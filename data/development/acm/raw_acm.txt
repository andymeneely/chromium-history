Locally decodable codes are error-correcting codes that admit efficient decoding algorithms; any bit of the original message can be recovered by looking at only a small number of locations of a corrupted codeword. The tradeoff between the rate of a code and the locality/efficiency of its decoding algorithms has been well studied, and it has widely been suspected that nontrivial locality must come at the price of low rate. A particular setting of potential interest in practice is codes of constant rate. For such codes, decoding algorithms with locality O(k∈) were known only for codes of rate ∈Ω(1/∈), where k is the length of the message. Furthermore, for codes of rate > 1/2, no nontrivial locality had been achieved.In this article, we construct a new family of locally decodable codes that have very efficient local decoding algorithms, and at the same time have rate approaching 1. We show that for every ∈ > 0 and α > 0, for infinitely many k, there exists a code C which encodes messages of length k with rate 1 − α, and is locally decodable from a constant fraction of errors using O(k∈) queries and time.These codes, which we call multiplicity codes, are based on evaluating multivariate polynomials and their derivatives. Multiplicity codes extend traditional multivariate polynomial codes; they inherit the local-decodability of these codes, and at the same time achieve better tradeoffs and flexibility in the rate and minimum distance.We study the problem of “privacy amplification”: key agreement between two parties who both know a weak secret w, such as a password. (Such a setting is ubiquitous on the internet, where passwords are the most commonly used security device.) We assume that the key agreement protocol is taking place in the presence of an active computationally unbounded adversary Eve. The adversary may have partial knowledge about w, so we assume only that w has some entropy from Eve’s point of view. Thus, the goal of the protocol is to convert this nonuniform secret w into a uniformly distributed string R that is fully secret from Eve. R may then be used as a key for running symmetric cryptographic protocols (such as encryption, authentication, etc.).Because we make no computational assumptions, the entropy in R can come only from w. Thus, such a protocol must minimize the entropy loss during its execution, so that R is as long as possible. The best previous results have entropy loss of Θ(κ2), where κ is the security parameter, thus requiring the password to be very long even for small values of κ. In this work, we present the first protocol for information-theoretic key agreement that has entropy loss linear in the security parameter. The result is optimal up to constant factors. We achieve our improvement through a somewhat surprising application of error-correcting codes for the edit distance.The protocol can be extended to provide also “information reconciliation,” that is, to work even when the two parties have slightly different versions of w (e.g., when biometrics are involved).Today’s hardware technology presents a new challenge in designing robust systems. Deep submicron VLSI technology introduces transient and permanent faults that were never considered in low-level system designs in the past. Still, robustness of that part of the system is crucial and needs to be guaranteed for any successful product. Distributed systems, on the other hand, have been dealing with similar issues for decades. However, neither the basic abstractions nor the complexity of contemporary fault-tolerant distributed algorithms match the peculiarities of hardware implementations.This article is intended to be part of an attempt striving to bridge over this gap between theory and practice for the clock synchronization problem. Solving this task sufficiently well will allow to build an ultra-robust high-precision clocking system for hardware designs like systems-on-chips in critical applications. As our first building block, we describe and prove correct a novel distributed, Byzantine fault-tolerant, probabilistically self-stabilizing pulse synchronization protocol, called FATAL, that can be implemented using standard asynchronous digital logic: Correct FATAL nodes are guaranteed to generate pulses (i.e., unnumbered clock ticks) in a synchronized way, despite a certain fraction of nodes being faulty. FATAL uses randomization only during stabilization and, despite the strict limitations introduced by hardware designs, offers optimal resilience and smaller complexity than all existing protocols. Finally, we show how to leverage FATAL to efficiently generate synchronized, self-stabilizing, high-frequency clocks.A Markov sequence is a basic statistical model representing uncertain sequential data, and it is used within a plethora of applications, including speech recognition, image processing, computational biology, radio-frequency identification (RFID), and information extraction. The problem of querying a Markov sequence is studied under the conventional semantics of querying a probabilistic database, where queries are formulated as finite-state transducers. Specifically, the complexity of two main problems is analyzed. The first problem is that of computing the confidence (probability) of an answer. The second is the enumeration of the answers in the order of decreasing confidence (with the generation of the top-k answers as a special case), or in an approximate order thereof. In particular, it is shown that enumeration in any subexponential-approximate order is generally intractable (even for some fixed transducers), and a matching upper bound is obtained through a proposed heuristic. Due to this hardness, a special consideration is given to restricted (yet common) classes of transducers that extract matches of a regular expression (subject to prefix and suffix constraints), and it is shown that these classes are, indeed, significantly more tractable.
We propose an algorithm for solving the maximum weighted stable set problem on claw-free graphs that runs in O(|V|(|E| + |V| log|V|))-time, drastically improving the previous best known complexity bound. This algorithm is based on a novel decomposition theorem for claw-free graphs, which is also introduced in the present article. Despite being weaker than the structural results for claw-free graphs given by Chudnovsky and Seymour [2005, 2008a, 2008b] our decomposition theorem is, on the other hand, algorithmic, that is, it is coupled with an O(|V||E|)-time algorithm that actually produces the decomposition.We bound the time it takes for a group of birds to stabilize in a standard flocking model. Each bird averages its velocity with its neighbors lying within a fixed radius. We resolve the worst-case complexity of this natural algorithm by providing asymptotically tight bounds on the time to equilibrium. We reduce the problem to two distinct questions in computational geometry and circuit complexity.Identifying complexity measures that bound the communication complexity of a {0,1}-valued matrix M is one the most fundamental problems in communication complexity. Mehlhorn and Schmidt [1982] were the first to suggest matrix-rank as one such measure. Among other things, they showed log rank F(M) CC(M) rankF2(M), where CC(M) denotes the (deterministic) communication complexity of the function associated with M, and the rank on the left-hand side is over any field F and on the right-hand side it is over the two-element field F2. For certain matrices M, communication complexity equals the right-hand side, and this completely settles the question of “communication complexity vs. F2-rank”.Here we reopen this question by pointing out that, when M has an additional natural combinatorial property---high discrepancy with respect to distributions which are uniform over submatrices---then communication complexity can be sublinear in F2-rank. Assuming the Polynomial Freiman-Ruzsa (PFR) conjecture in additive combinatorics, we show that CC(M) O(rank F2(M)/log rank F2(M)) for any matrix M which satisfies this combinatorial property.We also observe that if M has low rank over the reals, then it has low rank over F2 and it additionally satisfies this combinatorial property. As a corollary, our results also give the first (conditional) sublinear bound on communication complexity in terms of rank over the reals, a result improved later by Lovett [2014].Our proof is based on the study of the “approximate duality conjecture” which was suggested by Ben-Sasson and Zewi [2011] and studied there in connection to the PFR conjecture. First, we improve the bounds on approximate duality assuming the PFR conjecture. Then, we use the approximate duality conjecture (with improved bounds) to get our upper bound on the communication complexity of low-rank matrices.Consider the following two-player communication process to decide a language L: The first player holds the entire input x but is polynomially bounded; the second player is computationally unbounded but does not know any part of x; their goal is to decide cooperatively whether x belongs to L at small cost, where the cost measure is the number of bits of communication from the first player to the second player.For any integer d ≥ 3 and positive real ε, we show that, if satisfiability for n-variable d-CNF formulas has a protocol of cost O(nd − ε), then coNP is in NP/poly, which implies that the polynomial-time hierarchy collapses to its third level. The result even holds when the first player is conondeterministic, and is tight as there exists a trivial protocol for ε = 0. Under the hypothesis that coNP is not in NP/poly, our result implies tight lower bounds for parameters of interest in several areas, namely sparsification, kernelization in parameterized complexity, lossy compression, and probabilistically checkable proofs.By reduction, similar results hold for other NP-complete problems. For the vertex cover problem on n-vertex d-uniform hypergraphs, this statement holds for any integer d ≥ 2. The case d = 2 implies that no NP-hard vertex deletion problem based on a graph property that is inherited by subgraphs can have kernels consisting of O(k2 − ε) edges unless coNP is in NP/poly, where k denotes the size of the deletion set. Kernels consisting of O(k2) edges are known for several problems in the class, including vertex cover, feedback vertex set, and bounded-degree deletion.We propose a logic for true concurrency whose formulae predicate about events in computations and their causal dependencies. The induced logical equivalence is hereditary history-preserving bisimilarity, and fragments of the logic can be identified which correspond to other true concurrent behavioural equivalences in the literature: step, pomset and history-preserving bisimilarity. Standard Hennessy-Milner logic, and thus (interleaving) bisimilarity, is also recovered as a fragment. We also propose an extension of the logic with fixpoint operators, thus allowing to describe causal and concurrency properties of infinite computations. This work contributes to a rational presentation of the true concurrent spectrum and to a deeper understanding of the relations between the involved behavioural equivalences.In this article, we study the complexity of the problems: given a loop, described by linear constraints over a finite set of variables, is there a linear or lexicographical-linear ranking function for this loop? While existence of such functions implies termination, these problems are not equivalent to termination. When the variables range over the rationals (or reals), it is known that both problems are PTIME decidable. However, when they range over the integers, whether for single-path or multipath loops, the complexity has not yet been determined. We show that both problems are coNP-complete. However, we point out some special cases of importance of PTIME complexity. We also present complete algorithms for synthesizing linear and lexicographical-linear ranking functions, both for the general case and the special PTIME cases. Moreover, in the rational setting, our algorithm for synthesizing lexicographical-linear ranking functions extends existing ones, because our definition for such functions is more general, yet it has PTIME complexity.When defining computations over syntax as data, one often runs into tedious issues concerning α-equivalence and semantically correct manipulations of binding constructs. Here we study a semantic framework in which these issues can be dealt with automatically by the programming language. We take the user-friendly “nominal” approach in which bound objects are named. In particular, we develop a version of Scott domains within nominal sets and define two programming languages whose denotational semantics are based on those domains. The first language, λν-PCF, is an extension of Plotkin’s PCF with names that can be swapped, tested for equality and locally scoped; although simple, it already exposes most of the semantic subtleties of our approach. The second language, PNA, extends the first with name abstraction and concretion so that it can be used for metaprogramming over syntax with binders.For both languages, we prove a full abstraction result for nominal Scott domains analogous to Plotkin’s classic result about PCF and conventional Scott domains: two program phrases have the same observable operational behaviour in all contexts if and only if they denote equal elements of the nominal Scott domain model. This is the first full abstraction result we know of for languages combining higher-order functions with some form of locally scoped names which uses a domain theory based on ordinary extensional functions, rather than using the more intensional approach of game semantics.To obtain full abstraction, we need to add two functionals, one for existential quantification over names and one for “definite description” over names. Only adding one of them is not enough, as we give counter-examples to full abstraction in both cases.
The computation of the winning set for Büchi objectives in alternating games on graphs is a central problem in computer-aided verification with a large number of applications. The long-standing best known upper bound for solving the problem is Õ(n ⋅ m), where n is the number of vertices and m is the number of edges in the graph. We are the first to break the Õ(n ⋅ m) boundary by presenting a new technique that reduces the running time to O(n2). This bound also leads to O(n2)-time algorithms for computing the set of almost-sure winning vertices for Büchi objectives (1) in alternating games with probabilistic transitions (improving an earlier bound of Õ(n ⋅ m)), (2) in concurrent graph games with constant actions (improving an earlier bound of O(n3)), and (3) in Markov decision processes (improving for m>n4/3 an earlier bound of O(m ⋅ √m)). We then show how to maintain the winning set for Büchi objectives in alternating games under a sequence of edge insertions or a sequence of edge deletions in O(n) amortized time per operation. Our algorithms are the first dynamic algorithms for this problem. We then consider another core graph theoretic problem in verification of probabilistic systems, namely computing the maximal end-component decomposition of a graph. We present two improved static algorithms for the maximal end-component decomposition problem. Our first algorithm is an O(m ⋅ √m)-time algorithm, and our second algorithm is an O(n2)-time algorithm which is obtained using the same technique as for alternating Büchi games. Thus, we obtain an O(min &lcu;m ⋅ √m,n2})-time algorithm improving the long-standing O(n ⋅ m) time bound. Finally, we show how to maintain the maximal end-component decomposition of a graph under a sequence of edge insertions or a sequence of edge deletions in O(n) amortized time per edge deletion, and O(m) worst-case time per edge insertion. Again, our algorithms are the first dynamic algorithms for this problem.Many areas of computer science require answering questions about reachability in compactly described discrete transition systems. Answering such questions effectively requires techniques to be able to do so without building the entire system. In particular, heuristic search uses lower-bounding (“admissible”) heuristic functions to prune parts of the system known to not contain an optimal solution. A prominent technique for deriving such bounds is to consider abstract transition systems that aggregate groups of states into one. The key question is how to design and represent such abstractions. The most successful answer to this question are pattern databases, which aggregate states if and only if they agree on a subset of the state variables. Merge-and-shrink abstraction is a new paradigm that, as we show, allows to compactly represent a more general class of abstractions, strictly dominating pattern databases in theory. We identify the maximal class of transition systems, which we call factored transition systems, to which merge-and-shrink applies naturally, and we show that the well-known notion of bisimilarity can be adapted to this framework in a way that still guarantees perfect heuristic functions, while potentially reducing abstraction size exponentially. Applying these ideas to planning, one of the foundational subareas of artificial intelligence, we show that in some benchmarks this size reduction leads to the computation of perfect heuristic functions in polynomial time and that more approximate merge-and-shrink strategies yield heuristic functions competitive with the state of the art.Given topological spaces X,Y, a fundamental problem of algebraic topology is understanding the structure of all continuous maps X → Y. We consider a computational version, where X,Y are given as finite simplicial complexes, and the goal is to compute [X,Y], that is, all homotopy classes of such maps. We solve this problem in the stable range, where for some d ≥ 2, we have dim X ≤ 2d−2 and Y is (d-1)-connected; in particular, Y can be the d-dimensional sphere Sd. The algorithm combines classical tools and ideas from homotopy theory (obstruction theory, Postnikov systems, and simplicial sets) with algorithmic tools from effective algebraic topology (locally effective simplicial sets and objects with effective homology). In contrast, [X,Y] is known to be uncomputable for general X,Y, since for X=S1 it includes a well known undecidable problem: testing triviality of the fundamental group of Y.In follow-up papers, the algorithm is shown to run in polynomial time for d fixed, and extended to other problems, such as the extension problem, where we are given a subspace A ⊂ X and a map A → Y and ask whether it extends to a map X → Y, or computing the ℤ2-index—everything in the stable range. Outside the stable range, the extension problem is undecidable.This article presents the first tight bounds on the time complexity of shared-memory renaming, a fundamental problem in distributed computing in which a set of processes need to pick distinct identifiers from a small namespace.We first prove an individual lower bound of Ω(k) process steps for deterministic renaming into any namespace of size subexponential in k, where k is the number of participants. The bound is tight: it draws an exponential separation between deterministic and randomized solutions, and implies new tight bounds for deterministic concurrent fetch-and-increment counters, queues, and stacks. The proof is based on a new reduction from renaming to another fundamental problem in distributed computing: mutual exclusion. We complement this individual bound with a global lower bound of Ω(k log (k/c)) on the total step complexity of renaming into a namespace of size ck, for any c ≥ 1. This result applies to randomized algorithms against a strong adversary, and helps derive new global lower bounds for randomized approximate counter implementations, that are tight within logarithmic factors.On the algorithmic side, we give a protocol that transforms any sorting network into a randomized strong adaptive renaming algorithm, with expected cost equal to the depth of the sorting network. This gives a tight adaptive renaming algorithm with expected step complexity O(log k), where k is the contention in the current execution. This algorithm is the first to achieve sublinear time, and it is time-optimal as per our randomized lower bound. Finally, we use this renaming protocol to build monotone-consistent counters with logarithmic step complexity and linearizable fetch-and-increment registers with polylogarithmic cost.Multi-party communication complexity involves distributed computation of a function over inputs held by multiple distributed players. A key focus of distributed computing research, since the very beginning, has been to tolerate failures. It is thus natural to ask “If we want to compute a certain function in a fault-tolerant way, what will the communication complexity be?” For this question, this article will focus specifically on (i) tolerating node crash failures, and (ii) computing the function over general topologies (instead of, e.g., just cliques).One way to approach this question is to first develop results in a simpler failure-free setting, and then “amend” the results to take into account failures' impact. Whether this approach is effective largely depends on how big a difference failures can make. This article proves that the impact of failures is significant, at least for the Sum aggregate function in general topologies: As our central contribution, we prove that there exists (at least) an exponential gap between the non-fault-tolerant and fault-tolerant communication complexity of Sum. This gap attests that fault-tolerant communication complexity needs to be studied separately from non-fault-tolerant communication complexity, instead of being considered as an “amended” version of the latter. Such exponential gap is not obvious: For some other functions such as the Max aggregate function, the gap is only logarithmic.Part of our results are obtained via a novel reduction from a new two-party problem UnionSizeCP that we introduce. UnionSizeCP comes with a novel cycle promise, which is the key enabler of our reduction. We further prove that this cycle promise and UnionSizeCP likely play a fundamental role in reasoning about fault-tolerant communication complexity.
The inverted index is the backbone of modern web search engines. For each word in a collection of web documents, the index records the list of documents where this word occurs. Given a set of query words, the job of a search engine is to output a ranked list of the most relevant documents containing the query. However, if the query consists of an arbitrary string—which can be a partial word, multiword phrase, or more generally any sequence of characters—then word boundaries are no longer relevant and we need a different approach. In string retrieval settings, we are given a set D={d1, d2,d3, …, dD} of D strings with n characters in total taken from an alphabet set Σ = [σ], and the task of the search engine, for a given query pattern P of length p, is to report the “most relevant” strings in D containing P. The query may also consist of two or more patterns. The notion of relevance can be captured by a function score(P,dr), which indicates how relevant document dr is to the pattern P. Some example score functions are the frequency of pattern occurrences, proximity between pattern occurrences, or pattern-independent PageRank of the document.The first formal framework to study such kinds of retrieval problems was given by Muthukrishnan [SODA 2002]. He considered two metrics for relevance: frequency and proximity. He took a threshold-based approach on these metrics and gave data structures that use O(n log n) words of space. We study this problem in a somewhat more natural top-k framework. Here, k is a part of the query, and the top k most relevant (highest-scoring) documents are to be reported in sorted order of score. We present the first linear-space framework (i.e., using O(n) words of space) that is capable of handling arbitrary score functions with near-optimal O(p + klog k) query time. The query time can be made optimal O(p+k) if sorted order is not necessary. Further, we derive compact space and succinct space indexes (for some specific score functions). This space compression comes at the cost of higher query time. At last, we extend our framework to handle the case of multiple patterns. Apart from providing a robust framework, our results also improve many earlier results in index space or query time or both.We show that there exists an infinite word over the alphabet {0, 1, 3, 4} containing no three consecutive blocks of the same size and the same sum. This answers an open problem of Pirillo and Varricchio from 1994.The Chow parameters of a Boolean function f:{−1, 1}n → {−1, 1} are its n+1 degree-0 and degree-1 Fourier coefficients. It has been known since 1961 [Chow 1961; Tannenbaum 1961] that the (exact values of the) Chow parameters of any linear threshold function f uniquely specify f within the space of all Boolean functions, but until recently [O'Donnell and Servedio 2011] nothing was known about efficient algorithms for reconstructing f (exactly or approximately) from exact or approximate values of its Chow parameters. We refer to this reconstruction problem as the Chow Parameters Problem.Our main result is a new algorithm for the Chow Parameters Problem which, given (sufficiently accurate approximations to) the Chow parameters of any linear threshold function f, runs in time Õ(n2) ⋅ (1/ε)O(log2(1/ε)) and with high probability outputs a representation of an LTF f′ that is ε-close to f in Hamming distance. The only previous algorithm [O'Donnell and Servedio 2011] had running time poly(n) ⋅ 22Õ(1/ε2).As a byproduct of our approach, we show that for any linear threshold function f over {-1, 1}n, there is a linear threshold function f′ which is ε-close to f and has all weights that are integers of magnitude at most √n ⋅ (1/ε)O(log2(1/ε)). This significantly improves the previous best result of Diakonikolas and Servedio [2009] which gave a poly(n) ⋅ 2Õ(1/ε2/3) weight bound, and is close to the known lower bound of max{√n, (1/ε)Ω(log log (1/ε))} [Goldberg 2006; Servedio 2007]. Our techniques also yield improved algorithms for related problems in learning theory.In addition to being significantly stronger than previous work, our results are obtained using conceptually simpler proofs. The two main ingredients underlying our results are (1) a new structural result showing that for f any linear threshold function and g any bounded function, if the Chow parameters of f are close to the Chow parameters of g then f is close to g; (2) a new boosting-like algorithm that given approximations to the Chow parameters of a linear threshold function outputs a bounded function whose Chow parameters are close to those of f.Relational schema mappings have been extensively studied in connection with data integration and exchange problems, but mappings between XML schemas have not received the same amount of attention. Our goal is to develop a theory of expressive XML schema mappings. Such mappings should be able to use various forms of navigation in a document, and specify conditions on data values. We develop a language for XML schema mappings, and study both data exchange with such mappings and metadata management problems. Specifically, we concentrate on four types of problems: complexity of mappings, query answering, consistency issues, and composition.We first analyze the complexity of mappings, that is, recognizing pairs of documents such that one can be mapped into the other, and provide a classification based on sets of features used in mappings. Next, we chart the tractability frontier for the query answering problem. We show that the problem is tractable for expressive schema mappings and simple queries, but not vice versa. Then, we move to static analysis. We study the complexity of the consistency problem, that is, deciding whether it is possible to map some document of a source schema into a document of the target schema. Finally, we look at composition of XML schema mappings. We analyze its complexity and show that it is harder to achieve closure under composition for XML than for relational mappings. Nevertheless, we find a robust class of XML schema mappings that, in addition to being closed under composition, have good complexity properties with respect to the main data management tasks. Due to its good properties, we suggest this class as the class to use in applications of XML schema mappings.The coordination of a sequence of actions, to be performed in a linear temporal order in a distributed system, is studied. While in asynchronous message-passing systems such ordering of events requires the construction of message chains based on Lamport's happened-before relation, this is no longer true in the presence of time bounds on message delivery. Given such bounds, the mere passage of time can provide information about the occurrence of events at remote sites, without the need for explicit confirmation. A new causal structure called the centipede is introduced, and it is shown that centipedes must exist in every execution where linear ordering of actions is ensured. Centipedes capture the subtle interplay between the explicit information obtained via message chains, and the indirectly derived information gained by the passage of time, given the time bounds. Centipedes are defined using two relations. One is called syncausality, a slight generalisation of the happened-before relation. The other is a novel bound guarantee relation among events, that is based on the bounds on message transmission. In a precise sense, centipedes play a role in the synchronous setting analogous to that played by message chains in asynchronous systems. Our study is based on a knowledge-based analysis of distributed coordination. Temporally linear coordination is reduced to nested knowledge (knowledge about knowledge). Obtaining nested knowledge of a spontaneous event is, in turn, shown to require the existence of an appropriate centipede.In this article, we investigate the logical structure of memory models of theoretical and practical interest. Our main interest is in “the logic behind a fixed memory model”, rather than in “a model of any kind behind a given logical system”. As an effective language for reasoning about such memory models, we use the formalism of separation logic. Our main result is that for any concrete choice of heap-like memory model, validity in that model is undecidable even for purely propositional formulas in this language.The main novelty of our approach to the problem is that we focus on validity in specific, concrete memory models, as opposed to validity in general classes of models.Besides its intrinsic technical interest, this result also provides new insights into the nature of their decidable fragments. In particular, we show that, in order to obtain such decidable fragments, either the formula language must be severely restricted or the valuations of propositional variables must be constrained.In addition, we show that a number of propositional systems that approximate separation logic are undecidable as well. In particular, this resolves the open problems of decidability for Boolean BI and Classical BI.Moreover, we provide one of the simplest undecidable propositional systems currently known in the literature, called “Minimal Boolean BI”, by combining the purely positive implication-conjunction fragment of Boolean logic with the laws of multiplicative *-conjunction, its unit and its adjoint implication, originally provided by intuitionistic multiplicative linear logic. Each of these two components is individually decidable: the implication-conjunction fragment of Boolean logic is co-NP-complete, and intuitionistic multiplicative linear logic is NP-complete.All of our undecidability results are obtained by means of a direct encoding of Minsky machines.
The maximum cardinality and maximum weight matching problems can be solved in Õ(m√n) time, a bound that has resisted improvement despite decades of research. (Here m and n are the number of edges and vertices.) In this article, we demonstrate that this “m√n barrier” can be bypassed by approximation. For any ε > 0, we give an algorithm that computes a (1 − ε)-approximate maximum weight matching in O(mε−1 log ε−1) time, that is, optimal linear time for any fixed ε. Our algorithm is dramatically simpler than the best exact maximum weight matching algorithms on general graphs and should be appealing in all applications that can tolerate a negligible relative error.The class ACC consists of circuit families with constant depth over unbounded fan-in AND, OR, NOT, and MODm gates, where m > 1 is an arbitrary constant. We prove the following.---NEXP, the class of languages accepted in nondeterministic exponential time, does not have nonuniform ACC circuits of polynomial size. The size lower bound can be slightly strengthened to quasipolynomials and other less natural functions.---ENP, the class of languages recognized in 2O(n) time with an NP oracle, doesn’t have nonuniform ACC circuits of 2no(1) size. The lower bound gives an exponential size-depth tradeoff: for every d, m there is a δ > 0 such that ENP doesn’t have depth-d ACC circuits of size 2nδ with MODm gates.Previously, it was not known whether EXPNP had depth-3 polynomial-size circuits made out of only MOD6 gates. The high-level strategy is to design faster algorithms for the circuit satisfiability problem over ACC circuits, then prove that such algorithms entail these lower bounds. The algorithms combine known properties of ACC with fast rectangular matrix multiplication and dynamic programming, while the second step requires a strengthening of the author’s prior work.We prove that constraint satisfaction problems without the ability to count are solvable by the local consistency checking algorithm. This settles three (equivalent) conjectures: Feder--Vardi [SICOMP’98], Bulatov [LICS’04] and Larose--Zádori [AU’07].We give two different and simple constructions for dimensionality reduction in ℓ2 via linear mappings that are sparse: only an O(ϵ)-fraction of entries in each column of our embedding matrices are non-zero to achieve distortion 1 + ϵ with high probability, while still achieving the asymptotically optimal number of rows. These are the first constructions to provide subconstant sparsity for all values of parameters, improving upon previous works of Achlioptas [2003] and Dasgupta et al. [2010]. Such distributions can be used to speed up applications where ℓ2 dimensionality reduction is used.Normally, one thinks of probabilistic transition systems as taking an initial probability distribution over the state space into a new probability distribution representing the system after a transition. We, however, take a dual view of Markov processes as transformers of bounded measurable functions. This is very much in the same spirit as a “predicate-transformer” view, which is dual to the state-transformer view of transition systems. We redevelop the theory of labelled Markov processes from this viewpoint; in particular, we explore approximation theory. We obtain three main results.(i) It is possible to define bisimulation on general measure spaces and show that it is an equivalence relation. The logical characterization of bisimulation can be done straightforwardly and generally.(ii) A new and flexible approach to approximation based on averaging can be given. This vastly generalizes and streamlines the idea of using conditional expectations to compute approximations.(iii) We show that there is a minimal process bisimulation-equivalent to a given process, and this minimal process is obtained as the limit of the finite approximants.This article is concerned with the problem of implementing an unbounded timestamp object from multiwriter atomic registers, in an asynchronous distributed system of n processes with distinct identifiers where timestamps are taken from an arbitrary universe. Ellen et al. [2008] showed that √n/2 − O(1) registers are required for any obstruction-free implementation of long-lived timestamp systems from atomic registers (meaning processes can repeatedly get timestamps).We improve this existing lower bound in two ways. First we establish a lower bound of n/6 − 1 registers for the obstruction-free long-lived timestamp problem. Previous such linear lower bounds were only known for constrained versions of the timestamp problem. This bound is asymptotically tight; Ellen et al. [2008] constructed a wait-free algorithm that uses n − 1 registers. Second we show that √2n − log n − O(1) registers are required for any obstruction-free implementation of one-shot timestamp systems (meaning each process can get a timestamp at most once). We show that this bound is also asymptotically tight by providing a wait-free one-shot timestamp system that uses at most ⌈2√n⌉ registers, thus establishing a space complexity gap between one-shot and long-lived timestamp systems.Graph data appears in a variety of application domains, and many uses of it, such as querying, matching, and transforming data, naturally result in incompletely specified graph data, that is, graph patterns. While queries need to be posed against such data, techniques for querying patterns are generally lacking, and properties of such queries are not well understood.Our goal is to study the basics of querying graph patterns. The key features of patterns we consider here are node and label variables and edges specified by regular expressions. We provide a classification of patterns, and study standard graph queries on graph patterns. We give precise characterizations of both data and combined complexity for each class of patterns. If complexity is high, we do further analysis of features that lead to intractability, as well as lower-complexity restrictions. Since our patterns are based on regular expressions, query answering for them can be captured by a new automata model. These automata have two modes of acceptance: one captures queries returning nodes, and the other queries returning paths. We study properties of such automata, and the key computational tasks associated with them. Finally, we provide additional restrictions for tractability, and show that some intractable cases can be naturally cast as instances of constraint satisfaction problems.
We show that any explicit example for a tensor A : [n]r → F with tensor-rank ≥ nrċ(1−o(1)), where r = r(n) ≤ log n/log log n is super-constant, implies an explicit super-polynomial lower bound for the size of general arithmetic formulas over F. This shows that strong enough lower bounds for the size of arithmetic formulas of depth 3 imply super-polynomial lower bounds for the size of general arithmetic formulas.One component of our proof is a new approach for homogenization and multilinearization of arithmetic formulas, that gives the following results:We show that for any n-variate homogeneous polynomial f of degree r, if there exists a (fanin-2) formula of size s and depth d for f then there exists a homogeneous formula of size O((d+r+1 r) ċ s) for f. In particular, for any r ≤ O(log n), if there exists a polynomial size formula for f then there exists a polynomial size homogeneous formula for f. This refutes a conjecture of Nisan and Wigderson [1996] and shows that super-polynomial lower bounds for homogeneous formulas for polynomials of small degree imply super-polynomial lower bounds for general formulas.We show that for any n-variate set-multilinear polynomial f of degree r, if there exists a (fanin-2) formula of size s and depth d for f, then there exists a set-multilinear formula of size O((d + 2)r ċ s) for f. In particular, for any r ≤ O(log n/log log n), if there exists a polynomial size formula for f then there exists a polynomial size set-multilinear formula for f. This shows that super-polynomial lower bounds for set-multilinear formulas for polynomials of small degree imply super-polynomial lower bounds for general formulas.We present a clustering scheme that combines a mode-seeking phase with a cluster merging phase in the corresponding density map. While mode detection is done by a standard graph-based hill-climbing scheme, the novelty of our approach resides in its use of topological persistence to guide the merging of clusters. Our algorithm provides additional feedback in the form of a set of points in the plane, called a persistence diagram (PD), which provably reflects the prominences of the modes of the density. In practice, this feedback enables the user to choose relevant parameter values, so that under mild sampling conditions the algorithm will output the correct number of clusters, a notion that can be made formally sound within persistence theory. In addition, the output clusters have the property that their spatial locations are bound to the ones of the basins of attraction of the peaks of the density.The algorithm only requires rough estimates of the density at the data points, and knowledge of (approximate) pairwise distances between them. It is therefore applicable in any metric space. Meanwhile, its complexity remains practical: although the size of the input distance matrix may be up to quadratic in the number of data points, a careful implementation only uses a linear amount of memory and takes barely more time to run than to read through the input.An important question in the study of constraint satisfaction problems (CSP) is understanding how the graph or hypergraph describing the incidence structure of the constraints influences the complexity of the problem. For binary CSP instances (that is, where each constraint involves only two variables), the situation is well understood: the complexity of the problem essentially depends on the treewidth of the graph of the constraints [Grohe 2007; Marx 2010b]. However, this is not the correct answer if constraints with unbounded number of variables are allowed, and in particular, for CSP instances arising from query evaluation problems in database theory. Formally, if H is a class of hypergraphs, then let CSP(H) be CSP restricted to instances whose hypergraph is in H. Our goal is to characterize those classes of hypergraphs for which CSP(H) is polynomial-time solvable or fixed-parameter tractable, parameterized by the number of variables. Note that in the applications related to database query evaluation, we usually assume that the number of variables is much smaller than the size of the instance, thus parameterization by the number of variables is a meaningful question.The most general known property of H that makes CSP(H) polynomial-time solvable is bounded fractional hypertree width. Here we introduce a new hypergraph measure called submodular width, and show that bounded submodular width of H (which is a strictly more general property than bounded fractional hypertree width) implies that CSP(H) is fixed-parameter tractable. In a matching hardness result, we show that if H has unbounded submodular width, then CSP(H) is not fixed-parameter tractable (and hence not polynomial-time solvable), unless the Exponential Time Hypothesis (ETH) fails. The algorithmic result uses tree decompositions in a novel way: instead of using a single decomposition depending on the hypergraph, the instance is split into a set of instances (all on the same set of variables as the original instance), and then the new instances are solved by choosing a different tree decomposition for each of them. The reason why this strategy works is that the splitting can be done in such a way that the new instances are “uniform” with respect to the number extensions of partial solutions, and therefore the number of partial solutions can be described by a submodular function. For the hardness result, we prove via a series of combinatorial results that if a hypergraph H has large submodular width, then a 3SAT instance can be efficiently simulated by a CSP instance whose hypergraph is H. To prove these combinatorial results, we need to develop a theory of (multicommodity) flows on hypergraphs and vertex separators in the case when the function b(S) defining the cost of separator S is submodular, which can be of independent interest.The “learning with errors” (LWE) problem is to distinguish random linear equations, which have been perturbed by a small amount of noise, from truly uniform ones. The problem has been shown to be as hard as worst-case lattice problems, and in recent years it has served as the foundation for a plethora of cryptographic applications. Unfortunately, these applications are rather inefficient due to an inherent quadratic overhead in the use of LWE. A main open question was whether LWE and its applications could be made truly efficient by exploiting extra algebraic structure, as was done for lattice-based hash functions (and related primitives).We resolve this question in the affirmative by introducing an algebraic variant of LWE called ring-LWE, and proving that it too enjoys very strong hardness guarantees. Specifically, we show that the ring-LWE distribution is pseudorandom, assuming that worst-case problems on ideal lattices are hard for polynomial-time quantum algorithms. Applications include the first truly practical lattice-based public-key cryptosystem with an efficient security reduction; moreover, many of the other applications of LWE can be made much more efficient through the use of ring-LWE.The existence of quantum uncertainty relations is the essential reason that some classically unrealizable cryptographic primitives become realizable when quantum communication is allowed. One operational manifestation of these uncertainty relations is a purely quantum effect referred to as information locking [DiVincenzo et al. 2004]. A locking scheme can be viewed as a cryptographic protocol in which a uniformly random n-bit message is encoded in a quantum system using a classical key of size much smaller than n. Without the key, no measurement of this quantum state can extract more than a negligible amount of information about the message, in which case the message is said to be “locked”. Furthermore, knowing the key, it is possible to recover, that is “unlock”, the message.In this article, we make the following contributions by exploiting a connection between uncertainty relations and low-distortion embeddings of Euclidean spaces into slightly larger spaces endowed with the ℓ1 norm. We introduce the notion of a metric uncertainty relation and connect it to low-distortion embeddings of ℓ2 into ℓ1. A metric uncertainty relation also implies an entropic uncertainty relation. We prove that random bases satisfy uncertainty relations with a stronger definition and better parameters than previously known. Our proof is also considerably simpler than earlier proofs. We then apply this result to show the existence of locking schemes with key size independent of the message length. Moreover, we give efficient constructions of bases satisfying metric uncertainty relations. The bases defining these metric uncertainty relations are computable by quantum circuits of almost linear size. This leads to the first explicit construction of a strong information locking scheme. These constructions are obtained by adapting an explicit norm embedding due to Indyk [2007] and an extractor construction of Guruswami et al. [2009]. We apply our metric uncertainty relations to exhibit communication protocols that perform equality testing of n-qubit states. We prove that this task can be performed by a single message protocol using O(log2 n) qubits and n bits of communication, where the computation of the sender is efficient.We prove that multilinear (tensor) analogues of many efficiently computable problems in numerical linear algebra are NP-hard. Our list includes: determining the feasibility of a system of bilinear equations, deciding whether a 3-tensor possesses a given eigenvalue, singular value, or spectral norm; approximating an eigenvalue, eigenvector, singular vector, or the spectral norm; and determining the rank or best rank-1 approximation of a 3-tensor. Furthermore, we show that restricting these problems to symmetric tensors does not alleviate their NP-hardness. We also explain how deciding nonnegative definiteness of a symmetric 4-tensor is NP-hard and how computing the combinatorial hyperdeterminant is NP-, #P-, and VNP-hard.
We consider the problem of computing the rank of an m × n matrix A over a field. We present a randomized algorithm to find a set of r = rank(A) linearly independent columns in Õ(|A| + rω) field operations, where |A| denotes the number of nonzero entries in A and ω < 2.38 is the matrix multiplication exponent. Previously the best known algorithm to find a set of r linearly independent columns is by Gaussian elimination, with deterministic running time O(mnrω-2). Our algorithm is faster when r < max{m,n}, for instance when the matrix is rectangular. We also consider the problem of computing the rank of a matrix dynamically, supporting the operations of rank one updates and additions and deletions of rows and columns. We present an algorithm that updates the rank in Õ(mn) field operations. We show that these algorithms can be used to obtain faster algorithms for various problems in exact linear algebra, combinatorial optimization and dynamic data structure.An important tool in the study of the complexity of Constraint Satisfaction Problems (CSPs) is the notion of a relational clone, which is the set of all relations expressible using primitive positive formulas over a particular set of base relations. Post's lattice gives a complete classification of all Boolean relational clones, and this has been used to classify the computational difficulty of CSPs. Motivated by a desire to understand the computational complexity of (weighted) counting CSPs, we develop an analogous notion of functional clones and study the landscape of these clones. One of these clones is the collection of log-supermodular (lsm) functions, which turns out to play a significant role in classifying counting CSPs. In the conservative case (where all nonnegative unary functions are available), we show that there are no functional clones lying strictly between the clone of lsm functions and the total clone (containing all functions). Thus, any counting CSP that contains a single nontrivial non-lsm function is computationally as hard to approximate as any problem in #P. Furthermore, we show that any nontrivial functional clone (in a sense that will be made precise) contains the binary function “implies”. As a consequence, in the conservative case, all nontrivial counting CSPs are as hard to approximate as #BIS, the problem of counting independent sets in a bipartite graph. Given the complexity-theoretic results, it is natural to ask whether the “implies” clone is equivalent to the clone of lsm functions. We use the Möbius transform and the Fourier transform to show that these clones coincide precisely up to arity 3. It is an intriguing open question whether the lsm clone is finitely generated. Finally, we investigate functional clones in which only restricted classes of unary functions are available.We study the problem of identity testing for depth-3 circuits of top fanin k and degree d. We give a new structure theorem for such identities that improves the known deterministic dkO(k)-time blackbox identity test over rationals [Kayal and Saraf, 2009] to one that takes dO(k2)-time. Our structure theorem essentially says that the number of independent variables in a real depth-3 identity is very small. This theorem affirmatively settles the strong rank conjecture posed by Dvir and Shpilka [2006].We devise various algebraic tools to study depth-3 identities, and use these tools to show that any depth-3 identity contains a much smaller nucleus identity that contains most of the “complexity” of the main identity. The special properties of this nucleus allow us to get near optimal rank bounds for depth-3 identities. The most important aspect of this work is relating a field-dependent quantity, the Sylvester-Gallai rank bound, to the rank of depth-3 identities. We also prove a high-dimensional Sylvester-Gallai theorem for all fields, and get a general depth-3 identity rank bound (slightly improving previous bounds).The Counting Constraint Satisfaction Problem (#CSP(H)) over a finite relational structure H can be expressed as follows: given a relational structure G over the same vocabulary, determine the number of homomorphisms from G to H. In this article we characterize relational structures H for which (#CSP(H) can be solved in polynomial time and prove that for all other structures the problem is #P-complete.A central theme in distributed network algorithms concerns understanding and coping with the issue of locality. Yet despite considerable progress, research efforts in this direction have not yet resulted in a solid basis in the form of a fundamental computational complexity theory for locality. Inspired by sequential complexity theory, we focus on a complexity theory for distributed decision problems. In the context of locality, solving a decision problem requires the processors to independently inspect their local neighborhoods and then collectively decide whether a given global input instance belongs to some specified language.We consider the standard LOCAL model of computation and define LD(t) (for local decision) as the class of decision problems that can be solved in t communication rounds. We first study the intriguing question of whether randomization helps in local distributed computing, and to what extent. Specifically, we define the corresponding randomized class BPLD(t,p,q), containing all languages for which there exists a randomized algorithm that runs in t rounds, accepts correct instances with probability at least p, and rejects incorrect ones with probability at least q. We show that p2 + q = 1 is a threshold for the containment of LD(t) in BPLD(t,p,q). More precisely, we show that there exists a language that does not belong to LD(t) for any t=o(n) but does belong to BPLD(0,p,q) for any p,q ∈ (0,1) such that p2 + q ≤ 1. On the other hand, we show that, restricted to hereditary languages, BPLD(t,p,q)=LD(O(t)), for any function t, and any p, q ∈ (0,1) such that p2 + q > 1.In addition, we investigate the impact of nondeterminism on local decision, and establish several structural results inspired by classical computational complexity theory. Specifically, we show that nondeterminism does help, but that this help is limited, as there exist languages that cannot be decided locally nondeterministically. Perhaps surprisingly, it turns out that it is the combination of randomization with nondeterminism that enables to decide all languages in constant time. Finally, we introduce the notion of local reduction, and establish a couple of completeness results.We present a linear-time algorithm for deciding first-order (FO) properties in classes of graphs with bounded expansion, a notion recently introduced by Nešetřil and Ossona de Mendez. This generalizes several results from the literature, because many natural classes of graphs have bounded expansion: graphs of bounded tree-width, all proper minor-closed classes of graphs, graphs of bounded degree, graphs with no subgraph isomorphic to a subdivision of a fixed graph, and graphs that can be drawn in a fixed surface in such a way that each edge crosses at most a constant number of other edges. We deduce that there is an almost linear-time algorithm for deciding FO properties in classes of graphs with locally bounded expansion.More generally, we design a dynamic data structure for graphs belonging to a fixed class of graphs of bounded expansion. After a linear-time initialization the data structure allows us to test an FO property in constant time, and the data structure can be updated in constant time after addition/deletion of an edge, provided the list of possible edges to be added is known in advance and their simultaneous addition results in a graph in the class. All our results also hold for relational structures and are based on the seminal result of Nešetřil and Ossona de Mendez on the existence of low tree-depth colorings.The classical zero-one law for first-order logic on random graphs says that for every first-order property ϕ in the theory of graphs and every p ∈ (0,1), the probability that the random graph G(n, p) satisfies ϕ approaches either 0 or 1 as n approaches infinity. It is well known that this law fails to hold for any formalism that can express the parity quantifier: for certain properties, the probability that G(n,p) satisfies the property need not converge, and for others the limit may be strictly between 0 and 1.In this work, we capture the limiting behavior of properties definable in first order logic augmented with the parity quantifier, FO[⌖], over G(n,p), thus eluding the above hurdles. Specifically, we establish the following “modular convergence law”.For every FO[⌖] sentence ϕ, there are two explicitly computable rational numbers a0, a1, such that for i ∈ {0,1}, as n approaches infinity, the probability that the random graph G(2n+i, p) satisfies ϕ approaches ai.Our results also extend appropriately to FO equipped with Modq quantifiers for prime q.In the process of deriving this theorem, we explore a new question that may be of interest in its own right. Specifically, we study the joint distribution of the subgraph statistics modulo 2 of G(n,p): namely, the number of copies, mod 2, of a fixed number of graphs F1, …, Fℓ of bounded size in G(n,p). We first show that every FO[⌖] property ϕ is almost surely determined by subgraph statistics modulo 2 of the above type. Next, we show that the limiting joint distribution of the subgraph statistics modulo 2 depends only on n mod 2, and we determine this limiting distribution completely. Interestingly, both these steps are based on a common technique using multivariate polynomials over finite fields and, in particular, on a new generalization of the Gowers norm.The first step is analogous to the Razborov-Smolensky method for lower bounds for AC0 with parity gates, yet stronger in certain ways. For instance, it allows us to obtain examples of simple graph properties that are exponentially uncorrelated with every FO[⌖] sentence, which is something that is not known for AC0[⌖].In the study of deterministic distributed algorithms, it is commonly assumed that each node has a unique O(log n)-bit identifier. We prove that for a general class of graph problems, local algorithms (constant-time distributed algorithms) do not need such identifiers: a port numbering and orientation is sufficient.Our result holds for so-called simple PO-checkable graph optimisation problems; this includes many classical packing and covering problems such as vertex covers, edge covers, matchings, independent sets, dominating sets, and edge dominating sets. We focus on the case of bounded-degree graphs and show that if a local algorithm finds a constant-factor approximation of a simple PO-checkable graph problem with the help of unique identifiers, then the same approximation ratio can be achieved on anonymous networks.As a corollary of our result, we derive a tight lower bound on the local approximability of the minimum edge dominating set problem. By prior work, there is a deterministic local algorithm that achieves the approximation factor of 4--1/⌊Δ/2⌋ in graphs of maximum degree Δ. This approximation ratio is known to be optimal in the port-numbering model—our main theorem implies that it is optimal also in the standard model in which each node has a unique identifier.Our main technical tool is an algebraic construction of homogeneously ordered graphs: We say that a graph is (α,r)-homogeneous if its nodes are linearly ordered so that an α fraction of nodes have pairwise isomorphic radius-r neighbourhoods. We show that there exists a finite (α,r)-homogeneous 2k-regular graph of girth at least g for any α < 1 and any r, k, and g.
We close affirmatively a question that has been open for long time: decidability of the HOM problem. The HOM problem consists in determining, given a tree homomorphism H and a regular tree language L represented by a tree automaton, whether H(L) is regular. In order to decide the HOM problem, we develop new constructions and techniques that are interesting by themselves, and provide several significant intermediate results. For example, we prove that the universality problem is decidable for languages represented by tree automata with equality constraints, and that the equivalence and inclusion problems are decidable for images of regular languages through tree homomorphisms. Our contributions are based on the following new constructions. We describe a simple transformation for converting a tree automaton with equality constraints into a tree automaton with disequality constraints recognizing the complementary language. We also define a new class of tree automata with arbitrary disequality constraints and a particular kind of equality constraints. An automaton of this new class essentially recognizes the intersection of a tree automaton with disequality constraints and the image of a regular language through a tree homomorphism. We prove decidability of emptiness and finiteness for this class by a pumping mechanism. We combine the above constructions adequately to provide an algorithm deciding the HOM problem. This is the journal version of a paper presented in the 42nd ACM Symposium on Theory of Computing (STOC 2010). Here, we provide all proofs and examples. Moreover, we obtain better complexity results via the modification of some proofs and a careful complexity analysis. In particular, the obtained time complexity for the decision of HOM is a tower of three exponentials.Combinatorial auctions allow bidders to bid on bundles of items rather than just on single items. The winner determination problem in combinatorial auctions is the problem of determining the allocation of items to bidders such that the sum of the accepted bid prices is maximized. This problem is equivalent to the well-known maximum-weight set packing problem. Even though these problems are NP-hard in general, they can be solved in polynomial time on instances whose associated item graphs have bounded treewidth (called structured item graphs). However, the tractability of determining whether for a given problem instance a structured item graph of fixed treewidth exists (and if so, computing one efficiently) was an open problem.In this article, we solve this problem by proving that deciding the existence of structured item graphs is computationally intractable, even for treewidth 3. Motivated by this unfavorable complexity result, we investigate other structural restrictions, and we show that the notion of hypertree decomposition, a well-studied measure of hypergraph cyclicity, turns out to be most useful here. Indeed, we show that the winner determination problem is solvable in polynomial time on instances whose dual auction hypergraphs have bounded hypertree width. Our solution method is based on encoding winner determination via a constraint satisfaction optimization problem and on exhibiting an algorithm to solve this latter problem efficiently for such structurally restricted instances. The class of tractable instances identified by our approach, while being efficiently recognizable, properly contains the class of instances having a structured item graph. Moreover, on the larger class, our method solves winner determination with the same asymptotic complexity as the best algorithm proposed in the literature for the subclass of structured item graphs. Hypertree decompositions can equally profitably be applied to the maximum-weight independent set problem, which is the dual problem of maximum-weight set packing.The generalized nested dissection method, developed by Lipton et al. [1979], is a seminal method for solving a linear system Ax=b where A is a symmetric positive definite matrix. The method runs extremely fast whenever A is a well-separable matrix (such as matrices whose underlying support is planar or avoids a fixed minor). In this work, we extend the nested dissection method to apply to any nonsingular well-separable matrix over any field. The running times we obtain essentially match those of the nested dissection method. An important tool is a novel method for matrix sparsification that preserves determinants and minors, and that guarantees that constant powers of the sparsified matrix remain sparse.We present an all-pairs shortest path algorithm whose running time on a complete directed graph on n vertices whose edge weights are chosen independently and uniformly at random from [0,1] is O(n2), in expectation and with high probability. This resolves a long-standing open problem. The algorithm is a variant of the dynamic all-pairs shortest paths algorithm of Demetrescu and Italiano [2006]. The analysis relies on a proof that the number of locally shortest paths in such randomly weighted graphs is O(n2), in expectation and with high probability. We also present a dynamic version of the algorithm that recomputes all shortest paths after a random edge update in O(log2n) expected time.In the traditional data exchange setting, source instances are restricted to be complete in the sense that every fact is either true or false in these instances. Although natural for a typical database translation scenario, this restriction is gradually becoming an impediment to the development of a wide range of applications that need to exchange objects that admit several interpretations. In particular, we are motivated by two specific applications that go beyond the usual data exchange scenario: exchanging incomplete information and exchanging knowledge bases.In this article, we propose a general framework for data exchange that can deal with these two applications. More specifically, we address the problem of exchanging information given by representation systems, which are essentially finite descriptions of (possibly infinite) sets of complete instances. We make use of the classical semantics of mappings specified by sets of logical sentences to give a meaningful semantics to the notion of exchanging representatives, from which the standard notions of solution, space of solutions, and universal solution naturally arise. We also introduce the notion of strong representation system for a class of mappings, that resembles the concept of strong representation system for a query language. We show the robustness of our proposal by applying it to the two applications mentioned above: exchanging incomplete information and exchanging knowledge bases, which are both instantiations of the exchanging problem for representation systems. We study these two applications in detail, presenting results regarding expressiveness, query answering and complexity of computing solutions, and also algorithms to materialize solutions.This article presents a game semantics for higher-rank polymorphism, leading to a new model of the calculus System F, and a programming language which extends it with mutable variables. In contrast to previous game models of polymorphism, it is quite concrete, extending existing categories of games by a simple development of the notion of question/answer labelling and the associated bracketing condition to represent “copycat links” between positive and negative occurrences of type variables. Some well-known System F encodings of type constructors correspond in our model to simple constructions on games, such as the lifted sum.We characterize the generic types of our model (those for which instantiation reflects denotational equivalence), and show how to construct an interpretation in which all types are generic. We show how mutable variables (à la Scheme) may be interpreted in our model, allowing the definition of polymorphic objects with local state. By proving definability of finitary elements in this model using a decomposition argument, we establish a full abstraction result.
Divide-and-conquer recurrences are one of the most studied equations in computer science. Yet, discrete versions of these recurrences, namely for some known sequence an and given bj, bj, pj and δj, δj, present some challenges. The discrete nature of this recurrence (represented by the floor and ceiling functions) introduces certain oscillations not captured by the traditional Master Theorem, for example due to Akra and Bazzi [1998] who primary studied the continuous version of the recurrence. We apply powerful techniques such as Dirichlet series, Mellin-Perron formula, and (extended) Tauberian theorems of Wiener-Ikehara to provide a complete and precise solution to this basic computer science recurrence. We illustrate applicability of our results on several examples including a popular and fast arithmetic coding algorithm due to Boncelet for which we estimate its average redundancy and prove the Central Limit Theorem for the phrase length. To the best of our knowledge, discrete divide and conquer recurrences were not studied in this generality and such detail; in particular, this allows us to compare the redundancy of Boncelet’s algorithm to the (asymptotically) optimal Tunstall scheme.We consider the following network design problem. We are given an undirected graph G = (V,E) with edge costs c(e) and a set of terminal nodes W ⊆ V. A hose demand matrix is any symmetric matrix D, indexed by the terminals, such that for each i ∈ W, ∑j≠i Dij ≤ 1. We must compute the minimum-cost edge capacities that are able to support the oblivious routing of every hose matrix in the network. An oblivious routing template, in this context, is a simple path Pij for each pair i,j ∈ W. Given such a template, if we are to route a demand matrix D, then for each i,j, we send Dij units of flow along each Pij. Fingerhut et al. [1997] and Gupta et al. [2001] obtained a 2-approximation for this problem, using a solution template in the form of a tree. It has been widely asked and subsequently conjectured [Italiano et al. 2006] that this solution actually results in the optimal capacity for the single-path VPN design problem; this has become known as the VPN Conjecture. The conjecture has previously been proven for some restricted classes of graphs [Fingerhut et al. 1997; Fiorini et al. 2007; Grandoni et al. 2008; Hurkens et al. 2007]. Our main theorem establishes that this conjecture is true in general graphs. This also has the implication that the single-path VPN problem is solvable in polynomial time.A natural fractional version of the conjecture had also been proposed [Hurkens et al. 2007]. In this version, the routing may split flow between many paths, in specified proportions. We demonstrate that this multipath version of the conjecture is in fact false. The multipath and single path versions of the VPN problem are essentially direct analogues of the randomized and nonrandomized versions of oblivious routing schemes for minimizing congestion for permutation routing [Borodin and Hopcroft 1982; Valiant 1982].Dedicated to the memory of Stephen L. Bloom (1940--2010).Shared mutable objects pose grave challenges in reasoning, especially for information hiding and modularity. This article presents a novel technique for reasoning about error-avoiding partial correctness of programs featuring shared mutable objects, and investigates the technique by formalizing a logic. Using a first-order assertion language, the logic provides heap-local reasoning about mutation and separation, via ghost fields and variables of type “region” (finite sets of object references). A new form of frame condition specifies write, read, and allocation effects using region expressions; this supports a frame rule that allows a command to read state on which the framed predicate depends. Soundness is proved using a standard program semantics. The logic facilitates heap-local reasoning about object invariants, as shown here by examples. Part II of this article extends the logic with second-order framing which formalizes the hiding of data invariants.Dedicated to the memory of John C. Reynolds (1935--2013).The hiding of internal invariants creates a mismatch between procedure specifications in an interface and proof obligations on the implementations of those procedures. The mismatch is sound if the invariants depend only on encapsulated state, but encapsulation is problematic in contemporary software due to the many uses of shared mutable objects. The mismatch is formalized here in a proof rule that achieves flexibility via explicit restrictions on client effects, expressed using ghost state and ordinary first order assertions. The restrictions amount to a stateful frame condition that must be satisfied by any client; this dynamic encapsulation boundary complements conventional scope-based encapsulation. The technical development is based on a companion article, Part I, that presents Region Logic---a programming logic with stateful frame conditions for commands.We propose a novel verification method for higher-order functional programs based on higher-order model checking, or more precisely, model checking of higher-order recursion schemes (recursion schemes, for short). The most distinguishing feature of our verification method for higher-order programs is that it is sound, complete, and automatic for the simply typed λ-calculus with recursion and finite base types, and for various program verification problems such as reachability, flow analysis, and resource usage verification. We first show that a variety of program verification problems can be reduced to model checking problems for recursion schemes, by transforming a program into a recursion scheme that generates a tree representing all the interesting possible event sequences of the program. We then develop a new type-based model-checking algorithm for recursion schemes and implement a prototype recursion scheme model checker. To our knowledge, this is the first implementation of a recursion scheme model checker. Experiments show that our model checker is reasonably fast, despite the worst-case time complexity of recursion scheme model checking being hyperexponential in general. Altogether, the results provide a new, promising approach to verification of higher-order functional programs.In this article, we consider the semantic design and verified compilation of a C-like programming language for concurrent shared-memory computation on x86 multiprocessors. The design of such a language is made surprisingly subtle by several factors: the relaxed-memory behavior of the hardware, the effects of compiler optimization on concurrent code, the need to support high-performance concurrent algorithms, and the desire for a reasonably simple programming model. In turn, this complexity makes verified compilation both essential and challenging.We describe ClightTSO, a concurrent extension of CompCert’s Clight in which the TSO-based memory model of x86 multiprocessors is exposed for high-performance code, and CompCertTSO, a formally verified compiler from ClightTSO to x86 assembly language, building on CompCert. CompCertTSO is verified in Coq: for any well-behaved and successfully compiled ClightTSO source program, any permitted observable behavior of the generated assembly code (if it does not run out of memory) is also possible in the source semantics. We also describe some verified fence-elimination optimizations, integrated into CompCertTSO.
It has long been known that for the paging problem in its standard form, competitive analysis cannot adequately distinguish algorithms based on their performance: there exists a vast class of algorithms that achieve the same competitive ratio, ranging from extremely naive and inefficient strategies (such as Flush-When-Full), to strategies of excellent performance in practice (such as Least-Recently-Used and some of its variants). A similar situation arises in the list update problem: in particular, under the cost formulation studied by Martínez and Roura [2000] and Munro [2000] every list update algorithm has, asymptotically, the same competitive ratio. Several refinements of competitive analysis, as well as alternative performance measures have been introduced in the literature, with varying degrees of success in narrowing this disconnect between theoretical analysis and empirical evaluation.In this article, we study these two fundamental online problems under the framework of bijective analysis [Angelopoulos et al. 2007, 2008]. This is an intuitive technique that is based on pairwise comparison of the costs incurred by two algorithms on sets of request sequences of the same size. Coupled with a well-established model of locality of reference due to Albers et al. [2005], we show that Least-Recently-Used and Move-to-Front are the unique optimal algorithms for paging and list update, respectively. Prior to this work, only measures based on average-cost analysis have separated LRU and MTF from all other algorithms. Given that bijective analysis is a fairly stringent measure (and also subsumes average-cost analysis), we prove that in a strong sense LRU and MTF stand out as the best (deterministic) algorithms.A common approach to clustering data is to view data objects as points in a metric space, and then to optimize a natural distance-based objective such as the k-median, k-means, or min-sum score. For applications such as clustering proteins by function or clustering images by subject, the implicit hope in taking this approach is that the optimal solution for the chosen objective will closely match the desired “target” clustering (e.g., a correct clustering of proteins by function or of images by who is in them). However, most distance-based objectives, including those mentioned here, are NP-hard to optimize. So, this assumption by itself is not sufficient, assuming P ≠ NP, to achieve clusterings of low-error via polynomial time algorithms.In this article, we show that we can bypass this barrier if we slightly extend this assumption to ask that for some small constant c, not only the optimal solution, but also all c-approximations to the optimal solution, differ from the target on at most some ε fraction of points—we call this (c,ε)-approximation-stability. We show that under this condition, it is possible to efficiently obtain low-error clusterings even if the property holds only for values c for which the objective is known to be NP-hard to approximate. Specifically, for any constant c > 1, (c,ε)-approximation-stability of k-median or k-means objectives can be used to efficiently produce a clustering of error O(ε) with respect to the target clustering, as can stability of the min-sum objective if the target clusters are sufficiently large. Thus, we can perform nearly as well in terms of agreement with the target clustering as if we could approximate these objectives to this NP-hard value.Gödel Incompleteness Theorem leaves open a way around it, vaguely perceived for a long time but not clearly identified. (Thus, Gödel believed informal arguments can answer any math question.) Closing this loophole does not seem obvious and involves Kolmogorov complexity. (This is unrelated to, well studied before, complexity quantifications of the usual Gödel effects.) I consider extensions U of the universal partial recursive predicate (or, say, Peano Arithmetic). I prove that any U either leaves an n-bit input (statement) unresolved or contains nearly all information about the n-bit prefix of any r.e. real ρ (which is n bits for some ρ). I argue that creating significant information about a specific math sequence is impossible regardless of the methods used. Similar problems and answers apply to other unsolvability results for tasks allowing multiple solutions, for example, nonrecursive tilings.We study the complexity of valued constraint satisfaction problems (VCSPs) parametrized by a constraint language, a fixed set of cost functions over a finite domain. An instance of the problem is specified by a sum of cost functions from the language and the goal is to minimize the sum. Under the unique games conjecture, the approximability of finite-valued VCSPs is well understood, see Raghavendra [2008]. However, there is no characterization of finite-valued VCSPs, let alone general-valued VCSPs, that can be solved exactly in polynomial time, thus giving insights from a combinatorial optimization perspective.We consider the case of languages containing all possible unary cost functions. In the case of languages consisting of only {0,∞}-valued cost functions (i.e., relations), such languages have been called conservative and studied by Bulatov [2003, 2011] and recently by Barto [2011]. Since we study valued languages, we call a language conservative if it contains all finite-valued unary cost functions. The computational complexity of conservative valued languages has been studied by Cohen et al. [2006] for languages over Boolean domains, by Deineko et al. [2008] for {0,1}-valued languages (a.k.a Max-CSP), and by Takhanov [2010a] for {0,∞}-valued languages containing all finite-valued unary cost functions (a.k.a. Min-Cost-Hom).We prove a Schaefer-like dichotomy theorem for conservative valued languages: if all cost functions in the language satisfy a certain condition (specified by a complementary combination of STP and MJN multimorphisms), then any instance can be solved in polynomial time (via a new algorithm developed in this article), otherwise the language is NP-hard. This is the first complete complexity classification of general-valued constraint languages over non-Boolean domains. It is a common phenomenon that complexity classifications of problems over non-Boolean domains are significantly harder than the Boolean cases. The polynomial-time algorithm we present for the tractable cases is a generalization of the submodular minimization problem and a result of Cohen et al. [2008].Our results generalize previous results by Takhanov [2010a] and (a subset of results) by Cohen et al. [2006] and Deineko et al. [2008]. Moreover, our results do not rely on any computer-assisted search as in Deineko et al. [2008], and provide a powerful tool for proving hardness of finite-valued and general-valued languages.We study the complexity of gossip in an asynchronous, message-passing fault-prone distributed system. We show that an adaptive adversary can significantly hamper the spreading of a rumor, while an oblivious adversary cannot. The algorithmic techniques proposed in this article can be used for improving the message complexity of distributed algorithms that rely on an all-to-all message exchange paradigm and are designed for an asynchronous environment. As an example, we show how to improve the message complexity of asynchronous randomized consensus.In this article, we demonstrate that, ignoring computational constraints, it is possible to release synthetic databases that are useful for accurately answering large classes of queries while preserving differential privacy. Specifically, we give a mechanism that privately releases synthetic data useful for answering a class of queries over a discrete domain with error that grows as a function of the size of the smallest net approximately representing the answers to that class of queries. We show that this in particular implies a mechanism for counting queries that gives error guarantees that grow only with the VC-dimension of the class of queries, which itself grows at most logarithmically with the size of the query class.We also show that it is not possible to release even simple classes of queries (such as intervals and their generalizations) over continuous domains with worst-case utility guarantees while preserving differential privacy. In response to this, we consider a relaxation of the utility guarantee and give a privacy preserving polynomial time algorithm that for any halfspace query will provide an answer that is accurate for some small perturbation of the query. This algorithm does not release synthetic data, but instead another data structure capable of representing an answer for each query. We also give an efficient algorithm for releasing synthetic data for the class of interval queries and axis-aligned rectangles of constant dimension over discrete domains.Graph analysis is playing an increasingly important role in science and industry. Due to numerous limitations in sharing real-world graphs, models for generating massive graphs are critical for developing better algorithms. In this article, we analyze the stochastic Kronecker graph model (SKG), which is the foundation of the Graph500 supercomputer benchmark due to its favorable properties and easy parallelization. Our goal is to provide a deeper understanding of the parameters and properties of this model so that its functionality as a benchmark is increased. We develop a rigorous mathematical analysis that shows this model cannot generate a power-law distribution or even a lognormal distribution. However, we formalize an enhanced version of the SKG model that uses random noise for smoothing. We prove both in theory and in practice that this enhancement leads to a lognormal distribution. Additionally, we provide a precise analysis of isolated vertices, showing that the graphs that are produced by SKG might be quite different than intended. For example, between 50% and 75% of the vertices in the Graph500 benchmarks will be isolated. Finally, we show that this model tends to produce extremely small core numbers (compared to most social networks and other real graphs) for common parameter choices.Motivated by a recent conjecture concerning the expressiveness of declarative networking, we propose a formal computation model for “eventually consistent” distributed querying, based on relational transducers. A tight link has been conjectured between coordination-freeness of computations, and monotonicity of the queries expressed by such computations. Indeed, we propose a formal definition of coordination-freeness and confirm that the class of monotone queries is captured by coordination-free transducer networks. Coordination-freeness is a semantic property, but the syntactic class of “oblivious” transducers we define also captures the same class of monotone queries. Transducer networks that are not coordination-free are much more powerful.
Ye [2011] showed recently that the simplex method with Dantzig’s pivoting rule, as well as Howard’s policy iteration algorithm, solve discounted Markov decision processes (MDPs), with a constant discount factor, in strongly polynomial time. More precisely, Ye showed that both algorithms terminate after at most O(mn1−γ log n1−γ) iterations, where n is the number of states, m is the total number of actions in the MDP, and 0 < γ < 1 is the discount factor. We improve Ye’s analysis in two respects. First, we improve the bound given by Ye and show that Howard’s policy iteration algorithm actually terminates after at most O(m1−γ log n1−γ) iterations. Second, and more importantly, we show that the same bound applies to the number of iterations performed by the strategy iteration (or strategy improvement) algorithm, a generalization of Howard’s policy iteration algorithm used for solving 2-player turn-based stochastic games with discounted zero-sum rewards. This provides the first strongly polynomial algorithm for solving these games, solving a long standing open problem. Combined with other recent results, this provides a complete characterization of the complexity the standard strategy iteration algorithm for 2-player turn-based stochastic games; it is strongly polynomial for a fixed discount factor, and exponential otherwise.Performing random walks in networks is a fundamental primitive that has found applications in many areas of computer science, including distributed computing. In this article, we focus on the problem of sampling random walks efficiently in a distributed network and its applications. Given bandwidth constraints, the goal is to minimize the number of rounds required to obtain random walk samples.All previous algorithms that compute a random walk sample of length ℓ as a subroutine always do so naively, that is, in O(ℓ) rounds. The main contribution of this article is a fast distributed algorithm for performing random walks. We present a sublinear time distributed algorithm for performing random walks whose time complexity is sublinear in the length of the walk. Our algorithm performs a random walk of length ℓ in Õ(√ℓD) rounds (Õ hides polylog n factors where n is the number of nodes in the network) with high probability on an undirected network, where D is the diameter of the network. For small diameter graphs, this is a significant improvement over the naive O(ℓ) bound. Furthermore, our algorithm is optimal within a poly-logarithmic factor as there exists a matching lower bound [Nanongkai et al. 2011]. We further extend our algorithms to efficiently perform k independent random walks in Õ(√kℓD + k) rounds. We also show that our algorithm can be applied to speedup the more general Metropolis-Hastings sampling.Our random-walk algorithms can be used to speed up distributed algorithms in applications that use random walks as a subroutine. We present two main applications. First, we give a fast distributed algorithm for computing a random spanning tree (RST) in an arbitrary (undirected unweighted) network which runs in Õ(√mD) rounds with high probability (m is the number of edges). Our second application is a fast decentralized algorithm for estimating mixing time and related parameters of the underlying network. Our algorithm is fully decentralized and can serve as a building block in the design of topologically-aware networks.We give a test that can distinguish efficiently between product states of n quantum systems and states that are far from product. If applied to a state |ψ⟩ whose maximum overlap with a product state is 1 − ε, the test passes with probability 1 − Θ(ε), regardless of n or the local dimensions of the individual systems. The test uses two copies of |ψ⟩. We prove correctness of this test as a special case of a more general result regarding stability of maximum output purity of the depolarizing channel.A key application of the test is to quantum Merlin-Arthur games with multiple Merlins, where we obtain several structural results that had been previously conjectured, including the fact that efficient soundness amplification is possible and that two Merlins can simulate many Merlins: QMA(k) = QMA(2) for k ≥ 2. Building on a previous result of Aaronson et al., this implies that there is an efficient quantum algorithm to verify 3-SAT with constant soundness, given two unentangled proofs of Õ(√n) qubits. We also show how QMA(2) with log-sized proofs is equivalent to a large number of problems, some related to quantum information (such as testing separability of mixed states) as well as problems without any apparent connection to quantum mechanics (such as computing injective tensor norms of 3-index tensors). As a consequence, we obtain many hardness-of-approximation results, as well as potential algorithmic applications of methods for approximating QMA(2) acceptance probabilities.Finally, our test can also be used to construct an efficient test for determining whether a unitary operator is a tensor product, which is a generalization of classical linearity testing.Given samples from two distributions over an n-element set, we wish to test whether these distributions are statistically close. We present an algorithm which uses sublinear in n, specifically, O(n2/3ε−8/3 log n), independent samples from each distribution, runs in time linear in the sample size, makes no assumptions about the structure of the distributions, and distinguishes the cases when the distance between the distributions is small (less than {ε4/3n−1/3/32, εn−1/2/4}) or large (more than ε) in ℓ1 distance. This result can be compared to the lower bound of Ω(n2/3ε−2/3) for this problem given by Valiant [2008].Our algorithm has applications to the problem of testing whether a given Markov process is rapidly mixing. We present sublinear algorithms for several variants of this problem as well.The Steiner tree problem is one of the most fundamental NP-hard problems: given a weighted undirected graph and a subset of terminal nodes, find a minimum-cost tree spanning the terminals. In a sequence of papers, the approximation ratio for this problem was improved from 2 to 1.55 [Robins and Zelikovsky 2005]. All these algorithms are purely combinatorial. A long-standing open problem is whether there is an LP relaxation of Steiner tree with integrality gap smaller than 2 [Rajagopalan and Vazirani 1999].In this article we present an LP-based approximation algorithm for Steiner tree with an improved approximation factor. Our algorithm is based on a, seemingly novel, iterative randomized rounding technique. We consider an LP relaxation of the problem, which is based on the notion of directed components. We sample one component with probability proportional to the value of the associated variable in a fractional solution: the sampled component is contracted and the LP is updated consequently. We iterate this process until all terminals are connected. Our algorithm delivers a solution of cost at most ln(4) + ϵ < 1.39 times the cost of an optimal Steiner tree. The algorithm can be derandomized using the method of limited independence.As a by-product of our analysis, we show that the integrality gap of our LP is at most 1.55, hence answering the mentioned open question.
We investigate variants of Lloyd's heuristic for clustering high-dimensional data in an attempt to explain its popularity (a half century after its introduction) among practitioners, and in order to suggest improvements in its application. We propose and justify a clusterability criterion for data sets. We present variants of Lloyd's heuristic that quickly lead to provably near-optimal clustering solutions when applied to well-clusterable instances. This is the first performance guarantee for a variant of Lloyd's heuristic. The provision of a guarantee on output quality does not come at the expense of speed: some of our algorithms are candidates for being faster in practice than currently used variants of Lloyd's method. In addition, our other algorithms are faster on well-clusterable instances than recently proposed approximation algorithms, while maintaining similar guarantees on clustering quality. Our main algorithmic contribution is a novel probabilistic seeding process for the starting configuration of a Lloyd-type iteration.Let X be randomly chosen from {-1,1}n, and let Y be randomly chosen from the standard spherical Gaussian on ℝn. For any (possibly unbounded) polytope P formed by the intersection of k halfspaces, we prove that |Pr[X ∈ P] - Pr[Y ∈ P]| ≤ log8/5k ⋅ Δ, where Δ is a parameter that is small for polytopes formed by the intersection of “regular” halfspaces (i.e., halfspaces with low influence). The novelty of our invariance principle is the polylogarithmic dependence on k. Previously, only bounds that were at least linear in k were known. The proof of the invariance principle is based on a generalization of the Lindeberg method for proving central limit theorems and could be of use elsewhere.We give two important applications of our invariance principle, one from learning theory and the other from pseudorandomness.(1) A bound of logO(1)k ⋅ ε1/6 on the Boolean noise sensitivity of intersections of k “regular” halfspaces (previous work gave bounds linear in k). This gives a corresponding agnostic learning algorithm for intersections of regular halfspaces.(2) A pseudorandom generator (PRG) for estimating the Gaussian volume of polytopes with k faces within error δ and seed-length O(log n poly(log k,1/δ)).We also obtain PRGs with similar parameters that fool polytopes formed by intersection of regular halfspaces over the hypercube. Using our PRG constructions, we obtain the first deterministic quasi-polynomial time algorithms for approximately counting the number of solutions to a broad class of integer programs, including dense covering problems and contingency tables.We study the complexity of computing a query on a probabilistic database. We consider unions of conjunctive queries, UCQ, which are equivalent to positive, existential First Order Logic sentences, and also to nonrecursive datalog programs. The tuples in the database are independent random events. We prove the following dichotomy theorem. For every UCQ query, either its probability can be computed in polynomial time in the size of the database, or is #P-hard. Our result also has applications to the problem of computing the probability of positive, Boolean expressions, and establishes a dichotomy for such classes based on their structure. For the tractable case, we give a very simple algorithm that alternates between two steps: applying the inclusion/exclusion formula, and removing one existential variable. A key and novel feature of this algorithm is that it avoids computing terms that cancel out in the inclusion/exclusion formula, in other words it only computes those terms whose Mobius function in an appropriate lattice is nonzero. We show that this simple feature is a key ingredient needed to ensure completeness. For the hardness proof, we give a reduction from the counting problem for positive, partitioned 2CNF, which is known to be #P-complete. The hardness proof is nontrivial, and combines techniques from logic, classical algebra, and analysis.The algebraic/model theoretic design of static analyzers uses abstract domains based on representations of properties and pre-calculated property transformers. It is very efficient. The logical/proof theoretic approach uses SMT solvers/theorem provers and computation of property transformers on-the-fly. It is very expressive. We propose to unify both approaches, so that they can be combined to reach the sweet spot best adapted to a specific application domain in the precision/cost spectrum. We first give a new formalization of the proof theoretic approach in the abstract interpretation framework, introducing a semantics based on multiple interpretations to deal with the soundness of such approaches. Then we describe how to combine them with any other abstract interpretation-based analysis using an iterated reduction to combine abstractions. The key observation is that the Nelson-Oppen procedure, which decides satisfiability in a combination of logical theories by exchanging equalities and disequalities, computes a reduced product (after the state is enhanced with some new “observations” corresponding to alien terms). By abandoning restrictions ensuring completeness (such as disjointness, convexity, stably-infiniteness, or shininess, etc.), we can even broaden the application scope of logical abstractions for static analysis (which is incomplete anyway).The communication cost of algorithms (also known as I/O-complexity) is shown to be closely related to the expansion properties of the corresponding computation graphs. We demonstrate this on Strassen's and other fast matrix multiplication algorithms, and obtain the first lower bounds on their communication costs.In the sequential case, where the processor has a fast memory of size M, too small to store three n-by-n matrices, the lower bound on the number of words moved between fast and slow memory is, for a large class of matrix multiplication algorithms, Ω( (n/√M)ω0 ·M), where ω0 is the exponent in the arithmetic count (e.g., ω0 = lg 7 for Strassen, and ω0 = 3 for conventional matrix multiplication). With p parallel processors, each with fast memory of size M, the lower bound is asymptotically lower by a factor of p. These bounds are attainable both for sequential and for parallel algorithms and hence optimal.
Perfectly reliable message transmission (PRMT) is one of the fundamental problems in distributed computing. It allows a sender to reliably transmit a message to a receiver in an unreliable network, even in the presence of a computationally unbounded adversary. In this article, we study the inherent trade-off between the three important parameters of the PRMT protocols, namely, the network connectivity (n), the round complexity (r), and the communication complexity by considering the following generic question (which can be considered as the holy grail problem) in the context of the PRMT protocols.Given an n-connected network, a message of size ℓ (to be reliably communicated) and a limit c for the total communication allowed between the sender and the receiver, what is the minimum number of communication rounds required by a PRMT protocol to send the message, such that the communication complexity of the protocol is O(c)?We answer this interesting question by deriving a nontrivial lower bound on the round complexity. Moreover, we show that the lower bound is tight in the amortized sense, by designing a PRMT protocol whose round complexity matches the lower bound. The lower bound is the first of its kind, that simultaneously captures the inherent tradeoff between the three important parameters of a PRMT protocol.In this article we describe and analyze sublinear-time approximation algorithms for some optimization problems arising in machine learning, such as training linear classifiers and finding minimum enclosing balls. Our algorithms can be extended to some kernelized versions of these problems, such as SVDD, hard margin SVM, and L2-SVM, for which sublinear-time algorithms were not known before. These new algorithms use a combination of a novel sampling techniques and a new multiplicative update algorithm. We give lower bounds which show the running times of many of our algorithms to be nearly best possible in the unit-cost RAM model.The Lovász Local Lemma (LLL) is a powerful tool in probability theory to show the existence of combinatorial objects meeting a prescribed collection of “weakly dependent” criteria. We show that the LLL extends to a much more general geometric setting, where events are replaced with subspaces and probability is replaced with relative dimension, which allows to lower bound the dimension of the intersection of vector spaces under certain independence conditions.Our result immediately applies to the k-qsat problem (quantum analog of k-sat): For instance we show that any collection of rank-1 projectors, with the property that each qubit appears in at most 2k/(e ċ k) of them, has a joint satisfiable state.We then apply our results to the recently studied model of random k-qsat. Recent works have shown that the satisfiable region extends up to a density of 1 in the large k limit, where the density is the ratio of projectors to qubits. Using a hybrid approach building on work by Laumann et al. [2009, 2010] we greatly extend the known satisfiable region for random k-qsat to a density of Ω(2k/k2). Since our tool allows us to show the existence of joint satisfying states without the need to construct them, we are able to penetrate into regions where the satisfying states are conjectured to be entangled, avoiding the need to construct them, which has limited previous approaches to product states.We provide evidence that it is computationally difficult to approximate the partition function of the ferromagnetic q-state Potts model when q > 2. Specifically, we show that the partition function is hard for the complexity class #RHPi under approximation-preserving reducibility. Thus, it is as hard to approximate the partition function as it is to find approximate solutions to a wide range of counting problems, including that of determining the number of independent sets in a bipartite graph. Our proof exploits the first-order phase transition of the “random cluster” model, which is a probability distribution on graphs that is closely related to the q-state Potts model.We give a logical characterization of the polynomial-time properties of graphs embeddable in some surface. For every surface S, a property P of graphs embeddable in S is decidable in polynomial time if and only if it is definable in fixed-point logic with counting. It is a consequence of this result that for every surface S there is a k such that a simple combinatorial algorithm, namely “the k-dimensional Weisfeiler-Lehman algorithm”, decides isomorphism of graphs embeddable in S in polynomial time.We also present (without proof) generalizations of these results to arbitrary classes of graphs with excluded minors.
Let C denote one of the complexity classes “polynomial time,” “logspace,” or “nondeterministic logspace.” We introduce a logic L(C)inv and show generalizations and variants of the equivalence (L(C)inv captures C if and only if there is an almost C-optimal algorithm in C for the set Taut of tautologies of propositional logic). These statements are also equivalent to the existence of a listing of subsets in C of Taut by corresponding Turing machines and equivalent to the fact that a certain parameterized halting problem is in the parameterized complexity class XCuni.The rules governing the availability and quality of connections in a wireless network are described by physical models such as the signal-to-interference & noise ratio (SINR) model. For a collection of simultaneously transmitting stations in the plane, it is possible to identify a reception zone for each station, consisting of the points where its transmission is received correctly. The resulting SINR diagram partitions the plane into a reception zone per station and the remaining plane where no station can be heard.SINR diagrams appear to be fundamental to understanding the behavior of wireless networks, and may play a key role in the development of suitable algorithms for such networks, analogous perhaps to the role played by Voronoi diagrams in the study of proximity queries and related issues in computational geometry. So far, however, the properties of SINR diagrams have not been studied systematically, and most algorithmic studies in wireless networking rely on simplified graph-based models such as the unit disk graph (UDG) model, which conveniently abstract away interference-related complications, and make it easier to handle algorithmic issues, but consequently fail to capture accurately some important aspects of wireless networks.This article focuses on obtaining some basic understanding of SINR diagrams, their properties and their usability in algorithmic applications. Specifically, we have shown that assuming uniform power transmissions, the reception zones are convex and relatively well-rounded. These results are then used to develop an efficient approximation algorithm for a fundamental point location problem in wireless networks.We study the weighted version of the classic online paging problem where there is a weight (cost) for fetching each page into the cache. We design a randomized O(log k)-competitive online algorithm for this problem, where k is the cache size. This is the first randomized o(k)-competitive algorithm and its competitive ratio matches the known lower bound for the problem, up to constant factors. More generally, we design an O(log(k/(k − h + 1)))-competitive online algorithm for the version of the problem where the online algorithm has cache size k and it is compared to an optimal offline solution with cache size h ≤ k.Our solution is based on a two-step approach. We first obtain an O(log k)-competitive fractional algorithm based on an online primal-dual approach. Next, we obtain a randomized algorithm by rounding in an online manner the fractional solution to a probability distribution on the possible cache states. We also give an online primal-dual randomized O(log N)-competitive algorithm for the Metrical Task System problem (MTS) on a weighted star metric on N leaves.One-dimensional range queries, as one of the most basic type of queries in databases, have been studied extensively in the literature. For large databases, the goal is to build an external index that is optimized for disk block accesses (or I/Os). The problem is well understood in the static case. Theoretically, there exists an index of linear size that can answer a range query in O(1 + KB) I/Os, where K is the output size and B is the disk block size, but it is highly impractical. In practice, the standard solution is the B-tree, which answers a query in O(logB NM + KB) I/Os on a data set of size N, where M is the main memory size. For typical values of N, M, and B, logB NM can be considered a constant.However, the problem is still wide open in the dynamic setting, when insertions and deletions of records are to be supported. With smart buffering, it is possible to speed up updates significantly to o(1) I/Os amortized. Indeed, several dynamic B-trees have been proposed, but they all cause certain levels of degradation in the query performance, with the most interesting tradeoff point at O(1B log NM) I/Os for updates and O(log NM + KB) I/Os for queries. In this article, we prove that the query-update tradeoffs of all the known dynamic B-trees are optimal, when logB NM is a constant. This implies that one should not hope for substantially better solutions for all practical values of the parameters. Our lower bounds hold in a dynamic version of the indexability model, which is of independent interests. Dynamic indexability is a clean yet powerful model for studying dynamic indexing problems, and can potentially lead to more interesting lower bound results.
Noninteractive zero-knowledge (NIZK) proof systems are fundamental primitives used in many cryptographic constructions, including public-key encryption secure against chosen ciphertext attack, digital signatures, and various other cryptographic protocols. We introduce new techniques for constructing NIZK proofs based on groups with a bilinear map. Compared to previous constructions of NIZK proofs, our techniques yield dramatic reduction in the length of the common reference string (proportional to security parameter) and the size of the proofs (proportional to security parameter times the circuit size). Our novel techniques allow us to answer several long-standing open questions in the theory of noninteractive proofs. We construct the first perfect NIZK argument system for all NP. We construct the first universally composable NIZK argument for all NP in the presence of an adaptive adversary. We construct a non-interactive zap for all NP, which is the first that is based on a standard cryptographic security assumption.As advances in technology allow for the collection, storage, and analysis of vast amounts of data, the task of screening and assessing the significance of discovered patterns is becoming a major challenge in data mining applications. In this work, we address significance in the context of frequent itemset mining. Specifically, we develop a novel methodology to identify a meaningful support threshold s* for a dataset, such that the number of itemsets with support at least s* represents a substantial deviation from what would be expected in a random dataset with the same number of transactions and the same individual item frequencies. These itemsets can then be flagged as statistically significant with a small false discovery rate. We present extensive experimental results to substantiate the effectiveness of our methodology.We define when a linear-time temporal property is a fairness property with respect to a given system. This captures the essence shared by most fairness assumptions that are used in the specification and verification of reactive and concurrent systems, such as weak fairness, strong fairness, k-fairness, and many others. We provide three characterizations of fairness: a language-theoretic, a game-theoretic, and a topological characterization. It turns out that the fairness properties are the sets that are “large” from a topological point of view, that is, they are the co-meager sets in the natural topology of runs of a given system.This insight provides a link to probability theory where a set is “large” when it has measure 1. While these two notions of largeness are similar, they do not coincide in general. However, we show that they coincide for ω-regular properties and bounded Borel measures. That is, an ω-regular temporal property of a finite-state system has measure 1 under a bounded Borel measure if and only if it is a fairness property with respect to that system.The definition of fairness leads to a generic relaxation of correctness of a system in linear-time semantics. We define a system to be fairly correct if there exists a fairness assumption under which it satisfies its specification. Equivalently, a system is fairly correct if the set of runs satisfying the specification is topologically large. We motivate this notion of correctness and show how it can be verified in a system.Randomized algorithms are often enjoyed for their simplicity, but the hash functions used to yield the desired theoretical guarantees are often neither simple nor practical. Here we show that the simplest possible tabulation hashing provides unexpectedly strong guarantees.The scheme itself dates back to Zobrist in 1970 who used it for game playing programs. Keys are viewed as consisting of c characters. We initialize c tables H1, ..., Hc mapping characters to random hash codes. A key x = (x1, ..., xc) is hashed to H1[x1] ⊕ ⋯ ⊕ Hc[xc], where ⊕ denotes bit-wise exclusive-or.While this scheme is not even 4-independent, we show that it provides many of the guarantees that are normally obtained via higher independence, for example, Chernoff-type concentration, min-wise hashing for estimating set intersection, and cuckoo hashing.This article provides new worst-case bounds for the size and treewith of the result Q(D) of a conjunctive query Q applied to a database D. We derive bounds for the result size |Q(D)| in terms of structural properties of Q, both in the absence and in the presence of keys and functional dependencies. These bounds are based on a novel “coloring” of the query variables that associates a coloring number C(Q) to each query Q. Intuitively, each color used represents some possible entropy of that variable. Using this coloring number, we derive tight bounds for the size of Q(D) in case (i) no functional dependencies or keys are specified, and (ii) simple functional dependencies (keys) are given. These results generalize recent size-bounds for join queries obtained by Atserias et al. [2008]. In the case of arbitrary (compound) functional dependencies, we use tools from information theory to provide lower and upper bounds, establishing a close connection between size bounds and a basic question in information theory. Our new coloring scheme also allows us to precisely characterize (both in the absence of keys and with simple keys) the treewidth-preserving queries---the queries for which the treewidth of the output relation is bounded by a function of the treewidth of the input database. Finally, we give some results on the computational complexity of determining the size bounds, and of deciding whether the treewidth is preserved.
Informally, an obfuscator O is an (efficient, probabilistic) “compiler” that takes as input a program (or circuit) P and produces a new program O(P) that has the same functionality as P yet is “unintelligible” in some sense. Obfuscators, if they exist, would have a wide variety of cryptographic and complexity-theoretic applications, ranging from software protection to homomorphic encryption to complexity-theoretic analogues of Rice's theorem. Most of these applications are based on an interpretation of the “unintelligibility” condition in obfuscation as meaning that O(P) is a “virtual black box,” in the sense that anything one can efficiently compute given O(P), one could also efficiently compute given oracle access to P.In this work, we initiate a theoretical investigation of obfuscation. Our main result is that, even under very weak formalizations of the above intuition, obfuscation is impossible. We prove this by constructing a family of efficient programs P that are unobfuscatable in the sense that (a) given any efficient program P' that computes the same function as a program P ∈ p, the “source code” P can be efficiently reconstructed, yet (b) given oracle access to a (randomly selected) program P ∈ p, no efficient algorithm can reconstruct P (or even distinguish a certain bit in the code from random) except with negligible probability.We extend our impossibility result in a number of ways, including even obfuscators that (a) are not necessarily computable in polynomial time, (b) only approximately preserve the functionality, and (c) only need to work for very restricted models of computation (TC0). We also rule out several potential applications of obfuscators, by constructing “unobfuscatable” signature schemes, encryption schemes, and pseudorandom function families.We introduce the notion of a rational convex program (RCP) and we classify the known RCPs into two classes: quadratic and logarithmic. The importance of rationality is that it opens up the possibility of computing an optimal solution to the program via an algorithm that is either combinatorial or uses an LP-oracle. Next, we define a new Nash bargaining game, called ADNB, which is derived from the linear case of the Arrow-Debreu market model. We show that the convex program for ADNB is a logarithmic RCP, but unlike other known members of this class, it is nontotal.Our main result is a combinatorial, polynomial-time algorithm for ADNB. It turns out that the reason for infeasibility of logarithmic RCPs is quite different from that for LPs and quadratic RCPs. We believe that our ideas for surmounting the new difficulties will be useful for dealing with other nontotal RCPs as well. We give an application of our combinatorial algorithm for ADNB to an important “fair” throughput allocation problem on a wireless channel. Finally, we present a number of interesting questions that the new notion of RCP raises.We put forward a general theory of goal-oriented communication, where communication is not an end in itself, but rather a means to achieving some goals of the communicating parties. Focusing on goals provides a framework for addressing the problem of potential “misunderstanding” during communication, where the misunderstanding arises from lack of initial agreement on what protocol and/or language is being used in communication. In this context, “reliable communication” means overcoming any initial misunderstanding between parties towards achieving a given goal. Despite the enormous diversity among the goals of communication, we propose a simple model that captures all goals.In the simplest form of communication we consider, two parties, a user and a server, attempt to communicate with each other in order to achieve some goal of the user. We show that any goal of communication can be modeled mathematically by introducing a third party, which we call the referee, who hypothetically monitors the conversation between the user and the server and determines whether or not the goal has been achieved. Potential misunderstanding between the players is captured by allowing each player (the user/server) to come from a (potentially infinite) class of players such that each player is unaware which instantiation of the other it is talking to. We identify a main concept, which we call sensing, that allows goals to be achieved even under misunderstanding. Informally, sensing captures the user's ability (potentially using help from the server) to simulate the referee's assessment on whether the communication is achieving the goal. We show that when the user can sense progress, the goal of communication can be achieved despite initial misunderstanding. We also show that in certain settings sensing is necessary for overcoming such initial misunderstanding.Our results significantly extend the scope of the investigation started by Juba and Sudan (STOC 2008) who studied the foregoing phenomenon in the case of a single specific goal. Our study shows that their main suggestion, that misunderstanding can be detected and possibly corrected by focusing on the goal, can be proved in full generality.In this issue, the Invited Articles section is comprised of the article “Continuous Sampling from Distributed Streams” by Graham Cormode, Muthu Muthukrishnan, Ke Yi and Qin Zhang. This article was selected from the 29th ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, held in Indianapolis, Indiana, June 7--9, 2010. I thank the Program Committee of PODS 2010 and the PC Chair, Dirk Van Gucht, for their help in selecting this invited article. I am also grateful to JACM Associate Editor Phokion Kolaitis for his editorial work on this article.A fundamental problem in data management is to draw and maintain a sample of a large data set, for approximate query answering, selectivity estimation, and query planning. With large, streaming data sets, this problem becomes particularly difficult when the data is shared across multiple distributed sites. The main challenge is to ensure that a sample is drawn uniformly across the union of the data while minimizing the communication needed to run the protocol on the evolving data. At the same time, it is also necessary to make the protocol lightweight, by keeping the space and time costs low for each participant. In this article, we present communication-efficient protocols for continuously maintaining a sample (both with and without replacement) from k distributed streams. These apply to the case when we want a sample from the full streams, and to the sliding window cases of only the W most recent elements, or arrivals within the last w time units. We show that our protocols are optimal (up to logarithmic factors), not just in terms of the communication used, but also the time and space costs for each participant.
Probabilistic ω-automata are variants of nondeterministic automata over infinite words where all choices are resolved by probabilistic distributions. Acceptance of a run for an infinite input word can be defined using traditional acceptance criteria for ω-automata, such as Büchi, Rabin or Streett conditions. The accepted language of a probabilistic ω-automata is then defined by imposing a constraint on the probability measure of the accepting runs. In this paper, we study a series of fundamental properties of probabilistic ω-automata with three different language-semantics: (1) the probable semantics that requires positive acceptance probability, (2) the almost-sure semantics that requires acceptance with probability 1, and (3) the threshold semantics that relies on an additional parameter λ ∈ ]0,1[ that specifies a lower probability bound for the acceptance probability. We provide a comparison of probabilistic ω-automata under these three semantics and nondeterministic ω-automata concerning expressiveness and efficiency. Furthermore, we address closure properties under the Boolean operators union, intersection and complementation and algorithmic aspects, such as checking emptiness or language containment.This article presents constructions of useful concurrent data structures, including max registers and counters, with step complexity that is sublinear in the number of processes, n. This result avoids a well-known lower bound by having step complexity that is polylogarithmic in the number of values the object can take or the number of operations applied to it.The key step in these implementations is a method for constructing a max register, a linearizable, wait-free concurrent data structure that supports a write operation and a read operation that returns the largest value previously written. For fixed m, an m-valued max register is constructed from one-bit multi-writer multi-reader registers at a cost of at most &ceil;log m atomic register operations per write or read. An unbounded max register is constructed with cost O(min(log v, n)) to read or write a value v.Max registers are used to transform any monotone circuit into a wait-free concurrent data structure that provides write operations setting the inputs to the circuit and a read operation that returns the value of the circuit on the largest input values previously supplied. One application is a simple, linearizable, wait-free counter with a cost of O(min(log n log v, n)) to perform an increment and O(min(log v, n)) to perform a read, where v is the current value of the counter. For polynomially-many increments, this becomes O(log2 n), an exponential improvement on the best previously known upper bounds of O(n) for exact counting and O(n 4/5+&epsis;) for approximate counting.Finally, it is shown that the upper bounds are almost optimal. It is shown that for deterministic implementations, even if they are only required to satisfy solo-termination, min(&ceil;log m, n−1) is a lower bound on the worst-case complexity for an m-valued bounded max register, which is exactly equal to the upper bound for m ≤ 2n−1, and min(n−1, &ceil; log m - log(&ceil; log m + k)) is a lower bound for the read operation of an m-valued k-additive-accurate counter, which is a bounded counter in which a read operation is allowed to return a value within an additive error of ± k of the number of increment operations linearized before it. Furthermore, even in a solo-terminating randomized implementation of an n-valued max register with an oblivious adversary and global coins, there exist simple schedules in which, with high probability, the worst-case step complexity of a read operation is Ω(log n/log log n) if the write operations have polylogarithmic step complexity.In the renaming task, n+1 processes start with unique input names from a large space and must choose unique output names taken from a smaller name space, 0,1,…, K. To rule out trivial solutions, a protocol must be anonymous: the value chosen by a process can depend on its input name and on the execution, but not on the specific process ID.Attiya et al. [1990] showed that renaming has a wait-free solution when K≥ 2n. Several algebraic topology proofs of a lower bound stating that no such protocol exists when K < 2n have been published. In a companion article, we present the first completely combinatorial renaming lower bound proof stating if n + 1 is a primer power, then renaming is not wait-free solvable when K < 2n. In this article, we show that if n + 1 is not a primer power, then there exists a wait-free renaming protocol for K = 2n−1. Therefore the renaming lower bound for K < 2n is incorrect. More precisely, our main theorem states that there exists a wait-free renaming protocol for K < 2n if and only if n + 1 is not a prime power. We prove this result using the known equivalence of K-renaming for K = 2n − 1 and the weak symmetry breaking task: processes have no input values and the output values are 0 or 1, and it is required that in every execution in which all processes participate, at least one process decides 1 and at least one process decides 0.We construct finite groups whose Cayley graphs have large girth even with respect to a discounted distance measure that contracts arbitrarily long sequences of edges from the same color class (subgroup), and only counts transitions between color classes (cosets). These groups are shown to be useful in the construction of finite bisimilar hypergraph covers that avoid any small cyclic configurations. We present two applications to the finite model theory of the guarded fragment: a strengthening of the known finite model property for GF and the characterization of GF as the guarded bisimulation invariant fragment of first-order logic in the sense of finite model theory.
In the setting of secure two-party computation, two mutually distrusting parties wish to compute some function of their inputs while preserving, to the extent possible, various security properties such as privacy, correctness, and more. One desirable property is fairness which guarantees, informally, that if one party receives its output, then the other party does too. Cleve [1986] showed that complete fairness cannot be achieved in general without an honest majority. Since then, the accepted folklore has been that nothing non-trivial can be computed with complete fairness in the two-party setting.We demonstrate that this folklore belief is false by showing completely fair protocols for various nontrivial functions in the two-party setting based on standard cryptographic assumptions. We first show feasibility of obtaining complete fairness when computing any function over polynomial-size domains that does not contain an “embedded XOR”; this class of functions includes boolean AND/OR as well as Yao’s “millionaires’ problem”. We also demonstrate feasibility for certain functions that do contain an embedded XOR, though we prove a lower bound showing that any completely fair protocol for such functions must have round complexity super-logarithmic in the security parameter. Our results demonstrate that the question of completely fair secure computation without an honest majority is far from closed.We give a general technique to obtain approximation mechanisms that are truthful in expectation. We show that for packing domains, any α-approximation algorithm that also bounds the integrality gap of the LP relaxation of the problem by α can be used to construct an α-approximation mechanism that is truthful in expectation. This immediately yields a variety of new and significantly improved results for various problem domains and furthermore, yields truthful (in expectation) mechanisms with guarantees that match the best-known approximation guarantees when truthfulness is not required. In particular, we obtain the first truthful mechanisms with approximation guarantees for a variety of multiparameter domains. We obtain truthful (in expectation) mechanisms achieving approximation guarantees of O(√m) for combinatorial auctions (CAs), (1 + ε) for multiunit CAs with B = Ω(log m) copies of each item, and 2 for multiparameter knapsack problems (multi-unit auctions).Our construction is based on considering an LP relaxation of the problem and using the classic VCG mechanism to obtain a truthful mechanism in this fractional domain. We argue that the (fractional) optimal solution scaled down by α, where α is the integrality gap of the problem, can be represented as a convex combination of integer solutions, and by viewing this convex combination as specifying a probability distribution over integer solutions, we get a randomized, truthful in expectation mechanism. Our construction can be seen as a way of exploiting VCG in a computational tractable way even when the underlying social-welfare maximization problem is NP-hard.The accurate and efficient treatment of mutable data structures is one of the outstanding problem areas in automatic program verification and analysis. Shape analysis is a form of program analysis that attempts to infer descriptions of the data structures in a program, and to prove that these structures are not misused or corrupted. It is one of the more challenging and expensive forms of program analysis, due to the complexity of aliasing and the need to look arbitrarily deeply into the program heap. This article describes a method of boosting shape analyses by defining a compositional method, where each procedure is analyzed independently of its callers. The analysis algorithm uses a restricted fragment of separation logic, and assigns a collection of Hoare triples to each procedure; the triples provide an over-approximation of data structure usage. Our method brings the usual benefits of compositionality---increased potential to scale, ability to deal with incomplete programs, graceful way to deal with imprecision---to shape analysis, for the first time.The analysis rests on a generalized form of abduction (inference of explanatory hypotheses), which we call bi-abduction. Bi-abduction displays abduction as a kind of inverse to the frame problem: it jointly infers anti-frames (missing portions of state) and frames (portions of state not touched by an operation), and is the basis of a new analysis algorithm. We have implemented our analysis and we report case studies on smaller programs to evaluate the quality of discovered specifications, and larger code bases (e.g., sendmail, an imap server, a Linux distribution) to illustrate the level of automation and scalability that we obtain from our compositional method.This article makes number of specific technical contributions on proof procedures and analysis algorithms, but in a sense its more important contribution is holistic: the explanation and demonstration of how a massive increase in automation is possible using abductive inference.In this article, we describe a randomized Shellsort algorithm. This algorithm is a simple, randomized, data-oblivious version of the Shellsort algorithm that always runs in O(n log n) time and succeeds in sorting any given input permutation with very high probability. Taken together, these properties imply applications in the design of new efficient privacy-preserving computations based on the secure multiparty computation (SMC) paradigm. In addition, by a trivial conversion of this Monte Carlo algorithm to its Las Vegas equivalent, one gets the first version of Shellsort with a running time that is provably O(n log n) with very high probability.The Lovász Local Lemma (LLL) is a powerful tool that gives sufficient conditions for avoiding all of a given set of “bad” events, with positive probability. A series of results have provided algorithms to efficiently construct structures whose existence is non-constructively guaranteed by the LLL, culminating in the recent breakthrough of Moser and Tardos [2010] for the full asymmetric LLL. We show that the output distribution of the Moser-Tardos algorithm well-approximates the conditional LLL-distribution, the distribution obtained by conditioning on all bad events being avoided. We show how a known bound on the probabilities of events in this distribution can be used for further probabilistic analysis and give new constructive and nonconstructive results.We also show that when a LLL application provides a small amount of slack, the number of resamplings of the Moser-Tardos algorithm is nearly linear in the number of underlying independent variables (not events!), and can thus be used to give efficient constructions in cases where the underlying proof applies the LLL to super-polynomially many events. Even in cases where finding a bad event that holds is computationally hard, we show that applying the algorithm to avoid a polynomial-sized “core” subset of bad events leads to a desired outcome with high probability. This is shown via a simple union bound over the probabilities of non-core events in the conditional LLL-distribution, and automatically leads to simple and efficient Monte-Carlo (and in most cases RNC) algorithms. We demonstrate this idea on several applications. We give the first constant-factor approximation algorithm for the Santa Claus problem by making a LLL-based proof of Feige constructive. We provide Monte Carlo algorithms for acyclic edge coloring, nonrepetitive graph colorings, and Ramsey-type graphs. In all these applications, the algorithm falls directly out of the non-constructive LLL-based proof. Our algorithms are very simple, often provide better bounds than previous algorithms, and are in several cases the first efficient algorithms known.As a second type of application we show that the properties of the conditional LLL-distribution can be used in cases beyond the critical dependency threshold of the LLL: avoiding all bad events is impossible in these cases. As the first (even nonconstructive) result of this kind, we show that by sampling a selected smaller core from the LLL-distribution, we can avoid a fraction of bad events that is higher than the expectation. MAX k-SAT is an illustrative example of this.This work considers the quantum interactive proof system model of computation, which is the (classical) interactive proof system model’s natural quantum computational analogue. An exact characterization of the expressive power of quantum interactive proof systems is obtained: the collection of computational problems having quantum interactive proof systems consists precisely of those problems solvable by deterministic Turing machines that use at most a polynomial amount of space (or, more succinctly, QIP = PSPACE). This characterization is proved through the use of a parallelized form of the matrix multiplicative weights update method, applied to a class of semidefinite programs that captures the computational power of quantum interactive proof systems. One striking implication of this characterization is that quantum computing provides no increase in computational power whatsoever over classical computing in the context of interactive proof systems, for it is well known that the collection of computational problems having classical interactive proof systems coincides with those problems solvable by polynomial-space computations.
The k-means method is one of the most widely used clustering algorithms, drawing its popularity from its speed in practice. Recently, however, it was shown to have exponential worst-case running time. In order to close the gap between practical performance and theoretical analysis, the k-means method has been studied in the model of smoothed analysis. But even the smoothed analyses so far are unsatisfactory as the bounds are still super-polynomial in the number n of data points.In this article, we settle the smoothed running time of the k-means method. We show that the smoothed number of iterations is bounded by a polynomial in n and 1/σ, where σ is the standard deviation of the Gaussian perturbations. This means that if an arbitrary input data set is randomly perturbed, then the k-means method will run in expected polynomial time on that input set.We consider several variants of the job shop problem that is a fundamental and classical problem in scheduling. The currently best approximation algorithms have worse than logarithmic performance guarantee, but the only previously known inapproximability result says that it is NP-hard to approximate job shops within a factor less than 5/4. Closing this big approximability gap is a well-known and long-standing open problem.This article closes many gaps in our understanding of the hardness of this problem and answers several open questions in the literature. It is shown the first nonconstant inapproximability result that almost matches the best-known approximation algorithm for acyclic job shops. The same bounds hold for the general version of flow shops, where jobs are not required to be processed on each machine. Similar inapproximability results are obtained when the objective is to minimize the sum of completion times. It is also shown that the problem with two machines and the preemptive variant with three machines have no PTAS.We give the first polynomial-time approximation scheme (PTAS) for the Steiner forest problem on planar graphs and, more generally, on graphs of bounded genus. As a first step, we show how to build a Steiner forest spanner for such graphs. The crux of the process is a clustering procedure called prize-collecting clustering that breaks down the input instance into separate subinstances which are easier to handle; moreover, the terminals in different subinstances are far from each other. Each subinstance has a relatively inexpensive Steiner tree connecting all its terminals, and the subinstances can be solved (almost) separately. Another building block is a PTAS for Steiner forest on graphs of bounded treewidth. Surprisingly, Steiner forest is NP-hard even on graphs of treewidth 3. Therefore, our PTAS for bounded-treewidth graphs needs a nontrivial combination of approximation arguments and dynamic programming on the tree decomposition. We further show that Steiner forest can be solved in polynomial time for series-parallel graphs (graphs of treewidth at most two) by a novel combination of dynamic programming and minimum cut computations, completing our thorough complexity study of Steiner forest in the range of bounded-treewidth graphs, planar graphs, and bounded-genus graphs.Consider an n-vertex graph G = (V, E) of maximum degree Δ, and suppose that each vertex v ∈ V hosts a processor. The processors are allowed to communicate only with their neighbors in G. The communication is synchronous, that is, it proceeds in discrete rounds.In the distributed vertex coloring problem, the objective is to color G with Δ + 1, or slightly more than Δ + 1, colors using as few rounds of communication as possible. (The number of rounds of communication will be henceforth referred to as running time.) Efficient randomized algorithms for this problem are known for more than twenty years [Alon et al. 1986; Luby 1986]. Specifically, these algorithms produce a (Δ + 1)-coloring within O(log n) time, with high probability. On the other hand, the best known deterministic algorithm that requires polylogarithmic time employs O(Δ2) colors. This algorithm was devised in a seminal FOCS’87 paper by Linial [1987]. Its running time is O(log* n). In the same article, Linial asked whether one can color with significantly less than Δ2 colors in deterministic polylogarithmic time. By now, this question of Linial became one of the most central long-standing open questions in this area.In this article, we answer this question in the affirmative, and devise a deterministic algorithm that employs Δ1+o(1) colors, and runs in polylogarithmic time. Specifically, the running time of our algorithm is O(f(Δ)log Δ log n), for an arbitrarily slow-growing function f(Δ) = ω(1). We can also produce an O(Δ1+η)-coloring in O(log Δ log n)-time, for an arbitrarily small constant η > 0, and an O(Δ)-coloring in O(Δε log n) time, for an arbitrarily small constant ε > 0. Our results are, in fact, far more general than this. In particular, for a graph of arboricity a, our algorithm produces an O(a1+η)-coloring, for an arbitrarily small constant η > 0, in time O(log a log n).
We investigate a new class of geometric problems based on the idea of online error correction. Suppose one is given access to a large geometric dataset though a query mechanism; for example, the dataset could be a terrain and a query might ask for the coordinates of a particular vertex or for the edges incident to it. Suppose, in addition, that the dataset satisfies some known structural property P (for example, monotonicity or convexity) but that, because of errors and noise, the queries occasionally provide answers that violate P. Can one design a filter that modifies the query's answers so that (i) the output satisfies P; (ii) the amount of data modification is minimized? We provide upper and lower bounds on the complexity of online reconstruction for convexity in 2D and 3D.The work reported here lays the foundations of data exchange in the presence of probabilistic data. This requires rethinking the very basic concepts of traditional data exchange, such as solution, universal solution, and the certain answers of target queries. We develop a framework for data exchange over probabilistic databases, and make a case for its coherence and robustness. This framework applies to arbitrary schema mappings, and finite or countably infinite probability spaces on the source and target instances. After establishing this framework and formulating the key concepts, we study the application of the framework to a concrete and practical setting where probabilistic databases are compactly encoded by means of annotations formulated over random Boolean variables. In this setting, we study the problems of testing for the existence of solutions and universal solutions, materializing such solutions, and evaluating target queries (for unions of conjunctive queries) in both the exact sense and the approximate sense. For each of the problems, we carry out a complexity analysis based on properties of the annotation, for various classes of dependencies. Finally, we show that the framework and results easily and completely generalize to allow not only the data, but also the schema mapping itself to be probabilistic.We consider a fragment of XPath 1.0, where attribute and text values may be compared. We show that for any unary query ϕ in this fragment, the set of nodes that satisfy the query in a document t can be calculated in time O(|ϕ|3|t|). We show that for a query in a bigger fragment with Kleene star allowed, the same can be done in time O(2O(|ϕ|)|t|) or in time O(|ϕ|3|t|log|t|). Finally, we present algorithms for binary queries of XPath, which do a precomputation on the document and then output the selected pairs with constant delay.We describe an algorithm for Byzantine agreement that is scalable in the sense that each processor sends only Õ(&sqrt;n) bits, where n is the total number of processors. Our algorithm succeeds with high probability against an adaptive adversary, which can take over processors at any time during the protocol, up to the point of taking over arbitrarily close to a 1/3 fraction. We assume synchronous communication but a rushing adversary. Moreover, our algorithm works in the presence of flooding: processors controlled by the adversary can send out any number of messages. We assume the existence of private channels between all pairs of processors but make no other cryptographic assumptions. Finally, our algorithm has latency that is polylogarithmic in n. To the best of our knowledge, ours is the first algorithm to solve Byzantine agreement against an adaptive adversary, while requiring o(n2) total bits of communication.
Let f be a function on a set of variables V. For each x ∈ V, let c(x) be the cost of reading the value of x. An algorithm for evaluating f is a strategy for adaptively identifying and reading a set of variables U ⊆ V whose values uniquely determine the value of f. We are interested in finding algorithms which minimize the cost incurred to evaluate f in the above sense. Competitive analysis is employed to measure the performance of the algorithms. We address two variants of the above problem. We consider the basic model in which the evaluation algorithm knows the cost c(x), for each x ∈ V. We also study a novel model where the costs of the variables are not known in advance and some preemption is allowed in the reading operations. This model has applications, for example, when reading a variable coincides with obtaining the output of a job on a CPU and the cost is the CPU time.For the model where the costs of the variables are known, we present a polynomial time algorithm with the best possible competitive ratio γcf for each function f that is representable by a threshold tree and for each fixed cost function c(⋅). Remarkably, the best-known result for the same class of functions is a pseudo-polynomial algorithm with competitiveness 2 γcf. Still in the same model, we introduce the Linear Programming Approach (LPA), a framework that allows the design of efficient algorithms for evaluating functions. We show that different implementations of this approach lead in general to the best algorithms known so far—and in many cases to optimal algorithms—for different classes of functions considered before in the literature.Via the LPA, we are able to determine exactly the optimal extremal competitiveness of monotone Boolean functions. Remarkably, the upper bound which leads to this result, holds for a much broader class of functions, which also includes the whole set of Boolean functions.We also show how to extend the LPA (together with these results) to the model where the costs of the variables are not known beforehand. In particular, we show how to employ the extended LPA to design a polynomial-time optimal (with respect to competitiveness) algorithm for the class of monotone Boolean functions representable by threshold trees.We consider Fisher and Arrow--Debreu markets under additively separable, piecewise-linear, concave utility functions and obtain the following results. For both market models, if an equilibrium exists, there is one that is rational and can be written using polynomially many bits. There is no simple necessary and sufficient condition for the existence of an equilibrium: The problem of checking for existence of an equilibrium is NP-complete for both market models; the same holds for existence of an ε-approximate equilibrium, for ε = O(n−5). Under standard (mild) sufficient conditions, the problem of finding an exact equilibrium is in PPAD for both market models. Finally, building on the techniques of Chen et al. [2009a] we prove that under these sufficient conditions, finding an equilibrium for Fisher markets is PPAD-hard.This article is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the ℓ1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.This article focuses on computations on large graphs (e.g., the web-graph) where the edges of the graph are presented as a stream. The objective in the streaming model is to use small amount of memory (preferably sub-linear in the number of nodes n) and a smaller number of passes.In the streaming model, we show how to perform several graph computations including estimating the probability distribution after a random walk of length l, the mixing time M, and other related quantities such as the conductance of the graph. By applying our algorithm for computing probability distribution on the web-graph, we can estimate the PageRank p of any node up to an additive error of &sqrt;ε p+ε in Õ(&sqrt;M/α) passes and Õ(min(nα+1/ε&sqrt;M/α+(1/ε)Mα, α n&sqrt;Mα + (1/ε)&sqrt;M/α)) space, for any α ∈ (0,1]. Specifically, for ε = M/n, α = M−1/2, we can compute the approximate PageRank values in Õ(nM−1/4) space and Õ(M3/4) passes. In comparison, a standard implementation of the PageRank algorithm will take O(n) space and O(M) passes. We also give an approach to approximate the PageRank values in just Õ(1) passes although this requires Õ(nM) space.
This article gives an overview of the geometric complexity theory (GCT) approach towards the P vs. NP and related problems focusing on its main complexity theoretic results. These are: (1) two concrete lower bounds, which are currently the best known lower bounds in the context of the P vs. NC and permanent vs. determinant problems, (2) the Flip Theorem, which formalizes the self-referential paradox in the P vs. NP problem, and (3) the Decomposition Theorem, which decomposes the arithmetic P vs. NP and permanent vs. determinant problems into subproblems without self-referential difficulty, consisting of positivity hypotheses in algebraic geometry and representation theory and easier hardness hypotheses.We present several results about Delaunay triangulations (DTs) and convex hulls in transdichotomous and hereditary settings: (i) the DT of a planar point set can be computed in expected time O(sort(n)) on a word RAM, where sort(n) is the time to sort n numbers. We assume that the word RAM supports the shuffle operation in constant time; (ii) if we know the ordering of a planar point set in x- and in y-direction, its DT can be found by a randomized algebraic computation tree of expected linear depth; (iii) given a universe U of points in the plane, we construct a data structure D for Delaunay queries: for any P ⊆ U, D can find the DT of P in expected time O(|P| log log |U|); (iv) given a universe U of points in 3-space in general convex position, there is a data structure D for convex hull queries: for any P ⊆ U, D can find the convex hull of P in expected time O(|P| (log log |U|)2); (v) given a convex polytope in 3-space with n vertices which are colored with χ ≥ 2 colors, we can split it into the convex hulls of the individual color classes in expected time O(n (log log n)2).The results (i)--(iii) generalize to higher dimensions, where the expected running time now also depends on the complexity of the resulting DT. We need a wide range of techniques. Most prominently, we describe a reduction from DTs to nearest-neighbor graphs that relies on a new variant of randomized incremental constructions using dependent sampling.This article deals with the emulation of atomic read/write (R/W) storage in dynamic asynchronous message passing systems. In static settings, it is well known that atomic R/W storage can be implemented in a fault-tolerant manner even if the system is completely asynchronous, whereas consensus is not solvable. In contrast, all existing emulations of atomic storage in dynamic systems rely on consensus or stronger primitives, leading to a popular belief that dynamic R/W storage is unattainable without consensus.In this article, we specify the problem of dynamic atomic read/write storage in terms of the interface available to the users of such storage. We discover that, perhaps surprisingly, dynamic R/W storage is solvable in a completely asynchronous system: we present DynaStore, an algorithm that solves this problem. Our result implies that atomic R/W storage is in fact easier than consensus, even in dynamic systems.We analyze the convergence of randomized trace estimators. Starting at 1989, several algorithms have been proposed for estimating the trace of a matrix by 1/M∑i=1M ziT Azi, where the zi are random vectors; different estimators use different distributions for the zis, all of which lead to E(1/M∑i=1M ziT Azi) = trace(A). These algorithms are useful in applications in which there is no explicit representation of A but rather an efficient method compute zTAz given z. Existing results only analyze the variance of the different estimators. In contrast, we analyze the number of samples M required to guarantee that with probability at least 1−δ, the relative error in the estimate is at most &epsis;. We argue that such bounds are much more useful in applications than the variance. We found that these bounds rank the estimators differently than the variance; this suggests that minimum-variance estimators may not be the best.We also make two additional contributions to this area. The first is a specialized bound for projection matrices, whose trace (rank) needs to be computed in electronic structure calculations. The second is a new estimator that uses less randomness than all the existing estimators.
We present a novel definition of privacy in the framework of offline (retroactive) database query auditing. Given information about the database, a description of sensitive data, and assumptions about users' prior knowledge, our goal is to determine if answering a past user's query could have led to a privacy breach. According to our definition, an audited property A is private, given the disclosure of property B, if no user can gain confidence in A by learning B, subject to prior knowledge constraints. Privacy is not violated if the disclosure of B causes a loss of confidence in A. The new notion of privacy is formalized using the well-known semantics for reasoning about knowledge, where logical properties correspond to sets of possible worlds (databases) that satisfy these properties. Database users are modeled as either possibilistic agents whose knowledge is a set of possible worlds, or as probabilistic agents whose knowledge is a probability distribution on possible worlds.We analyze the new privacy notion, show its relationship with the conventional approach, and derive criteria that allow the auditor to test privacy efficiently in some important cases. In particular, we prove characterization theorems for the possibilistic case, and study in depth the probabilistic case under the assumption that all database records are considered a-priori independent by the user, as well as under more relaxed (or absent) prior-knowledge assumptions. In the probabilistic case we show that for certain families of distributions there is no efficient algorithm to test whether an audited property A is private given the disclosure of a property B, assuming P ≠ NP. Nevertheless, for many interesting families, such as the family of product distributions, we obtain algorithms that are efficient both in theory and in practice.The restless bandit problem is one of the most well-studied generalizations of the celebrated stochastic multi-armed bandit (MAB) problem in decision theory. In its ultimate generality, the restless bandit problem is known to be PSPACE-Hard to approximate to any nontrivial factor, and little progress has been made on this problem despite its significance in modeling activity allocation under uncertainty.In this article, we consider the Feedback MAB problem, where the reward obtained by playing each of n independent arms varies according to an underlying on/off Markov process whose exact state is only revealed when the arm is played. The goal is to design a policy for playing the arms in order to maximize the infinite horizon time average expected reward. This problem is also an instance of a Partially Observable Markov Decision Process (POMDP), and is widely studied in wireless scheduling and unmanned aerial vehicle (UAV) routing. Unlike the stochastic MAB problem, the Feedback MAB problem does not admit to greedy index-based optimal policies.We develop a novel duality-based algorithmic technique that yields a surprisingly simple and intuitive (2+&epsis;)-approximate greedy policy to this problem. We show that both in terms of approximation factor and computational efficiency, our policy is closely related to the Whittle index, which is widely used for its simplicity and efficiency of computation. Subsequently we define a multi-state generalization, that we term Monotone bandits, which remains subclass of the restless bandit problem. We show that our policy remains a 2-approximation in this setting, and further, our technique is robust enough to incorporate various side-constraints such as blocking plays, switching costs, and even models where determining the state of an arm is a separate operation from playing it.Our technique is also of independent interest for other restless bandit problems, and we provide an example in nonpreemptive machine replenishment. Interestingly, in this case, our policy provides a constant factor guarantee, whereas the Whittle index is provably polynomially worse.By presenting the first O(1) approximations for nontrivial instances of restless bandits as well as of POMDPs, our work initiates the study of approximation algorithms in both these contexts.We study models of incomplete information for XML, their computational properties, and query answering. While our approach is motivated by the study of relational incompleteness, incomplete information in XML documents may appear not only as null values but also as missing structural information. Our goal is to provide a classification of incomplete descriptions of XML documents, and separate features—or groups of features—that lead to hard computational problems from those that admit efficient algorithms. Our classification of incomplete information is based on the combination of null values with partial structural descriptions of documents. The key computational problems we consider are consistency of partial descriptions, representability of complete documents by incomplete ones, and query answering. We show how factors such as schema information, the presence of node ids, and missing structural information affect the complexity of these main computational problems, and find robust classes of incomplete XML descriptions that permit tractable query evaluation.
A schema mapping is a specification that describes how data structured under one schema (the source schema) is to be transformed into data structured under a different schema (the target schema). The notion of an inverse of a schema mapping is subtle, because a schema mapping may associate many target instances with each source instance, and many source instances with each target instance. In PODS 2006, Fagin defined a notion of the inverse of a schema mapping. This notion is tailored to the types of schema mappings that commonly arise in practice (those specified by “source-to-target tuple-generating dependencies”, or s-t tgds). We resolve the key open problem of the complexity of deciding whether there is an inverse. We also explore a number of interesting questions, including: What is the structure of an inverse? When is the inverse unique? How many nonequivalent inverses can there be? When does an inverse have an inverse? How big must an inverse be? Surprisingly, these questions are all interrelated. We show that for schema mappings M specified by full s-t tgds (those with no existential quantifiers), if M has an inverse, then it has a polynomial-size inverse of a particularly nice form, and there is a polynomial-time algorithm for generating it. We introduce the notion of “essential conjunctions” (or “essential atoms” in the full case), and show that they play a crucial role in the study of inverses. We use them to give greatly simplified proofs of some known results about inverses. What emerges is a much deeper understanding about this fundamental and complex operator.We give an algorithm to learn an intersection of k halfspaces in Rn whose normals span an l-dimensional subspace. For any input distribution with a logconcave density such that the bounding hyperplanes of the k halfspaces pass through its mean, the algorithm (&epsis;,δ)-learns with time and sample complexity bounded by (nkl/&epsis;)O(l) log 1/&epsis; δ. The hypothesis found is an intersection of O(k log (1/&epsis;)) halfspaces. This improves on Blum and Kannan's algorithm for the uniform distribution over a ball, in the time and sample complexity (previously doubly exponential) and in the generality of the input distribution.This article presents a novel generic technique for solving dataflow equations in interprocedural dataflow analysis. The technique is obtained by generalizing Newton's method for computing a zero of a differentiable function to ω-continuous semirings. Complete semilattices, the common program analysis framework, are a special class of ω-continuous semirings. We show that our generalized method always converges to the solution, and requires at most as many iterations as current methods based on Kleene's fixed-point theorem. We also show that, contrary to Kleene's method, Newton's method always terminates for arbitrary idempotent and commutative semirings. More precisely, in the latter setting the number of iterations required to solve a system of n equations is at most n.It has been known for some time that graph isomorphism reduces to the hidden subgroup problem (HSP). What is more, most exponential speedups in quantum computation are obtained by solving instances of the HSP. A common feature of the resulting algorithms is the use of quantum coset states, which encode the hidden subgroup. An open question has been how hard it is to use these states to solve graph isomorphism. It was recently shown by Moore et al. [2005] that only an exponentially small amount of information is available from one, or a pair of coset states. A potential source of power to exploit are entangled quantum measurements that act jointly on many states at once.We show that entangled quantum measurements on at least Ω(n log n) coset states are necessary to get useful information for the case of graph isomorphism, matching an information theoretic upper bound. This may be viewed as a negative result because in general it seems hard to implement a given highly entangled measurement. Our main theorem is very general and also rules out using joint measurements on few coset states for some other groups, such as GL(n,Fpm) and Gn where G is finite and satisfies a suitable property.
The article describes and analyzes NAMOA*, an algorithm for multiobjective heuristic graph search problems. The algorithm is presented as an extension of A*, an admissible scalar shortest path algorithm. Under consistent heuristics A* is known to improve its efficiency with more informed heuristics, and to be optimal over the class of admissible algorithms in terms of the set of expanded nodes and the number of node expansions. Equivalent beneficial properties are shown to prevail in the new algorithm.We prove that poly-sized AC0 circuits cannot distinguish a polylogarithmically independent distribution from the uniform one. This settles the 1990 conjecture by Linial and Nisan [1990]. The only prior progress on the problem was by Bazzi [2007], who showed that O(log2 n)-independent distributions fool poly-size DNF formulas. [Razborov 2008] has later given a much simpler proof for Bazzi's theorem.We show that the NP-Complete language 3Sat has a PCP verifier that makes two queries to a proof of almost-linear size and achieves subconstant probability of error ϵ=o(1). The verifier performs only projection tests, meaning that the answer to the first query determines at most one accepting answer to the second query. The number of bits representing a symbol in the proof depends only on the error ϵ. Previously, by the parallel repetition theorem, there were PCP Theorems with two-query projection tests, but only (arbitrarily small) constant error and polynomial size. There were also PCP Theorems with subconstant error and almost-linear size, but a constant number of queries that is larger than 2.As a corollary, we obtain a host of new results. In particular, our theorem improves many of the hardness of approximation results that are proved using the parallel repetition theorem. A partial list includes the following:(1) 3Sat cannot be efficiently approximated to within a factor of 7/8+o(1), unless P=NP. This holds even under almost-linear reductions. Previously, the best known NP-hardness factor was 7/8+ϵ for any constant ϵ>0, under polynomial reductions (Håstad).(2) 3Lin cannot be efficiently approximated to within a factor of 1/2+o(1), unless P=NP. This holds even under almost-linear reductions. Previously, the best known NP-hardness factor was 1/2+ϵ for any constant ϵ>0, under polynomial reductions (Håstad).(3) A PCP Theorem with amortized query complexity 1 + o(1) and amortized free bit complexity o(1). Previously, the best-known amortized query complexity and free bit complexity were 1+ϵ and ϵ, respectively, for any constant ϵ>0 (Samorodnitsky and Trevisan).One of the new ideas that we use is a new technique for doing the composition step in the (classical) proof of the PCP Theorem, without increasing the number of queries to the proof. We formalize this as a composition of new objects that we call Locally Decode/Reject Codes (LDRC). The notion of LDRC was implicit in several previous works, and we make it explicit in this work. We believe that the formulation of LDRCs and their construction are of independent interest.Description logics (DLs) and rules are formalisms that emphasize different aspects of knowledge representation: whereas DLs are focused on specifying and reasoning about conceptual knowledge, rules are focused on nonmonotonic inference. Many applications, however, require features of both DLs and rules. Developing a formalism that integrates DLs and rules would be a natural outcome of a large body of research in knowledge representation and reasoning of the last two decades; however, achieving this goal is very challenging and the approaches proposed thus far have not fully reached it. In this paper, we present a hybrid formalism of MKNF+ knowledge bases, which integrates DLs and rules in a coherent semantic framework. Achieving seamless integration is nontrivial, since DLs use an open-world assumption, while the rules are based on a closed-world assumption. We overcome this discrepancy by basing the semantics of our formalism on the logic of minimal knowledge and negation as failure (MKNF) by Lifschitz. We present several algorithms for reasoning with MKNF+ knowledge bases, each suitable to different kinds of rules, and establish tight complexity bounds.
We present new explicit constructions of deterministic randomness extractors, dispersers and related objects. We say that a distribution X on binary strings of length n is a δ-source if X assigns probability at most 2−δn to any string of length n. For every δ>0, we construct the following poly(n)-time computable functions:2-source disperser: D:({0, 1}n)2 → {0, 1} such that for any two independent δ-sources X1,X2 we have that the support of D(X1,X2) is {0, 1}.Bipartite Ramsey graph: Let N=2n. A corollary is that the function D is a 2-coloring of the edges of KN,N (the complete bipartite graph over two sets of N vertices) such that any induced subgraph of size Nδ by Nδ is not monochromatic.3-source extractor: E:({0, 1}n)3→ {0, 1} such that for any three independent δ-sources X1,X2,X3 we have that E(X1,X2,X3) is o(1)-close to being an unbiased random bit.No previous explicit construction was known for either of these for any δ<1/2, and these results constitute significant progress to long-standing open problems.A component in these results is a new construction of condensers that may be of independent interest: This is a function C:{0, 1}n → ({0, 1}n/c)d (where c and d are constants that depend only on δ) such that for every δ-source X one of the output blocks of C(X) is (exponentially close to) a 0.9-source. (This result was obtained independently by Ran Raz.)The constructions are quite involved and use as building blocks other new and known objects. A recurring theme in these constructions is that objects that were designed to work with independent inputs, sometimes perform well enough with correlated, high entropy inputs.The construction of the disperser is based on a new technique which we call “the challenge-response mechanism” that (in some sense) allows “identifying high entropy regions” in a given pair of sources using only one sample from the two sources.We consider the problem of embedding a metric into low-dimensional Euclidean space. The classical theorems of Bourgain, and of Johnson and Lindenstrauss say that any metric on n points embeds into an O(log n)-dimensional Euclidean space with O(log n) distortion. Moreover, a simple “volume” argument shows that this bound is nearly tight: a uniform metric on n points requires nearly logarithmic number of dimensions to embed with logarithmic distortion. It is natural to ask whether such a volume restriction is the only hurdle to low-dimensional embeddings. In other words, do doubling metrics, that do not have large uniform submetrics, and thus no volume hurdles to low dimensional embeddings, embed in low dimensional Euclidean spaces with small distortion?In this article, we give a positive answer to this question. We show how to embed any doubling metrics into O(log log n) dimensions with O(log n) distortion. This is the first embedding for doubling metrics into fewer than logarithmic number of dimensions, even allowing for logarithmic distortion.This result is one extreme point of our general trade-off between distortion and dimension: given an n-point metric (V,d) with doubling dimension dimD, and any target dimension T in the range Ω(dimD log log n) ≤ T ≤ O(log n), we show that the metric embeds into Euclidean space RT with O(log n &sqrt; dimD/T) distortion.This article determines the weakest failure detectors to implement shared atomic objects in a distributed system with crash-prone processes. We first determine the weakest failure detector for the basic register object. We then use that to determine the weakest failure detector for all popular atomic objects including test-and-set, fetch-and-add, queue, consensus and compare-and-swap, which we show is the same.Set constraints form a constraint system where variables range over the domain of sets of trees. They give a natural formalism for many problems in program analysis. Syntactically, set constraints are conjunctions of inclusions between expressions built over variables, constructors (constants and function symbols from a given signature) and a choice of set operators that defines the specific class of set constraints. In this article, we are interested in the class of set constraints with projections, which is the class with all Boolean operators (union, intersection and complement) and projections that in program analysis directly correspond to type destructors. We prove that the problem of existence of a solution of a system of set constraints with projections is in NEXPTIME, and thus that it is NEXPTIME-complete.We present a linear expected time algorithm for finding maximum cardinality matchings in sparse random graphs. This is optimal and improves on previous results by a logarithmic factor.Betweenness-Centrality measure is often used in social and computer communication networks to estimate the potential monitoring and control capabilities a vertex may have on data flowing in the network. In this article, we define the Routing Betweenness Centrality (RBC) measure that generalizes previously well known Betweenness measures such as the Shortest Path Betweenness, Flow Betweenness, and Traffic Load Centrality by considering network flows created by arbitrary loop-free routing strategies.We present algorithms for computing RBC of all the individual vertices in the network and algorithms for computing the RBC of a given group of vertices, where the RBC of a group of vertices represents their potential to collaboratively monitor and control data flows in the network. Two types of collaborations are considered: (i) conjunctive—the group is a sequences of vertices controlling traffic where all members of the sequence process the traffic in the order defined by the sequence and (ii) disjunctive—the group is a set of vertices controlling traffic where at least one member of the set processes the traffic. The algorithms presented in this paper also take into consideration different sampling rates of network monitors, accommodate arbitrary communication patterns between the vertices (traffic matrices), and can be applied to groups consisting of vertices and/or edges.For the cases of routing strategies that depend on both the source and the target of the message, we present algorithms with time complexity of O(n2m) where n is the number of vertices in the network and m is the number of edges in the routing tree (or the routing directed acyclic graph (DAG) for the cases of multi-path routing strategies). The time complexity can be reduced by an order of n if we assume that the routing decisions depend solely on the target of the messages.Finally, we show that a preprocessing of O(n2m) time, supports computations of RBC of sequences in O(kn) time and computations of RBC of sets in O(n3n) time, where k in the number of vertices in the sequence or the set.Personalized ranking systems and trust systems are an essential tool for collaboration in a multi-agent environment. In these systems, trust relations between many agents are aggregated to produce a personalized trust rating of the agents. In this article, we introduce the first extensive axiomatic study of this setting, and explore a wide array of well-known and new personalized ranking systems. We adapt several axioms (basic criteria) from the literature on global ranking systems to the context of personalized ranking systems, and fully classify the set of systems that satisfy all of these axioms. We further show that all these axioms are necessary for this result.
We present a new method for upper bounding the second eigenvalue of the Laplacian of graphs. Our approach uses multi-commodity flows to deform the geometry of the graph; we embed the resulting metric into Euclidean space to recover a bound on the Rayleigh quotient. Using this, we show that every n-vertex graph of genus g and maximum degree D satisfies λ2(G)=O((g+1)3D/n). This recovers the O(D/n) bound of Spielman and Teng for planar graphs, and compares to Kelner's bound of O((g+1)poly(D)/n), but our proof does not make use of conformal mappings or circle packings. We are thus able to extend this to resolve positively a conjecture of Spielman and Teng, by proving that λ2(G) = O(Dh6log h/n) whenever G is Kh-minor free. This shows, in particular, that spectral partitioning can be used to recover O(&sqrt;n)-sized separators in bounded degree graphs that exclude a fixed minor. We extend this further by obtaining nearly optimal bounds on λ2 for graphs that exclude small-depth minors in the sense of Plotkin, Rao, and Smith. Consequently, we show that spectral algorithms find separators of sublinear size in a general class of geometric graphs.Moreover, while the standard “sweep” algorithm applied to the second eigenvector may fail to find good quotient cuts in graphs of unbounded degree, our approach produces a vector that works for arbitrary graphs. This yields an alternate proof of the well-known nonplanar separator theorem of Alon, Seymour, and Thomas that states that every excluded-minor family of graphs has O(&sqrt;n)-node balanced separators.We observe that many important computational problems in NC1 share a simple self-reducibility property. We then show that, for any problem A having this self-reducibility property, A has polynomial-size TC0 circuits if and only if it has TC0 circuits of size n1+&epsis; for every &epsis;> 0 (counting the number of wires in a circuit as the size of the circuit). As an example of what this observation yields, consider the Boolean Formula Evaluation problem (BFE), which is complete for NC1 and has the self-reducibility property. It follows from a lower bound of Impagliazzo, Paturi, and Saks, that BFE requires depth d TC0 circuits of size n1+&epsis;d. If one were able to improve this lower bound to show that there is some constant &epsis;> 0 (independent of the depth d) such that every TC0 circuit family recognizing BFE has size at least n1+&epsis;, then it would follow that TC0 ≠ NC1. We show that proving lower bounds of the form n1+&epsis; is not ruled out by the Natural Proof framework of Razborov and Rudich and hence there is currently no known barrier for separating classes such as ACC0, TC0 and NC1 via existing “natural” approaches to proving circuit lower bounds.We also show that problems with small uniform constant-depth circuits have algorithms that simultaneously have small space and time bounds. We then make use of known time-space tradeoff lower bounds to show that SAT requires uniform depth d TC0 and AC0[6] circuits of size n1+c for some constant c depending on d.Protein structure analysis is one of the most important research issues in the post-genomic era, and faster and more accurate index data structures for such 3-D structures are highly desired for research on proteins. This article proposes a new data structure for indexing protein 3-D structures. For strings, there are many efficient indexing structures such as suffix trees, but it has been considered very difficult to design such sophisticated data structures against 3-D structures like proteins. Our index structure is based on the suffix tree and is called the geometric suffix tree. By using the geometric suffix tree for a set of protein structures, we can exactly search for all of their substructures whose RMSDs (root mean square deviations) or URMSDs (unit-vector root mean square deviations) to a given query 3-D structure are not larger than a given bound. Though there are O(N2) substructures in a structure of size N, our data structure requires only O(N) space for indexing all the substructures. We propose an O(N2) construction algorithm for it, while a naive algorithm would require O(N3) time to construct it. Moreover we propose an efficient search algorithm. Experiments show that we can search for similar structures much faster than previous algorithms if the RMSD threshold is not larger than 1Å. The experiments also show that the construction time of the geometric suffix tree is practically almost linear to the size of the database, when applied to a protein structure database.We present a fully dynamic randomized data structure that can answer queries about the convex hull of a set of n points in three dimensions, where insertions take O(log3n) expected amortized time, deletions take O(log6n) expected amortized time, and extreme-point queries take O(log2n) worst-case time. This is the first method that guarantees polylogarithmic update and query cost for arbitrary sequences of insertions and deletions, and improves the previous O(n&epsis;)-time method by Agarwal and Matoušek a decade ago. As a consequence, we obtain similar results for nearest neighbor queries in two dimensions and improved results for numerous fundamental geometric problems (such as levels in three dimensions and dynamic Euclidean minimum spanning trees in the plane).We present several new results regarding λs(n), the maximum length of a Davenport--Schinzel sequence of order s on n distinct symbols.First, we prove that λs(n) ≤ n ⋅ 2(1/t&excel;)α(n)t + O(α(n)t−1) for s ≥ 4 even, and λs(n) ≤ n ⋅ 2(1/t&excel;)α(n)t log2 α(n) + O(α(n)t) for s≥ 3 odd, where t = ⌊(s−2)/2⌋, and α(n) denotes the inverse Ackermann function. The previous upper bounds, by Agarwal et al. [1989], had a leading coefficient of 1 instead of 1/t&excel; in the exponent. The bounds for even s are now tight up to lower-order terms in the exponent. These new bounds result from a small improvement on the technique of Agarwal et al.More importantly, we also present a new technique for deriving upper bounds for λs(n). This new technique is very similar to the one we applied to the problem of stabbing interval chains [Alon et al. 2008]. With this new technique we: (1) re-derive the upper bound of λ3(n) ≤ 2n α(n) + O(n &sqrt;α(n)) (first shown by Klazar [1999]); (2) re-derive our own new upper bounds for general s and (3) obtain improved upper bounds for the generalized Davenport--Schinzel sequences considered by Adamec et al. [1992].Regarding lower bounds, we show that λ3(n) ≥ 2n α(n) − O(n) (the previous lower bound (Sharir and Agarwal, 1995) had a coefficient of 1/2), so the coefficient 2 is tight. We also present a simpler variant of the construction of Agarwal et al. [1989] that achieves the known lower bounds of λs(n) ≥ n ⋅ 2(1/t&excel;) α(n)t − O(α(n)t−1) for s ≥ 4 even.We study FO(MTC), first-order logic with monadic transitive closure, a logical formalism in between FO and MSO on trees. We characterize the expressive power of FO(MTC) in terms of nested tree-walking automata. Using the latter, we show that FO(MTC) is strictly less expressive than MSO, solving an open problem. We also present a temporal logic on trees that is expressively complete for FO(MTC), in the form of an extension of the XML document navigation language XPath with two operators: the Kleene star for taking the transitive closure of path expressions, and a subtree relativisation operator, allowing one to restrict attention to a specific subtree while evaluating a subexpression. We show that the expressive power of this XPath dialect equals that of FO(MTC) for Boolean, unary and binary queries. We also investigate the complexity of the automata model as well as the XPath dialect. We show that query evaluation be done in polynomial time (combined complexity), but that emptiness (or, satisfiability) is 2ExpTime-complete.Supervised learning—that is, learning from labeled examples—is an area of Machine Learning that has reached substantial maturity. It has generated general-purpose and practically successful algorithms and the foundations are quite well understood and captured by theoretical frameworks such as the PAC-learning model and the Statistical Learning theory framework. However, for many contemporary practical problems such as classifying web pages or detecting spam, there is often additional information available in the form of unlabeled data, which is often much cheaper and more plentiful than labeled data. As a consequence, there has recently been substantial interest in semi-supervised learning—using unlabeled data together with labeled data—since any useful information that reduces the amount of labeled data needed can be a significant benefit. Several techniques have been developed for doing this, along with experimental results on a variety of different learning problems. Unfortunately, the standard learning frameworks for reasoning about supervised learning do not capture the key aspects and the assumptions underlying these semi-supervised learning methods.In this article, we describe an augmented version of the PAC model designed for semi-supervised learning, that can be used to reason about many of the different approaches taken over the past decade in the Machine Learning community. This model provides a unified framework for analyzing when and why unlabeled data can help, in which one can analyze both sample-complexity and algorithmic issues. The model can be viewed as an extension of the standard PAC model where, in addition to a concept class C, one also proposes a compatibility notion: a type of compatibility that one believes the target concept should have with the underlying distribution of data. Unlabeled data is then potentially helpful in this setting because it allows one to estimate compatibility over the space of hypotheses, and to reduce the size of the search space from the whole set of hypotheses C down to those that, according to one's assumptions, are a-priori reasonable with respect to the distribution. As we show, many of the assumptions underlying existing semi-supervised learning algorithms can be formulated in this framework.After proposing the model, we then analyze sample-complexity issues in this setting: that is, how much of each type of data one should expect to need in order to learn well, and what the key quantities are that these numbers depend on. We also consider the algorithmic question of how to efficiently optimize for natural classes and compatibility notions, and provide several algorithmic results including an improved bound for Co-Training with linear separators when the distribution satisfies independence given the label.
We present a general approach for designing approximation algorithms for a fundamental class of geometric clustering problems in arbitrary dimensions. More specifically, our approach leads to simple randomized algorithms for the k-means, k-median and discrete k-means problems that yield (1+ϵ) approximations with probability ≥ 1/2 and running times of O(2(k/ϵ)O(1) dn). These are the first algorithms for these problems whose running times are linear in the size of the input (nd for n points in d dimensions) assuming k and ϵ are fixed. Our method is general enough to be applicable to clustering problems satisfying certain simple properties and is likely to have further applications.We introduce a theoretical framework for discovering relationships between two database instances over distinct and unknown schemata. This framework is grounded in the context of data exchange. We formalize the problem of understanding the relationship between two instances as that of obtaining a schema mapping so that a minimum repair of this mapping provides a perfect description of the target instance given the source instance. We show that this definition yields “intuitive” results when applied on database instances derived from each other by basic operations. We study the complexity of decision problems related to this optimality notion in the context of different logical languages and show that, even in very restricted cases, the problem is of high complexity.We present the nested Chinese restaurant process (nCRP), a stochastic process that assigns probability distributions to ensembles of infinitely deep, infinitely branching trees. We show how this stochastic process can be used as a prior distribution in a Bayesian nonparametric model of document collections. Specifically, we present an application to information retrieval in which documents are modeled as paths down a random tree, and the preferential attachment dynamics of the nCRP leads to clustering of documents according to sharing of topics at multiple levels of abstraction. Given a corpus of documents, a posterior inference algorithm finds an approximation to a posterior distribution over trees, topics and allocations of words to levels of the tree. We demonstrate this algorithm on collections of scientific abstracts from several journals. This model exemplifies a recent trend in statistical machine learning—the use of Bayesian nonparametric methods to infer distributions on flexible data structures.We present a novel clock synchronization algorithm and prove tight upper and lower bounds on the worst-case clock skew that may occur between any two participants in any given distributed system. More importantly, the worst-case clock skew between neighboring nodes is (asymptotically) at most a factor of two larger than the best possible bound. While previous results solely focused on the dependency of the skew bounds on the network diameter, we prove that our techniques are optimal also with respect to the maximum clock drift, the uncertainty in message delays, and the imposed bounds on the clock rates. The presented results all hold in a general model where both the clock drifts and the message delays may vary arbitrarily within pre-specified bounds.Furthermore, our algorithm exhibits a number of other highly desirable properties. First, the algorithm ensures that the clock values remain in an affine linear envelope of real time. A better worst-case bound on the accuracy with respect to real time cannot be achieved in the absence of an external timer. Second, the algorithm minimizes the number and size of messages that need to be exchanged in a given time period. Moreover, only a small number of bits must be stored locally for each neighbor. Finally, our algorithm can easily be adapted for a variety of other prominent synchronization models.A temporal constraint language is a set of relations that has a first-order definition in(Q;<), the dense linear order of the rational numbers. We present a complete complexity classification of the constraint satisfaction problem (CSP) for temporal constraint languages: if the constraint language is contained in one out of nine temporal constraint languages, then the CSP can be solved in polynomial time; otherwise, the CSP is NP-complete. Our proof combines model-theoretic concepts with techniques from universal algebra, and also applies the so-called product Ramsey theorem, which we believe will useful in similar contexts of constraint satisfaction complexity classification.An extended abstract of this article appeared in the proceedings of STOC'08.In the spirit of Landin, we present a calculus of dependent types to serve as the semantic foundation for a family of languages called data description languages. Such languages, which include pads, datascript, and packettypes, are designed to facilitate programming with ad hoc data, that is, data not in well-behaved relational or xml formats. In the calculus, each type describes the physical layout and semantic properties of a data source. In the semantics, we interpret types simultaneously as the in-memory representation of the data described and as parsers for the data source. The parsing functions are robust, automatically detecting and recording errors in the data stream without halting parsing. We show the parsers are type-correct, returning data whose type matches the simple-type interpretation of the specification. We also prove the parsers are “error-correct,” accurately reporting the number of physical and semantic errors that occur in the returned data. We use the calculus to describe the features of various data description languages, and we discuss how we have used the calculus to improve pads.The Lovász Local Lemma discovered by Erdős and Lovász in 1975 is a powerful tool to non-constructively prove the existence of combinatorial objects meeting a prescribed collection of criteria. In 1991, József Beck was the first to demonstrate that a constructive variant can be given under certain more restrictive conditions, starting a whole line of research aimed at improving his algorithm's performance and relaxing its restrictions. In the present article, we improve upon recent findings so as to provide a method for making almost all known applications of the general Local Lemma algorithmic.
Nearest neighbor searching is the problem of preprocessing a set of n point points in d-dimensional space so that, given any query point q, it is possible to report the closest point to q rapidly. In approximate nearest neighbor searching, a parameter ϵ > 0 is given, and a multiplicative error of (1 + ϵ) is allowed. We assume that the dimension d is a constant and treat n and ϵ as asymptotic quantities. Numerous solutions have been proposed, ranging from low-space solutions having space O(n) and query time O(log n + 1/ϵd−1) to high-space solutions having space roughly O((n log n)/ϵd) and query time O(log (n/ϵ)).We show that there is a single approach to this fundamental problem, which both improves upon existing results and spans the spectrum of space-time tradeoffs. Given a tradeoff parameter γ, where 2 ≤ γ ≤ 1/ϵ, we show that there exists a data structure of space O(nγd−1 log(1/ϵ)) that can answer queries in time O(log(nγ) + 1/(ϵγ)(d−1)/2. When γ = 2, this yields a data structure of space O(n log (1/ϵ)) that can answer queries in time O(log n + 1/ϵ(d−1)/2). When γ = 1/ϵ, it provides a data structure of space O((n/ϵd−1)log(1/ϵ)) that can answer queries in time O(log(n/ϵ)).Our results are based on a data structure called a (t,ϵ)-AVD, which is a hierarchical quadtree-based subdivision of space into cells. Each cell stores up to t representative points of the set, such that for any query point q in the cell at least one of these points is an approximate nearest neighbor of q. We provide new algorithms for constructing AVDs and tools for analyzing their total space requirements. We also establish lower bounds on the space complexity of AVDs, and show that, up to a factor of O(log (1/ϵ)), our space bounds are asymptotically tight in the two extremes, γ = 2 and γ = 1/ϵ.We show that the combinatorial complexity of the union of n “fat” tetrahedra in 3-space (i.e., tetrahedra all of whose solid angles are at least some fixed constant) of arbitrary sizes, is O(n2+ϵ), for any ϵ > 0;the bound is almost tight in the worst case, thus almost settling a conjecture of Pach et al. [2003]. Our result extends, in a significant way, the result of Pach et al. [2003] for the restricted case of nearly congruent cubes. The analysis uses cuttings, combined with the Dobkin-Kirkpatrick hierarchical decomposition of convex polytopes, in order to partition space into subcells, so that, on average, the overwhelming majority of the tetrahedra intersecting a subcell Δ behave as fat dihedral wedges in Δ. As an immediate corollary, we obtain that the combinatorial complexity of the union of n cubes in R3, having arbitrary side lengths, is O(n2+ϵ), for any ϵ > 0 (again, significantly extending the result of Pach et al. [2003]). Finally, our analysis can easily be extended to yield a nearly quadratic bound on the complexity of the union of arbitrarily oriented fat triangular prisms (whose cross-sections have arbitrary sizes) in R3.Mutual authentication and authenticated key exchange are fundamental techniques for enabling secure communication over public, insecure networks. It is well known how to design secure protocols for achieving these goals when parties share high-entropy cryptographic keys in advance of the authentication stage. Unfortunately, it is much more common for users to share weak, low-entropy passwords which furthermore may be chosen from a known space of possibilities (say, a dictionary of English words). In this case, the problem becomes much more difficult as one must ensure that protocols are immune to off-line dictionary attacks in which an adversary exhaustively enumerates all possible passwords in an attempt to determine the correct one.We propose a 3-round protocol for password-only authenticated key exchange, and provide a rigorous proof of security for our protocol based on the decisional Diffie-Hellman assumption. The protocol assumes only public parameters—specifically, a “common reference string”—which can be “hard-coded” into an implementation of the protocol; in particular, and in contrast to some previous work, our protocol does not require either party to pre-share a public key. The protocol is also remarkably efficient, requiring computation only (roughly) 4 times greater than “classical” Diffie-Hellman key exchange that provides no authentication at all. Ours is the first protocol for password-only authentication that is both practical and provably-secure using standard cryptographic assumptions.Consider an ordered, static tree T where each node has a label from alphabet Σ. Tree T may be of arbitrary degree and shape. Our goal is designing a compressed storage scheme of T that supports basic navigational operations among the immediate neighbors of a node (i.e. parent, ith child, or any child with some label,…) as well as more sophisticated path-based search operations over its labeled structure.We present a novel approach to this problem by designing what we call the XBW-transform of the tree in the spirit of the well-known Burrows-Wheeler transform for strings [1994]. The XBW-transform uses path-sorting to linearize the labeled tree T into two coordinated arrays, one capturing the structure and the other the labels. For the first time, by using the properties of the XBW-transform, our compressed indexes go beyond the information-theoretic lower bound, and support navigational and path-search operations over labeled trees within (near-)optimal time bounds and entropy-bounded space.Our XBW-transform is simple and likely to spur new results in the theory of tree compression and indexing, as well as interesting application contexts. As an example, we use the XBW-transform to design and implement a compressed index for XML documents whose compression ratio is significantly better than the one achievable by state-of-the-art tools, and its query time performance is order of magnitudes faster.
The generalized hypertree width GHW(H) of a hypergraph H is a measure of its cyclicity. Classes of conjunctive queries or constraint satisfaction problems whose associated hypergraphs have bounded GHW are known to be solvable in polynomial time. However, it has been an open problem for several years if for a fixed constant k and input hypergraph H it can be determined in polynomial time whether GHW(H) ≤ k. Here, this problem is settled by proving that even for k = 3 the problem is already NP-hard. On the way to this result, another long standing open problem, originally raised by Goodman and Shmueli [1984] in the context of join optimization is solved. It is proven that determining whether a hypergraph H admits a tree projection with respect to a hypergraph G is NP-complete. Our intractability results on generalized hypertree width motivate further research on more restrictive tractable hypergraph decomposition methods that approximate generalized hypertree decomposition (GHD). We show that each such method is dominated by a tractable decomposition method definable through a function that associates a set of partial edges to a hypergraph. By using one particular such function, we define the new Component Hypertree Decomposition method, which is tractable and strictly more general than other approximations to GHD published so far.XPath is a prominent W3C standard for navigating XML documents that has stimulated a lot of research into query answering and static analysis. In particular, query containment has been studied extensively for fragments of the 1.0 version of this standard, whereas little is known about query containment in (fragments of) the richer language XPath 2.0. In this article, we consider extensions of CoreXPath, the navigational core of XPath 1.0, with operators that are part of or inspired by XPath 2.0: path intersection, path equality, path complementation, for-loops, and transitive closure. For each combination of these operators, we determine the complexity of query containment, both with and without DTDs. It turns out to range from ExpTime (for extensions with path equality) and 2-ExpTime (for extensions with path intersection) to non-elementary (for extensions with path complementation or for-loops). In almost all cases, adding transitive closure on top has no further impact on the complexity. We also investigate the effect of dropping the upward and/or sibling axes, and show that this sometimes leads to a reduction in complexity. Since the languages we study include negation and conjunction in filters, our complexity results can equivalently be stated in terms of satisfiability. We also analyze the above languages in terms of succinctness.Concurrent with recent theoretical interest in the problem of metric embedding, a growing body of research in the networking community has studied the distance matrix defined by node-to-node latencies in the Internet, resulting in a number of recent approaches that approximately embed this distance matrix into low-dimensional Euclidean space. There is a fundamental distinction, however, between the theoretical approaches to the embedding problem and this recent Internet-related work: in addition to computational limitations, Internet measurement algorithms operate under the constraint that it is only feasible to measure distances for a linear (or near-linear) number of node pairs, and typically in a highly structured way. Indeed, the most common framework for Internet measurements of this type is a beacon-based approach one chooses uniformly at random a constant number of nodes (“beacons”) in the network, each node measures its distance to all beacons, and one then has access to only these measurements for the remainder of the algorithm. Moreover, beacon-based algorithms are often designed not for embedding but for the more basic problem of triangulation, in which one uses the triangle inequality to infer the distances that have not been measured.Here we give algorithms with provable performance guarantees for beacon-based triangulation and embedding. We show that in addition to multiplicative error in the distances, performance guarantees for beacon-based algorithms typically must include a notion of slack—a certain fraction of all distances may be arbitrarily distorted. For metric spaces of bounded doubling dimension (which have been proposed as a reasonable abstraction of Internet latencies), we show that triangulation-based distance reconstruction with a constant number of beacons can achieve multiplicative error 1 + δ on a 1 − ε fraction of distances, for arbitrarily small constants δ and ε. For this same class of metric spaces, we give a beacon-based embedding algorithm that achieves constant distortion on a 1 − ε fraction of distances; this provides some theoretical justification for the success of the recent Global Network Positioning algorithm of Ng and Zhang [2002], and it forms an interesting contrast with lower bounds showing that it is not possible to embed all distances in a doubling metric space with constant distortion. We also give results for other classes of metric spaces, as well as distributed algorithms that require only a sparse set of distances but do not place too much measurement load on any one node.We prove the following information-theoretic property about quantum states.Substate theorem: Let ρ and σ be quantum states in the same Hilbert space with relative entropy S(ρ ∥ σ) ≔ Tr ρ (log ρ− log σ) = c. Then for all ε > 0, there is a state ρ′ such that the trace distance ∥ρ′ − ρ∥tr ≔ Tr &sqrt;(ρ′ − ρ)2 ≤ ε, and ρ′/2O(c/ε2) ≤ σ.It states that if the relative entropy of ρ and σ is small, then there is a state ρ′ close to ρ, i.e. with small trace distance ∥ρ′ − ρ∥tr, that when scaled down by a factor 2O(c) ‘sits inside’, or becomes a ‘substate’ of, σ. This result has several applications in quantum communication complexity and cryptography. Using the substate theorem, we derive a privacy trade-off for the set membership problem in the two-party quantum communication model. Here Alice is given a subset A &subse; [n], Bob an input i ∈ [n], and they need to determine if i ∈ A.Privacy trade-off for set membership: In any two-party quantum communication protocol for the set membership problem, if Bob reveals only k bits of information about his input, then Alice must reveal at least n/2O(k) bits of information about her input.We also discuss relationships between various information theoretic quantities that arise naturally in the context of the substate theorem.Our main result is a reduction from worst-case lattice problems such as GapSVP and SIVP to a certain learning problem. This learning problem is a natural extension of the “learning from parity with error” problem to higher moduli. It can also be viewed as the problem of decoding from a random linear code. This, we believe, gives a strong indication that these problems are hard. Our reduction, however, is quantum. Hence, an efficient solution to the learning problem implies a quantum algorithm for GapSVP and SIVP. A main open question is whether this reduction can be made classical (i.e., nonquantum).We also present a (classical) public-key cryptosystem whose security is based on the hardness of the learning problem. By the main result, its security is also based on the worst-case quantum hardness of GapSVP and SIVP. The new cryptosystem is much more efficient than previous lattice-based cryptosystems: the public key is of size Õ(n2) and encrypting a message increases its size by a factor of Õ(n) (in previous cryptosystems these values are Õ(n4) and Õ(n2), respectively). In fact, under the assumption that all parties share a random bit string of length Õ(n2), the size of the public key can be reduced to Õ(n).
For more than 40 years, Branch & Reduce exponential-time backtracking algorithms have been among the most common tools used for finding exact solutions of NP-hard problems. Despite that, the way to analyze such recursive algorithms is still far from producing tight worst-case running time bounds. Motivated by this, we use an approach, that we call “Measure & Conquer”, as an attempt to step beyond such limitations. The approach is based on the careful design of a nonstandard measure of the subproblem size; this measure is then used to lower bound the progress made by the algorithm at each branching step. The idea is that a smarter measure may capture behaviors of the algorithm that a standard measure might not be able to exploit, and hence lead to a significantly better worst-case time analysis.In order to show the potentialities of Measure & Conquer, we consider two well-studied NP-hard problems: minimum dominating set and maximum independent set. For the first problem, we consider the current best algorithm, and prove (thanks to a better measure) a much tighter running time bound for it. For the second problem, we describe a new, simple algorithm, and show that its running time is competitive with the current best time bounds, achieved with far more complicated algorithms (and standard analysis).Our examples show that a good choice of the measure, made in the very first stages of exact algorithms design, can have a tremendous impact on the running time bounds achievable.In this article, we introduce the model of finite state probabilistic monitors (FPM), which are finite state automata on infinite strings that have probabilistic transitions and an absorbing reject state. FPMs are a natural automata model that can be seen as either randomized run-time monitoring algorithms or as models of open, probabilistic reactive systems that can fail. We give a number of results that characterize, topologically as well as with respect to their computational power, the sets of languages recognized by FPMs. We also study the emptiness and universality problems for such automata and give exact complexity bounds for these problems.The capability of the Random Access Machine (RAM) to execute any instruction in constant time is not realizable, due to fundamental physical constraints on the minimum size of devices and on the maximum speed of signals. This work explores how well the ideal RAM performance can be approximated, for significant classes of computations, by machines whose building blocks have constant size and are connected at a constant distance.A novel memory structure is proposed, which is pipelined (can accept a new request at each cycle) and hierarchical, exhibiting optimal latency a(x) = O(x1/d) to address x, in d-dimensional realizations.In spite of block-transfer or other memory-pipeline capabilities, a number of previous machine models do not achieve a full overlap of memory accesses. These are examples of machines with explicit data movement. It is shown that there are direct-flow computations (without branches and indirect accesses) that require time superlinear in the number of instructions, on all such machines.To circumvent the explicit-data-movement constraints, the Speculative Prefetcher (SP) and the Speculative Prefetcher and Evaluator (SPE) processors are developed. Both processors can execute any direct-flow program in linear time. The SPE also executes in linear time a class of loop programs that includes many significant algorithms. Even quicksort, a somewhat irregular, recursive algorithm admits a linear-time SPE implementation. A relation between instructions called address dependence is introduced, which limits memory-access overlap and can lead to superlinear time, as illustrated with the classical merging algorithm.We develop a single rounding algorithm for scheduling on unrelated parallel machines; this algorithm works well with the known linear programming-, quadratic programming-, and convex programming-relaxations for scheduling to minimize completion time, makespan, and other well-studied objective functions. This algorithm leads to the following applications for the general setting of unrelated parallel machines: (i) a bicriteria algorithm for a schedule whose weighted completion-time and makespan simultaneously exhibit the current-best individual approximations for these criteria; (ii) better-than-two approximation guarantees for scheduling to minimize the Lp norm of the vector of machine-loads, for all 1 < p < ∞; and (iii) the first constant-factor multicriteria approximation algorithms that can handle the weighted completion-time and any given collection of integer Lp norms. Our algorithm has a natural interpretation as a melding of linear-algebraic and probabilistic approaches. Via this view, it yields a common generalization of rounding theorems due to Karp et al. [1987] and Shmoys & Tardos [1993], and leads to improved approximation algorithms for the problem of scheduling with resource-dependent processing times introduced by Grigoriev et al. [2007].
We show that the sparsest cut in graphs with n vertices and m edges can be approximated within O(log2 n) factor in Õ(m + n3/2) time using polylogarithmic single commodity max-flow computations. Previous algorithms are based on multicommodity flows that take time Õ(m + n2). Our algorithm iteratively employs max-flow computations to embed an expander flow, thus providing a certificate of expansion. Our technique can also be extended to yield an O(log2 n)-(pseudo-) approximation algorithm for the edge-separator problem with a similar running time.We give an improved explicit construction of highly unbalanced bipartite expander graphs with expansion arbitrarily close to the degree (which is polylogarithmic in the number of vertices). Both the degree and the number of right-hand vertices are polynomially close to optimal, whereas the previous constructions of Ta-Shma et al. [2007] required at least one of these to be quasipolynomial in the optimal. Our expanders have a short and self-contained description and analysis, based on the ideas underlying the recent list-decodable error-correcting codes of Parvaresh and Vardy [2005].Our expanders can be interpreted as near-optimal “randomness condensers,” that reduce the task of extracting randomness from sources of arbitrary min-entropy rate to extracting randomness from sources of min-entropy rate arbitrarily close to 1, which is a much easier task. Using this connection, we obtain a new, self-contained construction of randomness extractors that is optimal up to constant factors, while being much simpler than the previous construction of Lu et al. [2003] and improving upon it when the error parameter is small (e.g., 1/poly(n)).Understanding the graph structure of the Internet is a crucial step for building accurate network models and designing efficient algorithms for Internet applications. Yet, obtaining this graph structure can be a surprisingly difficult task, as edges cannot be explicitly queried. For instance, empirical studies of the network of Internet Protocol (IP) addresses typically rely on indirect methods like traceroute to build what are approximately single-source, all-destinations, shortest-path trees. These trees only sample a fraction of the network's edges, and a paper by Lakhina et al. [2003] found empirically that the resulting sample is intrinsically biased. Further, in simulations, they observed that the degree distribution under traceroute sampling exhibits a power law even when the underlying degree distribution is Poisson.In this article, we study the bias of traceroute sampling mathematically and, for a very general class of underlying degree distributions, explicitly calculate the distribution that will be observed. As example applications of our machinery, we prove that traceroute sampling finds power-law degree distributions in both δ-regular and Poisson-distributed random graphs. Thus, our work puts the observations of Lakhina et al. on a rigorous footing, and extends them to nearly arbitrary degree distributions.Is it possible to predict how long an algorithm will take to solve a previously-unseen instance of an NP-complete problem? If so, what uses can be found for models that make such predictions? This article provides answers to these questions and evaluates the answers experimentally.We propose the use of supervised machine learning to build models that predict an algorithm's runtime given a problem instance. We discuss the construction of these models and describe techniques for interpreting them to gain understanding of the characteristics that cause instances to be hard or easy. We also present two applications of our models: building algorithm portfolios that outperform their constituent algorithms, and generating test distributions that emphasize hard problems.We demonstrate the effectiveness of our techniques in a case study of the combinatorial auction winner determination problem. Our experimental results show that we can build very accurate models of an algorithm's running time, interpret our models, build an algorithm portfolio that strongly outperforms the best single algorithm, and tune a standard benchmark suite to generate much harder problem instances.In a cost-sharing problem, several participants with unknown preferences vie to receive some good or service, and each possible outcome has a known cost. A cost-sharing mechanism is a protocol that decides which participants are allocated a good and at what prices. Three desirable properties of a cost-sharing mechanism are: incentive-compatibility, meaning that participants are motivated to bid their true private value for receiving the good; budget-balance, meaning that the mechanism recovers its incurred cost with the prices charged; and economic efficiency, meaning that the cost incurred and the value to the participants are traded off in an optimal way. These three goals have been known to be mutually incompatible for thirty years. Nearly all the work on cost-sharing mechanism design by the economics and computer science communities has focused on achieving two of these goals while completely ignoring the third.We introduce novel measures for quantifying efficiency loss in cost-sharing mechanisms and prove simultaneous approximate budget-balance and approximate efficiency guarantees for mechanisms for a wide range of cost-sharing problems, including all submodular and Steiner tree problems. Our key technical tool is an exact characterization of worst-case efficiency loss in Moulin mechanisms, the dominant paradigm in cost-sharing mechanism design.Obstruction-free implementations of concurrent objects are optimized for the common case where there is no step contention, and were recently advocated as a solution to the costs associated with synchronization without locks. In this article, we study this claim and this goes through precisely defining the notions of obstruction-freedom and step contention. We consider several classes of obstruction-free implementations, present corresponding generic object implementations, and prove lower bounds on their complexity. Viewed collectively, our results establish that the worst-case operation time complexity of obstruction-free implementations is high, even in the absence of step contention. We also show that lock-based implementations are not subject to some of the time-complexity lower bounds we present.
We consider a scenario where we want to query a large dataset that is stored in external memory and does not fit into main memory. The most constrained resources in such a situation are the size of the main memory and the number of random accesses to external memory. We note that sequentially streaming data from external memory through main memory is much less prohibitive.We propose an abstract model of this scenario in which we restrict the size of the main memory and the number of random accesses to external memory, but admit arbitrary sequential access. A distinguishing feature of our model is that it allows the usage of unlimited external memory for storing intermediate results, such as several hard disks that can be accessed in parallel.In this model, we prove lower bounds for the problem of sorting a sequence of strings (or numbers), the problem of deciding whether two given sets of strings are equal, and two closely related decision problems. Intuitively, our results say that there is no algorithm for the problems that uses internal memory space bounded by N1−ϵ and at most o(log N) random accesses to external memory, but unlimited “streaming access”, both for writing to and reading from external memory. (Here, N denotes the size of the input and ϵ is an arbitrary constant greater than 0.) We even permit randomized algorithms with one-sided bounded error. We also consider the problem of evaluating database queries and prove similar lower bounds for evaluating relational algebra queries against relational databases and XQuery and XPath queries against XML-databases.Motivated by reasoning tasks for XML languages, the satisfiability problem of logics on data trees is investigated. The nodes of a data tree have a label from a finite set and a data value from a possibly infinite set. It is shown that satisfiability for two-variable first-order logic is decidable if the tree structure can be accessed only through the child and the next sibling predicates and the access to data values is restricted to equality tests. From this main result, decidability of satisfiability and containment for a data-aware fragment of XPath and of the implication problem for unary key and inclusion constraints is concluded.We prove that Bimatrix, the problem of finding a Nash equilibrium in a two-player game, is complete for the complexity class PPAD (Polynomial Parity Argument, Directed version) introduced by Papadimitriou in 1991.Our result, building upon the work of Daskalakis et al. [2006a] on the complexity of four-player Nash equilibria, settles a long standing open problem in algorithmic game theory. It also serves as a starting point for a series of results concerning the complexity of two-player Nash equilibria. In particular, we prove the following theorems:—Bimatrix does not have a fully polynomial-time approximation scheme unless every problem in PPAD is solvable in polynomial time.—The smoothed complexity of the classic Lemke-Howson algorithm and, in fact, of any algorithm for Bimatrix is not polynomial unless every problem in PPAD is solvable in randomized polynomial time.Our results also have a complexity implication in mathematical economics:—Arrow-Debreu market equilibria are PPAD-hard to compute.The Minimum Weight Triangulation problem is to find a triangulation T* of minimum length for a given set of points P in the Euclidean plane. It was one of the few longstanding open problems from the famous list of twelve problems with unknown complexity status, published by Garey and Johnson [1979]. Very recently, the problem was shown to be NP-hard by Mulzer and Rote [2006]. In this article, we present a quasi-polynomial time approximation scheme for Minimum Weight Triangulation.We propose the model of nested words for representation of data with both a linear ordering and a hierarchically nested matching of items. Examples of data with such dual linear-hierarchical structure include executions of structured programs, annotated linguistic data, and HTML/XML documents. Nested words generalize both words and ordered trees, and allow both word and tree operations. We define nested word automata—finite-state acceptors for nested words, and show that the resulting class of regular languages of nested words has all the appealing theoretical properties that the classical regular word languages enjoys: deterministic nested word automata are as expressive as their nondeterministic counterparts; the class is closed under union, intersection, complementation, concatenation, Kleene-*, prefixes, and language homomorphisms; membership, emptiness, language inclusion, and language equivalence are all decidable; and definability in monadic second order logic corresponds exactly to finite-state recognizability. We also consider regular languages of infinite nested words and show that the closure properties, MSO-characterization, and decidability of decision problems carry over.The linear encodings of nested words give the class of visibly pushdown languages of words, and this class lies between balanced languages and deterministic context-free languages. We argue that for algorithmic verification of structured programs, instead of viewing the program as a context-free language over words, one should view it as a regular language of nested words (or equivalently, a visibly pushdown language), and this would allow model checking of many properties (such as stack inspection, pre-post conditions) that are not expressible in existing specification logics.We also study the relationship between ordered trees and nested words, and the corresponding automata: while the analysis complexity of nested word automata is the same as that of classical tree automata, they combine both bottom-up and top-down traversals, and enjoy expressiveness and succinctness benefits over tree automata.It has long been known [Chvátal and Sankoff 1975] that the average length of the longest common subsequence of two random strings of length n over an alphabet of size k is asymptotic to γkn for some constant γk depending on k. The value of these constants remains unknown, and a number of papers have proved upper and lower bounds on them. We discuss techniques, involving numerical calculations with recurrences on many variables, for determining lower and upper bounds on these constants. To our knowledge, the previous best-known lower and upper bounds for γ2 were those of Dančík and Paterson, approximately 0.773911 and 0.837623 [Dančík 1994; Dančík and Paterson 1995]. We improve these to 0.788071 and 0.826280. This upper bound is less than the γ2 given by Steele's old conjecture (see Steele [1997, page 3]) that γ2 = 2/(1 + &sqrt;2)≈ 0.828427. (As Steele points out, experimental evidence had already suggested that this conjectured value was too high.) Finally, we show that the upper bound technique described here could be used to produce, for any k, a sequence of upper bounds converging to γk, though the computation time grows very quickly as better bounds are guaranteed.We present a near-optimal reduction from approximately counting the cardinality of a discrete set to approximately sampling elements of the set. An important application of our work is to approximating the partition function Z of a discrete system, such as the Ising model, matchings or colorings of a graph. The typical approach to estimating the partition function Z(β*) at some desired inverse temperature β* is to define a sequence, which we call a cooling schedule, β0 = 0 < β1 < … < β ℓ = β* where Z(0) is trivial to compute and the ratios Z(βi+1)/Z(βi) are easy to estimate by sampling from the distribution corresponding to Z(βi). Previous approaches required a cooling schedule of length O*(ln A) where A=Z(0), thereby ensuring that each ratio Z(βi+1)/Z(βi) is bounded. We present a cooling schedule of length ℓ =O*(&sqrt; lnA).For well-studied problems such as estimating the partition function of the Ising model, or approximating the number of colorings or matchings of a graph, our cooling schedule is of length O*(&sqrt; n), which implies an overall savings of O*(n) in the running time of the approximate counting algorithm (since roughly ℓ samples are needed to estimate each ratio).A similar improvement in the length of the cooling schedule was recently obtained by Lovász and Vempala in the context of estimating the volume of convex bodies. While our reduction is inspired by theirs, the discrete analogue of their result turns out to be significantly more difficult. Whereas a fixed schedule suffices in their setting, we prove that in the discrete setting we need an adaptive schedule, that is, the schedule depends on Z. More precisely, we prove any nonadaptive cooling schedule has length at least O*(ln A), and we present an algorithm to find an adaptive schedule of length O*(&sqrt; ln A).
We give a O(&sqrt;log n)-approximation algorithm for the sparsest cut, edge expansion, balanced separator, and graph conductance problems. This improves the O(log n)-approximation of Leighton and Rao (1988). We use a well-known semidefinite relaxation with triangle inequality constraints. Central to our analysis is a geometric theorem about projections of point sets in Rd, whose proof makes essential use of a phenomenon called measure concentration.We also describe an interesting and natural “approximate certificate” for a graph's expansion, which involves embedding an n-node expander in it with appropriate dilation and congestion. We call this an expander flow.We study the multicut and the sparsest cut problems in directed graphs. In the multicut problem, we are a given an n-vertex graph G along with k source-sink pairs, and the goal is to find the minimum cardinality subset of edges whose removal separates all source-sink pairs. The sparsest cut problem has the same input, but the goal is to find a subset of edges to delete so as to minimize the ratio of the number of deleted edges to the number of source-sink pairs that are separated by this deletion. The natural linear programming relaxation for multicut corresponds, by LP-duality, to the well-studied maximum (fractional) multicommodity flow problem, while the standard LP-relaxation for sparsest cut corresponds to maximum concurrent flow. Therefore, the integrality gap of the linear programming relaxation for multicut/sparsest cut is also the flow-cut gap: the largest gap, achievable for any graph, between the maximum flow value and the minimum cost solution for the corresponding cut problem.Our first result is that the flow-cut gap between maximum multicommodity flow and minimum multicut is Ω˜(n1/7) in directed graphs. We show a similar result for the gap between maximum concurrent flow and sparsest cut in directed graphs. These results improve upon a long-standing lower bound of Ω(log n) for both types of flow-cut gaps. We notice that these polynomially large flow-cut gaps are in a sharp contrast to the undirected setting where both these flow-cut gaps are known to be Θ(log n). Our second result is that both directed multicut and sparsest cut are hard to approximate to within a factor of 2Ω(log1−&epsis; n) for any constant &epsis; > 0, unless NP ⊆ ZPP. This improves upon the recent Ω(log n/log log n)-hardness result for these problems. We also show that existence of PCP's for NP with perfect completeness, polynomially small soundness, and constant number of queries would imply a polynomial factor hardness of approximation for both these problems. All our results hold for directed acyclic graphs.Phylogeny is both a fundamental tool in biology and a rich source of fascinating modeling and algorithmic problems. Today's wealth of sequenced genomes makes it increasingly important to understand evolutionary events such as duplications, losses, transpositions, inversions, lateral transfers, and domain shuffling. We focus on the gene duplication event, that constitutes a major force in the creation of genes with new function [Ohno 1970; Lynch and Force 2000] and, thereby also, of biodiversity.We introduce the probabilistic gene evolution model, which describes how a gene tree evolves within a given species tree with respect to speciation, gene duplication, and gene loss. The actual relation between gene tree and species tree is captured by a reconciliation, a concept which we generalize for more expressiveness. The model is a canonical generalization of the classical linear birth-death process, obtained by replacing the interval where the process takes place by a tree.For the gene evolution model, we derive efficient algorithms for some associated probability distributions: the probability of a reconciled tree, the probability of a gene tree, the maximum probability reconciliation, the posterior probability of a reconciliation, and sampling reconciliations with respect to the posterior probability. These algorithms provides the basis for several applications, including species tree construction, reconciliation analysis, orthology analysis, biogeography, and host-parasite co-evolution.An arithmetic formula is multilinear if the polynomial computed by each of its subformulas is multilinear. We prove that any multilinear arithmetic formula for the permanent or the determinant of an n × n matrix is of size super-polynomial in n. Previously, super-polynomial lower bounds were not known (for any explicit function) even for the special case of multilinear formulas of constant depth.We give the first correct O(n log n) algorithm for finding a maximum st-flow in a directed planar graph. After a preprocessing step that consists in finding single-source shortest-path distances in the dual, the algorithm consists of repeatedly saturating the leftmost residual s-to-t path.This article presents a method for constructing hardware structures that perform a fixed permutation on streaming data. The method applies to permutations that can be represented as linear mappings on the bit-level representation of the data locations. This subclass includes many important permutations such as stride permutations (corner turn, perfect shuffle, etc.), the bit reversal, the Hadamard reordering, and the Gray code reordering.The datapath for performing the streaming permutation consists of several independent banks of memory and two interconnection networks. These structures are built for a given streaming width (i.e., number of inputs and outputs per cycle) and operate at full throughput for this streaming width.We provide an algorithm that completely specifies the datapath and control logic given the desired permutation and streaming width. Further, we provide lower bounds on the achievable cost of a solution and show that for an important subclass of permutations our solution is optimal.We apply our algorithm to derive datapaths for several important permutations, including a detailed example that carefully illustrates each aspect of the design process. Lastly, we compare our permutation structures to those of Järvinen et al. [2004], which are specialized for stride permutations.
We define Recursive Markov Chains (RMCs), a class of finitely presented denumerable Markov chains, and we study algorithms for their analysis. Informally, an RMC consists of a collection of finite-state Markov chains with the ability to invoke each other in a potentially recursive manner. RMCs offer a natural abstract model for probabilistic programs with procedures. They generalize, in a precise sense, a number of well-studied stochastic models, including Stochastic Context-Free Grammars (SCFG) and Multi-Type Branching Processes (MT-BP).We focus on algorithms for reachability and termination analysis for RMCs: what is the probability that an RMC started from a given state reaches another target state, or that it terminates? These probabilities are in general irrational, and they arise as (least) fixed point solutions to certain (monotone) systems of nonlinear equations associated with RMCs. We address both the qualitative problem of determining whether the probabilities are 0, 1 or in-between, and the quantitative problems of comparing the probabilities with a given bound, or approximating them to desired precision.We show that all these problems can be solved in PSPACE using a decision procedure for the Existential Theory of Reals. We provide a more practical algorithm, based on a decomposed version of multi-variate Newton's method, and prove that it always converges monotonically to the desired probabilities. We show this method applies more generally to any monotone polynomial system. We obtain polynomial-time algorithms for various special subclasses of RMCs. Among these: for SCFGs and MT-BPs (equivalently, for 1-exit RMCs) the qualitative problem can be solved in P-time; for linearly recursive RMCs the probabilities are rational and can be computed exactly in P-time.We show that our PSPACE upper bounds cannot be substantially improved without a breakthrough on long standing open problems: the square-root sum problem and an arithmetic circuit decision problem that captures P-time on the unit-cost rational arithmetic RAM model. We show that these problems reduce to the qualitative problem and to the approximation problem (to within any nontrivial error) for termination probabilities of general RMCs, and to the quantitative decision problem for termination (extinction) of SCFGs (MT-BPs).We consider the problem of storing a large file on a remote and unreliable server. To verify that the file has not been corrupted, a user could store a small private (randomized) “fingerprint” on his own computer. This is the setting for the well-studied authentication problem in cryptography, and the required fingerprint size is well understood. We study the problem of sublinear authentication: suppose the user would like to encode and store the file in a way that allows him to verify that it has not been corrupted, but without reading the entire file. If the user only wants to read q bits of the file, how large does the size s of the private fingerprint need to be? We define this problem formally, and show a tight lower bound on the relationship between s and q when the adversary is not computationally bounded, namely: s × q = Ω(n), where n is the file size. This is an easier case of the online memory checking problem, introduced by Blum et al. [1991], and hence the same (tight) lower bound applies also to that problem.It was previously shown that, when the adversary is computationally bounded, under the assumption that one-way functions exist, it is possible to construct much better online memory checkers. The same is also true for sublinear authentication schemes. We show that the existence of one-way functions is also a necessary condition: even slightly breaking the s × q = Ω(n) lower bound in a computational setting implies the existence of one-way functions.Living organisms function in accordance with complex mechanisms that operate in different ways depending on conditions. Darwin's theory of evolution suggests that such mechanisms evolved through variation guided by natural selection. However, there has existed no theory that would explain quantitatively which mechanisms can so evolve in realistic population sizes within realistic time periods, and which are too complex. In this article, we suggest such a theory. We treat Darwinian evolution as a form of computational learning from examples in which the course of learning is influenced only by the aggregate fitness of the hypotheses on the examples, and not otherwise by specific examples. We formulate a notion of evolvability that distinguishes function classes that are evolvable with polynomially bounded resources from those that are not. We show that in a single stage of evolution monotone Boolean conjunctions and disjunctions are evolvable over the uniform distribution, while Boolean parity functions are not. We suggest that the mechanism that underlies biological evolution overall is “evolvable target pursuit”, which consists of a series of evolutionary stages, each one inexorably pursuing an evolvable target in the technical sense suggested above, each such target being rendered evolvable by the serendipitous combination of the environment and the outcomes of previous evolutionary stages.In this article, we are interested in general techniques for designing mechanisms that approximate the social welfare in the presence of selfish rational behavior. We demonstrate our results in the setting of Combinatorial Auctions (CA). Our first result is a general deterministic technique to decouple the algorithmic allocation problem from the strategic aspects, by a procedure that converts any algorithm to a dominant-strategy ascending mechanism. This technique works for any single value domain, in which each agent has the same value for each desired outcome, and this value is the only private information. In particular, for “single-value CAs”, where each player desires any one of several different bundles but has the same value for each of them, our technique converts any approximation algorithm to a dominant strategy mechanism that almost preserves the original approximation ratio. Our second result provides the first computationally efficient deterministic mechanism for the case of single-value multi-minded bidders (with private value and private desired bundles). The mechanism achieves an approximation to the social welfare which is close to the best possible in polynomial time (unless P=NP). This mechanism is an algorithmic implementation in undominated strategies, a notion that we define and justify, and is of independent interest.
We study the impact of combinatorial structure in congestion games on the complexity of computing pure Nash equilibria and the convergence time of best response sequences. In particular, we investigate which properties of the strategy spaces of individual players ensure a polynomial convergence time. We show that if the strategy space of each player consists of the bases of a matroid over the set of resources, then the lengths of all best response sequences are polynomially bounded in the number of players and resources. We also prove that this result is tight, that is, the matroid property is a necessary and sufficient condition on the players' strategy spaces for guaranteeing polynomial-time convergence to a Nash equilibrium.In addition, we present an approach that enables us to devise hardness proofs for various kinds of combinatorial games, including first results about the hardness of market sharing games and congestion games for overlay network design. Our approach also yields a short proof for the PLS-completeness of network congestion games. In particular, we show that network congestion games are PLS-complete for directed and undirected networks even in case of linear latency functions.Many search algorithms are limited by the amount of memory available. Magnetic disk storage is over two orders of magnitude cheaper than semiconductor memory, and individual disks can hold up to a terabyte. We augment memory with magnetic disks to perform brute-force and heuristic searches that are orders of magnitude larger than any previous such searches. The main difficulty is detecting duplicate nodes, which is normally done with a hash table. Due to long disk latencies, however, randomly accessed hash tables are infeasible on disk, and are replaced by a mechanism we call delayed duplicate detection. In contrast to previous work, we perform delayed duplicate detection without sorting, which runs in time linear in the number of nodes in practice. Using this technique, we performed the first complete breadth-first searches of the 2 × 7, 3 × 5, 4 × 4, and 2 × 8 sliding-tile Puzzles, verifying the radius of the 4 × 4 puzzle and determining the radius of the others. We also performed the first complete breadth-first searches of the four-peg Towers of Hanoi problem with up to 22 discs, discovering a surprising anomaly regarding the radii of these problems. In addition, we performed the first heuristic searches of the four-peg Towers of Hanoi problem with up to 31 discs, verifying a conjectured optimal solution length to these problems. We also performed partial breadth-first searches of Rubik's Cube to depth ten in the face-turn metric, and depth eleven in the quarter-turn metric, confirming previous results.Given a set of demands in a directed graph, the directed congestion minimization problem is to route every demand with the objective of minimizing the heaviest load over all edges. We show that for any constant ϵ > 0, there is no Ω(log1−ϵ M)-approximation algorithm on networks of size M unless NP ⊆ ZPTIME(npolylog n). This bound is almost tight given the O(log M/log log M)-approximation via randomized rounding due to Raghavan and Thompson.We construct weak ε-nets of almost linear size for
certain types of point sets. Specifically, for planar point sets in
convex position we construct weak 1/r-nets of size O(rα(r)),
where α(r) denotes the inverse Ackermann function. For point
sets along the moment curve in ℝd we construct
weak 1/r-nets of size r · 2poly(α(r)),
where the degree of the polynomial in the exponent depends
(quadratically) on d.Our constructions result from a reduction to a new problem,
which we call stabbing interval chains with j-tuples. Given the
range of integers N = [1, n], an interval chain of length k is a
sequence of k consecutive, disjoint, nonempty intervals contained
in N. A j-tuple $\bar{P}$ = (p1,…,pj) is said to stab an
interval chain C = I1…Ik if each
pi falls on a different interval of C. The problem is to
construct a small-size family Z of j-tuples that stabs all
k-interval chains in N.Let z(j)k(n) denote the minimum size of
such a family Z. We derive almost-tight upper and lower bounds for
z(j)k(n) for every fixed j; our bounds
involve functions αm(n) of the inverse Ackermann
hierarchy. Specifically, we show that for j = 3 we have
z(3)k(n) =
Θ(nα$\lfloor$k/2$\rfloor$(n)) for all k ≥
6. For each j≥4, we construct a pair of functions
Pʹj(m), Qʹj(m), almost equal
asymptotically, such that z(j)Pʹj(m)(n)
= O(nαm(n)) and
z(j)Qʹj(m)(n) =
Ω(nαm(n)).
A distributed consensus algorithm allows n processes to reach a common decision value starting from individual inputs. Wait-free consensus, in which a process always terminates within a finite number of its own steps, is impossible in an asynchronous shared-memory system. However, consensus becomes solvable using randomization when a process only has to terminate with probability 1. Randomized consensus algorithms are typically evaluated by their total step complexity, which is the expected total number of steps taken by all processes.This article proves that the total step complexity of randomized consensus is Θ(n2) in an asynchronous shared memory system using multi-writer multi-reader registers. This result is achieved by improving both the lower and the upper bounds for this problem.In addition to improving upon the best previously known result by a factor of log2n, the lower bound features a greatly streamlined proof. Both goals are achieved through restricting attention to a set of layered executions and using an isoperimetric inequality for analyzing their behavior.The matching algorithm decreases the expected total step complexity by a log n factor, by leveraging the multi-writing capability of the shared registers. Its correctness proof is facilitated by viewing each execution of the algorithm as a stochastic process and applying Kolmogorov's inequality.The (parameterized) FEEDBACK VERTEX SET problem on directed graphs (i.e., the DFVS problem) is defined as follows: given a directed graph G and a parameter k, either construct a feedback vertex set of at most k vertices in G or report that no such a set exists. It has been a well-known open problem in parameterized computation and complexity whether the DFVS problem is fixed-parameter tractable, that is, whether the problem can be solved in time f(k)nO(1) for some function f. In this article, we develop new algorithmic techniques that result in an algorithm with running time 4k k! nO(1) for the DFVS problem. Therefore, we resolve this open problem.We give the first polynomial time algorithm for exactly computing an equilibrium for the linear utilities case of the market model defined by Fisher. Our algorithm uses the primal--dual paradigm in the enhanced setting of KKT conditions and convex programs. We pinpoint the added difficulty raised by this setting and the manner in which our algorithm circumvents it.We address optimization problems in which we are given contradictory pieces of input information and the goal is to find a globally consistent solution that minimizes the extent of disagreement with the respective inputs. Specifically, the problems we address are rank aggregation, the feedback arc set problem on tournaments, and correlation and consensus clustering. We show that for all these problems (and various weighted versions of them), we can obtain improved approximation factors using essentially the same remarkably simple algorithm. Additionally, we almost settle a long-standing conjecture of Bang-Jensen and Thomassen and show that unless NP⊆BPP, there is no polynomial time algorithm for the problem of minimum feedback arc set in tournaments.We revisit a problem introduced by Bharat and Broder almost a decade ago: How to sample random pages from the corpus of documents indexed by a search engine, using only the search engine's public interface? Such a primitive is particularly useful in creating objective benchmarks for search engines.The technique of Bharat and Broder suffers from a well-recorded bias: it favors long documents. In this article we introduce two novel sampling algorithms: a lexicon-based algorithm and a random walk algorithm. Our algorithms produce biased samples, but each sample is accompanied by a weight, which represents its bias. The samples, in conjunction with the weights, are then used to simulate near-uniform samples. To this end, we resort to four well-known Monte Carlo simulation methods: rejection sampling, importance sampling, the Metropolis--Hastings algorithm, and the Maximum Degree method.The limited access to search engines force our algorithms to use bias weights that are only “approximate”. We characterize analytically the effect of approximate bias weights on Monte Carlo methods and conclude that our algorithms are guaranteed to produce near-uniform samples from the search engine's corpus. Our study of approximate Monte Carlo methods could be of independent interest.Experiments on a corpus of 2.4 million documents substantiate our analytical findings and show that our algorithms do not have significant bias towards long documents. We use our algorithms to collect comparative statistics about the corpora of the Google, MSN Search, and Yahoo! search engines.
In the maximum constraint satisfaction problem (MAX CSP), one is given a finite collection of (possibly weighted) constraints on overlapping sets of variables, and the goal is to assign values from a given finite domain to the variables so as to maximize the number (or the total weight, for the weighted case) of satisfied constraints. This problem is NP-hard in general, and, therefore, it is natural to study how restricting the allowed types of constraints affects the approximability of the problem. In this article, we show that any MAX CSP problem with a finite set of allowed constraint types, which includes all fixed-value constraints (i.e., constraints of the form x = a), is either solvable exactly in polynomial time or else is APX-complete, even if the number of occurrences of variables in instances is bounded. Moreover, we present a simple description of all polynomial-time solvable cases of our problem. This description relies on the well-known algebraic combinatorial property of supermodularity.We present a deterministic, log-space algorithm that solves st-connectivity in undirected graphs. The previous bound on the space complexity of undirected st-connectivity was log4/3(⋅) obtained by Armoni, Ta-Shma, Wigderson and Zhou (JACM 2000). As undirected st-connectivity is complete for the class of problems solvable by symmetric, nondeterministic, log-space computations (the class SL), this algorithm implies that SL = L (where L is the class of problems solvable by deterministic log-space computations). Independent of our work (and using different techniques), Trifonov (STOC 2005) has presented an O(log n log log n)-space, deterministic algorithm for undirected st-connectivity.Our algorithm also implies a way to construct in log-space a fixed sequence of directions that guides a deterministic walk through all of the vertices of any connected graph. Specifically, we give log-space constructible universal-traversal sequences for graphs with restricted labeling and log-space constructible universal-exploration sequences for general graphs.In this article, we show that keeping track of history enables significant improvements in the communication complexity of dynamic network protocols. We present a communication optimal maintenance of a spanning tree in a dynamic network. The amortized (on the number of topological changes) message complexity is O(V), where V is the number of nodes in the network. The message size used by the algorithm is O(log |ID|) where |ID| is the size of the name space of the nodes. Typically, log |ID| = O(log V).Previous algorithms that adapt to dynamic networks involved Ω (E) messages per topological change—inherently paying for re-computation of the tree from scratch.Spanning trees are essential components in many distributed algorithms. Some examples include broadcast (dissemination of messages to all network nodes), multicast, reset (general adaptation of static algorithms to dynamic networks), routing, termination detection, and more. Thus, our efficient maintenance of a spanning tree implies the improvement of algorithms for these tasks. Our results are obtained using a novel technique to save communication. A node uses information received in the past in order to deduce present information from the fact that certain messages were NOT sent by the node's neighbor. This technique is one of our main contributions.Subtyping relations are usually defined either syntactically by a formal system or semantically by an interpretation of types into an untyped denotational model. This work shows how to define a subtyping relation semantically in the presence of Boolean connectives, functional types and dynamic dispatch on types, without the complexity of denotational models, and how to derive a complete subtyping algorithm.
The process of introducing security controls into a sensitive task, which we call secure task design in this article, consists of two steps: high-level security policy design and low-level enforcement scheme design. A high-level security policy states an overall requirement for a sensitive task. One example of a high-level security policy is a separation of duty policy, which requires a task to be performed by a team of at least k users. Unlike low-level enforcement schemes such as security constraints in workflows, a separation of duty policy states a high-level requirement about the task without referring to individual steps in the task. While extremely important and widely used, separation of duty policies state only requirements on the number of users involved in the task and do not capture the requirements on these users' attributes. In this article, we introduce a novel algebra that enables the formal specification of high-level policies that combine requirements on users' attributes with requirements on the number of users motivated by separation of duty considerations. We give the syntax and semantics of the algebra and study algebraic properties of its operators. After that, we study potential mechanisms to enforce high-level policies specified in the algebra and a number of computational problems related to policy analysis and enforcement.We prove a new discrete fixed point theorem for direction-preserving functions defined on integer points, based on a novel characterization of boundary conditions for the existence of fixed points. The theorem allows us to derive an improved algorithm for finding such a fixed point. We also develop a new lower bound proof technique. Together, they allow us to derive an asymptotic matching bound for the problem of finding a fixed point in a hypercube of any constantly bounded finite dimension.Exploring a linkage with the approximation version of the continuous fixed point problem, we obtain asymptotic matching bounds for the complexity of the approximate Brouwer fixed point problem in the continuous case for Lipschitz functions. It settles a fifteen-years-old open problem of Hirsch, Papadimitriou, and Vavasis by improving both the upper and lower bounds.Our characterization for the existence of a fixed point is also applicable to functions defined on nonconvex domains, which makes it a potentially useful tool for the design and analysis of algorithms for fixed points in general domains.We develop polynomial-time algorithms for finding correlated equilibria—a well-studied notion of rationality that generalizes the Nash equilibrium—in a broad class of succinctly representable multiplayer games, encompassing graphical games, anonymous games, polymatrix games, congestion games, scheduling games, local effect games, as well as several generalizations. Our algorithm is based on a variant of the existence proof due to Hart and Schmeidler, and employs linear programming duality, the ellipsoid algorithm, Markov chain steady state computations, as well as application-specific methods for computing multivariate expectations over product distributions.For anonymous games and graphical games of bounded tree-width, we provide a different polynomial-time algorithm for optimizing an arbitrary linear function over the set of correlated equilibria of the game. In contrast to our sweeping positive results for computing an arbitrary correlated equilibrium, we prove that optimizing over correlated equilibria is NP-hard in all of the other classes of games that we consider.The homomorphism preservation theorem (h.p.t.), a result in classical model theory, states that a first-order formula is preserved under homomorphisms on all structures (finite and infinite) if and only if it is equivalent to an existential-positive formula. Answering a long-standing question in finite model theory, we prove that the h.p.t. remains valid when restricted to finite structures (unlike many other classical preservation theorems, including the Łoś--Tarski theorem and Lyndon's positivity theorem). Applications of this result extend to constraint satisfaction problems and to database theory via a correspondence between existential-positive formulas and unions of conjunctive queries. A further result of this article strengthens the classical h.p.t.: we show that a first-order formula is preserved under homomorphisms on all structures if and only if it is equivalent to an existential-positive formula of equal quantifier-rank.
Some promising recent schemes for XML access control employ encryption for implementing security policies on published data, avoiding data duplication. In this article, we study one such scheme, due to Miklau and Suciu [2003]. That scheme was introduced with some intuitive explanations and goals, but without precise definitions and guarantees for the use of cryptography (specifically, symmetric encryption and secret sharing). We bridge this gap in the present work. We analyze the scheme in the context of the rigorous models of modern cryptography. We obtain formal results in simple, symbolic terms close to the vocabulary of Miklau and Suciu. We also obtain more detailed computational results that establish security against probabilistic polynomial-time adversaries. Our approach, which relates these two layers of the analysis, continues a recent thrust in security research and may be applicable to a broad class of systems that rely on cryptographic data protection.Data exchange is the problem of finding an instance of a target schema, given an instance of a source schema and a specification of the relationship between the source and the target. Theoretical foundations of data exchange have recently been investigated for relational data.In this article, we start looking into the basic properties of XML data exchange, that is, restructuring of XML documents that conform to a source DTD under a target DTD, and answering queries written over the target schema. We define XML data exchange settings in which source-to-target dependencies refer to the hierarchical structure of the data. Combining DTDs and dependencies makes some XML data exchange settings inconsistent. We investigate the consistency problem and determine its exact complexity.We then move to query answering, and prove a dichotomy theorem that classifies data exchange settings into those over which query answering is tractable, and those over which it is coNP-complete, depending on classes of regular expressions used in DTDs. Furthermore, for all tractable cases we give polynomial-time algorithms that compute target XML documents over which queries can be answered.We study the satisfiability problem associated with XPath in the presence of DTDs. This is the problem of determining, given a query p in an XPath fragment and a DTD D, whether or not there exists an XML document T such that T conforms to D and the answer of p on T is nonempty. We consider a variety of XPath fragments widely used in practice, and investigate the impact of different XPath operators on the satisfiability analysis. We first study the problem for negation-free XPath fragments with and without upward axes, recursion and data-value joins, identifying which factors lead to tractability and which to NP-completeness. We then turn to fragments with negation but without data values, establishing lower and upper bounds in the absence and in the presence of upward modalities and recursion. We show that with negation the complexity ranges from PSPACE to EXPTIME. Moreover, when both data values and negation are in place, we find that the complexity ranges from NEXPTIME to undecidable. Furthermore, we give a finer analysis of the problem for particular classes of DTDs, exploring the impact of various DTD constructs, identifying tractable cases, as well as providing the complexity in the query size alone. Finally, we investigate the problem for XPath fragments with sibling axes, exploring the impact of horizontal modalities on the satisfiability analysis.Data exchange deals with inserting data from one database into another database having a different schema. Fagin et al. [2005] have shown that among the universal solutions of a solvable data exchange problem, there exists—up to isomorphism—a unique most compact one, “the core”, and have convincingly argued that this core should be the database to be materialized. They stated as an important open problem whether the core can be computed in polynomial time in the general setting where the mapping between the source and target schemas is given by source-to-target constraints that are arbitrary tuple generating dependencies (tgds) and target constraints consisting of equality generating dependencies (egds) and a weakly acyclic set of tgds. In this article, we solve this problem by developing new methods for efficiently computing the core of a universal solution. This positive result shows that data exchange based on cores is feasible and applicable in a very general setting. In addition to our main result, we use the method of hypertree decompositions to derive new algorithms and upper bounds for query containment checking and computing cores of arbitrary database instances. We also show that computing the core of a data exchange problem is fixed-parameter intractable with respect to a number of relevant parameters, and that computing cores is NP-complete if the rule bodies of target tgds are augmented by a special predicate that distinguishes a null value from a constant data value.We construct binary codes for fingerprinting digital documents. Our codes for n users that are ε-secure against c pirates have length O(c2log(n/ε)). This improves the codes proposed by Boneh and Shaw [1998] whose length is approximately the square of this length. The improvement carries over to works using the Boneh--Shaw code as a primitive, for example, to the dynamic traitor tracing scheme of Tassa [2005].By proving matching lower bounds we establish that the length of our codes is best within a constant factor for reasonable error probabilities. This lower bound generalizes the bound found independently by Peikert et al. [2003] that applies to a limited class of codes. Our results also imply that randomized fingerprint codes over a binary alphabet are as powerful as over an arbitrary alphabet and the equal strength of two distinct models for fingerprinting.A triangulation of a planar point set S is a maximal plane straight-line graph with vertex set S. In the minimum-weight triangulation (MWT) problem, we are looking for a triangulation of a given point set that minimizes the sum of the edge lengths. We prove that the decision version of this problem is NP-hard, using a reduction from PLANAR 1-IN-3-SAT. The correct working of the gadgets is established with computer assistance, using dynamic programming on polygonal faces, as well as the β-skeleton heuristic to certify that certain edges belong to the minimum-weight triangulation.
A q-query Locally Decodable Code (LDC) encodes an n-bit message x as an N-bit codeword C(x), such that one can probabilistically recover any bit xi of the message by querying only q bits of the codeword C(x), even after some constant fraction of codeword bits has been corrupted.We give new constructions of three query LDCs of vastly shorter length than that of previous constructions. Specifically, given any Mersenne prime p = 2t − 1, we design three query LDCs of length N = exp(O(n1/t)), for every n. Based on the largest known Mersenne prime, this translates to a length of less than exp(O(n10 − 7)) compared to exp(O(n1/2)) in the previous constructions. It has often been conjectured that there are infinitely many Mersenne primes. Under this conjecture, our constructions yield three query locally decodable codes of length N = exp(nO(1/log log n)) for infinitely many n.We also obtain analogous improvements for Private Information Retrieval (PIR) schemes. We give 3-server PIR schemes with communication complexity of O(n10 − 7) to access an n-bit database, compared to the previous best scheme with complexity O(n1/5.25). Assuming again that there are infinitely many Mersenne primes, we get 3-server PIR schemes of communication complexity nO(1/log logn)) for infinitely many n.Previous families of LDCs and PIR schemes were based on the properties of low-degree multivariate polynomials over finite fields. Our constructions are completely different and are obtained by constructing a large number of vectors in a small dimensional vector space whose inner products are restricted to lie in an algebraically nice set. XPath expressions define navigational queries on XML data and are issued on XML documents to select sets of element nodes. Due to the wide use of XPath, which is embedded into several languages for querying and manipulating XML data, the problem of efficiently answering XPath queries has received increasing attention from the research community. As the efficiency of computing the answer of an XPath query depends on its size, replacing XPath expressions with equivalent ones having the smallest size is a crucial issue in this direction. This article investigates the minimization problem for a wide fragment of XPath (namely X P[✶]), where the use of the most common operators (child, descendant, wildcard and branching) is allowed with some syntactic restrictions. The examined fragment consists of expressions which have not been specifically studied in the relational setting before: neither are they mere conjunctive queries (as the combination of “//” and “*” enables an implicit form of disjunction to be expressed) nor do they coincide with disjunctive ones (as the latter are more expressive). Three main contributions are provided. The “global minimality” property is shown to hold: the minimization of a given XPath expression can be accomplished by removing pieces of the expression, without having to re-formulate it (as for “general” disjunctive queries). Then, the complexity of the minimization problem is characterized, showing that it is the same as the containment problem. Finally, specific forms of XPath expressions are identified, which can be minimized in polynomial time.We consider a failure-free, asynchronous message passing network with n links, where the processors are arranged on a ring or a chain. The processors are identically programmed but have distinct identities, taken from {0, 1,… ,M − 1}. We investigate the communication costs of three well studied tasks: Consensus, Leader, and MaxF (finding the maximum identity). We show that in chain and ring topologies, the message complexities of all three tasks are the same. Hence, we study a finer measure of complexity: the number of transmitted bits required to solve a task T, denoted BitC(T).We prove several new lower bounds (and some simple upper bounds) that imply the following results: For the two processors case, BitC(Consensus) = 2 and BitC(Leader) = BitC(MaxF) = 2log2 M ± O(1), where the gap between the lower and upper bounds is almost always 1. For a chain, BitC(Consensus) = Θ(n), BitC(Leader) = Θ(n + log M), and BitC(MaxF) = Θ(n log M). For the ring topology, we prove the lower bound of Ω(n log M) for Leader, and (hence) MaxF.We consider also a chain where the intermediate processors have no identities. We prove that BitC(Leader) = Θ(n log M), which is equal to n times the bit complexity of the problem for two processors. For the specific case when the chain length is even, we prove that BitC(Leader) = Θ(n), for both above settings. In addition, we show that for any algorithm solving MaxF, there exists an input, for which every execution has the bit complexity Ω(n log M) (this is not the case for Leader).In our proofs, we use both methods of distributed computing and of communication complexity theory, establishing new links between the two areas.XrML is becoming a popular language in industry for writing software licenses. The semantics for XrML is implicitly given by an algorithm that determines if a permission follows from a set of licenses. We focus on a fragment of the language and use it to highlight some problematic aspects of the algorithm. We then correct the problems, introduce formal semantics, and show that our semantics captures the (corrected) algorithm. Next, we consider the complexity of determining if a permission is implied by a set of XrML licenses. We prove that the general problem is undecidable, but it is polynomial-time computable for an expressive fragment of the language. We extend XrML to capture a wider range of licenses by adding negation to the language. Finally, we discuss the key differences between XrML and MPEG-21, an international standard based on XrML.Stirling [1996, 1998] proved the decidability of bisimilarity on so-called normed pushdown processes. This result was substantially extended by Sénizergues [1998, 2005] who showed the decidability of bisimilarity for regular (or equational) graphs of finite out-degree; this essentially coincides with weak bisimilarity of processes generated by (unnormed) pushdown automata where the ϵ-transitions can only deterministically pop the stack. The question of decidability of bisimilarity for the more general class of so called Type -1 systems, which is equivalent to weak bisimilarity on unrestricted ϵ-popping pushdown processes, was left open. This was repeatedly indicated by both Stirling and Sénizergues. Here we answer the question negatively, that is, we show the undecidability of bisimilarity on Type -1 systems, even in the normed case.We achieve the result by applying a technique we call Defender's Forcing, referring to the bisimulation games. The idea is simple, yet powerful. We demonstrate its versatility by deriving further results in a uniform way. First, we classify several versions of the undecidable problems for prefix rewrite systems (or pushdown automata) as Π01-complete or Σ11-complete. Second, we solve the decidability question for weak bisimilarity on PA (Process Algebra) processes, showing that the problem is undecidable and even Σ11-complete. Third, we show Σ11-completeness of weak bisimilarity for so-called parallel pushdown (or multiset) automata, a subclass of (labeled, place/transition) Petri nets.
It is known that if P and NP are different then there is an infinite hierarchy of different complexity classes that lie strictly between them. Thus, if P ≠ NP, it is not possible to classify NP using any finite collection of complexity classes. This situation has led to attempts to identify smaller classes of problems within NP where dichotomy results may hold: every problem is either in P or is NP-complete. A similar situation exists for counting problems. If P ≠#P, there is an infinite hierarchy in between and it is important to identify subclasses of #P where dichotomy results hold. Graph homomorphism problems are a fertile setting in which to explore dichotomy theorems. Indeed, Feder and Vardi have shown that a dichotomy theorem for the problem of deciding whether there is a homomorphism to a fixed directed acyclic graph would resolve their long-standing dichotomy conjecture for all constraint satisfaction problems. In this article, we give a dichotomy theorem for the problem of counting homomorphisms to directed acyclic graphs. Let H be a fixed directed acyclic graph. The problem is, given an input digraph G, determine how many homomorphisms there are from G to H. We give a graph-theoretic classification, showing that for some digraphs H, the problem is in P and for the rest of the digraphs H the problem is #P-complete. An interesting feature of the dichotomy, which is absent from previously known dichotomy results, is that there is a rich supply of tractable graphs H with complex structure.We present a general deterministic linear space reduction from priority queues to sorting implying that if we can sort up to n keys in S(n) time per key, then there is a priority queue supporting delete and insert in O(S(n)) time and find-min in constant time. Conversely, a priority queue can trivially be used for sorting: first insert all keys to be sorted, then extract them in sorted order by repeatedly deleting the minimum. Asymptotically, this settles the complexity of priority queues in terms of that of sorting.Previously, at SODA'96, such a result was presented by the author for the special case of monotone priority queues where the minimum is not allowed to decrease.Besides nailing down the complexity of priority queues to that of sorting, and vice versa, our result yields several improved bounds for linear space integer priority queues with find-min in constant time:Deterministically. We get O(log log n) update time using a sorting algorithm of Han from STOC'02. This improves the O((log log n)(log log log n)) update time of Han from SODA'01.Randomized. We get O(&sqrt;log log n) expected update time using a randomized sorting algorithm of Han and Thorup from FOCS'02. This improves the O(log log n) expected update time of Thorup from SODA'96.Deterministically in AC0 (without multiplication). For any ϵ > 0, we get O((log log n)1+ϵ) update time using an AC0 sorting algorithm of Han and Thorup from FOCS'02. This improves the O((log log n)2) update time of Thorup from SODA'98.Randomized in AC0. We get O(log log n) expected update time using a randomized AC0 sorting algorithm of Thorup from SODA'97. This improves the O((log log n)1+ϵ) expected update time of Thorup also from SODA'97.The above bounds assume that each integer is stored in a single word and that word operations take unit time as in the word RAM.We introduce a notion of finite testing, based on statistical hypothesis tests, via a variant of the well-known trace machine. Under this scenario, two processes are deemed observationally equivalent if they cannot be distinguished by any finite test. We consider processes modeled as image finite probabilistic automata and prove that our notion of observational equivalence coincides with the trace distribution equivalence proposed by Segala. Along the way, we give an explicit characterization of the set of probabilistic generalize the Approximation Induction Principle by defining an also prove limit and convex closure properties of trace distributions in an appropriate metric space.A snapshot object is an abstraction of the problem of obtaining a consistent view of the contents of shared memory in a distributed system, despite concurrent changes to the memory. There are implementations of m-component snapshot objects shared by n ≥ m processes using m registers. This is the minimum number of registers possible. We prove a time lower bound for implementations that use this minimum number of registers. It matches the time taken by the fastest such implementation. Our proof yields insight into the structure of any such implementation, showing that processes must access the registers in a very constrained way. We also prove a time lower bound for snapshot implementations using single-writer registers in addition to m historyless objects (such as registers and swap objects).A shared variable is an abstraction of persistent interprocess communication. Processors execute operations, often concurrently, on shared variables to exchange information among themselves. The behavior of operation executions is required to be “consistent” for effective interprocess communication. Consequently, a consistency specification of a shared variable describes some guarantees on the behavior of the operation executions. A Read/Write shared variable has two operations: a Write stores a specified value in the variable and a Read returns a value from the variable. For Read/Write variables, a consistency specification describes what values Reads may return. Using an intuitive notion of illegality of Reads, we propose a framework that facilitates specifying a large variety of Read/Write variables.From a high-volume stream of weighted items, we want to create a generic sample of a certain limited size that we can later use to estimate the total weight of arbitrary subsets. Applied to Internet traffic analysis, the items could be records summarizing the flows of packets streaming by a router. Subsets could be flow records from different time intervals of a worm attack whose signature is later determined. The samples taken in the past thus allow us to trace the history of the attack even though the worm was unknown at the time of sampling.Estimation from the samples must be accurate even with heavy-tailed distributions where most of the weight is concentrated on a few heavy items. We want the sample to be weight sensitive, giving priority to heavy items. At the same time, we want sampling without replacement in order to avoid selecting heavy items multiple times. To fulfill these requirements we introduce priority sampling, which is the first weight-sensitive sampling scheme without replacement that works in a streaming context and is suitable for estimating subset sums. Testing priority sampling on Internet traffic analysis, we found it to perform an order of magnitude better than previous schemes.Priority sampling is simple to define and implement: we consider a steam of items i = 0,…,n − 1 with weights wi. For each item i, we generate a random number αi ∈ (0,1] and create a priority qi = wi/αi. The sample S consists of the k highest priority items. Let τ be the (k + 1)th highest priority. Each sampled item i in S gets a weight estimate ŵi = max{wi, τ}, while nonsampled items get weight estimate ŵi = 0.Magically, it turns out that the weight estimates are unbiased, that is, E[ŵi] = wi, and by linearity of expectation, we get unbiased estimators over any subset sum simply by adding the sampled weight estimates from the subset. Also, we can estimate the variance of the estimates, and find, surprisingly, that the covariance between estimates ŵi and ŵj of different weights is zero.Finally, we conjecture an extremely strong near-optimality; namely that for any weight sequence, there exists no specialized scheme for sampling k items with unbiased weight estimators that gets smaller variance sum than priority sampling with k + 1 items. Szegedy settled this conjecture at STOC'06.
How does a search engine company decide what ads to display with each query so as to maximize its revenue? This turns out to be a generalization of the online bipartite matching problem. We introduce the notion of a trade-off revealing LP and use it to derive an optimal algorithm achieving a competitive ratio of 1−1/e for this problem.We show that {0, 1}d endowed with edit distance embeds into ℓ1 with distortion 2O(&sqrt;log d log log d). We further show efficient implementation of the embedding that yield solutions to various computational problems involving edit distance. These include sketching, communication complexity, nearest neighbor search. For all these problems, we improve upon previous bounds.Abduction is a fundamental mode of reasoning with applications in many areas of AI and Computer Science. The computation of abductive explanations is an important computational problem, which is at the core of early systems such as the ATMS and Clause Management Systems and is intimately related to prime implicate generation in propositional logic. Many algorithms have been devised for computing some abductive explanation, and the complexity of the problem has been well studied. However, little attention has been paid to the problem of computing multiple explanations, and in particular all explanations for an abductive query. We fill this gap and consider the computation of all explanations of an abductive query from a propositional Horn theory, or of a polynomial subset of them. Our study pays particular attention to the form of the query, ranging from a literal to a compound formula, to whether explanations are based on a set of abducible literals and to the representation of the Horn theory, either by a Horn conjunctive normal form (CNF) or model-based in terms of its characteristic models. For these combinations, we present either tractability results in terms of polynomial total-time algorithms, intractability results in terms of nonexistence of such algorithms (unless P = NP), or semi-tractability results in terms of solvability in quasi-polynomial time, established by polynomial-time equivalence to the problem of dualizing a monotone CNF expression. Our results complement previous results in the literature, and refute a longstanding conjecture by Selman and Levesque. They elucidate the complexity of generating all abductive explanations and shed light on related problems such as generating sets of restricted prime implicates of a Horn theory. The algorithms for tractable cases can be readily applied for generating a polynomial subset of explanations in polynomial time.Finding an equilibrium of an extensive form game of imperfect information is a fundamental problem in computational game theory, but current techniques do not scale to large games. To address this, we introduce the ordered game isomorphism and the related ordered game isomorphic abstraction transformation. For a multi-player sequential game of imperfect information with observable actions and an ordered signal space, we prove that any Nash equilibrium in an abstracted smaller game, obtained by one or more applications of the transformation, can be easily converted into a Nash equilibrium in the original game. We present an algorithm, GameShrink, for abstracting the game using our isomorphism exhaustively. Its complexity is õ(n2), where n is the number of nodes in a structure we call the signal tree. It is no larger than the game tree, and on nontrivial games it is drastically smaller, so GameShrink has time and space complexity sublinear in the size of the game tree. Using GameShrink, we find an equilibrium to a poker game with 3.1 billion nodes—over four orders of magnitude more than in the largest poker game solved previously. To address even larger games, we introduce approximation methods that do not preserve equilibrium, but nevertheless yield (ex post) provably close-to-optimal strategies.We present a bisimulation method for proving the contextual equivalence of packages in λ-calculus with full existential and recursive types. Unlike traditional logical relations (either semantic or syntactic), our development is “elementary,” using only sets and relations and avoiding advanced machinery such as domain theory, admissibility, and TT-closure. Unlike other bisimulations, ours is complete even for existential types. The key idea is to consider sets of relations—instead of just relations—as bisimulations.
A flow of a commodity is said to be confluent if at any node all the flow of the commodity leaves along a single edge. In this article, we study single-commodity confluent flow problems, where we need to route given node demands to a single destination using a confluent flow. Single- and multi-commodity confluent flows arise in a variety of application areas, most notably in networking; in fact, most flows in the Internet are (multi-commodity) confluent flows since Internet routing is destination based.We present near-tight approximation algorithms, hardness results, and existence theorems for minimizing congestion in single-commodity confluent flows. The maximum edge congestion of a single-commodity confluent flow occurs at one of the incoming edges of the destination. Therefore, finding a minimum-congestion confluent flow is equivalent to the following problem: given a directed graph G with k sinks and non-negative demands on all the nodes of G, determine a confluent flow that routes every node demand to some sink such that the maximum congestion at a sink is minimized.The main result of this article is a polynomial-time algorithm for determining a confluent flow with congestion at most 1 + ln(k) in G, if G admits a splittable flow with congestion at most 1. We complement this result in two directions. First, we present a graph G that admits a splittable flow with congestion at most 1, yet no confluent flow with congestion smaller than Hk, the kth harmonic number, thus establishing tight upper and lower bounds to within an additive constant less than 1. Second, we show that it is NP-hard to approximate the congestion of an optimal confluent flow to within a factor of (log2k)/2, thus resolving the polynomial-time approximability to within a multiplicative constant. We also consider a demand maximization version of the problem. We show that if G admits a splittable flow of congestion at most 1, then a variant of the congestion minimization algorithm yields a confluent flow in G with congestion at most 1 that satisfies 1/3 fraction of total demand.We show that the gap between confluent flows and splittable flows is much smaller, if the underlying graph is k-connected. In particular, we prove that k-connected graphs with k sinks admit confluent flows of congestion less than C + dmax, where C is the congestion of the best splittable flow, and dmax is the maximum demand of any node in G. The proof of this existence theorem is non-constructive and relies on topological techniques introduced by Lovász.This article provides a new conceptual perspective on survey propagation, which is an iterative algorithm recently introduced by the statistical physics community that is very effective in solving random k-SAT problems even with densities close to the satisfiability threshold. We first describe how any SAT formula can be associated with a novel family of Markov random fields (MRFs), parameterized by a real number ρ ∈ [0, 1]. We then show that applying belief propagation---a well-known “message-passing” technique for estimating marginal probabilities---to this family of MRFs recovers a known family of algorithms, ranging from pure survey propagation at one extreme (ρ = 1) to standard belief propagation on the uniform distribution over SAT assignments at the other extreme (ρ = 0). Configurations in these MRFs have a natural interpretation as partial satisfiability assignments, on which a partial order can be defined. We isolate cores as minimal elements in this partial ordering, which are also fixed points of survey propagation and the only assignments with positive probability in the MRF for ρ = 1. Our experimental results for k = 3 suggest that solutions of random formulas typically do not possess non-trivial cores. This makes it necessary to study the structure of the space of partial assignments for ρ < 1 and investigate the role of assignments that are very close to being cores. To that end, we investigate the associated lattice structure, and prove a weight-preserving identity that shows how any MRF with ρ > 0 can be viewed as a “smoothed” version of the uniform distribution over satisfying assignments (ρ = 0). Finally, we isolate properties of Gibbs sampling and message-passing algorithms that are typical for an ensemble of k-SAT problems.We consider the problem of finding a shortest cycle (freely) homotopic to a given simple cycle on a compact, orientable surface. For this purpose, we use a pants decomposition of the surface: a set of disjoint simple cycles that cut the surface into pairs of pants (spheres with three holes). We solve this problem in a framework where the cycles are closed walks on the vertex-edge graph of a combinatorial surface that may overlap but do not cross.We give an algorithm that transforms an input pants decomposition into another homotopic pants decomposition that is optimal: each cycle is as short as possible in its homotopy class. As a consequence, finding a shortest cycle homotopic to a given simple cycle amounts to extending the cycle into a pants decomposition and to optimizing it: the resulting pants decomposition contains the desired cycle. We describe two algorithms for extending a cycle to a pants decomposition. All algorithms in this article are polynomial, assuming uniformity of the weights of the vertex-edge graph of the surface.The well-definedness problem for a database query language consists of checking, given an expression and an input type, that the expression never yields a runtime error on any input adhering to the input type. In this article, we study the well-definedness problem for query languages on trees that are built from a finite set of partially defined base operations by adding variables, constants, conditionals, let bindings, and iteration. We identify properties of base operations that can make the problem undecidable and give restrictions that are sufficient to ensure decidability. As a direct result, we obtain a large fragment of XQuery for which well-definedness is decidable.The relationship between the length of a word and the maximum length of its unbordered factors is investigated in this article. Consider a finite word w of length n. We call a word bordered if it has a proper prefix, which is also a suffix of that word. Let μ(w) denote the maximum length of all unbordered factors of w, and let ∂(w) denote the period of w. Clearly, μ(w) ≤ ∂(w).We establish that μ(w) = ∂(w), if w has an unbordered prefix of length μ(w) and n ≥ 2μ(w) − 1. This bound is tight and solves the stronger version of an old conjecture by Duval [1983]. It follows from this result that, in general, n ≥ 3μ(w) − 3 implies μ(w) = ∂(w), which gives an improved bound for the question raised by Ehrenfeucht and Silberger in 1979.We study random submatrices of a large matrix A. We show how to approximately compute A from its random submatrix of the smallest possible size O(rlog r) with a small error in the spectral norm, where r = ‖A‖2F/‖A‖22 is the numerical rank of A. The numerical rank is always bounded by, and is a stable relaxation of, the rank of A. This yields an asymptotically optimal guarantee in an algorithm for computing low-rank approximations of A. We also prove asymptotically optimal estimates on the spectral norm and the cut-norm of random submatrices of A. The result for the cut-norm yields a slight improvement on the best-known sample complexity for an approximation algorithm for MAX-2CSP problems. We use methods of Probability in Banach spaces, in particular the law of large numbers for operator-valued random variables.
We present constant-factor approximation algorithms for several widely-studied NP-hard optimization problems in network design, including the multicommodity rent-or-buy, virtual private network design, and single-sink buy-at-bulk problems. Our algorithms are simple and their approximation ratios improve over those previously known, in some cases by orders of magnitude.We develop a general analysis framework to bound the approximation ratios of our algorithms. This framework is based on a novel connection between random sampling and game-theoretic cost sharing.The PCP theorem [Arora and Safra 1998; Arora et. al. 1998] says that every language in NP has a witness format that can be checked probabilistically by reading only a constant number of bits from the proof. The celebrated equivalence of this theorem and inapproximability of certain optimization problems, due to Feige et al. [1996], has placed the PCP theorem at the heart of the area of inapproximability.In this work, we present a new proof of the PCP theorem that draws on this equivalence. We give a combinatorial proof for the NP-hardness of approximating a certain constraint satisfaction problem, which can then be reinterpreted to yield the PCP theorem.Our approach is to consider the unsat value of a constraint system, which is the smallest fraction of unsatisfied constraints, ranging over all possible assignments for the underlying variables. We describe a new combinatorial amplification transformation that doubles the unsat-value of a constraint-system, with only a linear blowup in the size of the system. The amplification step causes an increase in alphabet-size that is corrected by a (standard) PCP composition step. Iterative application of these two steps yields a proof for the PCP theorem.The amplification lemma relies on a new notion of “graph powering” that can be applied to systems of binary constraints. This powering amplifies the unsat-value of a constraint system provided that the underlying graph structure is an expander.We also extend our amplification lemma towards construction of assignment testers (alternatively, PCPs of Proximity) which are slightly stronger objects than PCPs. We then construct PCPs and locally-testable codes whose length is linear up to a polylog factor, and whose correctness can be probabilistically verified by making a constant number of queries. Namely, we prove SAT ∈ PCP 1/2,1[log2(n⋅poly log n), O(1)].We introduce exponential search trees as a novel technique for converting static polynomial space search structures for ordered sets into fully-dynamic linear space data structures.This leads to an optimal bound of O(&sqrt;log n/log log n) for searching and updating a dynamic set X of n integer keys in linear space. Searching X for an integer y means finding the maximum key in X which is smaller than or equal to y. This problem is equivalent to the standard text book problem of maintaining an ordered set.The best previous deterministic linear space bound was O(log n/log log n) due to Fredman and Willard from STOC 1990. No better deterministic search bound was known using polynomial space.We also get the following worst-case linear space trade-offs between the number n, the word length W, and the maximal key U < 2W: O(min log log n + log n/logW, log log n ⋅ log log U/log log log U). These trade-offs are, however, not likely to be optimal.Our results are generalized to finger searching and string searching, providing optimal results for both in terms of n.In multiagent settings where the agents have different preferences, preference aggregation is a central issue. Voting is a general method for preference aggregation, but seminal results have shown that all general voting protocols are manipulable. One could try to avoid manipulation by using protocols where determining a beneficial manipulation is hard. Especially among computational agents, it is reasonable to measure this hardness by computational complexity. Some earlier work has been done in this area, but it was assumed that the number of voters and candidates is unbounded. Such hardness results lose relevance when the number of candidates is small, because manipulation algorithms that are exponential only in the number of candidates (and only slightly so) might be available. We give such an algorithm for an individual agent to manipulate the Single Transferable Vote (STV) protocol, which has been shown hard to manipulate in the above sense. This motivates the core of this article, which derives hardness results for realistic elections where the number of candidates is a small constant (but the number of voters can be large).The main manipulation question we study is that of coalitional manipulation by weighted voters. (We show that for simpler manipulation problems, manipulation cannot be hard with few candidates.) We study both constructive manipulation (making a given candidate win) and destructive manipulation (making a given candidate not win). We characterize the exact number of candidates for which manipulation becomes hard for the plurality, Borda, STV, Copeland, maximin, veto, plurality with runoff, regular cup, and randomized cup protocols. We also show that hardness of manipulation in this setting implies hardness of manipulation by an individual in unweighted settings when there is uncertainty about the others' votes (but not vice-versa). To our knowledge, these are the first results on the hardness of manipulation when there is uncertainty about the others' votes.Expectation is a central notion in probability theory. The notion of expectation also makes sense for other notions of uncertainty. We introduce a propositional logic for reasoning about expectation, where the semantics depends on the underlying representation of uncertainty. We give sound and complete axiomatizations for the logic in the case that the underlying representation is (a) probability, (b) sets of probability measures, (c) belief functions, and (d) possibility measures. We show that this logic is more expressive than the corresponding logic for reasoning about likelihood in the case of sets of probability measures, but equi-expressive in the case of probability, belief, and possibility. Finally, we show that satisfiability for these logics is NP-complete, no harder than satisfiability for propositional logic.
Equivalence of aggregate queries is investigated for the class of conjunctive queries with comparisons and the aggregate operators count, count-distinct, min, max, and sum. Essentially, this class contains unnested SQL queries with the above aggregate operators, with a where clause consisting of a conjunction of comparisons, and without a having clause. The comparisons are either interpreted over a domain with a dense order (like the rationals) or with a discrete order (like the integers). Characterizations of equivalence differ for the two cases. For queries with either max or min, equivalence is characterized in terms of dominance mappings, which can be viewed as a generalization of containment mappings. For queries with the count-distinct operator, a sufficient condition for equivalence is given in terms of equivalence of conjunctive queries under set semantics. For some special cases, it is shown that this condition is also necessary. For conjunctive queries with comparisons but without aggregation, equivalence under bag-set semantics is characterized in terms of isomorphism. This characterization essentially remains the same also for queries with the count operator. Moreover, this characterization also applies to queries with the sum operator if the queries have either constants or comparisons, but not both. In the general case (i.e., both comparisons and constants), the characterization of the equivalence of queries with the sum operator is more elaborate. All the characterizations given in the paper are decidable in polynomial space.We solve an open question of Milner [1984]. We define a set of so-called well-behaved finite automata that, modulo bisimulation equivalence, corresponds exactly to the set of regular expressions, and we show how to determine whether a given finite automaton is in this set. As an application, we consider the star height problem.We give an equational specification of the field operations on the rational numbers under initial algebra semantics using just total field operations and 12 equations. A consequence of this specification is that 0−1 = 0, an interesting equation consistent with the ring axioms and many properties of division. The existence of an equational specification of the rationals without hidden functions was an open question. We also give an axiomatic examination of the divisibility operator, from which some interesting new axioms emerge along with equational specifications of algebras of rationals, including one with the modulus function. Finally, we state some open problems, including: Does there exist an equational specification of the field operations on the rationals without hidden functions that is a complete term rewriting system?Measurement-based quantum computation has emerged from the physics community as a new approach to quantum computation where the notion of measurement is the main driving force of computation. This is in contrast with the more traditional circuit model that is based on unitary operations. Among measurement-based quantum computation methods, the recently introduced one-way quantum computer [Raussendorf and Briegel 2001] stands out as fundamental.We develop a rigorous mathematical model underlying the one-way quantum computer and present a concrete syntax and operational semantics for programs, which we call patterns, and an algebra of these patterns derived from a denotational semantics. More importantly, we present a calculus for reasoning locally and compositionally about these patterns. We present a rewrite theory and prove a general standardization theorem which allows all patterns to be put in a semantically equivalent standard form. Standardization has far-reaching consequences: a new physical architecture based on performing all the entanglement in the beginning, parallelization by exposing the dependency structure of measurements and expressiveness theorems.Furthermore we formalize several other measurement-based models, for example, Teleportation, Phase and Pauli models and present compositional embeddings of them into and from the one-way model. This allows us to transfer all the theory we develop for the one-way model to these models. This shows that the framework we have developed has a general impact on measurement-based computation and is not just particular to the one-way quantum computer.Given a matrix A, it is often desirable to find a good approximation to A that has low rank. We introduce a simple technique for accelerating the computation of such approximations when A has strong spectral features, that is, when the singular values of interest are significantly greater than those of a random matrix with size and entries similar to A. Our technique amounts to independently sampling and/or quantizing the entries of A, thus speeding up computation by reducing the number of nonzero entries and/or the length of their representation. Our analysis is based on observing that the acts of sampling and quantization can be viewed as adding a random matrix N to A, whose entries are independent random variables with zero-mean and bounded variance. Since, with high probability, N has very weak spectral features, we can prove that the effect of sampling and quantization nearly vanishes when a low-rank approximation to A + N is computed. We give high probability bounds on the quality of our approximation both in the Frobenius and the 2-norm.Say that a k-CNF a formula is p-satisfiable if there exists a truth assignment satisfying a fraction 1 − 2−k +p 2−k of its clauses (note that every k-CNF formula is 0-satisfiable). Let Fk(n, m) denote a random k-CNF formula on n variables with m clauses. For every k≥2 and every r>0 we determine p and δ=δ(k)=O(k2−k/2) such that with probability tending to 1 as n→∞, a random k-CNF formula Fk(n, rn) is p-satisfiable but not (p+δ)-satisfiable.
We give a complexity theoretic classification of homomorphism problems for graphs and, more generally, relational structures obtained by restricting the left hand side structure in a homomorphism. For every class C of structures, let HOM(C,−) be the problem of deciding whether a given structure A ∈C has a homomorphism to a given (arbitrary) structure ß. We prove that, under some complexity theoretic assumption from parameterized complexity theory, HOM(C,−) is in polynomial time if and only if C has bounded tree width modulo homomorphic equivalence.Translated into the language of constraint satisfaction problems, our result yields a characterization of the tractable structural restrictions of constraint satisfaction problems. Translated into the language of database theory, it implies a characterization of the tractable instances of the evaluation problem for conjunctive queries over relational databases.This article extends the termination proof techniques based on reduction orderings to a higher-order setting, by defining a family of recursive path orderings for terms of a typed lambda-calculus generated by a signature of polymorphic higher-order function symbols. These relations can be generated from two given well-founded orderings, on the function symbols and on the type constructors. The obtained orderings on terms are well founded, monotonic, stable under substitution and include β-reductions. They can be used to prove the strong normalization property of higher-order calculi in which constants can be defined by higher-order rewrite rules using first-order pattern matching. For example, the polymorphic version of Gödel's recursor for the natural numbers is easily oriented. And indeed, our ordering is polymorphic, in the sense that a single comparison allows to prove the termination property of all monomorphic instances of a polymorphic rewrite rule. Many nontrivial examples are given that exemplify the expressive power of these orderings. All have been checked by our implementation.This article is an extended and improved version of Jouannaud and Rubio [1999]. Polymorphic algebras have been made more expressive than in our previous framework. The intuitive notion of a polymorphic higher-order ordering has now been made precise. The higher-order recursive path ordering itself has been made much more powerful by replacing the congruence on types used there by an ordering on types satisfying some abstract properties. Besides, using a restriction of Dershowitz's recursive path ordering for comparing types, we can integrate both orderings into a single one operating uniformly on both terms and types.Speed scaling is a power management technique that involves dynamically changing the speed of a processor. We study policies for setting the speed of the processor for both of the goals of minimizing the energy used and the maximum temperature attained. The theoretical study of speed scaling policies to manage energy was initiated in a seminal paper by Yao et al. [1995], and we adopt their setting. We assume that the power required to run at speed s is P(s) = sα for some constant α > 1. We assume a collection of tasks, each with a release time, a deadline, and an arbitrary amount of work that must be done between the release time and the deadline. Yao et al. [1995] gave an offline greedy algorithm YDS to compute the minimum energy schedule. They further proposed two online algorithms Average Rate (AVR) and Optimal Available (OA), and showed that AVR is 2α − 1 αα-competitive with respect to energy. We provide a tight αα bound on the competitive ratio of OA with respect to energy.We initiate the study of speed scaling to manage temperature. We assume that the environment has a fixed ambient temperature and that the device cools according to Newton's law of cooling. We observe that the maximum temperature can be approximated within a factor of two by the maximum energy used over any interval of length 1/b, where b is the cooling parameter of the device. We define a speed scaling policy to be cooling-oblivious if it is simultaneously constant-competitive with respect to temperature for all cooling parameters. We then observe that cooling-oblivious algorithms are also constant-competitive with respect to energy, maximum speed and maximum power. We show that YDS is a cooling-oblivious algorithm. In contrast, we show that the online algorithms OA and AVR are not cooling-oblivious. We then propose a new online algorithm that we call BKP. We show that BKP is cooling-oblivious. We further show that BKP is e-competitive with respect to the maximum speed, and that no deterministic online algorithm can have a better competitive ratio. BKP also has a lower competitive ratio for energy than OA for α ≥5.Finally, we show that the optimal temperature schedule can be computed offline in polynomial-time using the Ellipsoid algorithm.We give polynomial-time quantum algorithms for three problems from computational algebraic number theory. The first is Pell's equation. Given a positive nonsquare integer d, Pell's equation is x2 − dy2 = 1 and the goal is to find its integer solutions. Factoring integers reduces to finding integer solutions of Pell's equation, but a reduction in the other direction is not known and appears more difficult. The second problem we solve is the principal ideal problem in real quadratic number fields. This problem, which is at least as hard as solving Pell's equation, is the one-way function underlying the Buchmann--Williams key exchange system, which is therefore broken by our quantum algorithm. Finally, assuming the generalized Riemann hypothesis, this algorithm can be used to compute the class group of a real quadratic number field.
A fundamental problem of distributed computing is that of simulating a secure broadcast channel, within the setting of a point-to-point network. This problem is known as Byzantine Agreement (or Generals) and has been the focus of much research. Lamport et al. [1982] showed that in order to achieve Byzantine Agreement in the plain model, more than two thirds of the participating parties must be honest. They further showed that by augmenting the network with a public-key infrastructure for digital signatures, it is possible to obtain protocols that are secure for any number of corrupted parties. The problem in this augmented model is called “authenticated Byzantine Agreement”.In this article, we consider the question of concurrent, parallel and sequential composition of authenticated Byzantine Agreement protocols with a single common setup. We present surprising impossibility results showing that:(1) Authenticated Byzantine Agreement protocols that remain secure under parallel or concurrent composition (even for just two executions) and tolerate a third or more corrupted parties, do not exist.(2) Deterministic authenticated Byzantine Agreement protocols that run for r rounds and tolerate a third or more corrupted parties, can remain secure for at most 2r − 1 sequential executions.In contrast, we present randomized protocols for authenticated Byzantine Agreement that remain secure under sequential composition, for any polynomial number of executions. We exhibit two such protocols. In the first protocol, an honest majority is required. In the second protocol, any number of parties may be corrupted; however, the complexity of the protocol is in the order of 2n · n! for n parties. In order to have this polynomial in the security parameter k (used for the signature scheme in the protocol), this requires the overall number of parties to be limited to O(log k/log log k). The above results are achieved due to a new protocol for authenticated Byzantine Generals for three parties that can tolerate any number of faulty parties and composes sequentially.Finally, we show that when the model is further augmented so that in each session, all the participating parties receive a common session identifier that is unique to that session, then any polynomial number of authenticated Byzantine agreement protocols can be concurrently executed, while tolerating any number of corrupted parties.Suffix trees and suffix arrays are widely used and largely interchangeable index structures on strings and sequences. Practitioners prefer suffix arrays due to their simplicity and space efficiency while theoreticians use suffix trees due to linear-time construction algorithms and more explicit structure. We narrow this gap between theory and practice with a simple linear-time construction algorithm for suffix arrays. The simplicity is demonstrated with a C++ implementation of 50 effective lines of code. The algorithm is called DC3, which stems from the central underlying concept of difference cover. This view leads to a generalized algorithm, DC, that allows a space-efficient implementation and, moreover, supports the choice of a space--time tradeoff. For any v ∈ [1,&nradic;], it runs in O(vn) time using O(n/&vradic;) space in addition to the input string and the suffix array. We also present variants of the algorithm for several parallel and hierarchical memory models of computation. The algorithms for BSP and EREW-PRAM models are asymptotically faster than all previous suffix tree or array construction algorithms.We first introduce Abstract DPLL, a rule-based formulation of the Davis--Putnam--Logemann--Loveland (DPLL) procedure for propositional satisfiability. This abstract framework allows one to cleanly express practical DPLL algorithms and to formally reason about them in a simple way. Its properties, such as soundness, completeness or termination, immediately carry over to the modern DPLL implementations with features such as backjumping or clause learning.We then extend the framework to Satisfiability Modulo background Theories (SMT) and use it to model several variants of the so-called lazy approach for SMT. In particular, we use it to introduce a few variants of a new, efficient and modular approach for SMT based on a general DPLL(X) engine, whose parameter X can be instantiated with a specialized solver SolverT for a given theory T, thus producing a DPLL(T) system. We describe the high-level design of DPLL(X) and its cooperation with SolverT, discuss the role of theory propagation, and describe different DPLL(T) strategies for some theories arising in industrial applications.Our extensive experimental evidence, summarized in this article, shows that DPLL(T) systems can significantly outperform the other state-of-the-art tools, frequently even in orders of magnitude, and have better scaling properties.Stochastic optimization problems attempt to model uncertainty in the data by assuming that the input is specified by a probability distribution. We consider the well-studied paradigm of 2-stage models with recourse: first, given only distributional information about (some of) the data one commits on initial actions, and then once the actual data is realized (according to the distribution), further (recourse) actions can be taken. We show that for a broad class of 2-stage linear models with recourse, one can, for any ε > 0, in time polynomial in 1/ε and the size of the input, compute a solution of value within a factor (1+ε) of the optimum, in spite of the fact that exponentially many second-stage scenarios may occur. In conjunction with a suitable rounding scheme, this yields the first approximation algorithms for 2-stage stochastic integer optimization problems where the underlying random data is given by a “black box” and no restrictions are placed on the costs in the two stages. Our rounding approach for stochastic integer programs shows that an approximation algorithm for a deterministic analogue yields, with a small constant-factor loss, provably near-optimal solutions for the stochastic generalization. Among the range of applications, we consider are stochastic versions of the multicommodity flow, set cover, vertex cover, and facility location problems.
We study the approximability of two natural NP-hard problems. The first problem is congestion minimization in directed networks. In this problem, we are given a directed graph and a set of source-sink pairs. The goal is to route all the pairs with minimum congestion on the network edges. The second problem is machine scheduling, where we are given a set of jobs, and for each job, there is a list of intervals on which it can be scheduled. The goal is to find the smallest number of machines on which all jobs can be scheduled such that no two jobs overlap in their execution on any machine. Both problems are known to be O(log n/log log n)-approximable via the randomized rounding technique of Raghavan and Thompson [1987]. However, until recently, only Max SNP hardness was known for each problem. We make progress in closing this gap by showing that both problems are Ω(log log n)-hard to approximate unless NP ⊆ DTIME(nO(log log log n)).Maximum likelihood (ML) is an increasingly popular optimality criterion for selecting evolutionary trees [Felsenstein 1981]. Finding optimal ML trees appears to be a very hard computational task, but for tractable cases, ML is the method of choice. In particular, algorithms and heuristics for ML take longer to run than algorithms and heuristics for the second major character based criterion, maximum parsimony (MP). However, while MP has been known to be NP-complete for over 20 years [Foulds and Graham, 1982; Day et al. 1986], such a hardness result for ML has so far eluded researchers in the field.An important work by Tuffley and Steel [1997] proves quantitative relations between the parsimony values of given sequences and the corresponding log likelihood values. However, a direct application of their work would only give an exponential time reduction from MP to ML. Another step in this direction has recently been made by Addario-Berry et al. [2004], who proved that ancestral maximum likelihood (AML) is NP-complete. AML “lies in between” the two problems, having some properties of MP and some properties of ML. Still, the AML proof is not directly applicable to the ML problem.We resolve the question, showing that “regular” ML on phylogenetic trees is indeed intractable. Our reduction follows the vertex cover reductions for MP [Day et al. 1986] and AML [Addario-Berry et al. 2004], but its starting point is an approximation version of vertex cover, known as gap vc. The crux of our work is not the reduction, but its correctness proof. The proof goes through a series of tree modifications, while controlling the likelihood losses at each step, using the bounds of Tuffley and Steel [1997]. The proof can be viewed as correlating the value of any ML solution to an arbitrarily close approximation to vertex cover.We show that there is no log⅓ − ϵ M approximation for the undirected Edge-Disjoint Paths problem unless NP ⊆ ZPTIME(npolylog(n)), where M is the size of the graph and ϵ is any positive constant. This hardness result also applies to the undirected All-or-Nothing Multicommodity Flow problem and the undirected Node-Disjoint Paths problem.“Experts algorithms” constitute a methodology for choosing actions repeatedly, when the rewards depend both on the choice of action and on the unknown current state of the environment. An experts algorithm has access to a set of strategies (“experts”), each of which may recommend which action to choose. The algorithm learns how to combine the recommendations of individual experts so that, in the long run, for any fixed sequence of states of the environment, it does as well as the best expert would have done relative to the same sequence. This methodology may not be suitable for situations where the evolution of states of the environment depends on past chosen actions, as is usually the case, for example, in a repeated non-zero-sum game.A general exploration-exploitation experts method is presented along with a proper definition of value. The definition is shown to be adequate in that it both captures the impact of an expert's actions on the environment and is learnable. The new experts method is quite different from previously proposed experts algorithms. It represents a shift from the paradigms of regret minimization and myopic optimization to consideration of the long-term effect of a player's actions on the environment. The importance of this shift is demonstrated by the fact that this algorithm is capable of inducing cooperation in the repeated Prisoner's Dilemma game, whereas previous experts algorithms converge to the suboptimal non-cooperative play. The method is shown to asymptotically perform as well as the best available expert. Several variants are analyzed from the viewpoint of the exploration-exploitation tradeoff, including explore-then-exploit, polynomially vanishing exploration, constant-frequency exploration, and constant-size exploration phases. Complexity and performance bounds are proven.The (vertex) connectivity κ of a graph is the smallest number of vertices whose deletion separates the graph or makes it trivial. We present the fastest known algorithm for finding κ. For a digraph with n vertices, m edges and connectivity κ the time bound is O((n + min{κ&frac52;, κn¾})m). This improves the previous best bound of O((n + min{κ3, κn&rcub)m). For an undirected graph both of these bounds hold with m replaced by κn. Expander graphs are useful for solving the following subproblem that arises in connectivity computation: A known set R of vertices contains two large but unknown subsets that are separated by some unknown set S of κ vertices; we must find two vertices of R that are separated by S.In this article, we study the problem of online market clearing where there is one commodity in the market being bought and sold by multiple buyers and sellers whose bids arrive and expire at different times. The auctioneer is faced with an online clearing problem of deciding which buy and sell bids to match without knowing what bids will arrive in the future. For maximizing profit, we present a (randomized) online algorithm with a competitive ratio of ln(pmax − pmin) + 1, when bids are in a range [pmin, pmax], which we show is the best possible. A simpler algorithm has a ratio twice this, and can be used even if expiration times are not known. For maximizing the number of trades, we present a simple greedy algorithm that achieves a factor of 2 competitive ratio if no money-losing trades are allowed. We also show that if the online algorithm is allowed to subsidize matches---match money-losing pairs if it has already collected enough money from previous pairs to pay for them---then it can actually be 1-competitive with respect to the optimal offline algorithm that is not allowed subsidy. That is, for maximizing the number of trades, the ability to subsidize is at least as valuable as knowing the future. We also consider objectives of maximizing buy or sell volume and social welfare. We present all of these results as corollaries of theorems on online matching in an incomplete interval graph.We also consider the issue of incentive compatibility, and develop a nearly optimal incentive-compatible algorithm for maximizing social welfare. For maximizing profit, we show that no incentive-compatible algorithm can achieve a sublinear competitive ratio, even if only one buy bid and one sell bid are alive at a time. However, we provide an algorithm that, under certain mild assumptions on the bids, performs nearly as well as the best fixed pair of buy and sell prices, a weaker but still natural performance measure. This latter result uses online learning methods, and we also show how such methods can be used to improve our “optimal” algorithms to a broader notion of optimality. Finally, we show how some of our results can be generalized to settings in which the buyers and sellers themselves have online bidding strategies, rather than just each having individual bids.
This article studies the protein side-chain packing problem using the tree-decomposition of a protein structure. To obtain fast and accurate protein side-chain packing, protein structures are modeled using a geometric neighborhood graph, which can be easily decomposed into smaller blocks. Therefore, the side-chain assignment of the whole protein can be assembled from the assignment of the small blocks. Although we will show that the side-chain packing problem is still NP-hard, we can achieve a tree-decomposition-based globally optimal algorithm with time complexity of O(Nnrottw + 1) and several polynomial-time approximation schemes (PTAS), where N is the number of residues contained in the protein, nrot the average number of rotamers for each residue, and tw = O(N2/3 log N) the treewidth of the protein structure graph. Experimental results indicate that after Goldstein dead-end elimination is conducted, nrot is very small and tw is equal to 3 or 4 most of the time. Based on the globally optimal algorithm, we developed a protein side-chain assignment program TreePack, which runs up to 90 times faster than SCWRL 3.0, a widely-used side-chain packing program, on some large test proteins in the SCWRL benchmark database and an average of five times faster on all the test proteins in this database. There are also some real-world instances that TreePack can solve but that SCWRL 3.0 cannot. The TreePack program is available at http://ttic.uchicago.edu/~jinbo/TreePack.htm.We initiate a systematic study of locally testable codes; that is, error-correcting codes that admit very efficient membership tests. Specifically, these are codes accompanied with tests that make a constant number of (random) queries into any given word and reject non-codewords with probability proportional to their distance from the code.Locally testable codes are believed to be the combinatorial core of PCPs. However, the relation is less immediate than commonly believed. Nevertheless, we show that certain PCP systems can be modified to yield locally testable codes. On the other hand, we adapt techniques that we develop for the construction of the latter to yield new PCPs.Our main results are locally testable codes and PCPs of almost-linear length. Specifically, we prove the existence of the following constructs:---Locally testable binary (linear) codes in which k information bits are encoded by a codeword of length k ⋅ exp(Õ(√(log k))). This improves over previous results that either yield codewords of exponential length or obtained almost quadratic length codewords for sufficiently large nonbinary alphabet.---PCP systems of almost-linear length for SAT. The length of the proof is n ⋅ exp(Õ(√(log n))) and verification in performed by a constant number (i.e., 19) of queries, as opposed to previous results that used proof length n(1 + O(1/q)) for verification by q queries.The novel techniques in use include a random projection of certain codewords and PCP-oracles that preserves local-testability, an adaptation of PCP constructions to obtain “linear PCP-oracles” for proving conjunctions of linear conditions, and design of PCPs with some new soundness properties---a direct construction of locally testable (linear) codes of subexponential length.Parallel independent disks can enhance the performance of external memory (EM) algorithms, but the programming task is often difficult. Each disk can service only one read or write request at a time; the challenge is to keep the disks as busy as possible. In this article, we develop a randomized allocation discipline for parallel independent disks, called randomized cycling. We show how it can be used as the basis for an efficient distribution sort algorithm, which we call randomized cycling distribution sort (RCD). We prove that the expected I/O complexity of RCD is optimal. The analysis uses a novel reduction to a scenario with significantly fewer probabilistic interdependencies. We demonstrate RCD's practicality by experimental simulations. Using the randomized cycling discipline, algorithms developed for the unrealistic multihead disk model can be simulated on the realistic parallel disk model for the class of multipass algorithms, which make a complete pass through their data before accessing any element a second time. In particular, algorithms based upon the well-known distribution and merge paradigms of EM computation can be optimally extended from a single disk to parallel disks.We show how to reduce the time overhead for implementing two-way movement on a singly linked list to O(nε) per operation without modifying the list and without making use of storage other than a finite number of pointers into the list. We also prove a matching lower bound.These results add precision to the intuitive feeling that doubly linked lists are more efficient than singly linked lists, and quantify the efficiency gap in a read-only situation. We further analyze the number of points of access into the list (pointers) necessary for obtaining a desired value of ε. We obtain tight tradeoffs which also separate the amortized and worst-case settings.Our upper bound implies that read-only programs with singly-linked input can do string matching much faster than previously expected.
In this article, we show several results obtained by combining the use of stable distributions with pseudorandom generators for bounded space. In particular:---We show that, for any p ∈ (0, 2], one can maintain (using only O(log n/ε2) words of storage) a sketch C(q) of a point q ∈ lnp under dynamic updates of its coordinates. The sketch has the property that, given C(q) and C(s), one can estimate ‖q − s‖p up to a factor of (1 + ε) with large probability. This solves the main open problem of Feigenbaum et al. [1999].---We show that the aforementioned sketching approach directly translates into an approximate algorithm that, for a fixed linear mapping A, and given x ∈ ℜn and y ∈ ℜm, estimates ‖Ax − y‖p in O(n + m) time, for any p ∈ (0, 2]. This generalizes an earlier algorithm of Wasserman and Blum [1997] which worked for the case p = 2.---We obtain another sketch function C′ which probabilistically embeds ln1 into a normed space lm1. The embedding guarantees that, if we set m = log(1/δ)O(1/ε), then for any pair of points q, s ∈ ln1, the distance between q and s does not increase by more than (1 + ε) with constant probability, and it does not decrease by more than (1 − ε) with probability 1 − δ. This is the only known dimensionality reduction theorem for the l1 norm. In fact, stronger theorems of this type (i.e., that guarantee very low probability of expansion as well as of contraction) cannot exist [Brinkman and Charikar 2003].---We give an explicit embedding of ln2 into lnO(log n)1 with distortion (1 + 1/nΘ(1)).We develop a new randomized rounding approach for fractional vectors defined on the edge-sets of bipartite graphs. We show various ways of combining this technique with other ideas, leading to improved (approximation) algorithms for various problems. These include:---low congestion multi-path routing;---richer random-graph models for graphs with a given degree-sequence;---improved approximation algorithms for: (i) throughput-maximization in broadcast scheduling, (ii) delay-minimization in broadcast scheduling, as well as (iii) capacitated vertex cover; and---fair scheduling of jobs on unrelated parallel machines.Protein-protein interactions, which form the basis for most cellular processes, result in the formation of protein interfaces. Believing that the local shape of proteins is crucial, we take a geometric approach and present a definition of an interface surface formed by two or more proteins as a subset of their Voronoi diagram. The definition deals with the difficult and important problem of specifying interface boundaries by invoking methods used in the alpha shape representation of molecules, the discrete flow on Delaunay simplices to define pockets and reconstruct surfaces, and the assessment of the importance of topological features. We present an algorithm to construct the surface and define a hierarchy that distinguishes core and peripheral regions. This hierarchy is shown to have correlation with hot-spots in protein-protein interactions. Finally, we study the geometric and topological properties of interface surfaces and show their high degree of contortion.We present the first lock-free implementation of an extensible hash table running on current architectures. Our algorithm provides concurrent insert, delete, and find operations with an expected O(1) cost. It consists of very simple code, easily implementable using only load, store, and compare-and-swap operations. The new mathematical structure at the core of our algorithm is recursive split-ordering, a way of ordering elements in a linked list so that they can be repeatedly “split” using a single compare-and-swap operation. Metaphorically speaking, our algorithm differs from prior known algorithms in that extensibility is derived by “moving the buckets among the items” rather than “the items among the buckets.” Though lock-free algorithms are expected to work best in multiprogrammed environments, empirical tests we conducted on a large shared memory multiprocessor show that even in non-multiprogrammed environments, the new algorithm performs as well as the most efficient known lock-based resizable hash-table algorithm, and in high load cases it significantly outperforms it.We present new results on the relation between purely symbolic context-free parsing strategies and their probabilistic counterparts. Such parsing strategies are seen as constructions of push-down devices from grammars. We show that preservation of probability distribution is possible under two conditions, viz. the correct-prefix property and the property of strong predictiveness. These results generalize existing results in the literature that were obtained by considering parsing strategies in isolation. From our general results, we also derive negative results on so-called generalized LR parsing.We consider the generalized on-line two-server problem in which each server moves in its own metric space. Requests for service arrive one-by-one and every request is represented by two points: one in each metric space. The problem is to move, at every request, one of the two servers to its request-point such that the total distance travelled by the two servers is minimized.The special case in which both metric spaces are the real line is known as the CNN-problem. It has been a well-known open question in on-line optimization if an algorithm with a constant-competitive ratio exists for this problem. We answer this question in the affirmative by providing a constant-competitive algorithm for the generalized two-server problem on any metric space.The basic result in this article is a characterization of competitiveness for metrical service systems that seems much easier to use when looking for a competitive algorithm. The existence of a competitive algorithm for the generalized two-server problem follows rather easily from this result.The nominal approach to abstract syntax deals with the issues of bound names and α-equivalence by considering constructions and properties that are invariant with respect to permuting names. The use of permutations gives rise to an attractively simple formalization of common, but often technically incorrect uses of structural recursion and induction for abstract syntax modulo α-equivalence. At the heart of this approach is the notion of finitely supported mathematical objects. This article explains the idea in as concrete a way as possible and gives a new derivation within higher-order classical logic of principles of α-structural recursion and induction for α-equivalence classes from the ordinary versions of these principles for abstract syntax trees.Device initialization is a difficult challenge in some proposed realizations of quantum computers, and as such, must be treated as a computational resource. The degree of initialization can be quantified by k, the number of clean qubits in the initial state of the register. In this article, we show that unless m∈O(k + log n), oblivious (gate-by-gate) simulation of an ideal m-qubit quantum circuit by an n-qubit circuit with k clean qubits is impossible. Effectively, this indicates that there is no avoiding physical initialization of a quantity of qubits proportional to that required by the best ideal quantum circuit.
Unions of conjunctive queries, also known as select-project-join-union queries, are the most frequently asked queries in relational database systems. These queries are definable by existential positive first-order formulas and are preserved under homomorphisms. A classical result of mathematical logic asserts that the existential positive formulas are the only first-order formulas (up to logical equivalence) that are preserved under homomorphisms on all structures, finite and infinite. The question of whether the homomorphism-preservation theorem holds for the class of all finite structures resisted solution for a long time. It was eventually shown that, unlike other classical preservation theorems, the homomorphism-preservation theorem does hold in the finite. In this article, we show that the homomorphism-preservation theorem holds also for several restricted classes of finite structures of interest in graph theory and database theory. Specifically, we show that this result holds for all classes of finite structures of bounded degree, all classes of finite structures of bounded treewidth, and, more generally, all classes of finite structures whose cores exclude at least one minor.We study the complexity and expressive power of conjunctive queries over unranked labeled trees represented using a variety of structure relations such as “child”, “descendant”, and “following” as well as unary relations for node labels. We establish a framework for characterizing structures representing trees for which conjunctive queries can be evaluated efficiently. Then we completely chart the tractability frontier of the problem and establish a dichotomy theorem for our axis relations, that is, we find all subset-maximal sets of axes for which query evaluation is in polynomial time and show that for all other cases, query evaluation is NP-complete. All polynomial-time results are obtained immediately using the proof techniques from our framework. Finally, we study the expressiveness of conjunctive queries over trees and show that for each conjunctive query, there is an equivalent acyclic positive query (i.e., a set of acyclic conjunctive queries), but that in general this query is not of polynomial size.Planar spatial datasets can be modeled by closed semi-algebraic sets in the plane. We establish a characterization of the topological properties of such datasets expressible in the relational calculus with real polynomial constraints. The characterization is in the form of a query language that can only point that can only talk about points in the set and the “cones” around these points.
In this article we present a theoretical analysis of the online Sum-of-Squares algorithm (SS) for bin packing along with several new variants. SS is applicable to any instance of bin packing in which the bin capacity B and item sizes s(a) are integral (or can be scaled to be so), and runs in time O(nB). It performs remarkably well from an average case point of view: For any discrete distribution in which the optimal expected waste is sublinear, SS also has sublinear expected waste. For any discrete distribution where the optimal expected waste is bounded, SS has expected waste at most O(log n). We also discuss several interesting variants on SS, including a randomized O(nB log B)-time online algorithm SS* whose expected behavior is essentially optimal for all discrete distributions. Algorithm SS* depends on a new linear-programming-based pseudopolynomial-time algorithm for solving the NP-hard problem of determining, given a discrete distribution F, just what is the growth rate for the optimal expected waste.The Constraint Satisfaction Problem (CSP) provides a common framework for many combinatorial problems. The general CSP is known to be NP-complete; however, certain restrictions on a possible form of constraints may affect the complexity and lead to tractable problem classes. There is, therefore, a fundamental research direction, aiming to separate those subclasses of the CSP that are tractable and those which remain NP-complete.Schaefer gave an exhaustive solution of this problem for the CSP on a 2-element domain. In this article, we generalise this result to a classification of the complexity of the CSP on a 3-element domain. The main result states that every subproblem of the CSP is either tractable or NP-complete, and the criterion separating them is that conjectured in Bulatov et al. [2005] and Bulatov and Jeavons [2001b]. We also characterize those subproblems for which standard constraint propagation techniques provide a decision procedure. Finally, we exhibit a polynomial time algorithm which, for a given set of allowed constraints, outputs if this set gives rise to a tractable problem class. To obtain the main result and the algorithm, we extensively use the algebraic technique for the CSP developed in Jeavons [1998b], Bulatov et al.[2005], and Bulatov and Jeavons [2001b].We present a new average case analysis for the problem of scheduling n jobs on m machines so that the sum of job completion times is minimized. Our goal is to use the concept of competitive ratio---which is a typical worst case notion---also within an average case analysis. We show that the classic SEPT scheduling strategy with Ω(n) worst-case competitive ratio achieves an average of O(1) under several natural distributions, among them the exponential distribution. Our analysis technique allows to also roughly estimate the probability distribution of the competitive ratio. Thus, our result bridges the gap between worst case and average case performance guarantee.We consider the sequence comparison problem, also known as “hidden” pattern problem, where one searches for a given subsequence in a text (rather than a string understood as a sequence of consecutive symbols). A characteristic parameter is the number of occurrences of a given pattern w of length m as a subsequence in a random text of length n generated by a memoryless source. Spacings between letters of the pattern may either be constrained or not in order to define valid occurrences. We determine the mean and the variance of the number of occurrences, and establish a Gaussian limit law and large deviations. These results are obtained via combinatorics on words, formal language techniques, and methods of analytic combinatorics based on generating functions. The motivations to study this problem come from an attempt at finding a reliable threshold for intrusion detections, from textual data processing applications, and from molecular biology.We revisit the problem of conveying classical messages by transmitting quantum states, and derive new, optimal bounds on the number of quantum bits required for this task. Much of the previous work on this problem, and on other communication tasks in the setting of bounded error entanglement-assisted communication, is based on sophisticated information theoretic arguments. Our results are derived from first principles, using a simple linear algebraic technique. A direct consequence is a tight lower bound for the Inner Product function that has found applications to privacy amplification in quantum key distribution protocols.
We establish the first polynomial time-space lower bounds for satisfiability on general models of computation. We show that for any constant c less than the golden ratio there exists a positive constant d such that no deterministic random-access Turing machine can solve satisfiability in time nc and space nd, where d approaches 1 when c does. On conondeterministic instead of deterministic machines, we prove the same for any constant c less than &2radic;.Our lower bounds apply to nondeterministic linear time and almost all natural NP-complete problems known. In fact, they even apply to the class of languages that can be solved on a nondeterministic machine in linear time and space n1/c.Our proofs follow the paradigm of indirect diagonalization. We also use that paradigm to prove time-space lower bounds for languages higher up in the polynomial-time hierarchy.We introduce a new framework for designing fixed-parameter algorithms with subexponential running time---2O(&kradic;) nO(1). Our results apply to a broad family of graph problems, called bidimensional problems, which includes many domination and problems such as vertex cover, feedback vertex set, minimum maximal matching, dominating set, edge dominating set, disk dimension, and many others restricted to bounded-genus graphs (phrased as bipartite-graph problem). Furthermore, it is fairly straightforward to prove that a problem is bidimensional. In particular, our framework includes, as special cases, all previously known problems to have such subexponential algorithms. Previously, these algorithms applied to planar graphs, single-crossing-minor-free graphs, and/or map graphs; we extend these results to apply to bounded-genus graphs as well. In a parallel development of combinatorial results, we establish an upper bound on the treewidth (or branchwidth) of a bounded-genus graph that excludes some planar graph H as a minor. This bound depends linearly on the size ¦V(H)¦ of the excluded graph H and the genus g(G) of the graph G, and applies and extends the graph-minors work of Robertson and Seymour.Building on these results, we develop subexponential fixed-parameter algorithms for dominating set, vertex cover, and set cover in any class of graphs excluding a fixed graph H as a minor. In particular, this general category of graphs includes planar graphs, bounded-genus graphs, single-crossing-minor-free graphs, and any class of graphs that is closed under taking minors. Specifically, the running time is 2O(&kracic;) nh, where h is a constant depending only on H, which is polynomial for k = O(log2 n). We introduce a general approach for developing algorithms on H-minor-free graphs, based on structural results about H-minor-free graphs at the heart of Robertson and Seymour's graph-minors work. We believe this approach opens the way to further development on problems in H-minor-free graphs.Representation independence formally characterizes the encapsulation provided by language constructs for data abstraction and justifies reasoning by simulation. Representation independence has been shown for a variety of languages and constructs but not for shared references to mutable state; indeed it fails in general for such languages. This article formulates representation independence for classes, in an imperative, object-oriented language with pointers, subclassing and dynamic dispatch, class oriented visibility control, recursive types and methods, and a simple form of module. An instance of a class is considered to implement an abstraction using private fields and so-called representation objects. Encapsulation of representation objects is expressed by a restriction, called confinement, on aliasing. Representation independence is proved for programs satisfying the confinement condition. A static analysis is given for confinement that accepts common designs such as the observer and factory patterns. The formalization takes into account not only the usual interface between a client and a class that provides an abstraction but also the interface (often called “protected”) between the class and its subclasses.We study a behavioral theory of Mobile Ambients, a process calculus for modelling mobile agents in wide-area networks, focussing on reduction barbed congruence. Our contribution is threefold. (1) We prove a context lemma which shows that only parallel and nesting contexts need be examined to recover this congruence. (2) We characterize this congruence using a labeled bisimilarity: this requires novel techniques to deal with asynchronous movements of agents and with the invisibility of migrations of secret locations. (3) We develop refined proof methods involving up-to proof techniques, which allow us to verify a set of algebraic laws and the correctness of more complex examples.
The critical resource that limits the application of best-first search is memory. We present a new class of best-first search algorithms that reduce the space complexity. The key idea is to store only the Open list of generated nodes, but not the Closed list of expanded nodes. The solution path can be recovered by a divide-and-conquer technique, either as a bidirectional or unidirectional search. For many problems, frontier search dramatically reduces the memory required by best-first search. We apply frontier search to breadth-first search of sliding-tile puzzles and the 4-peg Towers of Hanoi problem, Dijkstra's algorithm on a grid with random edge costs, and the A* algorithm on the Fifteen Puzzle, the four-peg Towers of Hanoi Problem, and optimal sequence alignment in computational biology.We show that the problems of approximating the shortest and closest vector in a lattice to within a factor of &nradic; lie in NP intersect coNP. The result (almost) subsumes the three mutually-incomparable previous results regarding these lattice problems: Banaszczyk [1993], Goldreich and Goldwasser [2000], and Aharonov and Regev [2003]. Our technique is based on a simple fact regarding succinct approximation of functions using their Fourier series over the lattice. This technique might be useful elsewhere---we demonstrate this by giving a simple and efficient algorithm for one other lattice problem (CVPP) improving on a previous result of Regev[2003]. An interesting fact is that our result emerged from a “dequantization” of our previous quantum result in Aharonov and Regev [2003]. This route to proving purely classical results might be beneficial elsewhere.The Johnson--Lindenstrauss lemma shows that any n points in Euclidean space (i.e., ℝn with distances measured under the ℓ2 norm) may be mapped down to O((log n)/ε2) dimensions such that no pairwise distance is distorted by more than a (1 + ε) factor. Determining whether such dimension reduction is possible in ℓ1 has been an intriguing open question. We show strong lower bounds for general dimension reduction in ℓ1. We give an explicit family of n points in ℓ1 such that any embedding with constant distortion D requires nΩ(1/D2) dimensions. This proves that there is no analog of the Johnson--Lindenstrauss lemma for ℓ1; in fact, embedding with any constant distortion requires nΩ(1) dimensions. Further, embedding the points into ℓ1 with (1+ε) distortion requires n½−O(ε log(1/ε)) dimensions. Our proof establishes this lower bound for shortest path metrics of series-parallel graphs. We make extensive use of linear programming and duality in devising our bounds. We expect that the tools and techniques we develop will be useful for future investigations of embeddings into ℓ1.Let p > 1 be any fixed real. We show that assuming NP ⊈ RP, there is no polynomial time algorithm that approximates the Shortest Vector Problem (SVP) in ℓp norm within a constant factor. Under the stronger assumption NP ⊈ RTIME(2poly(log n)), we show that there is no polynomial-time algorithm with approximation ratio 2(log n)1/2−ε where n is the dimension of the lattice and ε > 0 is an arbitrarily small constant.We first give a new (randomized) reduction from Closest Vector Problem (CVP) to SVP that achieves some constant factor hardness. The reduction is based on BCH Codes. Its advantage is that the SVP instances produced by the reduction behave well under the augmented tensor product, a new variant of tensor product that we introduce. This enables us to boost the hardness factor to 2(log n)1/2-ε.In a wireless network, a basestation transmits data to mobiles at time-varying, mobile-dependent rates due to the ever changing nature of the communication channels. In this article, we consider a wireless system in which the channel conditions and data arrival processes are governed by an adversary. We first consider a single server and a set of users. At each time step t, the server can only transmit data to one user. If user i is chosen, the transmission rate is ri(t). We say that the system is (w, ϵ)-admissible if in any window of w time steps the adversary can schedule the users so that the total data arriving to each user is at most 1−ϵ times the total service it receives.Our objective is to design online scheduling algorithms to ensure stability in an admissible system. We first show, somewhat surprisingly, that the admissibility condition alone does not guarantee the existence of a stable online algorithm, even in a subcritical system (i.e., ϵ > 0). For example, if the nonzero rates in an infinite rate set can be arbitrarily small, then a subcritical system can be unstable for any deterministic online algorithm.On a positive note, we present a tracking algorithm that attempts to mimic the behavior of the adversary. This algorithm ensures stability for all (w, ϵ)-admissible systems that are not excluded by our instability results. As a special case, if the rate set is finite, then the tracking algorithm is stable even for a critical system (i.e., ϵ = 0). Moreover, the queue sizes are independent of ϵ. For subcritical systems, we also show that a simpler max weight algorithm is stable as long as the user rates are bounded away from zero.The offline version of our problem resembles the problem of scheduling unrelated machines and can be modeled by an integer program. We present a rounding algorithm for its linear relaxation and prove that the rounding technique cannot be substantially improved.
We present the first in-place algorithm for sorting an array of size n that performs, in the worst case, at most O(nlog n) element comparisons and O(n) element transports.This solves a long-standing open problem, stated explicitly, for example, in Munro and Raman [1992], of whether there exists a sorting algorithm that matches the asymptotic lower bounds on all computational resources simultaneously.In the ASYMMETRIC k-CENTER problem, the input is an integer k and a complete digraph over n points together with a distance function obeying the directed triangle inequality. The goal is to choose a set of k points to serve as centers and to assign all the points to the centers, so that the maximum distance of any point from its center is as small as possible.We show that the ASYMMETRIC k-CENTER problem is hard to approximate up to a factor of log*n−O(1) unless NP ⊆ DTIME(nlog log n). Since an O(log*n)-approximation algorithm is known for this problem, this resolves the asymptotic approximability of ASYMMETRIC k-CENTER. This is the first natural problem whose approximability threshold does not polynomially relate to the known approximation classes. We also resolve the approximability threshold of the metric (symmetric) k-Center problem with costs.We design two compressed data structures for the full-text indexing problem that support efficient substring searches using roughly the space required for storing the text in compressed form.Our first compressed data structure retrieves the occ occurrences of a pattern P[1,p] within a text T[1,n] in O(p + occ log1+ε n) time for any chosen ε, 0<ε<1. This data structure uses at most 5nHk(T) + o(n) bits of storage, where Hk(T) is the kth order empirical entropy of T. The space usage is Θ(n) bits in the worst case and o(n) bits for compressible texts. This data structure exploits the relationship between suffix arrays and the Burrows--Wheeler Transform, and can be regarded as a compressed suffix array.Our second compressed data structure achieves O(p+occ) query time using O(nHk(T)logε n) + o(n) bits of storage for any chosen ε, 0<ε<1. Therefore, it provides optimal output-sensitive query time using o(nlog n) bits in the worst case. This second data structure builds upon the first one and exploits the interplay between two compressors: the Burrows--Wheeler Transform and the LZ78 algorithm.We study routing and scheduling in packet-switched networks. We assume an adversary that controls the injection time, source, and destination for each packet injected. A set of paths for these packets is admissible if no link in the network is overloaded. We present the first on-line routing algorithm that finds a set of admissible paths whenever this is feasible. Our algorithm calculates a path for each packet as soon as it is injected at its source using a simple shortest path computation. The length of a link reflects its current congestion. We also show how our algorithm can be implemented under today's Internet routing paradigms.When the paths are known (either given by the adversary or computed as above), our goal is to schedule the packets along the given paths so that the packets experience small end-to-end delays. The best previous delay bounds for deterministic and distributed scheduling protocols were exponential in the path length. In this article, we present the first deterministic and distributed scheduling protocol that guarantees a polynomial end-to-end delay for every packet.Finally, we discuss the effects of combining routing with scheduling. We first show that some unstable scheduling protocols remain unstable no matter how the paths are chosen. However, the freedom to choose paths can make a difference. For example, we show that a ring with parallel links is stable for all greedy scheduling protocols if paths are chosen intelligently, whereas this is not the case if the adversary specifies the paths.A directed multigraph is said to be d-regular if the indegree and outdegree of every vertex is exactly d. By Hall's theorem, one can represent such a multigraph as a combination of at most n2 cycle covers, each taken with an appropriate multiplicity. We prove that if the d-regular multigraph does not contain more than ⌊d/2⌋ copies of any 2-cycle then we can find a similar decomposition into n2 pairs of cycle covers where each 2-cycle occurs in at most one component of each pair. Our proof is constructive and gives a polynomial algorithm to find such a decomposition. Since our applications only need one such a pair of cycle covers whose weight is at least the average weight of all pairs, we also give an alternative, simpler algorithm to extract a single such pair.This combinatorial theorem then comes handy in rounding a fractional solution of an LP relaxation of the maximum Traveling Salesman Problem (TSP) problem. The first stage of the rounding procedure obtains two cycle covers that do not share a 2-cycle with weight at least twice the weight of the optimal solution. Then we show how to extract a tour from the 2 cycle covers, whose weight is at least 2/3 of the weight of the longest tour. This improves upon the previous 5/8 approximation with a simpler algorithm. Utilizing a reduction from maximum TSP to the shortest superstring problem, we obtain a 2.5-approximation algorithm for the latter problem, which is again much simpler than the previous one.For minimum asymmetric TSP, the same technique gives two cycle covers, not sharing a 2-cycle, with weight at most twice the weight of the optimum. Assuming triangle inequality, we then show how to obtain from this pair of cycle covers a tour whose weight is at most 0.842 log2 n larger than optimal. This improves upon a previous approximation algorithm with approximation guarantee of 0.999 log2 n. Other applications of the rounding procedure are approximation algorithms for maximum 3-cycle cover (factor 2/3, previously 3/5) and maximum asymmetric TSP with triangle inequality (factor 10/13, previously 3/4).Describing the static semantics of programming languages with attribute grammars is eased when the formalism allows direct dependencies to be induced between rules for nodes arbitrarily far away in the tree. Such direct non-local dependencies cannot be analyzed using classical methods, which enable efficient evaluation.This article defines an attribute grammar extension (“remote attribute grammars”) to permit references to objects with fields to be passed through the attribute system. Fields may be read and written through these references. The extension has a declarative semantics in the spirit of classical attribute grammars. It is shown that determining circularity of remote attribute grammars is undecidable.The article then describes a family of conservative tests of noncircularity and shows how they can be used to “schedule” a remote attribute grammar using standard techniques. The article discusses practical batch and incremental evaluation of remote attribute grammars.We provide a general boosting technique for Textual Data Compression. Qualitatively, it takes a good compression algorithm and turns it into an algorithm with a better compression performance guarantee. It displays the following remarkable properties: (a) it can turn any memoryless compressor into a compression algorithm that uses the “best possible” contexts; (b) it is very simple and optimal in terms of time; and (c) it admits a decompression algorithm again optimal in time. To the best of our knowledge, this is the first boosting technique displaying these properties.Technically, our boosting technique builds upon three main ingredients: the Burrows--Wheeler Transform, the Suffix Tree data structure, and a greedy algorithm to process them. Specifically, we show that there exists a proper partition of the Burrows--Wheeler Transform of a string s that shows a deep combinatorial relation with the kth order entropy of s. That partition can be identified via a greedy processing of the suffix tree of s with the aim of minimizing a proper objective function over its nodes. The final compressed string is then obtained by compressing individually each substring of the partition by means of the base compressor we wish to boost.Our boosting technique is inherently combinatorial because it does not need to assume any prior probabilistic model about the source emitting s, and it does not deploy any training, parameter estimation and learning. Various corollaries are derived from this main achievement. Among the others, we show analytically that using our booster, we get better compression algorithms than some of the best existing ones, that is, LZ77, LZ78, PPMC and the ones derived from the Burrows--Wheeler Transform. Further, we settle analytically some long-standing open problems about the algorithmic structure and the performance of BWT-based compressors. Namely, we provide the first family of BWT algorithms that do not use Move-To-Front or Symbol Ranking as a part of the compression process.
We propose and analyze a simple new randomized algorithm, called ResolveSat, for finding satisfying assignments of Boolean formulas in conjunctive normal form. The algorithm consists of two stages: a preprocessing stage in which resolution is applied to enlarge the set of clauses of the formula, followed by a search stage that uses a simple randomized greedy procedure to look for a satisfying assignment. Currently, this is the fastest known probabilistic algorithm for k-CNF satisfiability for k ≥ 4 (with a running time of O(20.5625n) for 4-CNF). In addition, it is the fastest known probabilistic algorithm for k-CNF, k ≥ 3, that have at most one satisfying assignment (unique k-SAT) (with a running time O(2(2 ln 2 − 1)n + o(n)) = O(20.386 … n) in the case of 3-CNF). The analysis of the algorithm also gives an upper bound on the number of the codewords of a code defined by a k-CNF. This is applied to prove a lower bounds on depth 3 circuits accepting codes with nonconstant distance. In particular we prove a lower bound Ω(21.282…√>i<n>/i<) for an explicitly given Boolean function of n variables. This is the first such lower bound that is asymptotically bigger than 2√>i<n>/i< + o(√>i<n>/i<).This article provides a detailed description of the automatic theorem prover Simplify, which is the proof engine of the Extended Static Checkers ESC/Java and ESC/Modula-3. Simplify uses the Nelson--Oppen method to combine decision procedures for several important theories, and also employs a matcher to reason about quantifiers. Instead of conventional matching in a term DAG, Simplify matches up to equivalence in an E-graph, which detects many relevant pattern instances that would be missed by the conventional approach. The article describes two techniques, error context reporting and error localization, for helping the user to determine the reason that a false conjecture is false. The article includes detailed performance figures on conjectures derived from realistic program-checking problems.Trust management is a form of distributed access control that allows one principal to delegate some access decisions to other principals. While the use of delegation greatly enhances flexibility and scalability, it may also reduce the control that a principal has over the resources it owns. Security analysis asks whether safety, availability, and other properties can be maintained while delegating to partially trusted principals. We show that in contrast to the undecidability of classical Harrison--Ruzzo--Ullman safety properties, our primary security properties are decidable. In particular, most security properties we study are decidable in polynomial time. The computational complexity of containment analysis, the most complicated security property we study, varies according to the expressive power of the trust management language.
We present an algorithm for directed acyclic graphs that breaks through the O(n2) barrier on the single-operation complexity of fully dynamic transitive closure, where n is the number of edges in the graph. We can answer queries in O(nε) worst-case time and perform updates in O(nω(1,ε,1)−ε+n1+ε) worst-case time, for any ε∈[0,1], where ω(1,ε,1) is the exponent of the multiplication of an n × nε matrix by an nε × n matrix. The current best bounds on ω(1,ε,1) imply an O(n0.575) query time and an O(n1.575) update time in the worst case. Our subquadratic algorithm is randomized, and has one-sided error. As an application of this result, we show how to solve single-source reachability in O(n1.575) time per update and constant time per query.In the late nineties, Erickson proved a remarkable lower bound on the decision tree complexity of one of the central problems of computational geometry: given n numbers, do any r of them add up to 0? His lower bound of Ω(n⌈r/2⌉), for any fixed r, is optimal if the polynomials at the nodes are linear and at most r-variate. We generalize his bound to s-variate polynomials for s > r. Erickson's bound decays quickly as r grows and never reaches above pseudo-polynomial: we provide an exponential improvement. Our arguments are based on three ideas: (i) a geometrization of Erickson's proof technique; (ii) the use of error-correcting codes; and (iii) a tensor product construction for permutation matrices.A “randomness extractor” is an algorithm that given a sample from a distribution with sufficiently high min-entropy and a short random seed produces an output that is statistically indistinguishable from uniform. (Min-entropy is a measure of the amount of randomness in a distribution.) We present a simple, self-contained extractor construction that produces good extractors for all min-entropies. Our construction is algebraic and builds on a new polynomial-based approach introduced by Ta-Shma et al. [2001b]. Using our improvements, we obtain, for example, an extractor with output length m = k/(log n)O(1/α) and seed length (1 + α)log n for an arbitrary 0 < α ≤ 1, where n is the input length, and k is the min-entropy of the input distribution.A “pseudorandom generator” is an algorithm that given a short random seed produces a long output that is computationally indistinguishable from uniform. Our technique also gives a new way to construct pseudorandom generators from functions that require large circuits. Our pseudorandom generator construction is not based on the Nisan-Wigderson generator [Nisan and Wigderson 1994], and turns worst-case hardness directly into pseudorandomness. The parameters of our generator match those in Impagliazzo and Wigderson [1997] and Sudan et al. [2001] and in particular are strong enough to obtain a new proof that P = BPP if E requires exponential size circuits.Our construction also gives the following improvements over previous work:---We construct an optimal “hitting set generator” that stretches O(log n) random bits into sΩ(1) pseudorandom bits when given a function on log n bits that requires circuits of size s. This yields a quantitatively optimal hardness versus randomness tradeoff for both RP and BPP and solves an open problem raised in Impagliazzo et al. [1999].---We give the first construction of pseudorandom generators that fool nondeterministic circuits when given a function that requires large nondeterministic circuits. This technique also give a quantitatively optimal hardness versus randomness tradeoff for AM and the first hardness amplification result for nondeterministic circuits.There has been considerable recent interest in probabilistic packet marking schemes for the problem of tracing a sequence of network packets back to an anonymous source. An important consideration for such schemes is the number of packet header bits that need to be allocated to the marking protocol. Let b denote this value. All previous schemes belong to a class of protocols for which b must be at least log n, where n is the number of bits used to represent the path of the packets. In this article, we introduce a new marking technique for tracing a sequence of packets sent along the same path. This new technique is effective even when b = 1. In other words, the sequence of packets can be traced back to their source using only a single bit in the packet header. With this scheme, the number of packets required to reconstruct the path is O(22n), but we also show that Ω(2n) packets are required for any protocol where b = 1. We also study the trade-off between b and the number of packets required. We provide a protocol and a lower bound that together demonstrate that for the optimal protocol, the number of packets required (roughly) increases exponentially with n, but decreases doubly exponentially with b. The protocol we introduce is simple enough to be useful in practice. We also study the case where the packets are sent along k different paths. For this case, we demonstrate that any protocol must use at least log(2k − 1) header bits. We also provide a protocol that requires ⌈log(2k + 1)⌉ header bits in some restricted scenarios. This protocol introduces a new coding technique that may be of independent interest.Normalization as a way of producing good relational database designs is a well-understood topic. However, the same problem of distinguishing well-designed databases from poorly designed ones arises in other data models, in particular, XML. While, in the relational world, the criteria for being well designed are usually very intuitive and clear to state, they become more obscure when one moves to more complex data models.Our goal is to provide a set of tools for testing when a condition on a database design, specified by a normal form, corresponds to a good design. We use techniques of information theory, and define a measure of information content of elements in a database with respect to a set of constraints. We first test this measure in the relational context, providing information-theoretic justification for familiar normal forms such as BCNF, 4NF, PJ/NF, 5NFR, DK/NF. We then show that the same measure applies in the XML context, which gives us a characterization of a recently introduced XML normal form called XNF. Finally, we look at information-theoretic criteria for justifying normalization algorithms.We study the complexity of two central XML processing problems. The first is XPath 1.0 query processing, which has been shown to be in PTIME in previous work. We prove that both the data complexity and the query complexity of XPath 1.0 fall into lower (highly parallelizable) complexity classes, while the combined complexity is PTIME-hard. Subsequently, we study the sources of this hardness and identify a large and practically important fragment of XPath 1.0 for which the combined complexity is LOGCFL-complete and, therefore, in the highly parallelizable complexity class NC2. The second problem is the complexity of validating XML documents against various typing schemes like Document Type Definitions (DTDs), XML Schema Definitions (XSDs), and tree automata, both with respect to data and to combined complexity. For data complexity, we prove that validation is in LOGSPACE and depends crucially on how XML data is represented. For the combined complexity, we show that the complexity ranges from LOGSPACE to LOGCFL, depending on the typing scheme.
Let G = (V,E) be an undirected weighted graph with |V| = n and |E| = m. Let k ≥ 1 be an integer. We show that G = (V,E) can be preprocessed in O(kmn1/k) expected time, constructing a data structure of size O(kn1+1/k), such that any subsequent distance query can be answered, approximately, in O(k) time. The approximate distance returned is of stretch at most 2k−1, that is, the quotient obtained by dividing the estimated distance by the actual distance lies between 1 and 2k−1. A 1963 girth conjecture of Erdós, implies that Ω(n1+1/k) space is needed in the worst case for any real stretch strictly smaller than 2k+1. The space requirement of our algorithm is, therefore, essentially optimal. The most impressive feature of our data structure is its constant query time, hence the name "oracle". Previously, data structures that used only O(n1+1/k) space had a query time of Ω(n1/k).Our algorithms are extremely simple and easy to implement efficiently. They also provide faster constructions of sparse spanners of weighted graphs, and improved tree covers and distance labelings of weighted or unweighted graphs.In this article, we present an approximation algorithm for solving the single source shortest paths problem on weighted polyhedral surfaces. We consider a polyhedral surface P as consisting of n triangular faces, where each face has an associated positive weight. The cost of travel through a face is the Euclidean distance traveled, multiplied by the face's weight. For a given parameter ϵ, 0 <ϵ < 1, the cost of the computed paths is at most 1 + ϵ times the cost of corresponding shortest paths. Our algorithm is based on a novel way of discretizing polyhedral surfaces and utilizes a generic greedy approach for computing shortest paths in geometric graphs obtained by such discretization. Its running time is O(C(P)n/&sqrt;ϵ log n/ϵ log 1/ϵ) time, where C(P) captures geometric parameters and the weights of the faces of P.Though extensions to the relational data model have been proposed in order to handle probabilistic information, there has been very little work to date on handling aggregate operators in such databases. In this article, we present a very general notion of an aggregate operator and show how classical aggregation operators (such as COUNT, SUM, etc.) as well as statistical operators (such as percentiles, variance, etc.) are special cases of this general definition. We devise a formal linear programming based semantics for computing aggregates over probabilistic DBMSs, develop algorithms that satisfy this semantics, analyze their complexity, and introduce several families of approximation algorithms that run in polynomial time. We implemented all of these algorithms and tested them on a large set of data to help determine when each one is preferable.We study and further develop two language-based techniques for analyzing security protocols. One is based on a typed process calculus; the other, on untyped logic programs. Both focus on secrecy properties. We contribute to these two techniques, in particular by extending the former with a flexible, generic treatment of many cryptographic operations. We also establish an equivalence between the two techniques.
Concurrent executions of a zero-knowledge protocol by a single prover (with one or more verifiers) may leak information and may not be zero-knowledge in toto. In this article, we study the problem of maintaining zero-knowledge.We introduce the notion of an (α, β) timing constraint: for any two processors P1 and P2, if P1 measures α elapsed time on its local clock and P2 measures β elapsed time on its local clock, and P2 starts after P1 does, then P2 will finish after P1 does. We show that if the adversary is constrained by an (α, β) assumption then there exist four-round almost concurrent zero-knowledge interactive proofs and perfect concurrent zero-knowledge arguments for every language in NP. We also address the more specific problem of Deniable Authentication, for which we propose several particularly efficient solutions. Deniable Authentication is of independent interest, even in the sequential case; our concurrent solutions yield sequential solutions without recourse to timing, that is, in the standard model.We introduce the use of Fourier analysis on lattices as an integral part of a lattice-based construction. The tools we develop provide an elegant description of certain Gaussian distributions around lattice points. Our results include two cryptographic constructions that are based on the worst-case hardness of the unique shortest vector problem. The main result is a new public key cryptosystem whose security guarantee is considerably stronger than previous results (O(n1.5) instead of O(n7)). This provides the first alternative to Ajtai and Dwork's original 1996 cryptosystem. Our second result is a family of collision resistant hash functions with an improved security guarantee in terms of the unique shortest vector problem. Surprisingly, both results are derived from one theorem that presents two indistinguishable distributions on the segment [0, 1). It seems that this theorem can have further applications; as an example, we use it to solve an open problem in quantum computation related to the dihedral hidden subgroup problem.The dynamic behavior of a network in which information is changing continuously over time requires robust and efficient mechanisms for keeping nodes updated about new information. Gossip protocols are mechanisms for this task in which nodes communicate with one another according to some underlying deterministic or randomized algorithm, exchanging information in each communication step. In a variety of contexts, the use of randomization to propagate information has been found to provide better reliability and scalability than more regimented deterministic approaches.In many settings, such as a cluster of distributed computing hosts, new information is generated at individual nodes, and is most "interesting" to nodes that are nearby. Thus, we propose distance-based propagation bounds as a performance measure for gossip mechanisms: a node at distance d from the origin of a new piece of information should be able to learn about this information with a delay that grows slowly with d, and is independent of the size of the network.For nodes arranged with uniform density in Euclidean space, we present natural gossip mechanisms, called spatial gossip, that satisfy such a guarantee: new information is spread to nodes at distance d, with high probability, in O(log1 + ϵ d) time steps. Such a bound combines the desirable qualitative features of uniform gossip, in which information is spread with a delay that is logarithmic in the full network size, and deterministic flooding, in which information is spread with a delay that is linear in the distance and independent of the network size. Our mechanisms and their analysis resolve a conjecture of Demers et al. [1987].We further show an application of our gossip mechanisms to a basic resource location problem, in which nodes seek to rapidly learn the location of the nearest copy of a resource in a network. This problem, which is of considerable practical importance, can be solved by a very simple protocol using Spatial Gossip, whereas we can show that no protocol built on top of uniform gossip can inform nodes of their approximately nearest resource within poly-logarithmic time. The analysis relies on an additional useful property of spatial gossip, namely that information travels from its source to sinks along short paths not visiting points of the network far from the two nodes.We study novel combinatorial properties of graphs that allow us to devise a completely new approach to dynamic all pairs shortest paths problems. Our approach yields a fully dynamic algorithm for general directed graphs with non-negative real-valued edge weights that supports any sequence of operations in O(n2log3n) amortized time per update and unit worst-case time per distance query, where n is the number of vertices. We can also report shortest paths in optimal worst-case time. These bounds improve substantially over previous results and solve a long-standing open problem. Our algorithm is deterministic, uses simple data structures, and appears to be very fast in practice.It is shown that a planar digraph can be preprocessed in near-linear time, producing a near-linear space oracle that can answer reachability queries in constant time. The oracle can be distributed as an O(log n) space label for each vertex and then we can determine if one vertex can reach another considering their two labels only.The approach generalizes to give a near-linear space approximate distances oracle for a weighted planar digraph. With weights drawn from {0, …, N}, it approximates distances within a factor (1 + ϵ) in O(log log (nN) + 1/ϵ) time. Our scheme can be extended to find and route along correspondingly short dipaths.We consider the problem of approximating a given m × n matrix A by another matrix of specified rank k, which is smaller than m and n. The Singular Value Decomposition (SVD) can be used to find the "best" such approximation. However, it takes time polynomial in m, n which is prohibitive for some modern applications. In this article, we develop an algorithm that is qualitatively faster, provided we may sample the entries of the matrix in accordance with a natural probability distribution. In many applications, such sampling can be done efficiently. Our main result is a randomized algorithm to find the description of a matrix D* of rank at most k so that holds with probability at least 1 − δ (where |·|F is the Frobenius norm). The algorithm takes time polynomial in k,1/ε, log(1/δ) only and is independent of m and n. In particular, this implies that in constant time, it can be determined if a given matrix of arbitrary size has a good low-rank approximation.
We show that the complexity of the vertical decomposition of an arrangement of n fixed-degree algebraic surfaces or surface patches in four dimensions is O(n4+ϵ), for any ϵ > 0. This improves the best previously known upper bound for this problem by a near-linear factor, and settles a major problem in the theory of arrangements of surfaces, open since 1989. The new bound can be extended to higher dimensions, yielding the bound O(n2d−4+ϵ), for any ϵ > 0, on the complexity of vertical decompositions in dimensions d ≥ 4. We also describe the immediate algorithmic applications of these results, which include improved algorithms for point location, range searching, ray shooting, robot motion planning, and some geometric optimization problems.Information extraction from websites is nowadays a relevant problem, usually performed by software modules called wrappers. A key requirement is that the wrapper generation process should be automated to the largest extent, in order to allow for large-scale extraction tasks even in presence of changes in the underlying sites. So far, however, only semi-automatic proposals have appeared in the literature.We present a novel approach to information extraction from websites, which reconciles recent proposals for supervised wrapper induction with the more traditional field of grammar inference. Grammar inference provides a promising theoretical framework for the study of unsupervised---that is, fully automatic---wrapper generation algorithms. However, due to some unrealistic assumptions on the input, these algorithms are not practically applicable to Web information extraction tasks.The main contributions of the article stand in the definition of a class of regular languages, called the prefix mark-up languages, that abstract the structures usually found in HTML pages, and in the definition of a polynomial-time unsupervised learning algorithm for this class. The article shows that, differently from other known classes, prefix mark-up languages and the associated algorithm can be practically used for information extraction purposes.A system based on the techniques described in the article has been implemented in a working prototype. We present some experimental results on known Websites, and discuss opportunities and limitations of the proposed approach.We initiate a study of bounded clock synchronization under a more severe fault model than that proposed by Lamport and Melliar-Smith [1985]. Realistic aspects of the problem of synchronizing clocks in the presence of faults are considered. One aspect is that clock synchronization is an on-going task, thus the assumption that some of the processors never fail is too optimistic. To cope with this reality, we suggest self-stabilizing protocols that stabilize in any (long enough) period in which less than a third of the processors are faulty. Another aspect is that the clock value of each processor is bounded. A single transient fault may cause the clock to reach the upper bound. Therefore, we suggest a bounded clock that wraps around when appropriate.We present two randomized self-stabilizing protocols for synchronizing bounded clocks in the presence of Byzantine processor failures. The first protocol assumes that processors have a common pulse, while the second protocol does not. A new type of distributed counter based on the Chinese remainder theorem is used as part of the first protocol.The traditional assumption about memory is that a read returns the value written by the most recent write. However, in a shared memory multiprocessor several processes independently and simultaneously submit reads and writes resulting in a partial order of memory operations. In this partial order, the definition of most recent write may be ambiguous. Memory consistency models have been developed to specify what values may be returned by a read given that memory operations may only be partially ordered. Before this work, consistency models were defined independently. Each model followed a set of rules which was separate from the rules of every other model. In our work, we have defined a set of four consistency properties. Any subset of the four properties yields a set of rules which constitute a consistency model. Every consistency model previously described in the literature can be defined based on our four properties. Therefore, we present these properties as a unfied theory of shared memory consistency.Our unified theory provides several benefits. First, we claim that these four properties capture the underlying structure of memory consistency. That is, the goal of memory consistency is to ensure certain declarative properties which can be intuitively understood by a programmer, and hence allow him or her to write a correct program. Our unified theory provides a uniform, formal definition of all previously described consistency models, and in addition some combinations of properties produce new models that have not yet been described. We believe these new models will prove to be useful because they are based on declarative properties which programmers desire to be enforced. Finally, we introduce the idea of selecting a consistency model as an on-line activity. Before our work, a shared memory program would run start to finish under a single consistency model. Our unified theory allows the consistency model to change as the program runs while maintaining a consistent definition of what values may be returned by each read.
Scheduling a sequence of jobs released over time when the processing time of a job is only known at its completion is a classical problem in CPU scheduling in time sharing operating systems. A widely used measure for the responsiveness of the system is the average flow time of the jobs, that is, the average time spent by jobs in the system between release and completion.The Windows NT and the Unix operating system scheduling policies are based on the Multilevel Feedback algorithm. In this article, we prove that a randomized version of the Multilevel Feedback algorithm is competitive for single and parallel machine systems, in our opinion providing one theoretical validation of the goodness of an idea that has proven effective in practice along the last two decades.The randomized Multilevel Feedback algorithm (RMLF) was first proposed by Kalyanasundaram and Pruhs for a single machine achieving an O(log n log log n) competitive ratio to minimize the average flow time against the on-line adaptive adversary, where n is the number of jobs that are released. We present a version of RMLF working for any number m of parallel machines. We show for RMLF a first O(log n log n/m) competitiveness result against the oblivious adversary on parallel machines. We also show that the same RMLF algorithm surprisingly achieves a tight O(log n) competitive ratio against the oblivious adversary on a single machine, therefore matching the lower bound for this case.Minimizing a convex function over a convex set in n-dimensional space is a basic, general problem with many interesting special cases. Here, we present a simple new algorithm for convex optimization based on sampling by a random walk. It extends naturally to minimizing quasi-convex functions and to other generalizations.We take a critical look at the relationship between the security of cryptographic schemes in the Random Oracle Model, and the security of the schemes that result from implementing the random oracle by so called "cryptographic hash functions".The main result of this article is a negative one: There exist signature and encryption schemes that are secure in the Random Oracle Model, but for which any implementation of the random oracle results in insecure schemes. In the process of devising the above schemes, we consider possible definitions for the notion of a "good implementation" of a random oracle, pointing out limitations and challenges.Given a function f as an oracle, the collision problem is to find two distinct indexes i and j such that f(i) = f(j), under the promise that such indexes exist. Since the security of many fundamental cryptographic primitives depends on the hardness of finding collisions, our lower bounds provide evidence for the existence of cryptographic primitives that are immune to quantum cryptanalysis. We prove that any quantum algorithm for finding a collision in an r-to-one function must evaluate the function Ω((n/r)1/3) times, where n is the size of the domain and r|n. This matches an upper bound of Brassard, Høyer, and Tapp. No lower bound better than constant was previously known. Our result also implies a quantum lower bound of Ω(n2/3) queries for the element distinctness problem, which is to determine whether n integers are all distinct. The best previous lower bound was Ω(&sqrt;n) queries.We present a general technique for approximating various descriptors of the extent of a set P of n points in Rd when the dimension d is an arbitrary fixed constant. For a given extent measure μ and a parameter ϵ > 0, it computes in time O(n + 1/ϵO(1)) a subset Q ⊆ P of size 1/ϵO(1), with the property that (1 − ϵ)μ(P) ≤ μ(Q) ≤ μ(P). The specific applications of our technique include ϵ-approximation algorithms for (i) computing diameter, width, and smallest bounding box, ball, and cylinder of P, (ii) maintaining all the previous measures for a set of moving points, and (iii) fitting spheres and cylinders through a point set P. Our algorithms are considerably simpler, and faster in many cases, than previously known algorithms.By a switch graph, we mean an undirected graph G = (P ⊍ W, E) such that all vertices in P (the plugs) have degree one and all vertices in W (the switches) have even degrees. We call G plane if G is planar and can be embedded such that all plugs are in the outer face. Given a set (s1, t1), …,(sk, tk) of pairs of plugs, the problem is to find edge-disjoint paths p1, …, pk such that every pi connects si with ti. The best asymptotic worst-case complexity known so far is quadratic in the number of vertices. In this article, a linear, and thus asymptotically optimal, algorithm is introduced. This result may be viewed as a concluding "keystone" for a number of previous results on various special cases of the problem.We present a polynomial-time randomized algorithm for estimating the permanent of an arbitrary n × n matrix with nonnegative entries. This algorithm---technically a "fully-polynomial randomized approximation scheme"---computes an approximation that is, with high probability, within arbitrarily small specified relative error of the true value of the permanent.
Dealing with the NP-complete Dominating Set problem on graphs, we demonstrate the power of data reduction by preprocessing from a theoretical as well as a practical side. In particular, we prove that Dominating Set restricted to planar graphs has a so-called problem kernel of linear size, achieved by two simple and easy-to-implement reduction rules. Moreover, having implemented our reduction rules, first experiments indicate the impressive practical potential of these rules. Thus, this work seems to open up a new and prospective way how to cope with one of the most important problems in graph theory and combinatorial optimization.We introduce the smoothed analysis of algorithms, which continuously interpolates between the worst-case and average-case analyses of algorithms. In smoothed analysis, we measure the maximum over inputs of the expected performance of an algorithm under small random perturbations of that input. We measure this performance in terms of both the input size and the magnitude of the perturbations. We show that the simplex algorithm has smoothed complexity polynomial in the input size and the standard deviation of Gaussian perturbations.We prove lower bounds of order n log n for both the problem of multiplying polynomials of degree n, and of dividing polynomials with remainder, in the model of bounded coefficient arithmetic circuits over the complex numbers. These lower bounds are optimal up to order of magnitude. The proof uses a recent idea of R. Raz [Proc. 34th STOC 2002] proposed for matrix multiplication. It reduces the linear problem of multiplying a random circulant matrix with a vector to the bilinear problem of cyclic convolution. We treat the arising linear problem by extending J. Morgenstern's bound [J. ACM 20, pp. 305--306, 1973] in a unitarily invariant way. This establishes a new lower bound on the bounded coefficient complexity of linear forms in terms of the singular values of the corresponding matrix. In addition, we extend these lower bounds for linear and bilinear maps to a model of circuits that allows a restricted number of unbounded scalar multiplications.We prove that satisfiability problem for word equations is in PSPACE.We motivate and develop a natural bicriteria measure for assessing the quality of a clustering that avoids the drawbacks of existing measures. A simple recursive heuristic is shown to have poly-logarithmic worst-case guarantees under the new measure. The main result of the article is the analysis of a popular spectral algorithm. One variant of spectral clustering turns out to have effective worst-case guarantees; another finds a "good" clustering, if one exists.
We prove that any Resolution proof for the weak pigeonhole principle, with n holes and any number of pigeons, is of length Ω(2nε), (for some global constant ε > 0). One corollary is that a certain propositional formulation of the statement NP ⊄ P/poly does not have short Resolution proofs.A collection of simple closed Jordan curves in the plane is called a family of pseudo-circles if any two of its members intersect at most twice. A closed curve composed of two subarcs of distinct pseudo-circles is said to be an empty lens if the closed Jordan region that it bounds does not intersect any other member of the family. We establish a linear upper bound on the number of empty lenses in an arrangement of n pseudo-circles with the property that any two curves intersect precisely twice. We use this bound to show that any collection of n x-monotone pseudo-circles can be cut into O(n8/5) arcs so that any two intersect at most once; this improves a previous bound of O(n5/3) due to Tamaki and Tokuyama. If, in addition, the given collection admits an algebraic representation by three real parameters that satisfies some simple conditions, then the number of cuts can be further reduced to O(n3/2(log n)O(α(s(n))), where α(n) is the inverse Ackermann function, and s is a constant that depends on the the representation of the pseudo-circles. For arbitrary collections of pseudo-circles, any two of which intersect exactly twice, the number of necessary cuts reduces still further to O(n4/3). As applications, we obtain improved bounds for the number of incidences, the complexity of a single level, and the complexity of many faces in arrangements of circles, of pairwise intersecting pseudo-circles, of arbitrary x-monotone pseudo-circles, of parabolas, and of homothetic copies of any fixed simply shaped convex curve. We also obtain a variant of the Gallai--Sylvester theorem for arrangements of pairwise intersecting pseudo-circles, and a new lower bound on the number of distinct distances under any well-behaved norm.We study the security of individual bits in an RSA encrypted message EN(x). We show that given EN(x), predicting any single bit in x with only a nonnegligible advantage over the trivial guessing strategy, is (through a polynomial-time reduction) as hard as breaking RSA. Moreover, we prove that blocks of O(log log N) bits of x are computationally indistinguishable from random bits. The results carry over to the Rabin encryption scheme.Considering the discrete exponentiation function gx modulo p, with probability 1 − o(1) over random choices of the prime p, the analog results are demonstrated. The results do not rely on group representation, and therefore applies to general cyclic groups as well. Finally, we prove that the bits of ax + b modulo p give hard core predicates for any one-way function f.All our results follow from a general result on the chosen multiplier hidden number problem: given an integer N, and access to an algorithm Px that on input a random a ∈ ZN, returns a guess of the ith bit of ax mod N, recover x. We show that for any i, if Px has at least a nonnegligible advantage in predicting the ith bit, we either recover x, or, obtain a nontrivial factor of N in polynomial time. The result also extends to prove the results about simultaneous security of blocks of O(log log N) bits.We describe efficient constructions for various cryptographic primitives in private-key as well as public-key cryptography. Our main results are two new constructions of pseudo-random functions. We prove the pseudo-randomness of one construction under the assumption that factoring (Blum integers) is hard while the other construction is pseudo-random if the decisional version of the Diffie--Hellman assumption holds. Computing the value of our functions at any given point involves two subset products. This is much more efficient than previous proposals. Furthermore, these functions have the advantage of being in TC0 (the class of functions computable by constant depth circuits consisting of a polynomial number of threshold gates). This fact has several interesting applications. The simple algebraic structure of the functions implies additional features such as a zero-knowledge proof for statements of the form "y = fs(x)" and "y &neq; fs(x)" given a commitment to a key s of a pseudo-random function fs.We study a novel genre of optimization problems, which we call segmentation problems, motivated in part by certain aspects of clustering and data mining. For any classical optimization problem, the corresponding segmentation problem seeks to partition a set of cost vectors into several segments, so that the overall cost is optimized. We focus on two natural and interesting (but MAXSNP-complete) problems in this class, the hypercube segmentation problem and the catalog segmentation problem, and present approximation algorithms for them. We also present a general greedy scheme, which can be specialized to approximate any segmentation problem.A descriptive complexity approach to random 3-SAT is initiated. We show that unsatisfiability of any significant fraction of random 3-CNF formulas cannot be certified by any property that is expressible in Datalog. Combined with the known relationship between the complexity of constraint satisfaction problems and expressibility in Datalog, our result implies that any constraint propagation algorithm working with small constraints will fail to certify unsatisfiability almost always. Our result is a consequence of designing a winning strategy for one of the players in the existential pebble game. The winning strategy makes use of certain extension axioms that we introduce and hold almost surely on a random 3-CNF formula. The second contribution of our work is the connection between finite model theory and propositional proof complexity. To make this connection explicit, we establish a tight relationship between the number of pebbles needed to win the game and the width of the Resolution refutations. As a consequence to our result and the known size--width relationship in Resolution, we obtain new proofs of the exponential lower bounds for Resolution refutations of random 3-CNF formulas and the Pigeonhole Principle.Fagin's theorem, the first important result of descriptive complexity, asserts that a property of graphs is in NP if and only if it is definable by an existential second-order formula. In this article, we study the complexity of evaluating existential second-order formulas that belong to prefix classses of existential second-order logic, where a prefix class is the collection of all existential second-order formulas in prenex normal form such that the second-order and the first-order quantifiers obey a certain quantifier pattern. We completely characterize the computational complexity of prefix classes of existential second-order logic in three different contexts: (1) over directed graphs, (2) over undirected graphs with self-loops and (3) over undirected graphs without self-loops. Our main result is that in each of these three contexts a dichotomy holds, that is to say, each prefix class of existential second-order logic either contains sentences that can express NP-complete problems, or each of its sentences expresses a polynomial-time solvable problem. Although the boundary of the dichotomy coincides for the first two cases, it changes, as one moves to undirected graphs without self-loops. The key difference is that a certain prefix class, based on the well-known Ackermann class of first-order logic, contains sentences that can express NP-complete problems over graphs of the first two types, but becomes tractable over undirected graphs without self-loops. Moreover, establishing the dichotomy over undirected graphs without self-loops turns out to be a technically challenging problem that requires the use of sophisticated machinery from graph theory and combinatorics, including results about graphs of bounded tree-width and Ramsey's theorem.
XPath is a language for navigating an XML document and selecting a set of element nodes. XPath expressions are used to query XML data, describe key constraints, express transformations, and reference elements in remote documents. This article studies the containment and equivalence problems for a fragment of the XPath query language, with applications in all these contexts.In particular, we study a class of XPath queries that contain branching, label wildcards and can express descendant relationships between nodes. Prior work has shown that languages that combine any two of these three features have efficient containment algorithms. However, we show that for the combination of features, containment is coNP-complete. We provide a sound and complete algorithm for containment that runs in exponential time, and study parameterized PTIME special cases. While we identify one parameterized class of queries for which containment can be decided efficiently, we also show that even with some bounded parameters, containment remains coNP-complete. In response to these negative results, we describe a sound algorithm that is efficient for all queries, but may return false negatives in some cases.Declustering schemes allocate data blocks among multiple disks to enable parallel retrieval. Given a declustering scheme D, its response time with respect to a query Q, rt(Q), is defined to be the maximum number of data blocks of the query stored by the scheme in any one of the disks. If |Q| is the number of data blocks in Q and M is the number of disks, then rt(Q) is at least ⌈|Q|/M⌉. One way to evaluate the performance of D with respect to a set of range queries Q is to measure its additive error---the maximum difference of rt(Q) from ⌈|Q|/M⌉ over all range queries Q ∈ Q.In this article, we consider the problem of designing declustering schemes for uniform multidimensional data arranged in a d-dimensional grid so that their additive errors with respect to range queries are as small as possible. It has been shown that for a fixed dimension d ≥ 2, any declustering scheme on an Md grid, a grid with length M on each dimension, will always incur an additive error with respect to range queries of Ω(log M) when d = 2 and Ω(logd−1/2 M) when d > 2.Asymptotically optimal declustering schemes exist for 2-dimensional data. However, the best general upper bound known so far for the worst-case additive errors of d-dimensional declustering schemes, d ≥ 3, is O(Md−1), which is large when compared to the lower bound. In this article, we propose two declustering schemes based on low-discrepancy points in d-dimensions. When d is fixed, both schemes have an additive error of O(logd−1 M) with respect to range queries, provided that certain conditions are satisfied: the first scheme requires that the side lengths of the grid grow at a rate polynomial in M, while the second scheme requires d ≥ 2 and M = pt where d ≤ p ≤ C, C a constant, and t is a positive integer such that t(d − 1) ≥ 2. These are the first multidimensional declustering schemes with additive errors proven to be near optimal.Research on information extraction from Web pages (wrapping) has seen much activity recently (particularly systems implementations), but little work has been done on formally studying the expressiveness of the formalisms proposed or on the theoretical foundations of wrapping. In this paper, we first study monadic datalog over trees as a wrapping language. We show that this simple language is equivalent to monadic second order logic (MSO) in its ability to specify wrappers. We believe that MSO has the right expressiveness required for Web information extraction and propose MSO as a yardstick for evaluating and comparing wrappers. Along the way, several other results on the complexity of query evaluation and query containment for monadic datalog over trees are established, and a simple normal form for this language is presented. Using the above results, we subsequently study the kernel fragment Elog− of the Elog wrapping language used in the Lixto system (a visual wrapper generator). Curiously, Elog− exactly captures MSO, yet is easier to use. Indeed, programs in this language can be entirely visually specified.
In this article, we will formalize the method of dual fitting and the idea of factor-revealing LP. This combination is used to design and analyze two greedy algorithms for the metric uncapacitated facility location problem. Their approximation factors are 1.861 and 1.61, with running times of O(m log m) and O(n3), respectively, where n is the total number of vertices and m is the number of edges in the underlying complete bipartite graph between cities and facilities. The algorithms are used to improve recent results for several variants of the problem.We study the problem of compressing massive tables within the partition-training paradigm introduced by Buchsbaum et al. [2000], in which a table is partitioned by an off-line training procedure into disjoint intervals of columns, each of which is compressed separately by a standard, on-line compressor like gzip. We provide a new theory that unifies previous experimental observations on partitioning and heuristic observations on column permutation, all of which are used to improve compression rates. Based on this theory, we devise the first on-line training algorithms for table compression, which can be applied to individual files, not just continuously operating sources; and also a new, off-line training algorithm, based on a link to the asymmetric traveling salesman problem, which improves on prior work by rearranging columns prior to partitioning. We demonstrate these results experimentally. On various test files, the on-line algorithms provide 35--55% improvement over gzip with negligible slowdown; the off-line reordering provides up to 20% further improvement over partitioning alone. We also show that a variation of the table compression problem is MAX-SNP hard.We prove that three apparently unrelated fundamental problems in distributed computing, cryptography, and complexity theory, are essentially the same problem. These three problems and brief descriptions of them follow. (1) The selective decommitment problem. An adversary is given commitments to a collection of messages, and the adversary can ask for some subset of the commitments to be opened. The question is whether seeing the decommitments to these open plaintexts allows the adversary to learn something unexpected about the plaintexts that are unopened. (2) The power of 3-round weak zero-knowledge arguments. The question is what can be proved in (a possibly weakened form of) zero-knowledge in a 3-round argument. In particular, is there a language outside of BPP that has a 3-round public-coin weak zero-knowledge argument? (3) The Fiat-Shamir methodology. This is a method for converting a 3-round public-coin argument (viewed as an identification scheme) to a 1-round signature scheme. The method requires what we call a "magic function" that the signer applies to the first-round message of the argument to obtain a second-round message (queries from the verifier). An open question here is whether every 3-round public-coin argument for a language outside of BPP has a magic function.It follows easily from definitions that if a 3-round public-coin argument system is zero-knowledge in the standard (fairly strong) sense, then it has no magic function. We define a weakening of zero-knowledge such that zero-knowledge ⇒ no-magic-function still holds. For this weakened form of zero-knowledge, we give a partial converse: informally, if a 3-round public-coin argument system is not weakly zero-knowledge, then some form of magic is possible for this argument system. We obtain our definition of weak zero-knowledge by a sequence of weakenings of the standard definition, forming a hierarchy. Intermediate forms of zero-knowledge in this hierarchy are reasonable ones, and they may be useful in applications. Finally, we relate the selective decommitment problem to public-coin proof systems and arguments at an intermediate level of the hierarchy, and obtain several positive security results for selective decommitment.This article introduces and explores the condition-based approach to solve the consensus problem in asynchronous systems. The approach studies conditions that identify sets of input vectors for which it is possible to solve consensus despite the occurrence of up to f process crashes. The first main result defines acceptable conditions and shows that these are exactly the conditions for which a consensus protocol exists. Two examples of realistic acceptable conditions are presented, and proved to be maximal, in the sense that they cannot be extended and remain acceptable. The second main result is a generic consensus shared-memory protocol for any acceptable condition. The protocol always guarantees agreement and validity, and terminates (at least) when the inputs satisfy the condition with which the protocol has been instantiated, or when there are no crashes. An efficient version of the protocol is then designed for the message passing model that works when f < n/2, and it is shown that no such protocol exists when f ≥ n/2. It is also shown how the protocol's safety can be traded for its liveness.The main result is a characterization of the generating sequences of the length of words in a regular language on k symbols. We say that a sequence s of integers is regular if there is a finite graph G with two vertices i, t such that sn is the number of paths of length n from i to t in G. Thus the generating sequence of a regular language is regular. We prove that a sequence s is the generating sequence of a regular language on k symbols if and only if both sequences s = (sn)n≥0 and t = (kn − sn)n≥0 are regular.
Allen's interval algebra is one of the best established formalisms for temporal reasoning. This article provides the final step in the classification of complexity for satisfiability problems over constraints expressed in this algebra. When the constraints are chosen from the full Allen's algebra, this form of satisfiability problem is known to be NP-complete. However, eighteen tractable subalgebras have previously been identified; we show here that these subalgebras include all possible tractable subsets of Allen's algebra. In other words, we show that this algebra contains exactly eighteen maximal tractable subalgebras, and reasoning in any fragment not entirely contained in one of these subalgebras is NP-complete. We obtain this dichotomy result by giving a new uniform description of the known maximal tractable subalgebras, and then systematically using a general algebraic technique for identifying maximal subalgebras with a given property.We consider the traveling salesman problem when the cities are points in &#x211D;d for some fixed d and distances are computed according to geometric distances, determined by some norm. We show that for any polyhedral norm, the problem of finding a tour of maximum length can be solved in polynomial time. If arithmetic operations are assumed to take unit time, our algorithms run in time O(nf−2 log n), where f is the number of facets of the polyhedron determining the polyhedral norm. Thus, for example, we have O(n2 log n) algorithms for the cases of points in the plane under the Rectilinear and Sup norms. This is in contrast to the fact that finding a minimum length tour in each case is NP-hard. Our approach can be extended to the more general case of quasi-norms with a not necessarily symmetric unit ball, where we get a complexity of O(n2f−2 log n).For the special case of two-dimensional metrics with f = 4 (which includes the Rectilinear and Sup norms), we present a simple algorithm with O(n) running time. The algorithm does not use any indirect addressing, so its running time remains valid even in comparison based models in which sorting requires Ω(n log n) time. The basic mechanism of the algorithm provides some intuition on why polyhedral norms allow fast algorithms.Complementing the results on simplicity for polyhedral norms, we prove that, for the case of Euclidean distances in &#x211D;d for d ≥ 3, the Maximum TSP is NP-hard. This sheds new light on the well-studied difficulties of Euclidean distances.We characterize the performance of difference coding for compressing sets and database relations through an analysis of the problem of estimating the number of bits needed for storing the spacings between values in sets of integers. We provide analytical expressions for estimating the effectiveness of difference coding when the elements of the sets or the attribute fields in database tuples are drawn from the uniform and Zipf distributions. We also examine the case where a uniformly distributed domain is combined with a Zipf distribution, and with an arbitrary distribution. We present limit theorems for most cases, and probabilistic convergence results in other cases. We also examine the effects of attribute domain reordering on the compression ratio. Our simulations show excellent agreement with theory.We study analogs of classical relational calculus in the context of strings. We start by studying string logics. Taking a classical model-theoretic approach, we fix a set of string operations and look at the resulting collection of definable relations. These form an algebra---a class of n-ary relations for every n, closed under projection and Boolean operations. We show that by choosing the string vocabulary carefully, we get string logics that have desirable properties: computable evaluation and normal forms. We identify five distinct models and study the differences in their model-theory and complexity of evaluation. We identify a subset of these models that have additional attractive properties, such as finite VC dimension and quantifier elimination.Once you have a logic, the addition of free predicate symbols gives you a string query language. The resulting languages have attractive closure properties from a database point of view: while SQL does not allow the full composition of string pattern-matching expressions with relational operators, these logics yield compositional query languages that can capture common string-matching queries while remaining tractable. For each of the logics studied in the first part of the article, we study properties of the corresponding query languages. We give bounds on the data complexity of queries, extend the normal form results from logics to queries, and show that the languages have corresponding algebras expressing safe queries.The state explosion problem remains a major hurdle in applying symbolic model checking to large hardware designs. State space abstraction, having been essential for verifying designs of industrial complexity, is typically a manual process, requiring considerable creativity and insight.In this article, we present an automatic iterative abstraction-refinement methodology that extends symbolic model checking. In our method, the initial abstract model is generated by an automatic analysis of the control structures in the program to be verified. Abstract models may admit erroneous (or "spurious") counterexamples. We devise new symbolic techniques that analyze such counterexamples and refine the abstract model correspondingly. We describe aSMV, a prototype implementation of our methodology in NuSMV. Practical experiments including a large Fujitsu IP core design with about 500 latches and 10000 lines of SMV code confirm the effectiveness of our approach.
We give a simple and new randomized primality testing algorithm by reducing primality testing for number n to testing if a specific univariate identity over Zn holds.We also give new randomized algorithms for testing if a multivariate polynomial, over a finite field or over rationals, is identically zero. The first of these algorithms also works over Zn for any n. The running time of the algorithms is polynomial in the size of arithmetic circuit representing the input polynomial and the error parameter. These algorithms use fewer random bits and work for a larger class of polynomials than all the previously known methods, for example, the Schwartz--Zippel test [Schwartz 1980; Zippel 1979], Chen--Kao and Lewin--Vadhan tests [Chen and Kao 1997; Lewin and Vadhan 1998].This article introduces the sieve, a novel building block that allows to adapt to the number of simultaneously active processes (the point contention) during the execution of an operation. We present an implementation of the sieve in which each sieve operation requires O(k log k) steps, where k is the point contention during the operation.The sieve is the cornerstone of the first wait-free algorithms that adapt to point contention using only read and write operations. Specifically, we present efficient algorithms for long-lived renaming, timestamping and collecting information.The theory of generalized functions is the foundation of the modern theory of partial differential equations (PDE). As computers are playing an ever-larger role in solving PDEs, it is important to know those operations involving generalized functions in analysis and PDE that can be computed on digital computers. In this article, we introduce natural concepts of computability on test functions and generalized functions, as well as computability on Schwartz test functions and tempered distributions. Type-2 Turing machines are used as the machine model [Weihrauch 2000]. It is shown here that differentiation and integration on distributions are computable operators, and various types of Fourier transforms and convolutions are also computable operators. As an application, it is shown that the solution operator of the distributional inhomogeneous three dimensional wave equation is computable.We describe a slightly subexponential time algorithm for learning parity functions in the presence of random classification noise, a problem closely related to several cryptographic and coding problems. Our algorithm runs in polynomial time for the case of parity functions that depend on only the first O(log n log log n) bits of input, which provides the first known instance of an efficient noise-tolerant algorithm for a concept class that is not learnable in the Statistical Query model of Kearns [1998]. Thus, we demonstrate that the set of problems learnable in the statistical query model is a strict subset of those problems learnable in the presence of noise in the PAC model.In coding-theory terms, what we give is a poly(n)-time algorithm for decoding linear k × n codes in the presence of random noise for the case of k = c log n log log n for some c > 0. (The case of k = O(log n) is trivial since one can just individually check each of the 2k possible messages and choose the one that yields the closest codeword.)A natural extension of the statistical query model is to allow queries about statistical properties that involve t-tuples of examples, as opposed to just single examples. The second result of this article is to show that any class of functions learnable (strongly or weakly) with t-wise queries for t = O(log n) is also weakly learnable with standard unary queries. Hence, this natural extension to the statistical query model does not increase the set of weakly learnable functions.In this article, we develop a general methodology, mainly based upon Lyapunov functions, to derive bounds on average delays, and on averages and variances of queue lengths in complex systems of queues. We apply this methodology to cell-based switches and routers, considering first output-queued (OQ) architectures, in order to provide a simple example of our methodology, and then both input-queued (IQ), and combined input/output queued (CIOQ) architectures. These latter switching architectures require a scheduling algorithm to select at each slot a subset of input-buffered cells that can be transferred toward output ports. Although the stability properties (i.e., the limit throughput) of IQ and CIOQ cell-based switches were already studied for several classes of scheduling algorithms, very few analytical results concerning cell delays or queue lengths are available in the technical literature. We concentrate on Maximum Weight Matching (MWM) and Maximal Size Matching (mSM) scheduling algorithms; while the former was proved to maximize throughput, the latter allows simpler implementation. The derived bounds are shown to be rather tight when compared to simulation results.We consider the problem of scheduling a collection of dynamically arriving jobs with unknown execution times so as to minimize the average flow time. This is the classic CPU scheduling problem faced by time-sharing operating systems where preemption is allowed. It is easy to see that every algorithm that doesn't unnecessarily idle the processor is at worst n-competitive, where n is the number of jobs. Yet there was no known nonclairvoyant algorithm, deterministic or randomized, with a competitive ratio provably O(n1−ε). In this article, we give a randomized nonclairvoyant algorithm, RMLF, that has competitive ratio O(log n log log n) against an oblivious adversary. RMLF is a slight variation of the multilevel feedback (MLF) algorithm used by the UNIX operating system, further justifying the adoption of this algorithm. It is known that every randomized nonclairvoyant algorithm is Ω(log n)-competitive, and that every deterministic nonclairvoyant algorithm is Ω(n1/3)-competitive.This article deals with randomized allocation processes placing sequentially n balls into n bins. We consider multiple-choice algorithms that choose d locations (bins) for each ball at random, inspect the content of these locations, and then place the ball into one of them, for example, in a location with minimum number of balls. The goal is to achieve a good load balancing. This objective is measured in terms of the maximum load, that is, the maximum number of balls in the same bin.Multiple-choice algorithms have been studied extensively in the past. Previous analyses typically assume that the d locations for each ball are drawn uniformly and independently from the set of all bins. We investigate whether a nonuniform or dependent selection of the d locations of a ball may lead to a better load balancing. Three types of selection, resulting in three classes of algorithms, are distinguished: (1) uniform and independent, (2) nonuniform and independent, and (3) nonuniform and dependent.Our first result shows that the well-studied uniform greedy algorithm (class 1) does not obtain the smallest possible maximum load. In particular, we introduce a nonuniform algorithm (class 2) that obtains a better load balancing. Surprisingly, this algorithm uses an unfair tie-breaking mechanism, called Always-Go-Left, resulting in an asymmetric assignment of the balls to the bins. Our second result is a lower bound showing that a dependent allocation (class 3) cannot yield significant further improvement.Our upper and lower bounds on the maximum load are tight up to additive constants, proving that the Always-Go-Left algorithm achieves an almost optimal load balancing among all sequential multiple-choice algorithm. Furthermore, we show that the results for the Always-Go-Left algorithm can be generalized to allocation processes with more balls than bins and even to infinite processes in which balls are inserted and deleted by an oblivious adversary.
We present a new approach to inference in Bayesian networks, which is based on representing the network using a polynomial and then retrieving answers to probabilistic queries by evaluating and differentiating the polynomial. The network polynomial itself is exponential in size, but we show how it can be computed efficiently using an arithmetic circuit that can be evaluated and differentiated in time and space linear in the circuit size. The proposed framework for inference subsumes one of the most influential methods for inference in Bayesian networks, known as the tree-clustering or jointree method, which provides a deeper understanding of this classical method and lifts its desirable characteristics to a much more general setting. We discuss some theoretical and practical implications of this subsumption.Let Hn be the height of a random binary search tree on n nodes. We show that there exist constants α = 4.311… and β = 1.953… such that E(Hn) = αln n − βln ln n + O(1), We also show that Var(Hn) = O(1).It is shown that all centralized absolute moments E|Hn − EHn|α (α ≥ 0) of the height Hn of binary search trees of size n and of the saturation level Hn′ are bounded. The methods used rely on the analysis of a retarded differential equation of the form Φ′(u) = −α−2Φ(u/α)2 with α > 1. The method can also be extended to prove the same result for the height of m-ary search trees. Finally the limiting behaviour of the distribution of the height of binary search trees is precisely determined.The Static Single Assignment (SSA) form is a program representation used in many optimizing compilers. The key step in converting a program to SSA form is called φ-placement. Many algorithms for φ-placement have been proposed in the literature, but the relationships between these algorithms are not well understood.In this article, we propose a framework within which we systematically derive (i) properties of the SSA form and (ii) φ-placement algorithms. This framework is based on a new relation called merge which captures succinctly the structure of a program's control flow graph that is relevant to its SSA form. The φ-placement algorithms we derive include most of the ones described in the literature, as well as several new ones. We also evaluate experimentally the performance of some of these algorithms on the SPEC92 benchmarks.Some of the algorithms described here are optimal for a single variable. However, their repeated application is not necessarily optimal for multiple variables. We conclude the article by describing such an optimal algorithm, based on the transitive reduction of the merge relation, for multi-variable φ-placement in structured programs. The problem for general programs remains open.
This article presents a class of approximation algorithms that extend the idea of bounded-complexity inference, inspired by successful constraint propagation algorithms, to probabilistic inference and combinatorial optimization. The idea is to bound the dimensionality of dependencies created by inference algorithms. This yields a parameterized scheme, called mini-buckets, that offers adjustable trade-off between accuracy and efficiency. The mini-bucket approach to optimization problems, such as finding the most probable explanation (MPE) in Bayesian networks, generates both an approximate solution and bounds on the solution quality. We present empirical results demonstrating successful performance of the proposed approximation scheme for the MPE task, both on randomly generated problems and on realistic domains such as medical diagnosis and probabilistic decoding.We prove the first time-space lower bound trade-offs for randomized computation of decision problems. The bounds hold even in the case that the computation is allowed to have arbitrary probability of error on a small fraction of inputs. Our techniques are extension of those used by Ajtai and by Beame, Jayram, and Saks that applied to deterministic branching programs. Our results also give a quantitative improvement over the previous results.Previous time-space trade-off results for decision problems can be divided naturally into results for functions with Boolean domain, that is, each input variable is {0,1}-valued, and the case of large domain, where each input variable takes on values from a set whose size grows with the number of variables.In the case of Boolean domain, Ajtai exhibited an explicit class of functions, and proved that any deterministic Boolean branching program or RAM using space S = o(n) requires superlinear time T to compute them. The functional form of the superlinear bound is not given in his paper, but optimizing the parameters in his arguments gives T = Ω(n log log n/log log log n) for S = O(n1−&epsis;). For the same functions considered by Ajtai, we prove a time-space trade-off (for randomized branching programs with error) of the form T = Ω(n &sqrt; log(n/S)/log log (n/S)). In particular, for space O(n1−&epsis;), this improves the lower bound on time to Ω(n&sqrt; log n/log log n).In the large domain case, we prove lower bounds of the form T = Ω(n&sqrt; log(n/S)/log log (n/S)) for randomized computation of the element distinctness function and lower bounds of the form T = Ω(n log (n/S)) for randomized computation of Ajtai's Hamming closeness problem and of certain functions associated with quadratic forms over large fields.We present the first complete problem for SZK, the class of promise problems possessing statistical zero-knowledge proofs (against an honest verifier). The problem, called Statistical Difference, is to decide whether two efficiently samplable distributions are either statistically close or far apart. This gives a new characterization of SZK that makes no reference to interaction or zero knowledge.We propose the use of complete problems to unify and extend the study of statistical zero knowledge. To this end, we examine several consequences of our Completeness Theorem and its proof, such as:---A way to make every (honest-verifier) statistical zero-knowledge proof very communication efficient, with the prover sending only one bit to the verifier (to achieve soundness error 1/2).---Simpler proofs of many of the previously known results about statistical zero knowledge, such as the Fortnow and Aiello--H&#949;stad upper bounds on the complexity of SZK and Okamoto's result that SZK is closed under complement.---Strong closure properties of SZK that amount to constructing statistical zero-knowledge proofs for complex assertions built out of simpler assertions already shown to be in SZK.---New results about the various measures of "knowledge complexity," including a collapse in the hierarchy corresponding to knowledge complexity in the "hint" sense.---Algorithms for manipulating the statistical difference between efficiently samplable distributions, including transformations that "polarize" and "reverse" the statistical relationship between a pair of distributions.This article provides necessary and sufficient conditions for deadlock-free unicast and multicast routing with the path-based routing model in interconnection networks that use the wormhole switching technique. The theory is developed around three central concepts: channel waiting, False Resource Cycles, and valid destination sets. The first two concepts are suitable extensions to those developed for unicast routing by two authors of this article; the third concept has been developed by Lin and Ni. The necessary and sufficient conditions relax the requirements for deadlock-free routing, compared to techniques that provide only a sufficient condition. These necessary and sufficient conditions can be applied in a straightforward manner to prove deadlock freedom of newly developed adaptive routing algorithms for collective communication, which in turn will help in developing efficient and correct routing algorithms. The latter point is illustrated by developing two routing algorithms for multicast communication on 2D mesh architectures. The first algorithm uses fewer resources (channels) than an algorithm proposed in the literature but achieves the same adaptiveness. The second algorithm provides fully adaptive routing for both unicast and multicast messages.
This contribution proposes a set of criteria that distinguish a grand challenge in science or engineering from the many other kinds of short-term or long-term research problems that engage the interest of scientists and engineers. As an example drawn from Computer Science, it revives an old challenge: the construction and application of a verifying compiler that guarantees correctness of a program before running it.I examine the question of why so few classes of quantum algorithms have been discovered. I give two possible explanations for this, and some thoughts about what lines of research might lead to the discovery of more quantum algorithms.Because many problems of general interest have natural nondeterministic algorithms and because computers act deterministically, it is important to understand the relationship between deterministic and nondeterministic time. Specifically, it is important to understand how quickly a deterministic computing device can determine the outcome of a nondeterministic calculation. So far, we have no general techniques that work any better than trying all step-by-step simulations, an exponential method.The most famous question concerning determinism versus nondeterminism is the P = NP question. However, this is a different question than "what is the relationship?" and it is possible that significant progress about the relationship can be achieved without answering the Pv = NP question. Thanks to efficient reductions from Turing machine simulation to SAT, the relationship question can be posed as a question about SAT. The problem of proving a nontrivial lower bound on the time required to solve SAT is just an instance of the larger problem of proving lower bounds for any natural problem. It is suggested that a study of generic problems might be a fruitful approach toward insights on such problems.Would physical laws permit the construction of computing machines that are capable of solving some problems much faster than the standard computational model? Recent evidence suggests that this might be the case in the quantum world. But the question is of great interest even in the realm of classical physics. In this article, we observe that there is fundamental tension between the Extended Church--Turing Thesis and the existence of numerous seemingly intractable computational problems arising from classical physics. Efforts to resolve this incompatibility could both advance our knowledge of the theory of computation, as well as serve the needs of scientific computing.
A number of efficient methods for evaluating first-order and monadic-second order queries on finite relational structures are based on tree-decompositions of structures or queries. We systematically study these methods.In the first part of the article, we consider arbitrary formulas on tree-like structures. We generalize a theorem of Courcelle [1990] by showing that on structures of bounded tree-width a monadic second-order formula (with free first- and second-order variables) can be evaluated in time linear in the structure size plus the size of the output.In the second part, we study tree-like formulas on arbitrary structures. We generalize the notions of acyclicity and bounded tree-width from conjunctive queries to arbitrary first-order formulas in a straightforward way and analyze the complexity of evaluating formulas of these fragments. Moreover, we show that the acyclic and bounded tree-width fragments have the same expressive power as the well-known guarded fragment and the finite-variable fragments of first-order logic, respectively.An exponential lower bound on the circuit complexity of deciding the weak monadic second-order theory of one successor (WS1S) is proved. Circuits are built from binary operations, or 2-input gates, which compute arbitrary Boolean functions. In particular, to decide the truth of logical formulas of length at most 610 in this second-order language requires a circuit containing at least 10125 gates. So even if each gate were the size of a proton, the circuit would not fit in the known universe. This result and its proof, due to both authors, originally appeared in 1974 in the Ph.D. thesis of the first author. In this article, the proof is given, the result is put in historical perspective, and the result is extended to probabilistic circuits.*We study a property of correctness of programs written in a shared-memory parallel language. This property is a semantic equivalence between the parallel program and its sequential version, that we define. We consider some standard parallel imperative language. Within this language, this correctness property follows from the preservation of data dependences by the control flow and the synchronizations. Our result makes use of the semantics of the sequential version only. Hence, through our result, checking the correctness of some parallel program boils down to verifying properties of some sequential program.We present a model that enables us to analyze the running time of an algorithm on a computer with a memory hierarchy with limited associativity, in terms of various cache parameters. Our cache model, an extension of Aggarwal and Vitter's I/O model, enables us to establish useful relationships between the cache complexity and the I/O complexity of computations. As a corollary, we obtain cache-efficient algorithms in the single-level cache model for fundamental problems like sorting, FFT, and an important subclass of permutations. We also analyze the average-case cache behavior of mergesort, show that ignoring associativity concerns could lead to inferior performance, and present supporting experimental evidence.We further extend our model to multiple levels of cache with limited associativity and present optimal algorithms for matrix transpose and sorting. Our techniques may be used for systematic exploitation of the memory hierarchy starting from the algorithm design stage, and for dealing with the hitherto unresolved problem of limited associativity.
Some important classical mechanisms considered in Microeconomics and Game Theory require the solution of a difficult optimization problem. This is true of mechanisms for combinatorial auctions, which have in recent years assumed practical importance, and in particular of the gold standard for combinatorial auctions, the Generalized Vickrey Auction (GVA). Traditional analysis of these mechanisms---in particular, their truth revelation properties---assumes that the optimization problems are solved precisely. In reality, these optimization problems can usually be solved only in an approximate fashion. We investigate the impact on such mechanisms of replacing exact solutions by approximate ones. Specifically, we look at a particular greedy optimization method. We show that the GVA payment scheme does not provide for a truth revealing mechanism. We introduce another scheme that does guarantee truthfulness for a restricted class of players. We demonstrate the latter property by identifying natural properties for combinatorial auctions and showing that, for our restricted class of players, they imply that truthful strategies are dominant. Those properties have applicability beyond the specific auction studied.Given a collection of contigs and mate-pairs. The Contig Scaffolding Problem is to order and orientate the given contigs in a manner that is consistent with as many mate-pairs as possible. This paper describes an efficient heuristic called the greedy-path merging algorithm for solving this problem. The method was originally developed as a key component of the compartmentalized assembly strategy developed at Celera Genomics. This interim approach was used at an early stage of the sequencing of the human genome to produce a preliminary assembly based on preliminary whole genome shotgun data produced at Celera and preliminary human contigs produced by the Human Genome Project.In a traditional classification problem, we wish to assign one of k labels (or classes) to each of n objects, in a way that is consistent with some observed data that we have about the problem. An active line of research in this area is concerned with classification when one has information about pairwise relationships among the objects to be classified; this issue is one of the principal motivations for the framework of Markov random fields, and it arises in areas such as image processing, biometry, and document analysis. In its most basic form, this style of analysis seeks to find a classification that optimizes a combinatorial function consisting of assignment costs---based on the individual choice of label we make for each object---and separation costs---based on the pair of choices we make for two "related" objects.We formulate a general classification problem of this type, the metric labeling problem; we show that it contains as special cases a number of standard classification frameworks, including several arising from the theory of Markov random fields. From the perspective of combinatorial optimization, our problem can be viewed as a substantial generalization of the multiway cut problem, and equivalent to a type of uncapacitated quadratic assignment problem.We provide the first nontrivial polynomial-time approximation algorithms for a general family of classification problems of this type. Our main result is an O(log k log log k)-approximation algorithm for the metric labeling problem, with respect to an arbitrary metric on a set of k labels, and an arbitrary weighted graph of relationships on a set of objects. For the special case in which the labels are endowed with the uniform metric---all distances are the same---our methods provide a 2-approximation algorithm.A new framework for analyzing online bin packing algorithms is presented. This framework presents a unified way of explaining the performance of algorithms based on the Harmonic approach. Within this framework, it is shown that a new algorithm, Harmonic++, has asymptotic performance ratio at most 1.58889. It is also shown that the analysis of Harmonic+1 presented in Richey [1991] is incorrect; this is a fundamental logical flaw, not an error in calculation or an omitted case. The asymptotic performance ratio of Harmonic+1 is at least 1.59217. Thus, Harmonic++ provides the best upper bound for the online bin packing problem to date.Temporal logic comes in two varieties: linear-time temporal logic assumes implicit universal quantification over all paths that are generated by the execution of a system; branching-time temporal logic allows explicit existential and universal quantification over all paths. We introduce a third, more general variety of temporal logic: alternating-time temporal logic offers selective quantification over those paths that are possible outcomes of games, such as the game in which the system and the environment alternate moves. While linear-time and branching-time logics are natural specification languages for closed systems, alternating-time logics are natural specification languages for open systems. For example, by preceding the temporal operator "eventually" with a selective path quantifier, we can specify that in the game between the system and the environment, the system has a strategy to reach a certain state. The problems of receptiveness, realizability, and controllability can be formulated as model-checking problems for alternating-time formulas. Depending on whether or not we admit arbitrary nesting of selective path quantifiers and temporal operators, we obtain the two alternating-time temporal logics ATL and ATL*.ATL and ATL* are interpreted over concurrent game structures. Every state transition of a concurrent game structure results from a choice of moves, one for each player. The players represent individual components and the environment of an open system. Concurrent game structures can capture various forms of synchronous composition for open systems, and if augmented with fairness constraints, also asynchronous composition. Over structures without fairness constraints, the model-checking complexity of ATL is linear in the size of the game structure and length of the formula, and the symbolic model-checking algorithm for CTL extends with few modifications to ATL. Over structures with weak-fairness constraints, ATL model checking requires the solution of 1-pair Rabin games, and can be done in polynomial time. Over structures with strong-fairness constraints, ATL model checking requires the solution of games with Boolean combinations of Büchi conditions, and can be done in PSPACE. In the case of ATL*, the model-checking problem is closely related to the synthesis problem for linear-time formulas, and requires doubly exponential time.
This paper investigates to what extent a purely symbolic approach to decision making under uncertainty is possible, in the scope of artificial intelligence. Contrary to classical approaches to decision theory, we try to rank acts without resorting to any numerical representation of utility or uncertainty, and without using any scale on which both uncertainty and preference could be mapped. Our approach is a variant of Savage's where the setting is finite, and the strict preference on acts is a partial order. It is shown that although many axioms of Savage theory are preserved and despite the intuitive appeal of the ordinal method for constructing a preference over acts, the approach is inconsistent with a probabilistic representation of uncertainty. The latter leads to the kind of paradoxes encountered in the theory of voting. It is shown that the assumption of ordinal invariance enforces a qualitative decision procedure that presupposes a comparative possibility representation of uncertainty, originally due to Lewis, and usual in nonmonotonic reasoning. Our axiomatic investigation thus provides decision-theoretic foundations to the preferential inference of Lehmann and colleagues. However, the obtained decision rules are sometimes either not very decisive or may lead to overconfident decisions, although their basic principles look sound. This paper points out some limitations of purely ordinal approaches to Savage-like decision making under uncertainty, in perfect analogy with similar difficulties in voting theory.We consider the possibility of encoding m classical bits into many fewer n quantum bits (qubits) so that an arbitrary bit from the original m bits can be recovered with good probability. We show that nontrivial quantum codes exist that have no classical counterparts. On the other hand, we show that quantum encoding cannot save more than a logarithmic additive factor over the best classical encoding. The proof is based on an entropy coalescence principle that is obtained by viewing Holevo's theorem from a new perspective.In the existing implementations of quantum computing, qubits are a very expensive resource. Moreover, it is difficult to reinitialize existing bits during the computation. In particular, reinitialization is impossible in NMR quantum computing, which is perhaps the most advanced implementation of quantum computing at the moment. This motivates the study of quantum computation with restricted memory and no reinitialization, that is, of quantum finite automata. It was known that there are languages that are recognized by quantum finite automata with sizes exponentially smaller than those of corresponding classical automata. Here, we apply our technique to show the surprising result that there are languages for which quantum finite automata take exponentially more states than those of corresponding classical automata.This paper argues that for many algorithms, and static analysis algorithms in particular, bottom-up logic program presentations are clearer and simpler to analyze, for both correctness and complexity, than classical pseudo-code presentations. The main technical contribution consists of two theorems which allow, in many cases, the asymptotic running time of a bottom-up logic program to be determined by inspection. It is well known that a datalog program runs in O(nk) time where k is the largest number of free variables in any single rule. The theorems given here are significantly more refined. A variety of algorithms are presented and analyzed as examples.We show how to use an interactive theorem prover, HOL, together with a model checker, SPIN, to prove key properties of distance vector routing protocols. We do three case studies: correctness of the RIP standard, a sharp real-time bound on RIP stability, and preservation of loop-freedom in AODV, a distance vector protocol for wireless networks. We develop verification techniques suited to routing protocols generally. These case studies show significant benefits from automated support in reduced verification workload and assistance in finding new insights and gaps for standard specifications.
We present two new algorithms for solving the All Pairs Shortest Paths (APSP) problem for weighted directed graphs. Both algorithms use fast matrix multiplication algorithms.The first algorithm solves the APSP problem for weighted directed graphs in which the edge weights are integers of small absolute value in Õ(n2+μ) time, where μ satisfies the equation ω(1, μ, 1) = 1 + 2μ and ω(1, μ, 1) is the exponent of the multiplication of an n × nμ matrix by an nμ × n matrix. Currently, the best available bounds on ω(1, μ, 1), obtained by Coppersmith, imply that μ < 0.575. The running time of our algorithm is therefore O(n2.575). Our algorithm improves on the &Otilede;(n(3c+ω)/2) time algorithm, where ω = ω(1, 1, 1) < 2.376 is the usual exponent of matrix multiplication, obtained by Alon et al., whose running time is only known to be O(n2.688).The second algorithm solves the APSP problem almost exactly for directed graphs with arbitrary nonnegative real weights. The algorithm runs in Õ((nω/&epsis;) log(W/&epsis;)) time, where &epsis; > 0 is an error parameter and W is the largest edge weight in the graph, after the edge weights are scaled so that the smallest non-zero edge weight in the graph is 1. It returns estimates of all the distances in the graph with a stretch of at most 1 + &epsis;. Corresponding paths can also be found efficiently.The subject of this article is differential compression, the algorithmic task of finding common strings between versions of data and using them to encode one version compactly by describing it as a set of changes from its companion. A main goal of this work is to present new differencing algorithms that (i) operate at a fine granularity (the atomic unit of change), (ii) make no assumptions about the format or alignment of input data, and (iii) in practice use linear time, use constant space, and give good compression. We present new algorithms, which do not always compress optimally but use considerably less time or space than existing algorithms. One new algorithm runs in O(n) time and O(1) space in the worst case (where each unit of space contains [log n] bits), as compared to algorithms that run in O(n) time and O(n) space or in O(n2) time and O(1) space. We introduce two new techniques for differential compression and apply these to give additional algorithms that improve compression and time performance. We experimentally explore the properties of our algorithms by running them on actual versioned data. Finally, we present theoretical results that limit the compression power of differencing algorithms that are restricted to making only a single pass over the data.The article investigates XML document specifications with DTDs and integrity constraints, such as keys and foreign keys. We study the consistency problem of checking whether a given specification is meaningful: that is, whether there exists an XML document that both conforms to the DTD and satisfies the constraints. We show that DTDs interact with constraints in a highly intricate way and as a result, the consistency problem in general is undecidable. When it comes to unary keys and foreign keys, the consistency problem is shown to be NP-complete. This is done by coding DTDs and integrity constraints with linear constraints on the integers. We consider the variations of the problem (by both restricting and enlarging the class of constraints), and identify a number of tractable cases, as well as a number of additional NP-complete ones. By incorporating negations of constraints, we establish complexity bounds on the implication problem, which is shown to be coNP-complete for unary keys and foreign keys.In completely symmetric systems that have homogeneous nodes (hosts, computers, or processors) with identical arrival processes, an optimal static load balancing scheme does not involve the forwarding of jobs among nodes. Using an appropriate analytic model of a distributed computer system, we examine the following three decision schemes for load balancing: completely distributed, intermediately distributed, and completely centralized. We show that there is no forwarding of jobs in the completely centralized and completely distributed schemes, but that in an intermediately distributed decision scheme, mutual forwarding of jobs among nodes is possible, leading to degradation in system performance for every decision maker. This result appears paradoxical, because by adding communication capacity to the system for the sharing of jobs between nodes, the overall system performance is degraded. We characterize conditions under which such paradoxical behavior occurs, and we give examples in which the degradation of performance may increase without bound. We show that the degradation reduces and finally disappears in the limit as the intermediately distributed decision scheme tends to a completely distributed one.We derive tight bounds on cache misses for evaluation of explicit stencil operators on rectangular grids. Our lower bound is based on the isoperimetric property of the discrete crosspolytope. Our upper bound is based on a good surface-to-volume ratio of a parallelepiped spanned by a reduced basis of the interference lattice of a grid. Measurements show that our algorithm typically reduces the number of cache misses by a factor of three, relative to a compiler optimized code. We show that stencil calculations on grids whose interference lattices have a short vector feature abnormally high numbers of cache misses. We call such grids unfavorable and suggest to avoid these in computations by appropriate padding. By direct measurements on a MIPS R10000 processor we show a good correlation between abnormally high numbers of cache misses and unfavorable three-dimensional grids.
We consider a modified notion of planarity, in which two nations of a map are considered adjacent when they share any point of their boundaries (not necessarily an edge, as planarity requires). Such adjacencies define a map graph. We give an NP characterization for such graphs, derive some consequences regarding sparsity and coloring, and survey some algorithmic results.
The Johnson--Lindenstrauss lemma states that n points in a
high-dimensional Hilbert space can be embedded with small
distortion of the distances into an O(log n)
dimensional space by applying a random linear transformation. We
show that similar (though weaker) properties hold for certain
random linear transformations over the Hamming cube. We use these
transformations to solve NP-hard clustering problems in the cube as
well as in geometric settings.More specifically, we address the
following clustering problem. Given n points in a larger set
(e.g., ℝd) endowed with a distance function (e.g.,
L2 distance), we would like to partition the data
set into k disjoint clusters, each with a "cluster center,"
so as to minimize the sum over all data points of the distance
between the point and the center of the cluster containing the
point. The problem is provably NP-hard in some high-dimensional
geometric settings, even for k = 2. We give polynomial-time
approximation schemes for this problem in several settings,
including the binary cube {0,1}d with Hamming distance,
and ℝd either with L1 distance,
or with L2 distance, or with the square of
L2 distance. In all these settings, the best
previous results were constant factor approximation guarantees.We
note that our problem is similar in flavor to the k-median
problem (and the related facility location problem), which has been
considered in graph-theoretic and fixed dimensional geometric
settings, where it becomes hard when k is part of the input.
In contrast, we study the problem when k is fixed, but the
dimension is part of the input.

The problem of finding a center string that is "close" to every
given string arises in computational molecular biology and coding
theory. This problem has two versions: the Closest String problem
and the Closest Substring problem. Given a set of strings S
= {s1, s2, ...,
sn}, each of length m, the Closest String
problem is to find the smallest d and a string s of length
m which is within Hamming distance d to each
si ε S. This problem comes from
coding theory when we are looking for a code not too far away from
a given set of codes. Closest Substring problem, with an additional
input integer L, asks for the smallest d and a string
s, of length L, which is within Hamming distance d
away from a substring, of length L, of each si. This problem
is much more elusive than the Closest String problem. The Closest
Substring problem is formulated from applications in finding
conserved regions, identifying genetic drug targets and generating
genetic probes in molecular biology. Whether there are efficient
approximation algorithms for both problems are major open questions
in this area. We present two polynomial-time approximation
algorithms with approximation ratio 1 + ε for any small
ε to settle both questions.
In this article, we define timed regular expressions, a formalism for specifying discrete behaviors augmented with timing information, and prove that its expressive power is equivalent to the timed automata of Alur and Dill. This result is the timed analogue of Kleene Theorem and, similarly to that result, the hard part in the proof is the translation from automata to expressions. This result is extended from finite to infinite (in the sense of Büchi) behaviors. In addition to these fundamental results, we give a clean algebraic framework for two commonly accepted formalisms for timed behaviors, time-event sequences and piecewise-constant signals.We view congestion control as a distributed primal--dual algorithm carried out by sources and links over a network to solve a global optimization problem. We describe a multilink multisource model of the TCP Vegas congestion control mechanism. The model provides a fundamental understanding of delay, fairness and loss properties of TCP Vegas. It implies that Vegas stabilizes around a weighted proportionally fair allocation of network capacity when there is sufficient buffering in the network. It clarifies the mechanism through which persistent congestion may arise and its consequences, and suggests how we might use REM active queue management to prevent it. We present simulation results that validate our conclusions.We consider the problem of routing traffic to optimize the performance of a congested network. We are given a network, a rate of traffic between each pair of nodes, and a latency function for each edge specifying the time needed to traverse the edge given its congestion; the objective is to route traffic such that the sum of all travel times---the total latency---is minimized.In many settings, it may be expensive or impossible to regulate network traffic so as to implement an optimal assignment of routes. In the absence of regulation by some central authority, we assume that each network user routes its traffic on the minimum-latency path available to it, given the network congestion caused by the other users. In general such a "selfishly motivated" assignment of traffic to paths will not minimize the total latency; hence, this lack of regulation carries the cost of decreased network performance.In this article, we quantify the degradation in network performance due to unregulated traffic. We prove that if the latency of each edge is a linear function of its congestion, then the total latency of the routes chosen by selfish network users is at most 4/3 times the minimum possible total latency (subject to the condition that all traffic must be routed). We also consider the more general setting in which edge latency functions are assumed only to be continuous and nondecreasing in the edge congestion. Here, the total latency of the routes chosen by unregulated selfish network users may be arbitrarily larger than the minimum possible total latency; however, we prove that it is no more than the total latency incurred by optimally routing twice as much traffic.We consider a distributed server system and ask which policy should be used for assigning jobs (tasks) to hosts. In our server, jobs are not preemptible. Also, the job's service demand is not known a priori. We are particularly concerned with the case where the workload is heavy-tailed, as is characteristic of many empirically measured computer workloads. We analyze several natural task assignment policies and propose a new one TAGS (Task Assignment based on Guessing Size). The TAGS algorithm is counterintuitive in many respects, including load unbalancing, non-work-conserving, and fairness. We find that under heavy-tailed workloads, TAGS can outperform all task assignment policies known to us by several orders of magnitude with respect to both mean response time and mean slowdown, provided the system load is not too high. We also introduce a new practical performance metric for distributed servers called server expansion. Under the server expansion metric, TAGS significantly outperforms all other task assignment policies, regardless of system load.
In 1975, Valiant showed that Boolean matrix multiplication can be used for parsing context-free grammars (CFGs), yielding the asympotically fastest (although not practical) CFG parsing algorithm known. We prove a dual result: any CFG parser with time complexity O(gn3-∈), where g is the size of the grammar and n is the length of the input string, can be efficiently converted into an algorithm to multiply m × m Boolean matrices in time O(m3-∈/3). Given that practical, substantially subcubic Boolean matrix multiplication algorithms have been quite difficult to find, we thus explain why there has been little progress in developing practical, substantially subcubic general CFG parsers. In proving this result, we also develop a formalization of the notion of parsing.
We establish that the algorithmic complexity of the minimum
spanning tree problem is equal to its decision-tree complexity.
Specifically, we present a deterministic algorithm to find a
minimum spanning tree of a graph with n vertices and
m edges that runs in time
O(T*(m,n)) where
T* is the minimum number of edge-weight
comparisons needed to determine the solution. The algorithm is
quite simple and can be implemented on a pointer machine.Although
our time bound is optimal, the exact function describing it is not
known at present. The current best bounds known for
T* are T*(m,n) =
Ω(m) and T*(m,n) =
O(m ∙ α(m,n)), where α is a
certain natural inverse of Ackermann's function.Even under the
assumption that T* is superlinear, we show that
if the input graph is selected from Gn,m,
our algorithm runs in linear time with high probability, regardless
of n, m, or the permutation of edge weights. The
analysis uses a new martingale for Gn,m
similar to the edge-exposure martingale for
Gn,p.
We develop a theoretical framework to characterize the hardness of indexing data sets on block-access memory devices like hard disks. We define an indexing workload by a data set and a set of potential queries. For a workload, we can construct an indexing scheme, which is a collection of fixed-sized subsets of the data. We identify two measures of efficiency for an indexing scheme on a workload: storage redundancy, r (how many times each item in the data set is stored), and access overhead, A (how many times more blocks than necessary does a query retrieve).For many interesting families of workloads, there exists a trade-off between storage redundancy and access overhead. Given a desired access overhead A, there is a minimum redundancy that any indexing scheme must exhibit. We prove a lower-bound theorem for deriving the minimum redundancy. By applying this theorem, we show interesting upper and lower bounds and trade-offs between A and r in the case of multidimensional range queries and set queries.Structured document databases can be naturally viewed as derivation trees of a context-free grammar. Under this view, the classical formalism of attribute grammars becomes a formalism for structured document query languages. From this perspective, we study the expressive power of BAGs: Boolean-valued attribute grammars with propositional logic formulas as semantic rules, and RAGs: relation-valued attribute grammars with first-order logic formulas as semantic rules. BAGs can express only unary queries; RAGs can express queries of any arity. We first show that the (unary) queries expressible by BAGs are precisely those definable in monadic second-order logic. We then show that the queries expressible by RAGs are precisely those definable by first-order inductions of linear depth, or, equivalently, those computable in linear time on a parallel machine with polynomially many processors. Further, we show that RAGs that only use synthesized attributes are strictly weaker than RAGs that use both synthesized and inherited attributes. We show that RAGs are more expressive than monadic second-order logic for queries of any arity. Finally, we discuss relational attribute grammars in the context of BAGs and RAGs. We show that in the case of BAGs this does not increase the expressive power, while different semantics for relational RAGs capture the complexity classes NP, coNP and UP ∩ coUP.Shared registers are basic objects used as communication mediums in asynchronous concurrent computation. A concurrent timestamp system is a higher typed communication object, and has been shown to be a powerful tool to solve many concurrency control problems. It has turned out to be possible to construct such higher typed objects from primitive lower typed ones. The next step is to find efficient constructions. We propose a very efficient wait-free construction of bounded concurrent timestamp systems from 1-writer shared registers. This finalizes, corrects, and extends a preliminary bounded multiwriter construction proposed by the second author in 1986. That work partially initiated the current interest in wait-free concurrent objects, and introduced a notion of discrete vector clocks in distributed algorithms.
The idea of preprocessing part of the input of a problem in order to improve efficiency has been employed by several researchers in several areas of computer science. In this article, we show sufficient conditions to prove that an intractable problem cannot be efficiently solved even allowing an exponentially long preprocessing phase. The generality of such conditions is shown by applying them to various problems coming from different fields. While the results may seem to discourage the use of compilation, we present some evidence that such negative results are useful in practice.We provide two algorithms for computing the volume of the convex polytope Ω : = {x ∈ ℝn+ | Ax ≤ b}, for A, ∈ ℝm×n, b ∈ ℝn. The computational complexity of both algorithms is essentially described by nm, which makes them especially attractive for large n and relatively small m, when the other methods with O(mn) complexity fail. The methodology, which differs from previous existing methods, uses a Laplace transform technique that is well suited to the half-space representation of Ω.
Recent trends in information management involve the periodic transcription of data onto secondary devices in a networked environment, and the proper scheduling of these transcriptions is critical for efficient data management. To assist in the scheduling process, we are interested in modeling data obsolescence, that is, the reduction of consistency over time between a relation and its replica. The modeling is based on techniques from the field of stochastic processes, and provides several stochastic models for content evolution in the base relations of a database, taking referential integrity constraints into account. These models are general enough to accommodate most of the common scenarios in databases, including batch insertions and lifespans both with and without memory. As an initial "proof of concept" of the applicability of our approach, we validate the insertion portion of our model framework via experiments with real data feeds. We also discuss a set of transcription protocols that make use of the proposed stochastic model.We introduce the concept of a class of graphs, or more generally, relational structures, being locally tree-decomposable. There are numerous examples of locally tree-decomposable classes, among them the class of planar graphs and all classes of bounded valence or of bounded tree-width. We also consider a slightly more general concept of a class of structures having bounded local tree-width.We show that for each property φ of structures that is definable in first-order logic and for each locally tree-decomposable class C of structures, there is a linear time algorithm deciding whether a given structure A ∈ C has property φ. For classes C of bounded local tree-width, we show that for every k ≥ 1 there is an algorithm solving the same problem in time O(n1+(1/k)) (where n is the cardinality of the input structure).We study extensions of the process algebra axiom system ACP with two recursive operations: the binary Kleene star *, which is defined by x*y = x(x*y + y, and the push-down operation $, defined by x$y = x((x$y)(x$y)) + y. In this setting it is easy to represent register machine computation, and an equational theory results that is not decidable. In order to increase the expressive power, abstraction is then added: with rooted branching bisimulation equivalence each computable process can be expressed, and with rooted ô-bisimilarity each semi-computable process that initially is finitely branching can be expressed. Moreover, with abstraction and a finite number of auxiliary actions these results can be obtained without binary Kleene star. Finally, we consider two alternatives for the push-down operation. Each of these gives rise to similar results.
Research on multimedia information retrieval (MIR) has recently witnessed a booming interest. A prominent feature of this research trend is its simultaneous but independent materialization within several fields of computer science. The resulting richness of paradigms, methods and systems may, on the long run, result in a fragmentation of efforts and slow down progress. The primary goal of this study is to promote an integration of methods and techniques for MIR by contributing a conceptual model that encompasses in a unified and coherent perspective the many efforts that are being produced under the label of MIR. The model offers a retrieval capability that spans two media, text and images, but also several dimensions: form, content and structure. In this way, it reconciles similarity-based methods with semantics-based ones, providing the guidelines for the design of systems that are able to provide a generalized multimedia retrieval service, in which the existing forms of retrieval not only coexist, but can be combined in any desired manner. The model is formulated in terms of a fuzzy description logic, which plays a twofold role: (1) it directly models semantics-based retrieval, and (2) it offers an ideal framework for the integration of the multimedia and multidimensional aspects of retrieval mentioned above. The model also accounts for relevance feedback in both text and image retrieval, integrating known techniques for taking into account user judgments. The implementation of the model is addressed by presenting a decomposition technique that reduces query evaluation to the processing of simpler requests, each of which can be solved by means of widely known methods for text and image retrieval, and semantic processing. A prototype for multidimensional image retrieval is presented that shows this decomposition technique at work in a significant case.We consider the problems of containment, equivalence, satisfiability and query-reachability for datalog programs with negation. These problems are important for optimizing datalog programs. We show that both query-reachability and satisfiability are decidable for programs with stratified negation provided that negation is applied only to EDB predicates or that all EDB predicates are unary. In the latter case, we show that equivalence is also decidable. The algorithms we present can also be used to push constraints from a given query to the EDB predicates. In showing our decidability results we describe a powerful tool, the query-tree, which is used for several optimization problems for datalog programs. Finally, we show that satisfiability is undecidable for datalog programs with unary IDB predicates, stratified negation and the interpreted predicate ≠.We present an algorithm for implementing binary operations (of any type) from unary load-linked (LL) and store-conditional (SC) operations. The performance of the algorithm is evaluated according to its sensitivity, measuring the distance between operations in the graph induced by conflicts, which guarantees that they do not influence the step complexity of each other. The sensitivity of our implementation is O(log* n), where n is the number of processors in the system. That is, operations that are Ω(log* n) apart in the graph induced by conflicts do not delay each other. Constant sensitivity is achieved for operations used to implement heaps and array-based linked lists.We also prove that there is a problem which can be solved in O(1) steps using binary LL/SC operations, but requires O(log log* n) operations if only unary LL/SC operations are used. This indicates a non-constant gap between unary and binary, LL/SC operations.We start with a mathematical definition of a real interval as a closed, connected set of reals. Interval arithmetic operations (addition, subtraction, multiplication, and division) are likewise defined mathematically and we provide algorithms for computing these operations assuming exact real arithmetic. Next, we define interval arithmetic operations on intervals with IEEE 754 floating point endpoints to be sound and optimal approximations of the real interval operations and we show that the IEEE standard's specification of operations involving the signed infinities, signed zeros, and the exact/inexact flag are such as to make a correct and optimal implementation more efficient. From the resulting theorems, we derive data that are sufficiently detailed to convert directly to a program for efficiently implementing the interval operations. Finally, we extend these results to the case of general intervals, which are defined as connected sets of reals that are not necessarily closed.We present a general framework for solving resource allocation and scheduling problems. Given a resource of fixed size, we present algorithms that approximate the maximum throughput or the minimum loss by a constant factor. Our approximation factors apply to many problems, among which are: (i) real-time scheduling of jobs on parallel machines, (ii) bandwidth allocation for sessions between two endpoints, (iii) general caching, (iv) dynamic storage allocation, and (v) bandwidth allocation on optical line and ring topologies. For some of these problems we provide the first constant factor approximation algorithm. Our algorithms are simple and efficient and are based on the local-ratio technique. We note that they can equivalently be interpreted within the primal-dual schema.
Knowledge compilation has been emerging recently as a new direction of research for dealing with the computational intractability of general propositional reasoning. According to this approach, the reasoning process is split into two phases: an off-line compilation phase and an on-line query-answering phase. In the off-line phase, the propositional theory is compiled into some target language, which is typically a tractable one. In the on-line phase, the compiled target is used to efficiently answer a (potentially) exponential number of queries. The main motivation behind knowledge compilation is to push as much of the computational overhead as possible into the off-line phase, in order to amortize that overhead over all on-line queries. Another motivation behind compilation is to produce very simple on-line reasoning systems, which can be embedded cost-effectively into primitive computational platforms, such as those found in consumer electronics.One of the key aspects of any compilation approach is the target language into which the propositional theory is compiled. Previous target languages included Horn theories, prime implicates/implicants and ordered binary decision diagrams (OBDDs). We propose in this paper a new target compilation language, known as decomposable negation normal form (DNNF), and present a number of its properties that make it of interest to the broad community. Specifically, we show that DNNF is universal; supports a rich set of polynomial--time logical operations; is more space-efficient than OBDDs; and is very simple as far as its structure and algorithms are concerned. Moreover, we present an algorithm for converting any propositional theory in clausal form into a DNNF and show that if the clausal form has a bounded treewidth, then its DNNF compilation has a linear size and can be computed in linear time (treewidth is a graph-theoretic parameter that measures the connectivity of the clausal form). We also propose two techniques for approximating the DNNF compilation of a theory when the size of such compilation is too large to be practical. One of the techniques generates a sound but incomplete compilation, while the other generates a complete but unsound compilation. Together, these approximations bound the exact compilation from below and above in terms of their ability to answer clausal entailment queries. Finally, we show that the class of polynomial--time DNNF operations is rich enough to support relatively complex AI applications, by proposing a specific framework for compiling model-based diagnosis systems.We introduce a new approach to modeling uncertainty based on plausibility measures. This approach is easily seen to generalize other approaches to modeling uncertainty, such as probability measures, belief functions, and possibility measures. We focus on one application of plausibility measures in this paper: default reasoning. In recent years, a number of different semantics for defaults have been proposed, such as preferential structures, &egr;-semantics, possibilistic structures, and &kgr;-rankings, that have been shown to be characterized by the same set of axioms, known as the KLM properties. While this was viewed as a surprise, we show here that it is almost inevitable. In the framework of plausibility measures, we can give a necessary condition for the KLM axioms to be sound, and an additional condition necessary and sufficient to ensure that the KLM axioms are complete. This additional condition is so weak that it is almost always met whenever the axioms are sound. In particular, it is easily seen to hold for all the proposals made in the literature.Problems of statistical inference involve the adjustment of sample observations so they fit some a priori rank requirements, or order constraints. In such problems, the objective is to minimize the deviation cost function that depends on the distance between the observed value and the modify value. In Markov random field problems, there is also a pairwise relationship between the objects. The objective in Markov random field problem is to minimize the sum of the deviation cost function and a penalty function that grows with the distance between the values of related pairs---separation function.We discuss Markov random fields problems in the context of a representative application---the image segmentation problem. In this problem, the goal is to modify color shades assigned to pixels of an image so that the penalty function consisting of one term due to the deviation from the initial color shade and a second term that penalizes differences in assigned values to neighboring pixels is minimized. We present here an algorithm that solves the problem in polynomial time when the deviation function is convex and separation function is linear; and in strongly polynomial time when the deviation cost function is linear, quadratic or piecewise linear convex with few pieces (where "e;few"e; means a number exponential in a polynomial function of the number of variables and constraints). The complexity of the algorithm for a problem on n pixels or variables, m adjacency relations or constraints, and range of variable values (colors) U, is O(T(n,m) + n log U) where T(n,m) is the complexity of solving the minimum s, t cut problem on a graph with n nodes and m arcs. Furthermore, other algorithms are shown to solve the problem with convex deviation and convex separation in running time O(mn log n log nU) and the problem with nonconvex deviation and convex separation in running time O(T(nU, mU). The nonconvex separation problem is NP-hard even for fixed value of U.For the family of problems with convex deviation functions and linear separation function, the algorithm described here runs in polynomial time which is demonstrated to be fastest possible.We describe efficient techniques for a number of parties to jointly generate an RSA key. At the end of the protocol an RSA modulus N = pq is publicly known. None of the parties know the factorization of N. In addition a public encryption exponent is publicly known and each party holds a share of the private exponent that enables threshold decryption. Our protocols are efficient in computation and communication. All results are presented in the honest but curious scenario (passive adversary).Deterministic fully dynamic graph algorithms are presented for connectivity, minimum spanning tree, 2-edge connectivity, and biconnectivity. Assuming that we start with no edges in a graph with n vertices, the amortized operation costs are O(log2 n) for connectivity, O(log4 n) for minimum spanning forest, 2-edge connectivity, and O(log5 n) biconnectivity.This paper presents a combinatorial polynomial-time algorithm for minimizing submodular functions, answering an open question posed in 1981 by Grötschel, Lovász, and Schrijver. The algorithm employs a scaling scheme that uses a flow in the complete directed graph on the underlying set with each arc capacity equal to the scaled parameter. The resulting algorithm runs in time bounded by a polynomial in the size of the underlying set and the length of the largest absolute function value. The paper also presents a strongly polynomial version in which the number of steps is bounded by a polynomial in the size of the underlying set, independent of the function values.We examine the number of queries to input variables that a quantum algorithm requires to compute Boolean functions on {0,1}N in the black-box model. We show that the exponential quantum speed-up obtained for partial functions (i.e., problems involving a promise on the input) by Deutsch and Jozsa, Simon, and Shor cannot be obtained for any total function: if a quantum algorithm computes some total Boolean function f with small error probability using T black-box queries, then there is a classical deterministic algorithm that computes f exactly with O(Ts6) queries. We also give asymptotically tight characterizations of T for all symmetric f in the exact, zero-error, and bounded-error settings. Finally, we give new precise bounds for AND, OR, and PARITY. Our results are a quantum extension of the so-called polynomial method, which has been successfully applied in classical complexity theory, and also a quantum extension of results by Nisan about a polynomial relationship between randomized and deterministic decision tree complexity.We prove optimal, up to an arbitrary ε > 0, inapproximability results for Max-E k-Sat for k ≥ 3, maximizing the number of satisfied linear equations in an over-determined system of linear equations modulo a prime p and Set Splitting. As a consequence of these results we get improved lower bounds for the efficient approximability of many optimization problems studied previously. In particular, for Max-E2-Sat, Max-Cut, Max-di-Cut, and Vertex cover.We introduce a new approach to constructing extractors. Extractors are algorithms that transform a "e;weakly random"e; distribution into an almost uniform distribution. Explicit constructions of extractors have a variety of important applications, and tend to be very difficult to obtain.We demonstrate an unsuspected connection between extractors and pseudorandom generators. In fact, we show that every pseudorandom generator of a certain kind is an extractor.A pseudorandom generator construction due to Impagliazzo and Wigderson, once reinterpreted via our connection, is already an extractor that beats most known constructions and solves an important open question. We also show that, using the simpler Nisan--Wigderson generator and standard error-correcting codes, one can build even better extractors with the additional advantage that both the construction and the analysis are simple and admit a short self-contained description.We study adding aggregate operators, such as summing up elements of a column of a relation, to logics with counting mechanisms. The primary motivation comes from database applications, where aggregate operators are present in all real life query languages. Unlike other features of query languages, aggregates are not adequately captured by the existing logical formalisms. Consequently, all previous approaches to analyzing the expressive power of aggregation were only capable of producing partial results, depending on the allowed class of aggregate and arithmetic operations.We consider a powerful counting logic, and extend it with the set of all aggregate operators. We show that the resulting logic satisfies analogs of Hanf's and Gaifman's theorems, meaning that it can only express local properties. We consider a database query language that expresses all the standard aggregates found in commercial query languages, and show how it can be translated into the aggregate logic, thereby providing a number of expressivity bounds, that do not depend on a particular class of arithmetic functions, and that subsume all those previously known. We consider a restricted aggregate logic that gives us a tighter capture of database languages, and also use it to show that some questions on expressivity of aggregation cannot be answered without resolving some deep problems in complexity theory.
Basic techniques to prove the unconditional security of quantum crypto graphy are described. They are applied to a quantum key distribution protocol proposed by Bennett and Brassard [1984]. The proof considers a practical variation on the protocol in which the channel is noisy and photos may be lost during the transmission. Each individual signal sent into the channel must contain a single photon or any two-dimensional system in the exact state described in the protocol. No restriction is imposed on the detector used at the receiving side of the channel, except that whether or not the received system is detected must be independent of the basis used to measure this system.The Burrows—Wheeler Transform (also known as Block-Sorting) is at the  base of compression algorithms that are the state of the art in lossless data compression. In this paper, we analyze two algorithms that use this technique. The first one is the original algorithm described by Burrows and Wheeler, which, despite its simplicity outperforms the Gzip compressor. The second one uses an additional run-length encoding step to improve compression. We prove that the compression ratio of both algorithms can be bounded in terms of the kth order empirical entropy of the input string for any k ≥ 0. We make no assumptions on the input and we obtain bounds which hold in the worst case that is for every possible input string. All previous results for  Block-Sorting algorithms were concerned with the average compression ratio and have been established assuming that the input comes from a finite-order Markov source.This paper deals with the evaluation of acyclic Boolean
conjunctive queries in relational databases. By well-known results
of Yannakakis[1981], this problem is solvable in polynomial time;
its precise complexity, however, has not been pinpointed so far. We
show that the problem of evaluating acyclic Boolean conjunctive
queries is complete for LOGCFL, the class of decision problems that
are logspace-reducible to a context-free language. Since LOGCFL is
contained in AC1 and NC2, the evaluation problem of acyclic Boolean
conjunctive queries is highly parallelizable. We present a parallel
database algorithm solving this problem with alogarithmic number of
parallel join operations. The algorithm is generalized to computing
the output of relevant classes of non-Boolean queries. We also show
that the acyclic versions of the following well-known database and
AI problems are all LOGCFL-complete: The Query Output Tuple problem
for conjunctive queries, Conjunctive Query Containment, Clause
Subsumption, and Constraint Satisfaction. The LOGCFL-completeness
result is extended to the class of queries of bounded tree width
and to other relevant query classes which are more general than the
acyclic queries.The difficulty of designing fault-tolerant distributed algorithms incr eases with the severity of failures that an algorithm must tolerate, especially for systems with synchronous message passing. This paper considers methods that automatically translate algorithms tolerant of simple crash failures into ones tolerant of more severe failures. These translations simplify the design task by allowing algorithm designers to assume that processors fail only by stopping. Such translations can be quantified by two measures: fault-tolerance, which is a measure of how many processors must remain correct for the translation to be correct, and round-complexity, which is a measure of how the translation increases the running time of an algorithm. Understanding   these translations and their limitations with respect to these measures can provide insight into the relative impact of different models of faculty behavior on the ability to provide fault-tolerant applications for systems with synchronous message passing.This paper considers translations fr om crash failures to each of the following types of more severe failures: omission to send messages; omission to send and receive messages; and totally arbitrary behavior. It shows that previously developed translaions to send-omission failures are optimal with respect to both fault-tolerance and round-complexity. It exhibits a hierarchy of translations to general (send/receive) omission failures that improves upon the fault-tolerance of previously developed translations. These translations   are optimal in that they cannot be improved with respect to one measure without negatively affecting the other; that is, the hierarchy of translations is matched by corresponding hierarchy of impossibility results. The paper also gives a hierarchy of translations to arbitrary failures that improves upon the round-complexity of previously developed translations. These translations are near-optimal;We show that a type system based on the intuitionistic modal logic S4  provides an expressive framework for specifying and analyzing computation stages in the context of typed &lgr;-calculi and functional languages. We directly demonstrate the sense in which our   l→□e -calculus captures staging, and also give a conservative embeddng of Nielson and Nielson's two-level functional language in our functional language Mini-ML  □ , thus proving  that binding-time correctness is equivalent to modal correctness on this fragment. In addition,   Mini-ML□    can also express immediate evaluation and sharing of code across multiple stages, thus supporting run-time code generation as well as partial evaluation.
The widthof a Resolution proof is defined to be the maximal number of literals in any clause of the proof. In this paper, we relate proof width to proof length (=size), in both general Resolution, and its tree-like variant. The following consequences of these relations reveal width as a crucial “resource” of Resolution proofs.In one direction, the relations allow us to give simple, unified proofs for almost all known exponential lower bounds on size of resolution proofs, as well as several interesting new ones. They all follow from width lower bounds, and we show how these follow from natural expansion property of clauses of the input tautology.In the other direction, the width-size relations naturally suggest a  simple dynamic programming procedure for automated theorem proving—one which simply searches for small width proofs. This relation guarantees that the runnuing time (and thus the size of the produced proof) is at most quasi-polynomial in the smallest tree-like proof. This algorithm is never much worse than any of the recursive automated provers (such as DLL) used in practice. In contrast, we present a family of tautologies on which it is exponentially faster.This paper presents new theorems to analyze divide-and-conquer recurrences, which improve other similar ones in several aspects. In particular, these theorems provide more information, free us almost completely from technicalities like floors and ceilings, and cover a wider set of toll functions and weight distributions, stochastic recurrences included.We consider the problem of scheduling unrelated parallel machines subject to release dates so as to minimize the total weighted completion time of jobs. The main contribution of this paper is a provably good convex quadratic programming relaxation of strongly polynomial size for this problem. The best previously known approximation algorithms are based on LP relaxations in time- or interval-indexed variables. Those LP relaxations, however, suffer from a huge number of variables. As a result of the convex quadratic programming approach we can give a very simple and easy to analyze 2-approximation algorithm which can be further improved to performance guarantee 3/2 in the absence of release dates. We also consider preemptive scheduling problems and derive approximation algorithms and  results on the power of preemption which improve upon the best previously known results for these settings. Finally, for the special case of two machines we introduce a more sophisticated semidefinite programming relaxation and apply the random hyperplane technique introduced by Goemans   and Williamson for the MaxCut problem; this leads to an improved 1.2752-approximation.We study an on-line problem that is motivated by the networking problem of dynamically adjusting of acknowledgments in the Transmission Control Protocol (TCP). We provide a theoretical model for this problem in which the goal is to send acks at a time that minimize a linear combination of the cost for the number of acknowledgments sent and the cost for the additional latency introduced by delaying acknowledgments. To study the usefulness of applying packet arrival time prediction to this problem, we assume there is an oracle that provides the algorithm with the times of the next L arrivals, for some L ≥ 0.We give two different objective functions for measuring the cost of a solution, each with its own measure of latency cost. For each   objective function we first give an O(n2)-time dynamic programming algorithm for optimally solving the off-line problem. Then we describe an on-line algorithm that greedily acknowledges exactly when the cost for an acknowledgment is less than the latency cost incurred by not acknowledging. We show that for this algorithm there is a sequence of n packet arrivals for which it is &OHgr; (***)-competitive for the first objective function, 2-competitive for the second function for L = 0, and 1-competitivefor the second function for L = 1. Next we present a second on-line algorithm which is a slight modification of the first, and we prove that it is 2-competitive for both objective functions for all   L. We also give lower bounds on the competitive ratio for any deterministic on-line algorithm. These results show that for each objective function, at least one of our algorithms is optimal.Finally, we give some initial empirical results using arrival sequences from real network traffic where we compare the two methods used in TCP for acknowledgment delay with our two on-line algorithms. In all cases we examine performance with L = 0 and L = 1.We present approximation algorithms for the metric uncapacitated facility location problem and the metric k-median problem achieving guarantees of 3 and 6 respectively. The distinguishing feature of our algorithms is their low running time: O(m logm) and O(m logm(L + log (n))) respectively, where n and m are the total number of vertices and edges in the underlying complete bipartite graph on cities and facilities. The main algorithmic ideas are a new extension of the primal-dual schema and the use of Lagrangian relaxation to derive approximation algorithms.This paper resolves a long-standing open problem on whether the concurrent write capability of parallel random access machine (PRAM) is essential for solving fundamental graph problems like connected components and minimum spanning trees in O(logn) time. Specifically, we present a new algorithm to solve these problems in O(logn) time using a linear number of processors on the exclusive-read exclusive-write PRAM. The logarithmic time bound is actually optimal since it is well known that even computing the “OR” of nbit requires &OHgr;(log n time on the exclusive-write PRAM. The efficiency achieved by the new algorithm is based on a new schedule which can exploit a high degree of parallelism.We prove a sufficient condition for the stability of dynamic packet routing algorithms. Our approach reduces the problem of steady state analysis to the easier and better understood question of static routing. We show that certain high probability and worst case bounds on the quasi-static (finite past) performance of a routing algorithm imply bounds on the performance of the dynamic version of that algorithm. Our technique is particularly useful in analyzing routing on networks with bounded buffers where complicated dependices make standard queuing techniques inapplicable.We present several applications of our approach. In all cases we start from a known static algorithm, and modify it to fit our framework. In particular we give the first dynamic algorithms for routing on   a butterfly or two-dimensional mesh with bounded buffers. Both the injection rate for which the algorithm is stable, and the expected time a packet spends in the system are optimal up to constant factors. Our approach is also applicable to the recently introduced adversarial input model.
We give a data structure that allows arbitrary insertions and deletions on a planar point set P and supports basic queries on the convex hull of P, such as membership and tangent-finding. Updates take O(log1+&egr;n) amori tzed time and queries take O (log n time each, where n is the maximum size of P and &egr; is any fixed positive constant. For some advanced queries such as bridge-finding, both our bounds increase to O(log3/2n). The only previous fully dynamic solution was by Overmars and van Leeuwen from 1981 and required  O(log2n) time per update and O(log   n) time per query.We consider packet routing when packets are injected continuously into a network. We develop an adversarial theory of queuing aimed at addressing some of the restrictions inherent in probabilistic analysis and queuing theory based on time-invariant stochastic generation. We examine the stability of queuing networks and policies when the arrival process is adversarial, and provide some preliminary results in this direction. Our approach sheds light on various queuing policies in simple networks, and paves the way for a systematic study of queuing with few or no probabilistic assumptions.In this paper, we analyze the behavior of packet-switched communication networks in which packets arrive dynamically at the nodes and are routed in discrete time steps across the edges. We focus on a basic adversarial model of packet arrival and path determination for which the time-averaged arrival rate of packets requiring the use of any edge is limited to be less than 1. This model can reflect the behavior of connection-oriented networks with transient connections (such as ATM networks) as well as connectionless networks (such as the Internet).We concentrate on greedy (also known as work-conserving) contention-resolution protocols. A crucial issue that arises in such a setting is that of stability—will the number of packets in the system remain     bounded, as the system runs for an arbitrarily long period of time? We study the universal stability of network (i.e., stability under all greedy protocols) and universal stability of protocols (i.e., stability in all networks). Once the stability of a system is granted, we focus on the two main parameters that characterize its performance: maximum queue size required and maximum end-to-end delay experienced by any packet.Among other things, we show:

Our results resolve several questions posed by Borodin et al., and provide the first examples of (i) a protocol that is stable for all networks, and (ii) a protocol that is not stable for all networks.Our results resolve several questions posed by Borodin et al., and provide the first examples of (i) a protocol that is stable for all networks, and (ii) a protocol that is not stable for all networks.We define order locality to be a property of clauses relative to a term ordering. This property generalizes the subformula property for proofs where the terms appearing in proofs can be bounded, under the given ordering, by terms appearing in the goal clause. We show that when a clause set is order local, then the complexity of its ground entailment problem is a function of its structure (e.g., full versus Horn clauses), and the ordering used. We prove that, in many cases, order locality is equivalent to a clause set being saturated under ordered resolution. This provides a means of using standard resolution theorem provers for testing order locality and transforming non-local clause sets into local ones. We have used the Saturate system to automatically establish complexity bounds for a number of nontrival entailment problems relative to complexity classes which include polynomial and exponential time and co-NP.In the context of mesh-like, parallel processing computers for (i) approximating continuous space and (ii) analog simulation of the motion of objects and waves in continuous space, the present paper is concerned with which mesh-like interconnection of processors might be particularly suitable for the task and why.Processor interconnection schemes based on nearest neighbor connections in geometric lattices are presented along with motivation. Then two major threads are exploded regarding which lattices would be good: the regular lattices, for their symmetry and other properties in common with continuous space, and the well-known root lattices, for being, in a sense, the lattices required for physically   natural basic algorithms for motion.The main theorem of the present paper implies that thewell-known lattice An is the regular lattice having the maximum number of nearest neighbors among the n-dimensional regular lattices. It is noted that the only n-dimensional lattices that are both regular and root are An and Zn (Zn is the lattice of n-cubes. The remainder of the paper specifies other desirable properties of An including other ways it is superior to Zn for our purposes.
We study integrated prefetching and caching problems following the work of Cao et al. [1995] and Kimbrel and Karlin [1996]. Cao et al. and Kimbrel and Karlin gave approximation algorithms for minimizing the total elapsed time in single and parallel disk settings. The total elapsed time is the sum of the processor stall times and the length of the request sequence to be served.We show that an optimum prefetching/caching schedule for a single disk problem can be computed in polynomial time, thereby settling an open question by Kimbrel and Karlin. For the parallel disk problem, we give an approximation algorithm for minimizing stall time. The solution uses a few extra memory blocks in cache. Stall time is an important and harder to approximate measure for this problem. All of  our algorithms are based on a new approach which involves formulating the prefetching/caching problems as linear programs.The suffix tree of a string is the fundamental data structure of combinatorial pattern matching. We present a recursive technique for building suffix trees that yields optimal algorithms in different computational models. Sorting is an inherent bottleneck in building suffix trees and our algorithms match the sorting lower bound. Specifically, we present the following results. (1) Weiner [1973], who introduced the data structure, gave an optimal 0(n)-time algorithm for building the suffix tree of an n-character string drawn from a constant-size alphabet. In the comparison model, there is a trivial &Ogr;(n log n)-time lower bound based on sorting, and Weiner's algorithm matches this bound. For integer  alphabets, the fastest known algorithm is the O(n log n)time comparison-based algorithm, but no super-linear lower bound is known. Closing this gap is the main open question in stringology. We settle this open problem by giving a linear time reduction to sorting for building suffix trees. Since sorting is a lower-bound for building suffix trees, this algorithm is time-optimal in every alphabet mode. In particular, for an alphabet consisting of integers in a polynomial range we get the first known linear-time algorithm. (2) All previously known algorithms for building suffix trees exhibit a marked absence of locality of reference, and thus they tend to elicit many page faults (I/Os) when indexing very long strings. They are therefore  unsuitable for building suffix trees in secondary storage devices, where I/Os dominate the overall computational cost. We give a linear-I/O reduction to sorting for suffix tree construction. Since sorting is a trivial I/O-lower bound for building suffix trees, our algorithm is I/O-optimal.A simple variant of a priority queue, called a soft heap, is introduced. The data structure supports the usual operations: insert, delete, meld, and findmin. Its novelty is to beat the logarithmic bound on the complexity of a heap in a comparison-based model. To break this information-theoretic barrier, the entropy of the data structure is reduced by artifically raising the values of certain keys. Given any mixed sequence of n operations, a soft heap with error rate &egr; (for any 0 < &egr; ≤ 1/2) ensures that, at any time, at most &egr;n of its items have their keys raised. The amortized complexity of each operation is constant, except for insert, which takes 0(log 1/&egr;)time. The soft heap is optimal for  any value of &egr; in a comparison-based model. The data structure is purely pointer-based. No arrays are move items across the data structure not individually, as is customary, but in groups, in a data-structuring equivalent of “car pooling.” Keys must be raised as a result, in order to preserve the heap ordering of the data structure. The soft heap can be used to compute exact or approximate medians and percentiles optimally. It is also useful for approximate sorting and for computing minimum spanning trees of general graphs.A deterministic algorithm for computing a minimum spanning tree of a connected graph is presented. Its running time is 0(m &agr;(m, n)), where  &agr; is the classical functional inverse of Ackermann's function and n (respectively, m) is the number of vertices (respectively, edges). The algorithm is comparison-based : it uses pointers, not arrays, and it makes no numeric assumptions on the edge costs.We study contention resolution in a multiple-access channel such as the Ethernet channel. In the model that we consider, n users generate messages for the channel according to a probability distribution. Raghavan and Upfal have given a protocol in which the expected delay (time to get serviced) of every message is O(log n) when messages are generated according to a Bernoulli distribution with generation rate up to about 1/10. Our main results are the following protocols: (a) one in which the expected average message delay is O(1) when messages are generated according to a Bernoulli distribution with a generation rate smaller than 1/e, and (b) one in which the expected delay of any message is O(1) for an analogous  model in which users are synchronized (i.e., they agree about the time), there are potentially an infinite number of users, and messages are generated according to a Poisson distribution with generation rate up to 1/e. (Each message constitutes a new user.)To achieve (a), we first show how to simulate (b) using n synchronized users, and then show how to build the synchronization into the protocol.
Many combinatorial search problems can be expressed as 'constraint satisfaction problems'. This class of problems is known to be NP-hard in general, but a number of restricted constraint classes have been identified which ensure tractability. This paper presents the first general results on combining tractable constraint classes to obtain larger, more general, tractable classes. We give examples to show that many known examples of tractable constraint classes, from a wide variety of different contexts, can be constructed from simpler tractable classes using a general method. We also construct several new tractable classes that have not previously been identified.An architecture is described for designing systems that acquire and ma nipulate large amounts of unsystematized, or so-called commonsense, knowledge. Its aim is to exploit to the full those aspects of computational learning that are known to offer powerful solutions in the acquisition and maintenance of robust knowledge bases. The architecture makes explicit the requirements on the basic computational tasks that are to be performed and is designed to make this computationally tractable even for very large databases. The main claims are that (i) the basic learning and deduction tasks are provably tractable and (ii) tractable learning offers viable approaches to a range of issues that have been previously identified as problematic for artificial intelligence systems that are programmed. Among the issues that learning offers to resolve are robustness to inconsistencies, robustness to incomplete information and resolving among alternatives. Attribute-efficient learning algorithms, which allow learning from few examples in large dimensional systems, are fundamental to the approach. Underpinning the overall architecture is a new principled approach to manipulating relations in learning systems. This approach, of independently quantified arguments, allows propositional learning algorithms to be applied systematically to learning relational concepts in polynomial time and in modular fashion.A silver is a tetrahedon whose four vertices lie close to a plane and  whose orthogonal projection to that plane is a convex quadrilateral with no short edge. Silvers are notoriously common in 3-dimensional Delaunay triangulations even for well-spaced point sets. We show that, if the Delaunay triangulation has the ratio property introduced in Miller et al. [1995], then there is an assignment of weights so the weighted Delaunay traingulation contains no silvers. We also give an algorithm to compute such a weight assignment.We demonstrate an &Ohgr;(pn1+1/p ) lower bound on the average-case running time (uniform distribution) of p-pass Shellsort. This is the first nontrivial general lower bound for average-case Shellsort.We prove tight bounds on the time needed to solve k-set agreement. In this problem, each processor starts with an arbitrary input value taken from a fixed set, and halts after choosing an output value. In every execution, at most k distinct output values may be chosen, and every processor's output value must be some processor's input value. We analyze this problem in a synchronous, message-passing model where processors fail by crashing. We prove a lower bound of   f/k+1  degree of coordination required, and the number of faults tolerated, even in idealized models like the synchronous model. The proof of this result is interesting because it is the first to  apply topological techniques to the synchronous model.We consider comparator networks M that are used repeatedly: while the output produced by M is not sorted, it is fed again into M.  Sorting algorithms working in this way are called periodic. The number of parallel steps performed during a single run of M is called its period, the sorting time of M is the total number of parallel steps that are necessary to sort in the worst case. Periodic sorting networks have the advantage that they need little hardware (control logic, wiring, area) and that they are adaptive. We are interested in comparator networks of a constant period, due to their potential applications in hardware design.Previously,   very little was known on such networks. The fastest solutions required time O(n&egr;) where the depth was roughly 1/&egr;. We introduce a general method called periodification scheme that converts automatically an arbitrary sorting network that sorts n items in time T(n) and that has layout area A(n) into a sorting network that has period 5, sorts ***(n • T(n) items in time O(T(<n)• log n), and has layout area O(A(n)) • T(n)). In particular, applying this scheme to Batcher's algorithms, we get practical period 5 comparator networks that sort in time    O(log3n). For theoretical interest, one may use the AKS netork resulting in a period 5 comparator network with runtime O(log2n).
We present a novel divide-and-conquer paradigm for approximating NP-hard graph optimization problems. The paradigm models graph optimization problems that satisfy two properties: First, a divide-and-conquer approach is applicable. Second, a fractional spreading metric is computable in polynomial time. The spreading metric assigns lengths to either edges or vertices of the input graph, such that all subgraphs for which the optimization problem is nontrivial have large diameters. In addition, the spreading metric provides a lower bound,  t , on the cost of solving the optimization problem. We present a polynomial time approximation algorithm for problems modeled by our paradigm whose approximation factor is O(min{log     t,
 log log  t , log k log log k}) where k denotes the number of “interesting” vertices in the problem instance, and is at most the number of vertices.We present seven problems that can be formulated to fit the paradigm. For all these problems our algorithm improves previous results. The problems are: (1) linear arrangement; (2) embedding a graph in a d-dimensional mesh; (3) interval graph completion; (4) minimizing storage-time product; (5) subset feedback sets in directed graphs and multicuts in circular networks; (6) symmetric multicuts in directed networks; (7) balanced partitions and    p-separators (for small values of p) in directed graphs.We introduce resource augmentation as a method for analyzing online scheduling problems. In resource augmentation analysis the on-line scheduler is given more resources, say faster processors or more processors, than the adversary. We apply this analysis to two well-known on-line scheduling problems, the classic uniprocessor CPU scheduling problem 1 |ri, pmtn|&Sgr; Fi, and the best-effort firm real-time scheduling problem 1|ri, pmtn| &Sgr; wi( 1- Ui). It is known that there are no constant competitive nonclairvoyant on-line algorithms for these problems. We show that there are simple on-line scheduling algorithms for    these problems that are constant competitive if the online scheduler is equipped with a slightly faster processor than the adversary. Thus, a moderate increase in processor speed effectively gives the on-line scheduler the power of clairvoyance. Furthermore, the on-line scheduler can be constant competitive on all inputs that are not closely correlated with processor speed. We also show that the performance of an on-line scheduler is best-effort real time scheduling can be significantly improved if the system is designed in such a way that the laxity of every job is proportional to its length. We rework parts of the classical relational theory when the underlying domain is a structure with some interpreted operations that can be used in queries. We identify parts of the classical theory that go through 'as before' when interpreted structure is present, parts that go through only for classes of nicely behaved structures, and parts that only arise in the interpreted case. The first category include a number of results on language equivalence and expressive power characterizations for the active-domain semantics for a variety of logics. Under this semantics, quantifiers range over elements of a relational database. The main kind of results we prove here are generic collapse results: for generic queries, adding operations beyond order, does not give us extra   power.The second category includes results on the natural semantics, under which quantifiers range over the entire interpreted structure. We prove, for a variety of structures, natural-active collapse  results, showing that using unrestricted quantification does not give us any extra power. Moreover, for a variety of structures, including the real field, we give a set of algorithms for eliminating unbounded quantifications in favor of bounded ones. Furthermore, we extend these collapse results to a new class of higher-order logics that mix unbounded and bounded quantification. We give a set of normal forms for these logics, under special conditions on the interpreted structures. As a by-product, we obtain an elementary proof of the fact that parity test  is  not definable in the relational calculus with polynomial inequality constraints. We also give examples of structures with nice model-theoretic properties over which the natural-active collapse fails. Controlled stochastic systems occur in science engineering, manufacturing, social sciences, and many other cntexts. If the systems is modeled as a Markov decision process (MDP) and will run ad infinitum, the optimal control policy can be computed in polynomial time using linear programming. The problems considered here assume that the time that the process will run is finite, and based on the size of the input. There are mny factors that compound the complexity of computing the optimal policy. For instance, there are many factors that compound the complexity of this computation. For instance, if the controller does not have complete information about the state of the system, or if the system is represented in some very succint manner, the optimal policy is provably    not computable in time polynomial in the size of the input. We analyze the computational complexity of evaluating policies and of determining whether a sufficiently good policy exists for a MDP, based on a number of confounding factors, including the observability of the system state; the succinctness of the representation; the type of policy; even the number of actions relative to the number of states. In almost every case, we show that the decision problem is complete for some known complexity class. Some of these results are familiar from work by Papadimitriou and Tsitsiklis and others, but some, such as our PL-completeness proofs, are surprising. We include proofs of completeness for natural problems in the as yet little-studied classes NPPP.Motivated by a growing need to understand the computational potential  of quantum devices we suggest an approach to the relevant issues via quantum logic and its model theory. By isolating such notions as quantum parallelism and interference within a model-theoretic setting, quite divorced from their customary physical trappings, we seek to lay bare their logical underpinnings and possible computational ramifications.In the first part of the paper, a brief account of the relevant model  theory is given, and some new results are derived. In the second part, we model the simplest classical gate, namely the N-gate, propose a quantization scheme (which translates between classical and quantum models, and from which emerges a logical interpretation of the notion of quantum  parallelism), and apply it to the classical N-gate model. A class of physical instantiations of the resulting quantum N-gate model is also briefly discussed.The objective pursued in this paper is two-fold. The first part addresses the following combinatorial problem: is it possible to construct an infinite sequence over n letters where each letter is distributed as “evenly” as possible and appears with a given rate? The second objective of the paper is to use this construction in the framework of optimal routing in queuing networks. We show under rather general assumptions that the optimal deterministic routing in stochastic event graphs is such a sequence.The narrowing relation over terms constitutes the basis of the most important operational semantics of languages that integrate functional and logic programming paradigms. It also plays an important role in the definition of some algorithms of unification modulo equational theories that are defined by confluent term rewriting systems. Due to the inefficiency of simple narrowing, many refined narrowing strategies have been proposed in the last decade. This paper presents a new narrowing strategy that is optimal in several respects. For this purpose, we propose a notion of a needed narrowing step that, for inductively sequential rewrite systems, extends the Huet and Lévy notion of a needed reduction step. We define a strategy, based on this notion, that computes only needed  narrowing steps. Our strategy is sound and complete for a large class of rewrite systems, is optimal with respect to the cost measure that counts the number of distinct steps of a derivation, computes only incomparable and disjoint unifiers, and is efficiently implemented by unification.
Multivariate resultants generalize the Sylvester resultant of two polynomials and characterize the solvability of a polynomial system. They also reduce the computation of all common roots to a problem in linear algebra. We propose a determinantal formula for the sparse resultant of an arbitrary system of n + 1 polynomials in n variables. This resultant generalizes the classical one and has significantly lower degree for polynomials that are sparse in the sense that their mixed volume is lower than their Bézout number. Our algorithm uses a mixed polyhedral subdivision of the Minkowski sum of the Newton polytopes in order to construct a Newton matrix. Its determinant is a nonzero multiple of the sparse resultant and the latter equals the GCD of   at most n + 1 such determinants. This construction implies a restricted version of an effective sparse Nullstellensatz. For an arbitrary specialization of the coefficients, there are two methods that use one extra variable and yield the sparse resultant. This is the first algorithm to handle the general case with complexity polynomial in the resultant degree and simply exponential in n. We conjecture its extension to producing an exact rational expression for the sparse resultant.The need for computationally efficient decision-making techniques  together with the desire to simplify the processes of knowledge acquisition and agent specification have led various researchers in artificial intelligence to examine qualitative decision tools. However, the adequacy of such tools is not clear. This paper investigates the foundations of  maximin, minmax regret, and competitive ratio, three central qualitative decision criteria, by characterizing those behaviors that could result from their use. This 
characterizaton provides two important insights: (1)under what conditions can we employ an agent model based on these basic qualitative decision criteria, and (2) how “rational” are these decision procedures. For the     competitive ratio criterion in particular, this latter issue is of central importance to our understanding of current work on on-line algorithms. Our main result is a constructive representation theorem that uses two choice axioms to characterize maximin, minmax regret, and competitive ratio.Classically, several properties and relations of words, such as “being a power of the same word” can be expressed by using word equations. This paper is devoted to a general study of the expressive power of word equations. As main results we prove theorems which allow us to show that certain properties of words are not expressible as components of solutions of word equations. In particular, “the primitiveness” and “the equal length” are such properties, as well as being “any word over a proper subalphabet”.We study the learnability of multiplicity automata in Angluin's 
exact learning model, and we investigate its applications. Our starting point is a known theorem from automata theory relating the number of states in a minimal multiplicity automaton for a function to the rank of its Hankel matrix. With this theorem in hand, we present a new simple algorithm for learning multiplicity automata with improved time and query complexity, and we prove the learnability of various concept classes. These include (among others): 
-The class of disjoint DNF, and more generally satisfy-O(1) DNF.
-The class of polynomials over finite fields.
-The class of bounded-degree polynomials over infinite fields.
-The class of XOR of terms.
-Certain classes of boxes in high 
   dimensions.
In addition, we obtain the best query complexity for several classes known to be learnable by other methods such as decision trees and polynomials over GF(2).While multiplicity automata are shown to be useful to prove the learnability of some subclasses of DNF formulae and various other classes, we study the limitations of this method. We prove that this method cannot be used to resolve the learnability of some other open problems such as the learnability of general DNF formulas or even k-term DNF for k = &ohgr;(log n) or satisfy-s DNF formulas for s = &ohgr;(1). These results are proven by exhibiting functions in the above classes that require multiplicity automata    with super-polynomial number of states.We investigate parametric polymorphism in 
message-based concurrent programming, focusing on behavioral equivalences in a typed process calculus analogous to the polymorphic lambda-calculus of Girard and Reynolds.Polymorphism constrains the power of observers by preventing them from  directly manipulating data values whose types are abstract, leading to notions of equivalence much coarser than the standard untyped ones. We study the nature of these constraints through simple examples of concurrent abstract data types and develop basic theoretical machinery for establishing bisimilarity of polymorphic processes.We also observe some surprising interactions between polymorphism and  aliasing, drawing examples from both the polymorphic pi-calculus  and ML.
We initiate a graph-theoretic study of privacy in distributed environments with mobile eavesdroppers ("bugs"). For two privacy tasks—distributed database maintenance and message transmission—a computationally unbounded adversary “plays an eavesdrpping game,” coordinating the moment of the bugs among the sites to learn the current memory contents. Many different adversaries are considered, motivated by differences in eavesdropping technologies. We characterize the feasibility of the two privacy tasks combinatorially, construct protocols for the feasible cases, and analyze their computational complexity.A crashing network protocol is an asynchronous  protocol whose memory does not survive crashes. We show that a crashing network protocol that works over unreliable links can be driven to arbitrary global states, where each node is in a state reached in some (possibly different) execution, and each link has an arbitrary mixture of packets sent in (possibly different) executions. Our theorem considerably generalizes an earlier result, due to Fekete et al., which states that there is no correct crashing Data Link Protocol. For example, we prove that there is no correct crashing protocol for token passing and for many other resource allocation protocols such as k-exclusion, and the drinking and dining philosophers problems. We further characterize the  reachable states caused by crash failures using reliable non-FIFO and reliable FIFO links. We show that with reliable non-FIFO links any acyclic subset of nodes and links can be driven to arbitrary states. We show that with reliable FIFO links, only nodes can be driven to arbitrary states. Overall, we show a strict hierarchy in terms of the set of states reachable by crash failures in the three link models.We present a deterministic algorithm that computes st-connectivity in undirected graphs using O(log 4/3n) space. This improves the previous O(log3/2n) bound of Nisan et al. [1992].Translating linear temporal logic formulas to automata has proven to be an effective approach for implementing linear-time model-checking, and for obtaining many extensions and improvements to this verification method. On the other hand, for branching temporal logic, automata-theoretic techniques have long been thought to introduce an exponential penalty, making them essentially useless for model-checking. Recently, Bernholtz and Grumberg [1993] have shown that this exponential penalty can be avoided, though they did not match the linear complexity of non-automata-theoretic algorithms. In this paper, we show that alternating tree automata are the key to a comprehensive automata-theoretic framework for branching temporal logics. Not only can they be used to obtain  optimal decision procedures, as was shown by Muller et al., but, as we show here, they also make it possible to derive optimal model-checking algorithms. Moreover, the simple combinatorial structure that emerges from the automata-theoretic approach opens up new possibilities for the implementation of branching-time model checking and has enabled us to derive improved space complexity bounds for this long-standing problem.Completeness is an ideal, although uncommon, feature of abstract interpretations, formalizing the intuition that, relatively to the properties encoded by the underlying abstract domains, there is no loss of information accumulated in abstract computations. Thus, complete abstract interpretations can be rightly understood as optimal. We deal with both pointwise completeness, involving generic semantic operations, and (least) fixpoint completeness. Completeness and fixpoint completeness are shown to be properties that depend on the underlying abstract domains only. Our primary goal is then to solve the problem of making abstract interpretations complete by minimally extending or restricting the underlying abstract domains. Under the weak and reasonable hypothesis of dealing with  continuous semantic operations, we provide constructive characterizations for the least complete extensions and the greatest complete restrictions of abstract domains. As far as fixpoint completeness is concerned, for merely monotone semantic operators, the greatest restrictions of abstract domains are constructively characterized, while it is shown that the existence of least extensions of abstract domains cannot be, in general, guaranteed, even under strong hypotheses. These methodologies, which in finite settings give rise to effective algorithms, provide advanced formal tools for manipulating and comparing abstract interpretations, useful both in static program analysis and in semantics design. A number of examples illustrating these techniques are given.
The k-server problem is a generalization of the paging problems, and is the most studied problem in the area of competive online problems.  The Harmonic algorithm is a very natural and simple randomized algorithm for the k-server problem.  We give a simple proof that the Harmonic k-server algorithm is competitive.  The competitive ratio we prove is the best currently known fo the algorithm.  The Harmonic algorithm is memoryless and time-efficient.  This is the only such algorithm known to be competitive for the k-server problem.We identify and study a natural and frequently occurring subclass of Concurrent Read, Exclusive Write Parallel Random Access Machines (CREW-PRAMs). Called Concurrent Read, Owner Write, or CROW-PRAMS, these are machines in which each global memory location is assigned a unique “owner” processor, which is the only processor allowed to write into it.  Considering the difficulties that would be involved in physically realizinga full CREW-PRAM model and demonstrate its stability under several definitional changes.  Second, we precisely characterize the power of the CROW-PRAM by showing that the class of languages recognizable by it in time O(log n) (and implicity with a polynomial number of processors) is exactly the class LOGDCFL of languages  log space reducible to deterministic context-free languages.  Third, using the same basic machinery, we show that the recognition problem for deterministic context-free languages can be solved quickly on a deterministic auxilliary pushdown automation having random access to its input tape, a log n space work tape, and pushdown store of small maximum height. For example, time O(n1 + &egr;) is achievable with pushdown height O(log2 n). These result extend and unify work of von Braunmöhl, Cook, Mehlhorn, and Verbeek, Klein and Reif; and Rytter.We significantly improve known time bounds for solving the minimum cut problem on undirected graphs.  We use a "semiduality" between minimum cuts and maximum spanning tree packings combined with our previously developed random sampling techniques. We give a randomized (Monte Carlo) algorithm that finds a minimum cut in an m-edge, n-vertex graph with high probability in O(m log3  n) time. We also give a simpler randomized algorithm that finds all minimum cuts with high probability in O(m log3 n) time. This variant has an optimal RNC parallelization.  Both variants improve on the previous best time bound of O(n2 log3 n). Other applications of the tree-packing approach are new, nearly tight bounds on the number of near-minimum cuts a graph may have and a new data structure for representing them in a space-efficient manner.Existential second-order logic (ESO) and monadic second-order logic(MSO) have attracted much interest in logic  and computer science. ESO is a much expressive logic over successor structures than MSO. However, little was known about the relationship between MSOand syntatic  fragments of ESO. We shed light on this issue by completely characterizing this relationship for the prefix classes of ESO over strings, (i.e., finite successor structures). Moreover, we determine the complexity of model checking over strings, for all ESO-prefix classes.  Let ESO(   Q  ) denote the prefix class containing all sentences of the shape    ∃RQ4  , where R is a list of          predicate variables,  Q is a first-order predicate qualifier from the prefix set    Q  and    4   is quantifier-free. We show that ESO(   ∃*∀∃*  ) and ESO(   ∃*∀∀  ) are the maximal standard ESO-prefix classes contained in MSO, thus expressing only regular languages. We further prove the following dichotomy theorem: An ESO prefix-class either expresses only regular languages (and is thus in MSO), or it expresses some NP-complete languages. We also give a precise characterization of those ESO-prefix classes that are equivalent to MSO over strings, and of the ESO-prefix classes which are closed under complementation on strings.Shortest paths computations constitute one of the most fundamental network problems. Nonetheless, known parallel shortest-paths algorithms are generally inefficient: they perform significantly more work (product of time and processors) than their sequential counterparts. This gap, known in the literature as the “transitive closure bottleneck,” poses a long-standing open problem. Our main result is an   Omne0+s m+n1+e0    work polylog-time randomized algorithm that computes paths within (1 + O(1/polylog n) of shortest from s source nodes to all other nodes            in weighted undirected networks with n nodes and m edges (for any fixed &egr;0>0). This work bound nearly matches the    O&d5;sm    sequential time. In contrast, previous polylog-time algorithms required nearly   minO&d5; n3,O&d5; m2    work (even when s=1), and previous near-linear work algorithms required near-O(n) time. We also present faster        sequential algorithms that provide good approximate distances only between “distant” vertices: We obtain an    Om+snne 0   time algorithm that computes paths of weight (1+O(1/polylog n) dist + O(wmax polylog n), where dist is the corresponding distance and wmax is the maximum edge weight. Our chief instrument, which is of independent interest, are efficient constructions of sparse hop sets.  A (d,&egr;)-hop set of a network      G=(V,E) is a set E* of new weighted edges such that mimimum-weight d-edge paths in    V,E∪E*   have weight within (1+&egr;) of the respective distances in G. We construct hop sets of size   On1+e0    where &egr;=O(1/polylog n) and d=O(polylog n).In a linearly-typed functional language, one can define functions that consume their arguments in the process of computing their results. This is reminiscent of state transformations in imperative languages, where execition of an assignment statement alters the contents of the store. We explore this connection by translating two variations on Algol 60 into a purely functional language with polymorphic linear types. On the one hand, the translations lead to a semantic analysis of Algol-like programs, in terms of a model of the linear language. On the other hand, they demonstrate that a linearly-typed functional language can be at least as expressive as Algol.

We describe an efficient, purely functional implementation of deques with catenation. In addition to being an intriguing problem in its own right, finding a purely functional implementation of catenable deques is required to add certain sophisticated programming constructs to functional programming languages. Our solution has a worst-case running time of O(1) for each push, pop, inject, eject and catenation. The best previously known solution has an O(log*k) time bound for the kth deque operation. Our solution is not only faster but simpler. A key idea used in our result is an algorithmic technique related to the redundant digital representations used to avoid carry propagation in binary counting.The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authorative” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages” that join them together in  the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link-based analysis.In a timestamping system, processors repeatedly choose timestamps so that the order of the timestamps obtained reflects the real-time order in which they were requested. Concurrent timestamping systems permit requests by multiple processors to be issued concurrently; in bounded timestamping systems the sizes of the timestamps and the size and number of shared variables are bounded. An algorithm is wait-free if there exists an a priori bound on the number of steps a processor must take in order to make progress, independent of the action or inaction of other processors. Letting n denote the number of procesors, we construct a simple wait-free bounded concurrent timestamping system    requiring O(n) steps (accesses to shared memory) for a processor to read the current timestamps and determine the order among them, and O(n) steps to generate a timestamp, independent of the actions of the other processors. In addition, we introduce and implement the traceable use abstraction, a new primitive providing “inventory control” over values introduced by processors in the course of an algorithm execution. This abstraction has proved to be of great value in converting unbounded algorithms to bounded ones {Attiya and Rachman 1998; Dwork et al. 1992; 1993].Consider the set   H   of all linear (or affine) transformations between two vector spaces over a finite field F. We study how good   H is as a class of hash functions, namely we consider hashing a set S of size n into a range having the same cardinality n by a randomly chosen function from   H and look at the expected size of the largest hash bucket.   H is a universal class of hash functions for any finite field, but with respect to our measure different fields behave differently.If the finite  field   F has n elements, then there is a bad set S   ⊂ F2 of size n with expected maximal bucket size   H(n1/3). If n is a perfect square, then there is even a bad set with largest bucket size always at least    n. (This is worst possible, since with respect to a universal class of hash functions every set of size n has expected largest bucket size below    n +   1/2.)If, however, we consider the field of  two  elements, then we get much better bounds. The best previously known upper bound on the  expected size of the largest bucket for this class was  O(2   log n). We reduce this upper bound to O(log n log logn). Note that this is not far from the guarantee for a random function. There, the average largest bucket would be &THgr;(log n/ log log n).In the course of our proof we develop a tool which may be of independent interest. Suppose we have a subset S of a vector space D over   Z2, and consider a random linear mapping of  D to a smaller vector space R. If the cardinality of S is larger than   c&egr;|R|log|R|, then with probability 1 - &egr;, the image of S will cover all elements in the range.In this paper, we prove various results about PAC learning in the presence of malicious noise. Our main interest is the sample size behavior of learning algorithms. We prove the first nontrivial sample complexity lower bound in this model by showing that order of &egr;/&Dgr;2 + d/&Dgr; (up to logarithmic factors) examples are necessary for PAC learning any target class of {0,1}-valued functions of VC dimension d, where &egr; is the desired accuracy and &eegr; = &egr;/(1 + &egr;) - &Dgr; the malicious noise rate (it is well known that any nontrivial target class cannot be PAC learned with accuracy &egr; and malicious noise rate &eegr; ≥ &egr;/(1 + &egr;), this irrespective to sample complexity). We also show that this result  cannot be significantly improved in general by presenting efficient learning algorithms for the class of all subsets of d elements and the class of unions of at most d intervals on the real line. This is especialy interesting as we can also show that the popular minimum disagreement strategy needs samples of size d &egr;/&Dgr;2, hence is not optimal with respect to sample size. We then discuss the use of randomized hypotheses. For these the bound &egr;/(1 + &egr;) on the noise rate is no longer true and is replaced by 2&egr;/(1 + 2&egr;). In fact, we present a generic algorithm using randomized hypotheses that can tolerate noise rates slightly larger than &egr;/(1 + &egr;) while using samples of size  d/&egr; as in the noise-free case. Again one observes a quadratic powerlaw (in this case d&egr;/&Dgr;2, &Dgr; = 2&egr;/(1 + 2&egr;) - &eegr;) as &Dgr; goes to zero. We show upper and lower bounds of this order.This paper studies the problem of efficiently schedulling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is “work stealing,” in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies.Specifically, our analysis shows that the expected time to execute a fully strict computation on P processors using our work-stealing scheduler is T1/P + O(T  ∞ , where  T1 is the minimum serial execution time of the multithreaded computation and (T  ∞ is the minimum execution time with an infinite number of processors. Moreover, the space required by the execution is at most S1P, where  S1 is the minimum serial space requirement. We also show that the expected total communication of the algorithm is at most O(PT  ∞ ( 1 +  nd)Smax), where Smax is the size of the largest activation record of any thread and  nd is the maximum number of times that any thread synchronizes with its parent. This communication bound justifies the folk wisdom that work-stealing schedulers are more communication efficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.We develop principles and rules for achieving secrecy properties in security protocols. Our approach is based on traditional classification techniques, and extends those techniques to handle concurrent processes that use shared-key cryptography. The rules have the form of typing rules for a basic concurrent language with cryptographic primitives, the spi calculus. They guarantee that, if a protocol typechecks, then it does not leak its secret inputs.
Evolution can be mathematically modelled by a stochastic process that operates on the DNA of species. Such models are based on the established theory that the DNA sequences, or genomes, of all extant species have been derived from the genome of the common ancestor of all species by a process of random mutation and natural selection.A stochastic model of evolution can be used to construct phylogenies, or evolutionary trees, for a set of species. Maximum Likelihood Estimation (MLE) methods seek the evolutionary tree which is most likely to have produced the DNA under consideration. While these methods are intellectually satisfying, they have not been widely accepted because of their computational intractability.In this paper, we address the intractability of MLE   methods as follows: We introduce a metric on stochastic process models of evolution. We show that this metric is meaningful by proving that in order for any algorithm to distinguish between two stochastic models that are close according to this metric, it needs to be given many observations. We complement this result with a simple and efficient algorithm for inverting the stochastic process of evolution, that is, for building a tree from observations on two-state characters. (We will use the same techniques in a subsequent paper to solve the problem for multistate characters, and hence for building a tree from DNA sequence data.) The tree we build is provably close, in our metric, to the tree generating the data and gets closer as more observations become available.Though there  have been many heuristics suggested for the problem of finding good approximations to the most likely tree, our algorithm is the first one with a guaranteed convergence rate, and further, this rate is within a polynomial of the lower-bound rate we establish. Ours is also the first polynomial-time algorithm that is proven to converge at all to the correct tree.We present a primality proving algorithm—a probablistic primality test that produces short certificates of primality on prime inputs. We prove that the test runs in expected polynomial time for all but a vanishingly small fraction of the primes. As a corollary, we obtain an algorithm for generating large certified primes with distribution statistically close to uniform. Under the conjecture that the gap between consecutive primes is bounded by some polynomial in their size, the test is shown to run in expected polynomial time for all primes, yielding a Las Vegas primality test.Our test is based on a new methodology for applying group theory to the problem of prime certification, and the application of this methodology using groups generated by elliptic curves over  finite fields.We note that our methodology and methods have been subsequently used and improved upon, most notably in the primality proving algorithm of Adleman and Huang using hyperelliptic curves and in practical primality provers using elliptic curves.The pairing heap is well regarded as an efficient data structure for implementing priority queue operations. It is included in the GNU C++ library. Strikingly simple in design, the pairing heap data structure nonetheless seems difficult to analyze, belonging to the genre of self-adjusting  data structures. With its design originating as a self-adjusting analogue of the Fibonacci heap, it has been previously conjectured that the pairing heap provides constrant amortized time decrease-key operations, and experimental studies have supported this conjecture. This paper demonstrates, contrary to conjecture, that the pairing heap requires more than constant amortized time to perform decrease-key operations. Moreover, new experimental findings are presented that reveal detectable growth in  the amortized cost of the decrease-key operation.Second, a unifying framework is developed that includes both pairing heaps and Fibonacci heaps. The parameter of interest in this framework is the storage capacity available in the nodes of the data structure for auxiliary balance information fields. In this respect Fibonacci heaps require log log n bits per node when n items are present. This is shown to be asymptotically optimal for data structures that achieve the same asymptotic performance bounds as Fibonacci heaps and fall within this framework.This paper solves a longstanding open problem in fully dynamic algorithms: We present the first fully dynamic algorithms that maintain connectivity, bipartiteness, and approximate minimum spanning trees in polylogarithmic time per edge insertion or deletion. The algorithms are designed using a new dynamic technique that combines a novel graph decomposition with randomization. They are Las-Vegas type randomized algorithms which use simple data structures and have a small constant factor.Let n denote the number of nodes in the graph. For a sequence of &OHgr;(m0) operations, where m0 is the number of edges in the initial graph, the expected time for p updates is     O(p log3 n) (througout the paper the logarithms are based 2) for connectivity and bipartiteness. The worst-case time for one query is O(log n/log log n). For the k-edge witness problem (“Does the removal of k given edges disconnect the graph?”) the expected time for p updates is O(p log3 n) and the expected time for q queries is O(qk log3 n). Given a graph with k different weights, the minimum spanning tree can be maintained during   a   sequence of p updates in expected time O(pk log3 n). This implies an algorithm to maintain a 1 + &egr;-approximation of the minimum spanning tree 
in expected time
 O((p log3 n logU)/&egr;) for p updates, where the weights of the edges are between 1 and U.We introduce a search problem called “mutual search” where k agents, arbitrarily distributed over n sites, are required to locate one another by posing queries of the form “Anybody at site i?”. We ask for the least number of queries that is necessary and sufficient. For the case of two agents using deterministic protocols, we obtain the following worst-case results: In an oblivious setting (where all pre-planned queries are executed), there is no savings: n−1 queries are required and are sufficient. In a nonoblivious setting, we can exploit the paradigm of “no news is also news” to obtain significant savings: in the synchronous case 0.586n queries are    required; in the asynchronous case  0.896n queries suffice and a fortiori  0.536n queries are required; for   on  agents using a synchronous deterministic protocol less than n queries suffice; there is a simple randomized protocol for two agents with worst-case expected 0.5n queries and all radomized protocols require at least 0.25n worst-case expected queries. The graph-theoretic framework we formulate for expressing and analyzing algorithms for this problem may be of independent interest.In this paper, we give a new algorithm for quantifier elimination in the first order theory of real closed fields that improves the complexity of the best known algorithm for this problem till now. Unlike previously known algorithms [Basu et al. 1996; Renegar 1992; Heintz et al. 1990], the combinatorial part of the complexity (the part depending on the number of polynomials in the input) of this new algorithm is independent of the number of free variables. Moreover, under the assumption that each polynomial in the input depends only on a constant number of the free variables, the algebraic part of the complexity(the part depending on the degrees of the input polynomials) can also be made independent of the number of free variables. This new feature of our algorithm allow us to obtain   a new algorithm for a variant of the quantifier elimination problem. We give an almost optimal algorithm for this new problem, which we call the uniform quantifier elimination problem.Using tthe uniform quantifier elimination algorithm, we give an algorithm for solving a problem arising in the field of constraint databases with real polynomial constraints. We give an algorithm for converting a query with natural domain semantics to an equivalent one with active domain semantics. A nonconstructive version of this result was proved in Benedikt et al. [1998]. Very recently, a constructive proof was also given independently in Benedikt and Libkin [1997]. However, complexity issues were not considered and no algorithm with a reasonable complexity bound was known  for  this latter problem till now.We also point out interesting logical consequences of this algorithmic result, concerning the expressive power of a constraint query language over the reals. This leads to simpler and constructive proofs for these inexpressibility results than the ones known before.Moreover, our improved algorithm for performing quantifier elimination immediately leads to  improved algorithms for several problems for which quantifier elimination is a basic step, for example, the problem of computing the closure of a given semi-algebraic set.The problem of finding the circular attributes in an grammar is considered. Two algorithms are proposed: the first is polynomial but yields conservative results while the second is exact but is potentially expontial. It is also shown that finding the circular attributes is harder than testing circularity.
We focus on a rich axiomatization for actions in the situation 
calculus that includes, among other features, a solution to the frame problem for deterministic actions. Our work is foundational in nature, directed at simplifying the entailment problem for these axioms. Specifically, we make four contributions to the metatheory of situation calculus axiomatizations of dynamical systems:(1) We prove that the above-mentioned axiomatization for actions has a relative satisfiability property; the full axiomatization is satisfiable iff the axioms for the initial state are.(2)We define the concept of regression relative to these axioms, and prove a soundness and completeness theorem for a regression-based approach to the entailment problem for a wide class of  queries.(3) Our formalization of the situation calculus requires certain foundational axioms specifying the domain of situations. These include an induction axiom, whose presence complicates human and automated reasoning in the situation calculus. We characterize various classes of sentences whose proofs do not require induction, and in some cases, some of the other foundational axioms.(4)We prove that the logic programming language GOLOG never requires any of the foundational axioms for the evaluation of programs.The single-source shortest paths problem (SSSP) is one of the classic problems in algorithmic graph theory: given a positively weighted graph G with a source vertex s, find the shortest path from s to all other vertices in the graph.Since 1959, all theoretical developments in SSSP for general directed and undirected graphs have been based on Dijkstra's algorithm, visiting the vertices in order of increasing distance from s. Thus, any implementation of Dijkstra's algorithm sorts the vertices according to their distances from s. However, we do not know how to sort in linear time.Here, a deterministic linear time and linear space algorithm is presented for the undirected single  source shortest paths problem with positive integer weights. The algorithm avoids the sorting bottleneck by building a hierarchical bucketing structure, identifying vertex pairs that may be visited in any order.The approximate string matching problem is to find all locations at which a query of lengthm matches a substring of a text of length n with k-or-fewer differences. Simple and practical bit-vector algorithms have been designed for this problem, most notably the one used in agrep. These algorithms compute a bit representation of the current state-set of the k-difference automaton for the query, and asymptotically run in either O(nm/w) or O(nm log &sgr;/w) time where w is the word size of the machine (e.g., 32 or 64 in practice), and &sgr; is the size of the pattern alphabet. Here we present an algorithm  of comparable simplicity that  requires only O(nm/w) time by virtue of computing a bit representation of the relocatable dynamic programming matrix for the problem. Thus, the algorithm's performance is independent of k, and it is found to be more efficient than the previous results for many choices of k and 
smallm.Moreover, because the algorithm is not 
dependent on k, it can be used to rapidly compute blocks of the dynamic programming matrix as in the 4-Russians algorithm of Wu et al.(1996). This gives rise to an O(kn/w) expected-time algorithm for the case where m may be arbitrarily large. In practice this new algorithm,  that computes a region of the dynamic progr amming (d.p.) matrx w entries at a time using the basic algorithm as a subroutine is significantly faster  than our previous 4-Russians algorithm, that computes the same region 4 or 5 entries at a time using table lookup. This performance improvement yields a code that is either superior or competitive with all existing algorithms except for some filtration algorithms that are superior when k/m is sufficiently small.The structural tree-based mapping algorithm is an efficient and 
popular technique for technology mapping. In order to make good use of this mapping technique in FTGA design, it is desirable to design FPGA logic modules based on Boolan functions which can be represented by a tree of gates (i.e., series-parallel or SP functions). Thakur and Wong [1996a; 1996b] studied this issue and they demonstrated the advantages of designing logic modules as universal SP functions, that is, SP functions which can implement all SP functions with a certain number of inputs. The number of variables in the universal function corresponds to the number of inputs to the FPGA module, so it is desirable to have as few variables as possible in the constructed functions. The universal SP functions presented in  Thakur and Wong [1996a; 1966b] were designed manually. Recently, there is an algorithm that can generate these functions automatically [Young and Wong 1997], but the number of variables in the generated functions grows exponentially. In this paper, we present an algorithm to generate, for each n > 0, a universal SP function fn for implementing all SP functions with n inputs or less. The number of variables in fn is less than n2.376 and the constructions are the smallest possible when n is small (n ≤ 7). We also derived a nontrival lower bound on the sizes of the optimal universal SP functions  (&OHgr;(n log n)).
We consider the problem of deciding whether a polygonal knot in 3-dimensional Euclidean space is unknotted, ie., capable of being continuously deformed without self-intersection so that it lies in a plane. We show that this problem, UNKNOTTING PROBLEM is in NP. We also consider the problem, SPLITTING PROBLEM of determining whether two or more such polygons can be split, or continuously deformed without self-intersection so that they occupy both sides of a plane without intersecting it. We show that it also is in NP. Finally, we show that the problem of determining the genus of a polygonal knot (a generalization of the problem of determining whether it is unknotted) is in PSPACE. We also give exponential worst-case running time bounds for deterministic algorithms to solve each of these problems. These algorithms are based on the use of normal surfaces and decision procedures due to W. Haken, with recent extensions by W. Jaco and J. L. Tollefson.A number of current technologies allow for the determination of interatomic distance information in structures such as proteins and RNA. Thus, the reconstruction of a three-dimensional set of points using information about its interpoint distances has become a task of basic importance in determining molecular structure. The distance measurements one obtains from techniques such as NMR are typically sparse and error-prone, greatly complicating the reconstruction task. Many of these errors result in distance measurements that can be safely assumed to lie within certain fixed tolerances. But a number of sources of systematic error in these experiments lead to inaccuracies in the data that are very hard to quantify; in effect, one must treat certain entries of the measured distance  matrix as being arbitrarily “corrupted.”The existence of arbitrary errors leads to an interesting sort of error-correction problem—how many corrupted entries in a distance matrix can be efficiently corrected to produce a consistent three-dimensional structure? For the case of an n × n matrix in which every entry is specified, we provide a randomized algorithm running in time O(n log n) that enumerates all structures consistent with at most (1/2-&egr;)n errors per row, with high probability. In the case of randomly located errors, we can correct errors of the same density in a sparse matrix-one in which only a &bgr; fraction of the entries in each row are given, for any constant &bgr;gt;0.We introduce a new text-indexing data structure, the String B-Tree, that can be seen as a link between some traditional external-memory and string-matching data structures. In a short phrase, it is a combination of B-trees and Patricia tries for internal-node indices that is made more effective by adding extra pointers to speed up search and update operations. Consequently, the String B-Tree overcomes the theoretical limitations of inverted files, B-trees, prefix B-trees, suffix arrays, compacted tries and suffix trees. String B-trees have the same worst-case performance as B-trees but they manage unbounded-length strings and perform much more powerful search operations such as the ones supported by suffix trees. String B-trees are also effective in main memory (RAM  model) because they improve the online suffix tree search on a dynamic set of strings. They also can be successfully applied to database indexing and software duplication.Many high-level parallel programming languages allow for fine-grained parallelism. As in the popular work-time framework for parallel algorithm design, programs written in such languages can express the full parallelism in the program without specifying the mapping of program tasks to processors. A common concern in executing such programs is to schedule tasks to processors dynamically so as to minimize not only the execution time, but also the amount of space (memory) needed. Without careful scheduling, the parallel execution on p processors can use a factor of p or larger more space than a sequential implementation of the same program.This paper first identifies a class of parallel schedules that are provably efficient in both time and      space. For any computation with w units of work and critical path length
 d, and for any sequential schedule that takes space s1, we provide a parallel schedule that takes fewer than w/p + d steps on p processors and requires less than s1 +   p˙d  space. This matches the lower bound that we show, and significantly improves upon the best previous bound of   s1˙p  spaces for the common case where d<<s1.The paper then describes a scheduler for implementing high-level languages with nested  parallelism, that generates schedules in this class.  During program execution, as the structure of the computation is revealed, the scheduler keeps track of the active tasks, allocates the tasks to the processors, and performs the necessary task synchronization. The scheduler is itself a parallel algorithm, and incurs at most a constant factor overhead in time  and space, even when the scheduling granularity is individual units of work. The algorithm is the first efficient solution to the scheduling problem discussed here, even if space considerations are ignored.
Genomes frequently evolve by reversals &rgr;(i,j) that transform a gene order &pgr;1 … &pgr;i&pgr;i+1 … &pgr;j-1&pgr;j … &pgr;n into &pgr;1 … &pgr;i&pgr;j-1 … &pgr;i+1&pgr;j … &pgr;n. Reversal distance between permutations &pgr; and &sgr;is the minimum number of reversals to transform &pgr; into &Agr;. Analysis of genome rearrangements in molecular biology started in the late 1930's, when Dobzhansky  and Sturtevant published a milestone paper presenting a rearrangement scenario with 17 inversions between the species of Drosophilia. Analysis of genomes evolving by inversions leads to a combinatorial problem of sorting by reversals studied in detail recently. We study sorting of signed permutations by reversals, a problem that adequately models rearrangements in a small genomes like chloroplast or mitochondrial DNA. The previously suggested approximation algorithms for sorting signed permutations by reversals compute the reversal distance between permutations with an astonishing accuracy for both simulated and biological data. We prove a duality theorem explaining this intriguing performance and show that there exists a  “hidden” parameter that allows one to compute the reversal distance between signed permutations in polynomial time.This paper introduces compressed certificates for planarity, biconnectivity and triconnectivity in planar graphs, and proves many structural properties of certificates in planar graphs. As an application of our compressed certificates, we develop efficient dynamic planar algorithms. In particular, we consider the following three operations on a planar graph G: (i) insert an edge if the resultant graph remains planar; (ii) delete an edge; and (iii) test whether an edge could be added to the graph without violating planarity. We show how to support each of the above operations in O(n2/3) time, where n is the number of vertices in the graph. The bound for tests and deletions is worst-case, while the bound for  insertions is amortized. This is the first algorithm for this problem with sub-linear running time, and it affirmatively answers a question posed in Epstein et al. [1992]. We use our compressed certificates for biconnectivity and triconnectivity to maintain the biconnected and triconnected components of a dynamic planar graph. The time bounds are the same: O(n2/3) worst-case time per edge deletion, O(n2/3) amortized time per edge insertion, and O(n2/3) amortized time per edge insertion, and O(n2/3)worst-case time to check whether two vertices are either biconnected or triconnected.This paper analyzes a recently published algorithm for page replacement in hierarchical paged memory systems [O'Neil et al. 1993]. The algorithm is called the LRU-K method, and reduces to the well-known LRU (Least Recently Used) method for K = 1. Previous work [O'Neil et al. 1993; Weikum et al. 1994; Johnson and Shasha 1994] has shown the effectiveness for K > 1 by simulation, especially in the most common case of K = 2. The basic idea in LRU-K is to keep track of the times of the last K references to memory pages, and to use this statistical information to rank-order the pages as to their expected future behavior. Based on this the page replacement policy decision is made: which  memory-resident page to replace when a newly accessed page must be read into memory. In the current paper, we prove, under the assumptions of the independent reference model, that LRU-K is optimal. Specifically we show: given the times of the (up to) K most recent references to each disk page, no other algorithm A making decisions to keep pages in a memory buffer holding n - 1 pages based on this infomation can improve on the expected number of I/Os to access pages over the LRU-K algorithm using a memory buffer holding n pages. The proof uses the Bayesian formula to relate the space of actual page probabilities of the model to the space of observable page numbers on which the replacement decision is  acutally made.This paper has two agendas. One is to develop the foundations of round-off in computation. The other is to describe an algorithm for deciding feasibility for polynomial systems of equations and inequalities together with its complexity analysis and its round-off properties. Each role reinforces the other.
Consider a set of S of n data points  in real d-dimensional space, Rd, where distances are measured using any Minkowski metric. In nearest neighbor searching, we preprocess S into a data structure, so that given any query point q ∈ Rd, is the closest point of S to q can be reported quickly. Given any positive real &egr;, data point p is a (1 +&egr;)-approximate nearest neighbor of q if its distance from q is within a factor of (1 + &egr;) of the distance to the true nearest neighbor. We show that it is possible to preprocess a    set of n points in     Rd in O(dn log n) time and O(dn) space, so that given a query point  q  ∈ Rd, and &egr; > 0, a (1 + &egr;)-approximate nearest neighbor of q can be computed in O(cd, &egr; log n) time, where cd,&egr;≤d  1 + 6d/e;d is a factor depending only on dimension and &egr;. In general, we show that given an integer k ≥ 1, (1 + &egr;)-approximations  to the  k nearest neighbors of q can  be computed in additional O(kd log n) time.The following problems that arise in the computation of electrostatic forces and in the Boundary Element Method are considered. Given two convex interior-disjoint polyhedra in 3-space endowed with a volume charge density which is a polynomial in the Cartesian coordinates of R3, compute the Coulomb force acting  on them. Given two interior-disjoint polygons in 3-space endowed with a surface charge density which is polynomial in the Cartesian coordinates of R3, compute the normal component of the Coulomb force acting on them. For both problems adaptive Gaussian approximation algorithms are given, which, for n Gaussian points, in time O(n), achieve absolute error     Oc-n   for a constant c > 1. Such a result improves upon previously known best asymptotic bounds. This result is achieved by blending techniques from integral geometry, computational geometry and numerical analysis. In particular, integral geometry is used in order to represent the forces as integrals whose kernal is free from singularities.Publicly accessible databases are an indispensable resource for retrieving up-to-date information. But they also pose a significant risk to the privacy of the user, since a curious database operator can follow the user's queries and infer what the user is after. Indeed, in cases where the users' intentions are to be kept secret, users are often cautious about accessing the database. It can be shown that when accessing a single database, to completely guarantee the privacy of the user, the whole database should be down-loaded; namely n bits should be communicated (where n is the number of bits in the database).In this work, we investigate whether by replicating the database, more efficient solutions to the private retrieval problem can be  obtained. We describe schemes that enable a user to access k replicated copies of a database (k≥2) and privately retrieve information stored in the database. This means that each individual server (holding a replicated copy of the database) gets no information on the identity of the item retrieved by the user. Our schemes use the replication to gain substantial saving. In particular, we present a two-server scheme with communication complexity O(n1/3).In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of “robust” learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a  noise rate approaching the  information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.We propose inference systems for binary relations that satisfy composition laws such as transitivity. Our inference mechanisms are based on standard techniques from term rewriting and represent a refinement of chaining methods as they are used in the context of resolution-type theorem proving. We establish the refutational completeness of these calculi and prove that our methods are compatible with the usual simplification techniques employed in refutational theorem provers, such as subsumption or tautology deletion. Various optimizations of the basic chaining calculus will be discussed for theories with equality and for total orderings. A key to the practicality of chaining methods is the extent to which so-called variable chaining can be avoided. We demonstrate that rewrite  techniques considerably restrict variable chaining and that further restrictions are possible if the transitive relation under consideration satisfies additional properties, such as symmetry. But we also show that variable chaining cannot be completely avoided in general.A class of parallel algorithms for evaluating game trees is presented. These algorithms parallelize a standard sequential algorithm for evaluating AND/OR trees and the &agr;-&bgr; pruning procedure for evaluating MIN/MAX trees. It is shown that, uniformly on all instances of uniform AND/OR trees, the parallel AND/OR tree algorithm achieves an asymptotic linear speedup using a polynomial number of processors in the height of the tree. The analysis of linear speedup using more than a linear number of processors is due to J. Harting. A numerical lower bound rigorously establishes a good speedup for the uniform AND/OR trees with parameters that are typical in practice. The performance of the parallel &agr;-&bgr; algorithm on best-ordered MIN/MAX trees is analyzed.
We present a polynomial time approximation scheme for Euclidean TSP in fixed dimensions. For every fixed c > 1 and given any n nodes in R 2, a randomized version of the scheme finds a (1 + 1/c)-approximation to the optimum traveling salesman tour in O(n(log n)O(c)) time. When the nodes are in R d, the running time increases to O(n(log n)(O(d c))d-1). For every fixed c,    d the running time is n • poly(logn), that is nearly linear in n. The algorithmm can be derandomized, but this increases the running time by  a  factor O(nd). The previous best approximation algorithm for the problem (due to Christofides) achieves a 3/2-aproximation in polynomial time.We also give similar approximation schemes for some other NP-hard Euclidean problems: Minimum Steiner Tree, k-TSP, and k-MST. (The running times of the algorithm for k-TSP and k-MST involve an additional multiplicative factor k.) The previous best approximation algorithms for all these problems    achieved a constant-factor approximation. We also give efficient approximation schemes for Euclidean Min-Cost Matching, a problem that can be solved exactly in polynomial time.All our algorithms also work, with almost no modification, when distance is measured using any geometric norm (such as ℓ p for p ≥ 1 or other Minkowski norms). They also have simple parallel (i.e., NC) implementations.We introduce a new approach to the maximum flow problem. This approach is based on assigning arc lengths based on the residual flow value and the residual arc capacities. Our approach leads to an O(min(n2/3, m1/2)m log(n2/m) log U) time bound for a network with n vertices, m arcs, and integral arc capacities in the range [1, …, U]. This is a fundamental improvement over the previous time bounds. We also improve bounds for the Gomory-Hu tree problem, the parametric flow problem, and the approximate s-t  cut problem.We demonstrate the power of object identities (oids) as a database query language primitive. We develop an object-based data model, whose structural part generalizes most of the known complex-object data models: cyclicity is allowed in both its schemas and instances. Our main contribution is the operational part of the data model, the query language IQL, which uses oids for three critical purposes: (1) to represent data-structures with sharing and cycles, (2) to manipulate sets, and (3) to express any computable database query. IQL can be type checked, can be evaluated bottom-up, and naturally generalizes most popular rule-based languages. The model can also be extended to incorporate type inheritance, without changes to IQL.  Finally, we investigate an analogous value-based data  model, whose structural part is founded on regular infinte trees and whose operational part is IQL.The “waite-free hierarchy” provides a classification of multiprocessor synchronization primitives based on the values of n for which there are deterministic wait-free implementations of n-process consensus using instances of these objects and read-write registers. In a randomized wait-free setting, this classification is degenerate, since n-process consensus can be solved using only O(n) read-write registers.In this paper, we propose a classification of synchronization primitives based on the space complexity of randomized solutions to n-process consensus. A historyless object, such as a read-write register, a   swap register, or a test&set register, is an object whose state depends only on the lost nontrivial operation thate was applied to it. We show that, using historyless objects, &OHgr;(n  object instances are necessary to solve n-process consensus. This lower bound holds even if the objects have unbounded size and the termination requirement is nondeterministic solo termination, a property strictly weaker than randomized wait-freedom.We then use this result to related the randomized space complexity of basic multiprocessor synchronization primitives such as shared counters, fetch&add registers, and compare&swap registers. Viewed collectively, our results   imply that there is a separation based on space complexity for synchronization primitives in randomized computation, and that this separation differs from that implied by the deterministic “wait-free hierarchy.”We present an efficient algorithm for PAC-learning a very general class of geometric concepts over R d for fixed d. More specifically, let T  be any set of s halfspaces. Let x =(x1, …, xd) be an arbitrary point in R d. With each 
t∈T  we associate a boolean indicator function It(x) which is 1 if and only if x is in the halfspace t. The  concept class, Cds , that we study consists of all  concepts formed by any Boolean function over It1, …, Its for   ti ∈T . This class is much more general than any geometric concept class known to be PAC-learnable. Our results can be extended easily to learn efficiently any Boolean combination of a polynomial number of concepts selected from any concept class C  over R  given that the VC-dimension of C  has dependence only on d and there is a polynomial time algorithm to determine if there is a concept from  C  consistent with a given set of labeled examples. We also present a statistical query version of our algorithm that can tolerate random classification noise. Finally we present a generalization of the  standard &egr;-net result of Haussler and Welzl [1987] and apply it to give an alternative noise-tolerant algorithm for  d = 2 based on geometric subdivisions.
Directory-based coherence protocols in shared-memory multiprocessors are so complex that verification techniques based on automated procedures are required to establish their correctness. State enumeration approaches are well-suited to the verification of cache protocols but they face the problem of state space explosion, leading to unacceptable verification time and memory consumption even for small system configurations. One way to manage this complexity and make the verification feasible is to map the system model to verify onto a symbolic state model (SSM). Since the number of symbolic states is considerably less than the number of system states, an exhaustive state search becomes possible, even for large-scale sytems and complex protocols.In this paper, we develop the  concepts and notations to verifiy some properties of a directory-based protocol designed for non-FIFO interconnection networks. We compare the verification of the protocol with SSM and with the Stanford Mur  4 , a verification tool enumerating system states. We show that SSM is much more efficient in terms of verification time and memory consumption and therefore holds that promise of verifying much more complex protocols. A unique feature of SSM is that it verifies protocols for any system size and therefore provides reliable verification results in one run of the tool.A database query is finite if its result consists of a finite sets tuples. For queries formulated as sets of pure Horn rules, the problem of determining finiteness is, in general, undecidable.In this paper, we consider superfiniteness—a stronger kind of finiteness, which applies to Horn queries whose function symbols are replaced by the abstraction of infinite relations with finiteness constraints (abbr., FC's). We show that superfiniteness is not only decidable but also axiomatizable, and the axiomatization yields an effective decision procedure. Although there are finite queries that are not superfinite, we demonstrate that superfinite queries represent an interesting and  nontrivial subclass within the class of all finite queries.The we turn to the issue of inference of finiteness constraints—an important practical problem that is instrumental in deciding if a query is evaluable by a bottom-up algorithm. Although it is not known whether FC-entailment is decidable for sets of function-free Horn rules, we show that super-entailment, a stronger form of entailment, is decidable. We also show how a decision procedure for super-entailment can be used to enhance tests for query finiteness.Given a collection

 F
 of subsets of S =
{1,…,n}, set
cover is the problem of selecting as few as possible
subsets from   F such that their union covers
S,, and max
k-cover is the problem of selecting
k subsets from
  F such that their union has maximum cardinality. Both these problems are
NP-hard.   We prove that (1 - o(1)) ln
n is a threshold below  

 which set
cover cannot be approximated efficiently, unless NP has slightly
superpolynomial time algorithms. This closes the gap (up to low-order
terms) between the ratio of approximation achievable by the greedy
alogorithm (which is (1 - o(1)) ln
n), and provious results of Lund and Yanakakis, that showed hardness of
approximation within a ratio of 

log2
n/2≃0.72
 ln n. For max
k-cover, we show an approximation
threshold of (1 - 1/e)(up to
low-order terms), under assumption that 

P≠NP
.In this paper, we consider the question of determining whether a function f has property P or is &egr;-far from any function with property P. A property testing algorithm is given a sample of the value of f on instances drawn according to some distribution. In some cases, it is also allowed to query f on instances of its choice. We study this question for different properties and establish some connections to problems in learning theory and approximation.In particular, we focus our attention on testing graph properties. Given access to a graph G in the form of being able to query whether an edge exists or not between a pair of vertices, we devise algorithms to test whether the underlying graph has  properties such as being bipartite, k-Colorable, or having a p-Clique (clique of density p with respect to the vertex set). Our graph property testing algorithms are probabilistic and make assertions that are correct with high probability, while making a number of queries that is independent of the size of the graph. Moreover, the property testing algorithms can be used to efficiently (i.e., in time linear in the number of vertices) construct partitions of the graph that correspond to the property being tested, if it holds for the input graph.
This paper examines numerical issues in computing solutions to networks of stochastic automata. It is well-known that when the matrices that represent the automata contain only constant values, the cost of performing the operation basic to all iterative solution methods, that of matrix-vector multiply, is given by    rN=i=1N ni×i=1N ni    where ni is the number of states in  the ith automaton and N is the number of automata in the network. We introduce the concept of a generalized tensor product and prove a number of  lemmas concerning this product. The result of these lemmas allows us to show that this relatively small number of operations is sufficient in many practical cases of interest in which the automata contain functional and not simply constant transitions. Furthermore, we show how the automata should be ordered to achieve this.We examine a class of collective coin-flipping games that arises from randomized distributed algorithms with halting failures. In these games, a sequence of local coin flips is generated, which must be combined to form a single global coin flip. An adversary monitors the game and may attempt to bias its outcome by hiding the result of up to t local coin flips. We show that to guarantee at most constant bias, &ohgr;(t2) local coins are needed, even if (a) the local coins can have arbitrary distributions and ranges, (b) the adversary is required to decide immediately wheter to hide or reveal each local coin, and (c) the game can detect which local coins have been hidden. If the   adversary is permitted to control the outcome of the coin except for cases whose probability is polynomial in t, &ohgr;(t2/log2t) local coins are needed. Combining this fact with an extended version of the well-known Fischer-Lynch-Paterson impossibility proof of deterministic consensus, we show that given an adaptive adversary, any t-resilient asynchronous consensus protocol requires &ohgr;(t2/log2t) local coin flips in any model that can be simulated deterministically using atomic registers. This gives the first nontrivial lower bound on the total work required by wait-free consensus and is tight to within   logarithmic factors.Wait-free implementations of shared objects tolerate the failure of processes, but not the failure of base objects from which they are implemented. We consider the problem of implementing shared objects that tolerate the failure of both processes and base objects.We identify two classes of object failures: responsive and nonresponsive. With responsive failures, a faulty object responds to every operation, but its responses may be incorrect. With nonresponsive failures, a faulty object may also “hang” without responding. In each class, we define crash, omission, and arbitrary modes of failure.We show that all responsive failure modes can be tolerated. More precisely, for all   responsive failure modes  F, object types T, and t &ohgr; 0, we show how to implement a shared object of type T which is t-tolerant for  F. Such an object remains correct and wait-free even if up to t base objects fail according to  F. In contrast to responsive failures, we show that even the most benign non-responsive failure mode cannot be tolerated. We also show that randomization can be used to circumvent this impossibility result.Graceful degradation is a desirable property of fault-tolerant implementations: the implemented object never fails more severely than the base objects it is derived from, even if all the base objects fail. For several failure modes, we show  wheter  this property can be achieved, and, if so, how.We show that every language in NP has a probablistic verifier that checks membership proofs for it using logarithmic number of random bits and by examining a constant number of bits in the proof. If a string is in the language, then there exists a proof such that the verifier accepts with probability 1 (i.e., for every choice of its random string). For strings not in the language, the verifier rejects every provided “proof” with probability at least 1/2. Our result builds upon and improves a recent result of Arora and Safra [1998] whose verifiers examine a nonconstant number of bits in the proof (though this number is a very slowly growing function of the input length).As a consequence, we prove that no MAX SNP-hard problem has a polynomial  time approximation scheme, unless NP = P. The class MAX SNP was defined by Papadimitriou and Yannakakis [1991] and hard problems for this class include vertex cover, maximum satisfiability, maximum cut, metric TSP, Steiner trees and shortest superstring. We also improve upon the clique hardness results of Feige et al. [1996] and Arora and Safra [1998] and show that there exists a positive &egr; such that approximating the maximum clique size in an N-vertex graph to within a factor of N&egr; is NP-hard.
We consider the problem faced by a robot that must explore and learn an unknown room with obstacles in it. We seek algorithms that achieve a bounded ratio of the worst-case distance traversed in order to see all visible points of the environment (thus creating a map), divided by the optimum distance needed to verify the map, if we had it in the beginning. The situation is complicated by the fact that the latter off-line problem (the problem of optimally verifying a map) is NP-hard. Although we show that there is no such “competitive” algorithm for general obstacle courses, we give a competitive algorithm for the case of a polygonal room with a bounded number of obstacles in it.  We restrict ourselves to the rectilinear case, where each side of the obstacles and the room  is parallel to one of the coordinates, and the robot must also move either parallel or perpendicular to the sides. (In a subsequent paper, we will discuss the extension to polygons of general shapes.)We also discuss the off-line problem for simple rectilinear polygons and find an optimal solution (in the L1 metric) in polynomial time, in the case where the entry and the exit are different points.We consider the problem of coloring k-colorable graphs with the fewest possible colors. We present a randomized polynomial time algorithm that colors a 3-colorable graph on n vertices with min{O(&Dgr;1/3 log1/2 &Dgr; log n), O(n1/4 log1/2 n)} colors where &Dgr; is the maximum degree of any vertex. Besides giving the best known approximation ratio in terms of n, this marks the first nontrivial approximation result as a function of the maximum degree &Dgr;. This result can be generalized to k-colorable graphs to obtain a coloring using  min{O(&Dgr;1-2/k log1/2 &Dgr; log n), O(n1−3/(k+1) log1/2 n)} colors. Our results are inspired by the recent work of Goemans and Williamson who used an algorithm for semidefinite optimization problems, which generalize linear programs, to obtain improved approximations for the MAX CUT and MAX 2-SAT problems. An intriguing outcome of our work is a duality relationship established between the value of the optimum solution to our semidefinite program and the Lovász &thgr;-function. We show lower bounds on the gap between the optimum solution of our semidefinite  program and the actual chromatic number; by duality this also demonstrates interesting new facts about the &thgr;-function.Recent developments in analyzing molecular structures and representing solid models using simplicial complexes have further enhanced the need for computing structural information about simplicial complexes in R3. This paper develops basic techniques required to manipulate and analyze structures of complexes in R3.A new approach to analyze simplicial complexes in Euclidean 3-space R3 is described. First, methods from topology are used to analyze triangulated 3-manifolds in R3. Then, it is shown that these methods can, in fact, be applied to arbitrary simplicial complexes in R3 after (simulating) the process  of thickening a complex to a 3-manifold homotopic to it. As a consequence considerable structural information about the complex can be determined and certain discrete problems solved as well. For example, it is shown how to determine homology groups, as well as concrete representations of their generators, for a given complex in R3In this paper, we present randomized algorithms over binary search trees such that: (a) the insertion of a set of keys, in any fixed order, into an initially empty tree always produces a random binary search tree; (b) the deletion of any key from a random binary search tree results in a random binary search tree; (c) the random choices made by the algorithms are based upon the sizes of the subtrees of the tree; this implies that we can support accesses by rank without additional storage requirements or modification of the data structures; and (d) the cost of any elementary operation, measured as the number of visited nodes, is the same as the expected cost of its standard deterministic counterpart; hence, all search and update operations have guaranteed expected cost O(log  n), but now irrespective of any assumption on the input distribution.Consider an on-line scheduling problem in which a set of abstract processes are competing for the use of a number of resources. Further assume that it is either prohibitively expensive or impossible for any two of the processes to directly communicate with one another. If several processes simultaneously attempt to allocate a particular resource (as may be expected to occur, since the processes cannot easily coordinate their allocations), then none succeed. In such a framework, it is a challenge to design efficient contention resolution protocols.Two recently-proposed approaches to the problem of PRAM emulation give rise to scheduling problems of the above kind. In  one approach, the resources (in this case, the shared memory cells) are duplicated and distributed randomly.  We analyze a simple and efficient deterministic algorithm for accessing some subset of the duplicated resources. In the other approach, we analyze how quickly we can access the given (nonduplicated) resource using a simple randomized strategy. We obtain precise bounds on the performance of both strategies. We anticipate that our results with find other applications.
The expressive power of first-order query languages with several classes of equality and inequality constraints is studied in this paper. We settle the conjecture that recursive queries such as parity test and transitive closure cannot be expressed in the relational calculus augmented with polynomial inequality constraints over the reals. Furthermore, noting that relational queries exhibit several forms of genericity, we establish a number of collapse results of the following form: The class of generic Boolean queries expressible in the relational calculus augmented with a given class of constraints coincides with the class of queries expressible in the relational calculus (with or without an order relation). We prove such results for both the natural and active-domain semantics. As  a consequence, the relational calculus augmented with polynomial inequalities expresses the same classes of generic Boolean queries under both the natural and active-domain semantics.In the course of proving these results for the active-domin semantics, we establish Ramsey-type theorems saying that any query involving certain kinds of constraints coincides with a constraint-free query on databases whose elements come from a certain infinite subset of the domain. To prove the collapse results for the natural semantics, we make use of techniques from nonstandard analysis and from the model theory of ordered structures.This paper presents and proves correct a distributed algorithm that implements a sequentially consistent collection of shared read/update objects. This algorithm is a generalization of one used in the Orca shared object system. The algorithm caches objects in the local memory of processors according to application needs; each read operation accesses a single copy of the object, while each update accesses all copies. The algorithm uses broadcast communication when it sends messages to replicated copies of an object, and it uses point-to-point communication when a message is sent to a single copy, and when a reply is returned. Copies of all objects are kept consistent using a strategy based on sequence numbers for broadcasts.The algorithm is presented in two layers. The  lower layer uses the given broadcast and point-to-point communication services, plus sequence numbers, to provide a new communication service called a context multicast channel. The higher layer uses a context multicast channel to manage the object replication in a consistent fashion. Both layers and their combination are described and verified formally, using the I/O automation model for asynchronous concurrent systems.We give a new characterization of NP: the class NP contains exactly those languages L for which membership proofs (a proof that an input x is in L) can be verified probabilistically in polynomial time using logarithmic number of random bits and by reading sublogarithmic number of bits from the proof.We discuss implications of this characterization; specifically, we show that approximating Clique and Independent Set, even in a very weak sense, is NP-hard.An (N, M, T)-OR-disperser is a bipartite multigraph G=(V, W, E) with |V| = N, and |W| = M, having the following expansion property: any subset of V having at least T vertices has a neighbor set of size at least M/2. For any pair of constants &xgr;, &lgr;, 1 ≥ &xgr; > &lgr; ≥ 0, any sufficiently large N, and for any T ≥ 2(logN) M ≤ 2(log N)&lgr;, we give an explicit elementary construction of an (N, M,    T)-OR-disperser such that the out-degree of any vertex in V is at most polylogarithmic in N. Using this with known applications of OR-dispersers yields several results. First, our construction implies that the complexity class Strong-RP defined by Sipser, equals RP. Second, for any fixed &eegr; > 0, we give the first polynomial-time simulation of RP algorithms using the output of any “&eegr;-minimally random” source. For any integral R > 0, such a source accepts a single request for an R-bit string and generates the string according to a distribution that assigns probability at most 2−R&eegr; to any string. It is minimally random in the sense that  any weaker source is insufficient to do a black-box polynomial-time simulation of RP algorithms.A finite automaton—the so-called neuromaton, realized by a finite discrete recurrent neural network, working in parallel computation mode, is considered. Both the size of neuromata (i.e., the number of neurons) and their descriptional complexity (i.e., the number of bits in the neuromaton representation) are studied. It is proved that a constraint time delay of the neuromaton output does not play a role within a polynomial descriptional complexity. It is shown that any regular language given by a regular expression of length n is recognized by a neuromaton with &THgr;(n) neurons. Further, it is proved that this network size is, in the worst case, optimal. On the other hand, generally there is not an equivalent polynomial length regular     expression for a given neuromaton. Then, two specialized constructions of neural acceptors of the optimal descriptional complexity &THgr;(n) for a single n-bit string recognition are described. They both require O(n1/2) neurons and either O(n) connections with constant weights or O(n1/2) edges with weights of the 
 O2n  
 Hopfield condition stating when a regular language is a Hopfield language, is formulated. A construction of a Hopfield neuromaton is presented for a regular language satisfying the Hopfield    condition. The class of Hopfield languages is shown to be closed under union, intersection, concatenation and complement, and it is not closed under iteration. Finally, the problem whether a regular language given by a neuromaton (or by a Hopfield acceptor) is nonempty, is proved to be PSPACE-complete. As a consequence, the same result for a neuromaton equivalence problem is achieved.We show that quick hitting set generators can replace quick pseudorandom generators to derandomize any probabilistic two-sided error algorithms. Up to now quick hitting set generators have been known as the general and uniform derandomization method for probabilistic one-sided error algorithms, while quick pseudorandom generators as the generators as the general and uniform method to derandomize probabilistic two-sided error algorithms.Our method is based on a deterministic algorithm that, given a Boolean circuit C and given access to a hitting set generator, constructs a discrepancy set for C. The main novelty is that the discrepancy set depends on C, so the new derandomization method is not uniform (i.e., not oblivious).The algorithm works in time exponential in k(p(n)) where k(*) is the price of the hitting set generator and p(*) is a polynomial function in the size of C. We thus prove that if a logarithmic price quick hitting set generator exists then BPP = P.
Most complexity measures for concurrent algorithms for asynchronous shared-memory architectures focus on process steps and memory consumption. In practice, however, performance of multiprocessor algorithms is heavily influenced by contention, the extent to which processess access the same location at the same time. Nevertheless, even though contention is one of the principal considerations affecting the performance of real algorithms on real multiprocessors, there are no formal tools for analyzing the contention of asynchronous shared-memory algorithms.This paper introduces the first formal complexity model for contention in shared-memory multiprocessors. We focus on the standard multiprocessor architecture in which n asynchronous processes  communicate by applying read, write, and read-modify-write operations to a shared memory. To illustrate the utility of our model, we use it to derive two kinds of results: (1) lower bounds on contention for well-known basic problems such as agreement and mutual exclusion, and (2) trade-offs between the length of the critical path (maximal number of accesses to shared variables performed by a single process in executing the algorithm) and contention for these algorithms. Furthermore, we give the first formal contention analysis of a variety of counting networks, a class of concurrent data structures inplementing shared counters. Experiments indicate that certain counting networks outperform conventional single-variable counters at high levels of  contention. Our analysis provides the first formal model explaining this phenomenon.In 1876, Lewis Carroll proposed a voting system in which the winner is the candidate who with the fewest changes in voters' preferences becomes a Condorcet winner—a candidate who beats all other candidates in pairwise majority-rule elections. Bartholdi, Tovey, and Trick provided a lower bound—NP-hardness—on the computational complexity of determining the election winner in Carroll's system. We provide a stronger lower bound and an upper bound that matches our lower bound. In particular, determining the winner in Carroll's system is complete for parallel access to NP, that is, it is complete for Theta_2p for which it becomes the most natural complete problem known. It follows that determining the winner in   
  Carroll's elections is not NP-complete unless the polynomial hierarchy collapses.We review the field of result-checking, discussing simple checkers and self-correctors. We argue that such checkers could profitably be incorporated in software as an aid to efficient debugging and enhanced reliability. We consider how to modify traditional checking methodologies to make them more appropriate for use in real-time, real-number computer systems. In particular, we suggest that checkers should be allowed to use stored randomness: that is, that they should be allowed to generate, preprocess, and store random bits prior to run-time, and then to use this information repeatedly in a series of run-time checks. In a case study of checking a general real-number linear transformation (e.g., a Fourier Transform), we present a simple checker which uses stored randomness, and a  self-corrector which is particularly efficient if stored randomness is employed.We introduce a method to describe systems and their components by functional specification techniques. We define notions of interface and interaction refinement for interactive systems and their components. These notions of refinement allow us to change both the syntactic (the number of channels and sorts of messages at the channels) and the semantic interface (causality flow between messages and interaction granularity) of an interactive system component. We prove that these notions of refinement are compositional with respect to sequential and parallel composition of system components, communication feedback and recursive declarations of system components. According to these proofs, refinements of networks can be accomplished in a modular way by refining their compponents. We  generalize the notions of refinement to refining contexts. Finally, full abstraction for specifications is defined, and compositionality with respect to this abstraction is shown, too.
Inspired by the success of the distributed computing community in apply logics of knowledge and time to reasoning about distributed protocols, we aim for a similarly powerful and high-level abstraction when reasoning about control problems involving uncertainty. This paper concentrates on robot motion planning with uncertainty in both control and sensing, a problem that has already been well studied within the robotics community. First, a new and natural problem in this domain is defined: does there exists a sound and complete termination condition for a motion, given initial and goal locations? If yes, how to construct it? Then we define a high-level language, a logic of time and knowledge, which we use to reason about termination conditions and to state general conditions for the existence of sound and complete termination conditions in a broad domain. Finally, we show that sound termination conditions that are optimal in a precise sense provide a natural example of knowledge-based programs with multiple implementations.We provide data strutures that maintain a graph as edges are inserted and deleted, and keep track of the following properties with the following times: minimum spanning forests, graph connectivity, graph 2-edge connectivity, and bipartiteness in timeO(n1/2) per change; 3-edge connectivity, in time O(n2/3) per change; 4-edge connectivity, in time O(n&agr;(n)) per change; k-edge connectivity for constant k, in time O(nlogn) per change;2-vertex connectivity, and 3-vertex connectivity, in the O(n) per change; and  4-vertex connectivity, in time O(n&agr;(n)) per change. Further results speed up the insertion times to match the bounds of known partially dynamic algorithms.All our algorithms are based on a new technique that transforms an algorithm for sparse graphs into one that will work on any graph, which we call sparsification.We introduce a new framework for the study of reasoning. The Learning (in order) to Reason approach developed here views learning as an integral part of the inference process, and suggests that learning and reasoning should be studied together.The Learning to Reason framework combines the interfaces to the world used by known learning models with the reasoning task and a performance criterion suitable for it. In this framework, the intelligent agent is given access to its favorite learning interface, and is also given a grace period in with it can interact with this interface and construct a representation KB of the world W. The reasoning performance is measured only after this period, when the agent is presented with queries &agr; from some query     language, relevant to the world, and has to answer whether W implies &agr;.The approach is meant to overcome the main computational difficulties in the traditional treatment of reasoning which stem from its separation from the “world”. Since the agent interacts with the world when construction its knowledge representation it can choose a representation that is useful for the task at hand. Moreover, we can now make explicit the dependence of the reasoning performance on the environment the agent interacts with.We show how previous results from learning theory and reasoning fit into this framwork and illustrate the usefulness of the Learning to Reason approach by exhibiting new results that are not possible in the traditional setting. First,   we give Learning to Reason algorithms for classes of propositional languages for which there are no efficient reasoning algorithms, when represented as a traditional (formula-based) knowledge base. Second, we exhibit a Learning to Reason algorithm for a class of propositional languages that is not know to be learnable in the traditional sense.We study the extent to which complex hardware can speed up routing. Specifically, we consider the following questions. How much does adaptive routing improve over oblivious routing? How much does randomness help? How does it help if each node can have a large number of neighbors? What benefit is available if a node can send packets to several neighbors within a single time step? Some of these features require complex networking hardware, and it is thus important to investigate whether the performance justifies the investment. By varying these hardware parameters, we obtain a hierarchy of time bounds for worst-case permutation routing.Some parallel algorithms have the property that, as they are allowed to take more time, the total work that they do is reduced. This paper describes several algorithms with this property. These algorithms solve important problems on directed graphs, including breadth-first search, topological sort, strong connectivity, and and the single source shorest path problem. All of the algorithms run on the EREW PRAM model of parallel computer, except the algorithm for strong connectivity, which runs on the probabilistic EREW PRAM.
Many combinatorial search problems can be expressed as “constraint satisfaction problems” and this class of problems is known to be NP-complete in general. In this paper, we investigate the subclasses that arise from restricting the possible constraint types. We first show that any set of constraints that does not give rise to an NP-complete class of problems must satisfy a certain type of algebraic closure condition. We then investigate all the different possible forms of this algebraic closure property, and establish which of these are sufficient to ensure tractability. As examples, we show that all known classes of tractable constraints over finite domains can be characterized by such an algebraic closure property. Finally, we describe a simple computational procedure that can be used to determine the closure properties of a given set of constraints. This procedure involves solving a particular constraint satisfaction problem, which we call an “indicator problem.”Constraint networks are a simple representation and reasoning framework with diverse applications. In this paper, we identify two new complementary properties on the restrictiveness of the constraints in a network—constraint tightness and constraint looseness—and we show their usefulness for estimating the level of local consistency needed to ensure global consistency, and for estimating the level of local consistency present in a network. In particular, we present a sufficient condition, based on constraint tightness and the level of local consistency, that guarantees that a solution can be found in a backtrack-free manner. The condition can be useful in applications where a knowledge base will be queried over and over and the  preprocessing costs can be amortized over many queries. We also present a sufficient condition for local consistency, based on constraint looseness, that is straightforward and inexpensive to determine. The condition can be used to estimate the level of local consistency of a network. This in turn can be used in deciding whether it would be useful to preprocess the network before a backtracking search, and in deciding which local consistency conditions, if any, still need to be enforced if we want to ensure that a solution can be found in a backtrack-free manner. Two definitions of local consistency are employed in characterizing the conditions: the traditional variable-based notion and a recently introduced definition of local consistency called relational  consistency.Given a convex polytope P with
n faces in 

R3
, points 

s,t∈6P
, and a parameter 

0<e≤1
, we present an algorithm that constructs a path on


6P
 from s to
t whose length is at most


1+edP
s,t
, where 

dPs,t
  is the length of the shortest path between
s and
t on 

6P
. The algorithm runs in 

Onlog1/e+
1/e3
 time, and is relatively simple. The running time is


On+1/e3
 if we only want the approximate shortest path
distance and not the path itself. We also present an extension of the
algorithm that computes approximate shortest path distances from a given
source point on 

6P
  to all vertices of
P.We present an algorithm for finding the minimum cut of an undirected edge-weighted graph. It is simple in every respect. It has a short and compact description, is easy to implement, and has a surprisingly simple proof of correctness. Its runtime matches that of the fastest algorithm known. The runtime analysis is straightforward. In contrast to nearly all approaches so far, the algorithm uses no flow techniques. Roughly speaking, the algorithm consists of about |V| nearly identical phases each of which is a maximum adjacency search.The problem of implementing a shared object of one type from shared objects of other types has been extensively researched. Recent focus has mostly been on wait-free implementations, which permit every process to complete its operations on implemented objects, regardless of the speeds of other processes. It is known that shared objects of different types have differing abilities to support wait-free implementations. It is therefore natural to want to arrange types in a hierarchy that reflects their relative abilities to support wait-free implementations. In this paper, we formally define robustness and other desirable properties of hierarchies. Roughly speaking, a hierarchy is robust if each type is “stronger” than any combination of lower level types. We study two specific hierarchies: one, that we call hrm in which the level of a type is based on the ability of an unbounded number of objects of that type, and another hierarchy, that we call hr1, in which a type's level is based on the ability of a fixed number of objects of that type. We prove that resource bounded hierarchies, such as hr1 and its variants, are not robust. We also establish the unique importance of hrm: every nontrivial robust hierarchy, if one exists, is necessarily a “coarsening” of hrm.Learnability in Valiant's PAC learning model has been shown to be strongly related to the existence of uniform laws of large numbers. These laws define a distribution-free convergence property of means to expectations uniformly over classes of random variables. Classes of real-valued functions enjoying such a property are also known as uniform Glivenko-Cantelli classes. In this paper, we prove, through a generalization of Sauer's lemma that may be interesting in its own right, a new characterization of uniform Glivenko-Cantelli classes. Our characterization yields Dudley, Gine´, and Zinn's previous characterization as a corollary. Furthermore, it is the first based on a Gine´, and Zinn's previous characterization as a corollary. Furthermore, it is the first based on a simple combinatorial quantity generalizing the Vapnik-Chervonenkis dimension. We apply this result to obtain the weakest combinatorial condition known to imply PAC learnability in the statistical regression (or “agnostic”) framework. Furthermore, we find a characterization of learnability in the probabilistic concept model, solving an open problem posed by Kearns and Schapire. These results show that the accuracy parameter plays a crucial role in determining the effective complexity of the learner's hypothesis class.
In this paper, we develop a framework for computing upper and lower bounds of an exponential form for a large class of single resource systems with Markov additive inputs. Specifically, the bounds are on quantities such as backlog, queue length, and response time. Explicit or computable expressions for our bounds are given in the context of queuing theory and numerical comparisons with other bounds and exact results are presented. The paper concludes with two applications to admission control in multimedia systems.We investigate a problem arising in the computer-aided design of cars, planes, ships, trains, and other motor vehicles and machines: refine a mesh of curved polygons, which approximates the surface of a workpiece, into quadrilaterals so that the resulting mesh is suitable for a numerical analysis.  This mesh refinement problem turns out to be strongly NP-hardIn commercial CAD systems, this problem is usually solved using a gree
dy approach.  However, these algorithms leave the user a lot of patchwork to do afterwards.  We introduce a new global approach, which is based on network flow techniques.  Abstracting from all geometric and numerical aspects, we obtain an undirected graph with upper and lower capacities on the edges and some additional node constraints.  We reduce this problem to a sequence of bidirected flwo problems (or, equivalently, to b-matching problems).  For the first time, network flow techniques are applied to a mesh refinement problem.This approach avoids the local traps of greedy approaches and yields solutions that require significantly less additional patchwork.
We analyze algorithms that predict a binary value by combining the predictions of several prediction strategies, called experts. Our analysis is for worst-case situations, i.e., we make no assumptions about the way the sequence of bits to be predicted is generated.  We measure the performance of the algorithm by the difference between the expected number of  mistakes it makes on the bit sequence and the expected number of mistakes made by the best expert on this sequence, where the expectation is taken with respect to the randomization in the predictins.  We show that the minimum achievable difference is on the order of the square root of the number of mistakes of the best expert, and we give efficient algorithms that achieve this.  Our upper and lower bounds have matching leading constants in most cases.  We then show how this leads to certain kinds of pattern recognition/learning algorithms with performance bounds that improve on the best results currently know in this context.  We also compare our analysis to the case in which log loss is used instead of the expected number of mistakes.In this paper we study the problem of on-line allocation of routes to virtual circuits (both point-to-point and multicast) where the goal is to route all requests while minimizing the required bandwidth.  We concentrate on the case of Permanent virtual circuits (i.e., once a circuit is established it exists forever), and describe an algorithm that achieves on O (log n) competitive ratio with respect to maximum congestin, where nis the number of nodes in the network.  Informally, our results show that instead of knowing all of the future requests, it is sufficient to increase the bandwidth of the communication links by an O (log n) factor.  We also show that this result is tight, that is, for any on-line algorithm there exists a scenario in which ***(log n) increase in  bandwidth is necessary in directed networks.We view virtual circuit routing as a generalization of an on-line load balancing problem, defined as follows: jobs arrive on line and each job must be assigned to one of the machines immediately upon arrival.  Assigning a job to a machine increases the machine's load by an amount that depends both on the job and on the machine.  The goal is to minimize the maximum load. For the related machines case, we describe the first algorithm that achieves constant competitive ratio. for the unrelated case (with nmachines), we describe a new method that yields O(logn)-competitive algorithm.  This stands in contrast to the natural greed approach, whose competitive ratio is exactly n.Strictness analysis is an important technique for optimization of lazy functional languages.  It is well known that all strictness analysis methods are incomplete, i.e., fail to report some strictness properties.  In this paper, we provide a precise and formal characterization of the loss of information that leads to this incompletenss.  Specifically, we establish the following characterization theorem for Mycroft's strictness analysis method and a generalization of this method, called ee-analysis, that reasons about exhaustive evaluation in nonflat domains: Mycroft's method will  deduce a strictness property for program P iff the property is independent of any constant appearing in any evaluation of P. To prove this, we specify a small set of equations, called E-axioms, that capture the information loss in Mycroft's method and develop a new proof technique called E-rewriting. E-rewriting extends the standard notion of rewriting to permit the use of reductions using E-axioms interspersed with standard reduction steps.  E-axioms are a syntactic characterization of information loss and E-rewriting provides and algorithm-independent proof technique for characterizing the power of analysis methods.  It can be used to answer questions on completeness and incompleteness of Mycroft's method on certain natural classes of programs. Finally, the techniques developed in this paper provide a general principle for establishing similar results for other analysis methods such as those based on abstract interpretation.  As a demonstration of the generality of our technique, we give a characterization theorem for another variation of Mycroft's method called dd-analysis.
We introduce a general framework for constraint satisfaction and optimization where classical CSPs, fuzzy CSPs, weighted CSPs, partial constraint satisfaction, and others can be easily cast.  The framework is based on a semiring structure, where the set of the semiring specifies the 
values to be associated with each tuple of values of the variable domain, and the two semiring operations (+ and X) model constraint projection and combination respectively. Local consistency  algorithms, as usually used for classical CSPs, can be exploited in this general framework as well, provided that certain conditions on the semiring operations are satisfied. We then show how this framework can be used to model both old and new constraint solving and optimization schemes, thus allowing one to both  formally justify many informally taken choices in existing schemes, and to prove that local consistency techniques can be used also in newly defined schemes.We show that a Turing machine with two single-head one-dimensional tapes cannot recognize the set.Object-oriented applications of database systems require database transformations involoving nonstandard functionalities such as set manipulation and object creation, that is, the introduction of new domain  elements.  To deal with thse functionalities, Abiteboul and Kanellakis [1989] introduced the “determinate” transformations as a generalization of the standard domain-preserving transformations.  The obvious extensions of complete standard database programming languages, however, are not complete for the determinate transformations.  To remedy this mismatch, the “constructive” transformations are proposed.  It is shown that the constructive transformations are precisely the transformations that can be expressed in said extensions of complete standard languages.  Thereto, a close correspondence between object creation and  the construction of hereditarily finite sets is established.A restricted version of the main completeness result for the case where only list manipulations are involved is also presented.This paper addresses and answers a fundamental question about resolution.  Informally, what is gained with respect to the search for a proof by performing a single resolution step? It is first shown that any unsatisfiable formula  may be decomposed into regular formulas provable in linear time (by resolution).  A relevant resolution step strictly reduces at least one of the formulas in the decomposition while an irrelevant one does not contribute to the proof in any way.  the relevance of this insight into the nature of resolution and of the unsatisfiability problem for the development of proof strategies and for complexity considerations are briefly discussed.The decomposition also provides a technique for establishing completeness proofs for refinements of resolution.  As a first application, connection-graph resolution is shown to be strongly complete. This settles a problem that remained open for two decades despite many proff attempts.  The result is relevant for theorem proving because without strong completeness a connection graph resolution prover might run into an infinite loop even on the ground level.Consider an array of Processing Elements [PEs], connected by a 2-dimensional grid network, and holding at most one operand of an expression in each PE. Suppose that each PE is allowed, in any one parallel step, to receive one item of data from any of its four immediate neighbors, and to transmit one datum, as well.  How can an associative operator, such as addition, combine all the operands, using as little time for communciation as possible? An expression using such a single operator is termed a uniform expression.  When the total number of communication links used is the measure of goodness, this problem becomes a Steiner Tree problem, in the Manhattan Distance metric.  When the measure is minimizing the parallel time to completion, a method for solving this problem is given which is optimal to within an additive constant of two time-steps.  The method has applications when the operands are matrices, spread over an array of PEs, as well.  Some lower bounds for this problem, in more general networks, are also proven.
A collection of n balls in d dimensions forms a k-ply system if no point in the space is covered by more than k balls. We show that for every k-ply system &Ggr;, there is a sphere S that intersects at most O(k1/dn1−1/d) balls of &Ggr; and divides the remainder of &Ggr; into two parts: those in the interior and those in the exterior of the sphere S, respectively, so that the larger part contains at most (1−1/(d+2))n balls. This bound of   (O(k1/dn1−1/d) is the best possible in both n and k. We also present a simple randomized algorithm to find such a sphere in O(n) time. Our result implies that every  k-nearest neighbor graphs of n points in  d dimensions has a separator of size O(k1/dn1−1/d). In conjunction with a result of Koebe that every triangulated planar graph is isomorphic to the intersection graph of a disk-packing, our result not only gives a new   geometric proof of the planar separator theorem of Lipton and Tarjan, but also generalizes it to higher dimensions. The separator algorithm can be used for point location and geometric divide and conquer in a fixed dimensional space.We establish a general connection between fixpoint logic and complexity. On one side, we have fixpoint logic, parameterized by the choices of 1st-order operators (inflationary or noninflationary) and iteration constructs (deterministic, nondeterministic, or alternating). On the other side, we have the complexity classes between P and EXPTIME. Our parameterized fixpoint logics capture the complexity classes P, NP, PSPACE, and EXPTIME, but equally is achieved only over ordered structures.There is, however, an inherent mismatch between complexity and logic—while computational devices work on encodings of problems, logic is applied directly to the underlying mathematical structures. To overcome this mismatch, we use a theory of relational complexity, which bridges the gap between standard complexity and fixpoint logic. On one hand, we show that questions about containments among standard complexity classes can be translated to questions about containments among relational complexity classes. On the other hand, the expressive power of fixpoint logic can be precisely characterized in terms of relational complexity classes. This tight, three-way relationship among fixpoint logics, relational complexity and standard complexity yields in a uniform way logical analogs to all containments among the complexity classes P, NP, PSPACE, and EXPTIME. The logical formulation shows that some of the most tantalizing questions in complexity theory boil down to a single question: the relative power of inflationary vs. noninflationary 1st-order operators.Computing the natural join of a set of relations is an important operation in relational database systems. The ordering of joins determines to a large extent the computation time of the join. Since the number of possible orderings could be very large, query optimizers first reduce the search space by using various heuristics and then try to select an optimal ordering of joins. Avoiding Cartesian products is a common heuristic for reducing the search space, but it cannot guarantee optimal ordering in its search space, because the cheapest Cartesian-product-free (CPF for short) ordering could be significantly worse than an optimal non-CPF ordering by a factor of an arbitrarily large number. In this paper, we use programs consisting of joins, semijoins, and  projections for computing the join of some relations, and we introduce a novel algorithm that derives programs from CPF orderings of joins. We show that there exists a CPF ordering from which our algorithm derives a program whose cost is within a constant factor of the cost of an optimal ordering. Thus, our result demonstrates the effectiveness of avoiding Cartesian products as a heuristic for restricting the search space of orderings of joins.A basic task in distributed computation is the maintenance at each processor of the network, of a current and accurate copy of a common database. A primary example is the maintenance, for routing and other purposes, of a record of the current topology of the system.Such a database must be updated in the wake of locally generated changes to its contents. Due to previous disconnections of parts of the network, a maintenance protocol may need to update processors holding widely varying versions of the database.We provide a deterministic protocol for this problem, with only polylogarithmic overhead in both time and communication complexities.  Previous deterministic solutions required polynomial overhead in at least one of these measures.
We present algorithms for efficient searching of regular expressions on preprocessed text, using a Patricia tree as a logical model for the index. We obtain searching algorithms that run in logarithmic expected time in the size of the text for a wide subclass of regular expressions, and in sublinear expected time for any regular expression. This is the first such algorithm to be found with this complexity.Recurrent neural networks that are trained to behave like deterministic finite-state automata (DFAs) can show deteriorating performance when tested on long strings. This deteriorating performance can be attributed to the instability of the internal representation of the learned DFA states. The use of a sigmoidel discriminant function together with the recurrent structure contribute to this instability. We prove that a simple algorithm can construct second-order recurrent neural networks with a sparse interconnection topology and sigmoidal discriminant function such that the internal DFA state representations are stable, that is, the constructed network correctly classifies strings of arbitrary length. The algorithm is based on encoding strengths of weights directly into the neural network. We derive a relationship between the weight strength and the number of DFA states for robust string classification. For a DFA with n state and minput alphabet symbols, the constructive algorithm generates a “programmed” neural network with O(n) neurons and O(mn) weights. We compare our algorithm to other methods proposed in the literature.This paper studies the problem of dedicating routes to connections in optical networks. In optical networks, the vast bandwidth available in an optical fiber is utilized by partitioning it into several channels, each at a different optical wavelength. A connection between two nodes is assigned a specific wavelength, with the constraint that no two connections sharing a link in the network can be assigned the same wavelength. This paper considers optical networks with and without switches, and different types of routing in these networks. It presents optimal or near-optimal constructions of optical networks in these cases and algorithms for routing connections, specifically permutation routing for the networks constructed here.In this paper, a new algorithm for performing quantifier elimination from first order formulas over real closed fields in given. This algorithm improves the complexity of the asymptotically fastest algorithm for this problem, known to this data. A new feature of this algorithm is that the role of the algebraic part (the dependence on the degrees of the imput polynomials) and the combinatorial part (the dependence on the number of polynomials) are sparated. Another new feature is that the degrees of the polynomials in the equivalent quantifier-free formula that is output, are independent of the number of input polynomials. As special cases of this algorithm new and improved algorithms for deciding a sentence in the first order theory over real closed fields, and also for solving the existential problem in the first order theory over real closed fields, are obtained.We analyze the optimization effect of the “magic sets” rewriting technique for datalog queries and present some supplementary or alternative techniques that avoid many shortcomings of the basic technique. Given a magic sets rewritten query, the set of facts generated for the original, nonmagic predicates by the seminaive bottom-up evaluation is characterized precisely. It is shown that—because of the additional magic facts—magic sets processing may result in generating an order of magnitude more facts than the straightforward naive evaluation. A refinement of magic sets in factorized magic sets is defined. These magic sets retain most of the efficiency of original magic sets in regards to the number of nonmagic facts generated and have the property that a linear-time bound with respect to seminaive evaluation is guaranteed in all cases. An alternative technique for magic sets, called envelopes, which has several desirable properties over magic sets, is introduced. Envelope predicates are never recursive with the original predicates; thus, envelopes can be computed as a preprocessing task. Envelopes also allow the utilization of multiple sideways information passing strategies (sips) for a rule. An envelope-transformed program may be “readorned” according to another choice of sips and reoptimized by magic sets (or envelopes), thus making possible an optimization effect that cannot be achieved by magic sets based on a particular choice of sips.
Caching and prefetching are important mechanisms for speeding up access time to data on secondary storage. Recent work in competitive online algorithms has uncovered several promising new algorithms for caching. In this paper, we apply a form of the competitive philosophy for the first time to the problem of prefetching to develop an optimal universal prefetcher in terms of fault rate, with particular applications to large-scale databases and hypertext systems. Our prediction algorithms with particular applications to large-scale databases and hypertext systems. Our prediction algorithms for prefetching are novel in that they are based on data compression techniques that are both theoretically optimal and good in practice. Intuitively, in order to compress data effectively, you have  to be able to predict future data well, and thus good data compressors should be able to predict well for purposes of prefetching. We show for powerful models such as Markov sources and mthe order Markov sources that the page fault rate incurred by our prefetching algorithms are optimal in the limit for almost all sequences of page requests.Balancing networks, originally introduced by Aspnes et al. (Proceedings of the 23rd Annual ACM Symposium on Theory of Computing, pp. 348-358, May 1991), represent a new class of distributed, low-contention data structures suitable for solving many fundamental multi-processor coordination problems that can be expressed as balancing problems. In this work, we present a mathematical study of the combinatorial structure of balancing networks, and a variety of its applications.Our study identifies important combinatorial transfer parameters of balancing networks. In turn, necessary and sufficient combinatorial conditions are established, expressed in terms of transfer parameters, which precisely characterize many important and  well studied classes of balancing networks such as counting networks and smoothing networks. We propose these combinatorial conditions to be “balancing analogs” of the well known Zero-One principle holding for sorting networksWithin the combinatorial framework we develop, our first application is in deriving combinatorial conditions, involving the transfer parameters, which precisely delimit the boundary between counting networks and sorting networks.We investigate the query complexity of exact learning in the membership and (proper) equivalence query model. We give a complete characterization of concept classes that are learnable with a polynomial number of polynomial sized queries in this model. We give applications of this characterization, including results on learning a natural subclass of DNF formulas, and on learning with membership queries alone. Query complexity has previously been used to prove lower bounds on the time complexity of exact learning. We show a new relationship between query complexity and time complexity in exact learning: If any “honest” class is exactly and properly learnable with polynomial query complexity, but not learnable in polynomial time, then P = NP. In particular, we show that an honest class is exactly polynomial-query learnable if and only if it is learnable using an oracle for &Ggr;p4.We present a general theory for the use of negative premises in the rules of Transition System Specifications (TSSs). We formulate a criterion that should be satisfied by a TSS in order to be meaningful, that is, to unequivocally define a transition relation. We also provide powerful techniques for proving that a TSS satisfies this criterion, meanwhile constructing this transition relation. Both the criterion and the techniques originate from logic programming [van Gelder et al. 1988; Gelfond and Lifschitz 1988] to which TSSs are close. In an appendix we provide an extensive comparison between them.As in Groote [1993], we show that the bisimulation relation induced by a TSS is a congruence, provided that it is in ntyft/ntyxt-format and can be proved meaningful using our techniques. We also considerably extend the conservativity theorems of Groote[1993] and Groote and Vaandrager [1992]. As a running example, we study the combined addition of priorities and abstraction to Basic Process Algebra (BPA). Under some reasonable conditions we show that this TSS is indeed meaningful, which could not be shown by other methods [Bloom et al. 1995; Groote 1993]. Finally, we provide a sound and complete axiomatization for this example.
This paper present a new approach to finding minimum cuts in undirected graphs. The fundamental principle is simple: the edges in a graph's minimum cut form an extremely small fraction of the graph's edges. Using this idea, we give a randomized, strongly polynomial algorithm that finds the minimum cut in an arbitrarily weighted undirected graph with high probability. The algorithm runs in O(n2log3n) time, a significant improvement over the previous O˜(mn) time bounds based on maximum flows. It is simple and intuitive and uses no complex data structures. Our algorithm can be parallelized to run in RNC with n2 processors; this gives the first proof   that the minimum cut problem can be solved in RNC. The algorithm does more than find a single minimum cut; it finds all of them.With minor modifications, our algorithm solves two other problems of interest. Our algorithm finds all cuts with value within a multiplicative factor of &agr; of the minimum cut's in expected O˜(n2&agr;) time, or in RNC with n2&agr; processors. The problem of finding a minimum multiway cut of graph into r pieces is solved in expected O˜(n2(r-1)) time, or in RNC with n2(r-1) processors. The “trace” of the    algorithm's execution on these two problems forms a new compact data structure for representing all small cuts and all multiway cuts in a graph. This data structure can be efficiently transformed into the more standard cactus representing for minimum cuts.Product-form queuing network models have been widely used to model systems with shared resources such as computer systems (both centralized and distributed), communication networks, and flexible manufacturing systems. Closed multichain product-form networks are inherently more difficult to analyze than open networks, due to the effect of normalization. Results in workload characterization for closed networks in the literature are often for networks having special structures and only specific performance measures have been considered.In this article, we drive certain properties (insensitivity of conditional state probability distributions and fractional-linearity of Markov reward functions) for a broad class of closed multichain product-form networks. These properties are derived using the most basic flow balance conditions of product-form networks. Then we show how these basic properties can be applied in obtaining error bounds when similar customers are clustered together to speed up computation.The exponent of periodicity is an important factor in estimates of complexity of word-unification algorithms. We prove that the exponent of periodicity of a minimal solution of a word equation is of order 21.07d, where d is the length of the equation. We also give a lower bound 20.29d so our upper bound is almost optimal and exponentially better than the original bound (6d)22d4+ 2. Consequently, our result implies an exponential improvement of known upper bounds on complexity of word-unification algorithms.We determine what information about failures is necessary and sufficient to solve Consensus in asynchronous distributed systems subject to crash failures. In Chandra and Toueg [1996], it is shown that W, a failure detector that provides surprisingly little information about which processes have crashed, is sufficient to solve Consensus in asynchronous systems with a majority of correct processes. In this paper, we prove that to solve Consensus, any failure detector has to provide at least as much information as W. Thus, W is indeed the weakest failure detector for solving Consensus in asynchronous systems with a majority of correct processes.Sharing data between multiple asynchronous users—each of which can atomically read and write the data—is a feature that may help to increase the amount of parallelism in distributed systems. An algorithm implementing this feature is presented. The main construction of an n-user atomic variable directly from single-writer, single-reader atomic variables uses O(n) control bits and O(n) accesses per Read/Write running in O(1) parallel time.
Probabilistic reasoning suffers from NP-hard implementations. In particular, the amount of probabilistic information necessary to the computations is often overwhelming. For example, the size of conditional probability tables in Bayesian networks has long been a limiting factor in the general use of these networks.We present a new approach for manipulating the probabilistic information given. This approach avoids being overwhelmed by essentially compressing the information using approximation functions called linear potential functions. We can potentially reduce the information from a combinatorial amount to roughly linear in  the number of random variable assigments. Furthermore, we can compute these functions through closed form equations. As it turns out, our approximation method is quite general and may be applied to other data compression problems.Software protection is one of the most important issues concerning computer practice. There exist many heuristics and ad-hoc methods for protection, but the problem as a whole has not received the theoretical treatment it deserves. In this paper, we provide theoretical treatment of software protection. We reduce the problem of software protection to the problem of efficient simulation on oblivious RAM.A machine is oblivious if thhe sequence in which it accesses memory locations is equivalent for any two inputs with the same running time. For example, an oblivious Turing Machine is one for which the movement of the heads on the tapes is identical for each computation. (Thus, the movement is independent of the actual input.) What is the slowdown in the running time of a machine, if it is required to be oblivious? In 1979, Pippenger and Fischer showed how a two-tape oblivious Turing Machine can simulate, on-line, a one-tape Turing Machine, with a logarithmic slowdown in the running time. We show an analogous result for the random-access machine (RAM) model of computation. In particular, we show how to do an on-line simulation of an arbitrary RAM by a probabilistic oblivious RAM with a polylogaithmic slowdown in the running time. On the other hand, we show that a logarithmic slowdown is a lower bound.Though numerous multimedia systems exist in the commercial market today, relatively little work has been done on developing the mathematical foundation of multimedia technology. We attempt to take some initial steps towards the development of a theoretical basis for a multimedia information system. To do so, we develop the motion of a structured multimedia database system. We begin by defining a mathematical model of a media-instance. A media-instance may be thought of as “glue” residing on top of a specific physical media-representation (such as video, audio, documents, etc). Using this “glue”, it is possible to define a general purpose logical query language to query multimedia data. This glue consists of a set of “states” (e.g., video frames, audio tracks, etc.) and “features”, together with relationships between states and/or features. A structured multimedia database system imposes a certain mathematical structures on the set of features/states. Using this notion of a structure, we are able to define indexing structures for processing queries, methods to relax queries when answers do not exist to those queries, as well as sound, complete and terminating procedures to answer such queries (and their relaxations, when appropriate). We show how a media-presentation can be generated by processing a sequence of queries, and furthermore we show that when these queries are extended to include constraints, then these queries can not only generate presentations, but also generate temporal synchronization properties and spatial layout properties for such presentations. We describe the architecture of a prototype multimedia database system based on the principles described in this paper.We present a linear-time algorithm to decide for any fixed deterministic context-free language L and input string w whether wis a suffix of some string in L. In contrast to a previously published technique, the decision procedure may be extended to produce syntactic structures (parses) without an increase in time complexity. We also show how this algorithm may be applied to pocess incorrect input in linear time.In comparative concurrency semantics, one usually distinguishes between linear time and branching time semantic equivalences. Milner's notion of observatin equivalence is often mentioned as the standard example of a branching time equivalence. In this paper we investigate whether observation equivalence really does respect the branching structure of processes, and find that in the presence of the unobservable action &tgr;  of CCS this is not the case.Therefore, the notion of branching bisimulation equivalence is introduced which strongly preserves the branching structure of processes, in the sense that it preserves computations together with the potentials in all intermediate states that are passed through,  even if silent moves are involved. On closed CCS-terms branching bisimulation congruence can be completely axiomatized by the single axion scheme: a.(&tgr;.(y+z)+y)=a.(y+z) (where a ranges over all actions) and the usual loaws for strong congruence.We also establish that for sequential processes observation equivalence is not preserved under refinement of actions, whereas branching bisimulation is.For a large class of processes, it turns out that branching bisimulation and observation equivalence are the same. As far as we know, all protocols that have been verified in the setting of observation equivalence happen to fit in this class, and hence are also valid in the stronger setting of branching bisimulation equivalence.
Computational efficiency is a central concern in the design of knowledge representation systems. In order to obtain efficient systems, it has been suggested that one should limit the form of the statements in the knowledge base or use an incomplete inference mechanism. The former approach is often too restrictive for practical applications, whereas the latter leads to uncertainty about exactly what can and cannot be inferred from the knowledge base. We present a third alternative, in which knowledge given in a general representation language is translated (compiled) into a tractable form—allowing for efficient subsequent query answering.We show how propositional logical theories can be compiled into Horn theories that approximate the original information.  The approximations bound the original theory from below and above in terms of logical strength. The procedures are extended to other tractable languages (for example, binary clauses) and to the first-order case. Finally, we demonstrate the generality of our approach by compiling concept descriptions in a general frame-based language into a tractable form.We introduce the concept of unreliable failure detectors and study how they can be used to solve Consensus in asynchronous systems with crash failures. We characterise unreliable failure detectors in terms of two properties—completeness and accuracy. We show that Consensus can be solved even with unreliable failure detectors that make an infinite number of mistakes, and determine which ones can be used to solve Consensus despite any  number of crashes, and which ones require a majority of correct processes. We prove that Consensus and Atomic Broadcast are reducible to each other in asynchronous systems with crash failures; thus, the above results also apply to Atomic Broadcast. A companion paper shows that one of the failure detectors introduced here is the weakest failure detector for solving Consensus [Chandra et al. 1992].The contribution of this paper is two-fold. First, a connection is established between approximating the size of the largest clique in a graph and multi-prover interactive proofs. Second, an efficient multi-prover interactive proof for NP languages is constructed, where the verifier uses very few random bits and communication bits. Last, the connection between cliques and efficient multi-prover interaction proofs, is shown to yield hardness results on the complexity of approximating the size of the largest clique in a graph.Of independent interest is our proof of correctness for the multilinearity test of functions.We present optimal algorithms for sorting on parallel CREW and EREW versions of the pointer machine model. Intuitively, one can view our methods as being based on a parallel mergesort using linked lists rather than arrays (the usual parallel data structure). We also show how to exploit the “locality” of our approach to solve the set expression evaluation problem, a problem with applications to database querying and logic-programming in O(log n) time using O(n) processors. Interestingly, this is an asymptotic improvement over what seems possible using previous techniques.Categorical combinators [Curien 1986/1993; Hardin 1989; Yokouchi 1989] and more recently &lgr;&sgr;-calculus [Abadi 1991; Hardin and Le´vy 1989], have been introduced to provide an explicit treatment of substitutions in the &lgr;-calculus. We reintroduce here the ingredients of these calculi in a self-contained and stepwise way, with a special emphasis on confluence properties. The main new results of the paper with respect to Curien [1986/1993], Hardin [1989], Abadi [1991], and Hardin and Le´vy [1989] are the following:(1) We present a confluent weak calculus of substitutions, where no variable clashes can be feared;
(2) We solve a conjecture raised in Abadi [1991]: &lgr;&sgr;-calculus is not confluent (it is confluent on ground terms only).This  unfortunate result is “repaired” by presenting a confluent version of &lgr;&sgr;-calculus, named the &lgr;Env-caldulus in Hardin and Le´vy [1989], called here the confluent &lgr;&sgr;-calculus.
This paper introduces a new distributed data object called Resource Controller that provides an abstraction for managing the consumption of a global resource in a distributed system. Examples of resources that may be managed by such an object include; number of messages sent, number of nodes participating in the protocol, and total CPU time consumed.The Resource Controller object is accessed through a procedure that can be invoked at any node in the network. Before consuming a unit of resource at some node, the controlled algorithm should invoke the procedure at this node, requesting a permit or a rejection.The key characteristics of the Resource Controller object are the constraints that it imposes on the  global resource consumption.  An (M,  W)-Controller guarantees that the total number of permits granted is at most M; it also ensures that, if a request is rejected, then at least M—W permits are eventually granted, even if no more requests are made after the rejected one.In this paper, we describe several message and space-efficient implementations of the Resource Controller object. In particular, we present an (M, W)-Controller whose message complexity is O(n log2n log(M/(W + 1)) where n is the total number of nodes. This is in contrast to the O(nM) message complexity of a fully centralized  controller which maintains a global counter of the  number of granted permits at some distinguished node and relays all the requests to the node.SLD resolution with negation as finite failure (SLDNF) reflects the procedural interpretation of predicate calculus as a programming language and forms the computational basis for Prolog systems. Despite its advantages for stack-based memory management, SLDNF is often not appropriate for query  evaluation for three reasons: (a) it may not terminate due to infinite positive recursion; (b) it may be terminate due to infinite recursion through negation; and (c) it may repeatedly evaluate the same literal in a rule body, leading to unacceptable performance.We address all three problems for goal-oriented query evaluation of general logic programs by presenting tabled evaluation with delaying, called SLG resolution.  It has three distinctive features:(i) SLG resolution is a partial deduction procedure, consisting of seven fundamental transformations.  A query is transformed step by step into a set of answers.  The use of transformations separates logical issues of query evaluation from procedural ones.  SLG allows an arbitrary computation rule for selecting a literal from a rule body and an arbitrary control strategy for selecting transformations to apply.(ii) SLG resolution is sound and search space complete with respect to the well-founded partial model for all non-floundering queries, and preserves all three-valued stable models.  To evaluate a query under differenc three-valued stable models, SLG resolution can be enhanced by further processing of the answers of subgoals relevant to a query.(iii) SLG   resolution avoids both positive and negative loops and always terminates for programs with the bounded-term-size property.  It has a polynomial time data complexity for well-founded negation of function-free programs.  Through a delaying mechanism for handling ground negative literals involved in loops, SLG resolution avoids the repetition of any of its derivation steps.Restricted forms of SLG resolution are identified for definite, locally stratified, and modularly stratified programs, shedding light on the role each transformation plays.Let M(m,n) be the minimum number of comparatorsneeded in a comparator network that merges m elements   x1≤x2≤&cdots;≤xm  and n elements   y1≤y2≤&cdots;≤yn , where   n≥m . Batcher's odd-even merge yields the following upper bound:     Mm,n≤1 2m+nlog 2m+on;         in particular,     Mn,n≤nlog 2n+On.   We prove the following lower bound that matches the upper bound above asymptotically as   n≥m→∞:      Mm,n≥1 2m+nlog 2m-Om;     in particular,     Mn,n≥nlog   2n-On.   Our proof technique extends to give similarily tight lower bounds for the size of monotone Boolean circuits for merging, and for the size of switching networks capable of realizing the set of permutations that arise from merging.

A new algorithm is developed for calculating normalization constants (partition functions) and moments of product-form steady-state distributions of closed queuing networks and related models. The essential idea is to numerically invert the generating function of the normalization constant and related generating functions appearing in expressions for the moments. It is known that the generating function of the normalization constant often has a remarkably simple form, but numerical inversion evidently has not been considered before. For p-dimensional transforms, as occur with queuing networks having p closed chains, the algorithm recursively performs p one-dimensional inversions. The required computation grows exponentially in the  dimension, but the dimension can often be reduced by exploiting conditional decomposition based on special structure. For large populations, the inversion algorithm is made more efficient by computing large sums using Euler summation. The inversion algorithm also has a very low storage requirement. A key ingredient in the inversion algorithm is scaling. An effective static scaling is developed for multichain closed queuing networks with only single-server and (optionally) infinite-server queues. An important feature of the inversion algorithm is a self-contained accuracy check, which allows the results to be verified in the absence of alternative algorithms.We prove that the work function algorithm for the k-server problem has a competitive ratio at most 2k−1. Manasse et al. [1988] conjectured that the competitive ratio for the k-server problem is exactly k (it is trivially at least k); previously the best-known upper bound was exponential in k. Our proof involves three crucial ingredients: A quasiconvexity property of work functions, a duality lemma that uses quasiconvexity to characterize the configuration that achieve maximum increase of the work function, and a potential function that exploits the duality lemma.Implementation of programming language interpreters, proving theorem of the form A=B, implementation of abstract data types, and program optimization are all problems that can be reduced to the problem of finding a normal form for an expression with respect to a finite set of equations. In 1980, Chew proposed an elegant congruence closure based simplifier (CCNS) for computing with regular systems, which stores the history of it computations in a compact data structure. In 1990, Verma and Ramakrishnan showed that it can also be used for noetherian systems with no overlaps.In this paper, we develop a general theory of using CCNS for computing normal forms and present several applications. Our results are more powerful and widely applicable than earlier work. We present an independent set of postulates and prove that CCNS can be used for any system that satisfies them. (This proof is based on the notion of strong closure). We then show that CCNS can be used for consistent convergent systems and for various kinds of priority rewrite systems. This is the first time that the applicability of CCNS has been shown for priority systems. Finally, we present a new and simpler translation scheme for converting convergent systems into effectively nonoverlapping convergent priority systems. Such a translation scheme has been proposed earlier, but we show that it is incorrect.Because CCNS requires some strong properties of the given system, our demonstration of its wide applicability is both difficult and surprising. The tension between demands imposed by CCNS and our efforts to satisfy them gives our work much general significance. Our results are partly achieved through the idea of effectively simulating “bad” systems by almost-equivalent “good” ones, partly through our theory that substantially weakens the demands, and partly through the design of a powerful and unifying reduction proof method.This paper deals with the problem of maintaining a distributed directory server, that enables us to keep track of mobile users in a distributed network. The paper introduces the graph-theoretic concept of regional matching, and demonstrates how finding a regional matching with certain parameters enables efficient tracking. The communication overhead of our tracking mechanism is within a polylogarithmic factor of the lower bound.We consider the sequence transmission problem, that is, the problem of transmitting an infinite sequence of messages x1x2x3… over a channel that can both lose and reorder packets. We define performance measures, ideal transmission cost and recovery cost, for protocols that solve the sequence transmission problem. Ideal transmission cost measures the number of packets needed to deliver xn when the channel is behaving ideally and recovery cost measures how long it takes, in terms of number of messages delivered, for the ideal transmission cost to take hold once the channel begins behaving ideally. We also define lookahead, which measures the number of messages   the sender can be ahead of the receiver in the protocol. We show that any protocol with constant recovery cost and lookahead requires linear ideal transmission cost. We describe a protocol, Plin, that has ideal transmission cost 2n, recovery cost 1, and lookahead 0.The spectral method is the best currently known technique to prove lower bounds on expansion. Ramanujan graphs, which have asymptotically optimal second eigenvalue, are the best-known explicit expanders. The spectral method yielded a lower bound of k/4 on the expansion of linear-sized subsets of k-regular Ramanujan graphs. We improve the lower bound on the expansion of Ramanujan graphs to approximately k/2. Moreover, we construct a family of k-regular graphs with asymptotically optimal second eigenvalue and linear expansion equal to k/2. This shows that k/2 is the best bound one can obtain using the second eigenvalue method. We also show an upper bound of roughly     1+k-1  on the average degree of linear-sized induced subgraphs of Ramanujan graphs. This compares positively with the classical bound   2k-1 . As a byproduct, we obtain improved results on random walks on expanders and construct selection networks (respectively, extrovert graphs) of smaller size (respectively, degree) than was previously known.In propositional logic, several problems, such as satisfiability, MAX SAT and logical inference, can be formulated as integer programs. In this paper, we consider sets of clauses for which the corresponding integer programs can be solved as linear programs. We prove that balanced sets of clauses have this property.
Since Konolige's translation of default logic into strongly grounded autoepistemic logic, several other variants of Moore's original autoepistemic logic that embody default logic have been studied. All these logics differ significantly from Moore's autoepistemic logic (standard AEL) in that expansions are subject to additional groundedness-conditions. Hence, the question naturally arises whether default logic can be translated into standard AEL at all. We show that a modular translation is not possible. However, we are able to construct a faithful polynomial-time translation from default logic into standard AEL, which is nonmodular. Our translation exploits the self-referentiality of AEL. It uses as an important intermediate step an embedding of Marek's and Truszczyn´ski's  nonmonotonic logic N into standard AEL. It follows from our results that the expressive power of standard AEL is strictly greater than that of default logic.We propose a novel formalism, called Frame Logic (abbr., F-logic), that accounts in a clean and declarative fashion for most of the structural aspects of object-oriented and frame-based languages. These features include object identity, complex objects, inheritance, polymorphic types, query methods, encapsulation, and others. In a sense, F-logic stands in the same relationship to the object-oriented paradigm as classical predicate calculus stands to relational programming. F-logic has a model-theoretic semantics and a sound and complete resolution-based proof theory. A small number of fundamental concepts that come from object-oriented programming have direct representation in F-logic; other, secondary aspects of this paradigm are easily modeled as well. The paper  also discusses semantic issues pertaining to programming with a deductive object-oriented language based on a subset of F-logic.We determine the complexity of testing whether a finite state, sequential or concurrent probabilistic program satisfies its specification expressed in linear-time temporal logic. For sequential programs, we present an algorithm that runs in time linear in the program and exponential in the specification, and also show that the problem is in PSPACE, matching the known lower bound. For concurrent programs, we show that the problem can be solved in time polynomial in the program and doubly exponential in the specification, and prove that it is complete for double exponential time. We also address these questions for specifications described by &ohgr;-automata or formulas in extended temporal logic.Given a pattern string, we describe a way to preprocess it. We design a CRCW-PRAM constant time optimal parallel algorithm for finding all occurrences of the (preprocessed) pattern in any given text.We present an algorithm for sorting efficiently with parallel two-level memories. Our main result is an elegant, easy-to-implement, optimal, deterministic algorithm for external sorting with D disk drives. This result answers in the affirmative the open problem posed by Vitter and Shriver of whether an optimal algorithm exists that is deterministic. Our measure of performance is the number of parallel input/output (I/O) operations, in which each of the D disks can simultaneously transfer a block of B contiguous records. We assume that internal memory can hold M records. Our algorithm sorts N records in the optimal bound of &thgr;((N/BD) log(N/B)/    log(M/B)) deterministically, and thus improves upon Vitter and Shriver's optimal randomized algorithm as well as the well-known deterministic but nonoptimal technique of disk striping. It is also practical to implement.
Constraint networks have been shown to be useful in formulating such diverse problems as scene labeling, natural language parsing, and temporal reasoning. Given a constraint network, we often wish to (i) find a solution that satisfies the constraints and (ii) find the corresponding minimal network where the constraints are as explicit as possible. Both tasks are known to be NP-complete in the general case. Task (1) is usually solved using a backtracking algorithm, and task (ii) is often solved only approximately by enforcing various levels of local consistency. In this paper, we identify a property of binary constraint called row convexity and show its usefulness in deciding when a form of local consistency called path consistency is sufficient to guarantee that a network is both minimal and globally consistent. Globally consistent networks have the property that a solution can be found without backtracking. We show that one can test for the row convexity property efficiently and we show, by examining applications of constraint networks discussed in the literature, that our results are useful in practice. Thus, we identify a class of binary constraint networks for which we can solve both tasks (i) and (ii) efficiently. Finally, we generalize the results for binary constraint networks to networks with nonbinary constraints.We consider multiprocessing systems where processes make independent, Poisson distributed resource requests with mean arrival time 1. We assume that resources are not released. It is shown that the expected deadlock time is never less than 1, no matter how many processes and resources are in the system. Also, the expected number of processes blocked by deadlock time is one-half more than half the number of initially active processes. We obtain expressions for system statistics such as expected deadlock time, expected total processing time, and system efficiency, in terms of Abel sums. We derive asymptotic expressions for these statistics in the case of systems with many processes and the case of systems with a fixed number of processes. In the latter, generalizations of the Ramanujan Q-function arise. we use singularity analysis to obtain asymptotics of coefficients of generalized Q-functions.The existence of Nash equilibria in noncooperative flow control in a general product-form network shared by K users is investigated. The performance objective of each user is to maximize its average throughput subject to an upper bound on its average time-delay. Previous attempts to study existence of equilibria for this flow control model were not successful, partly because the time-delay constraints couple the strategy spaces of the individual users in a way that does not allow the application of standard equilibrium existence theorems from the game theory literature. To overcome this difficulty, a more general approach to study the existence of Nash equilibria for decentralized control schemes is introduced. This approach is based on directly proving the existence  of a fixed point of the best reply correspondence of the underlying game. For the investigated flow control model, the best reply correspondence is shown to be a function, implicitly defined by means of K interdependent linear programs. Employing an appropriate definition for continuity of the set of optimal solutions of parameterized linear programs, it is shown that, under appropriate conditions, the best reply function is continuous. Brouwer's theorem implies, then, that the best reply function has a fixed point.Two deterministic routing networks are presented: the pruned butterfly and the sorting fat-tree. Both networks are area-universal, that is, they can simulate any other routing network fitting in similar area with polylogarithmic slowdown. Previous area-universal networks were either for the off-line problem, where the message set to be routed is known in advance and substantial precomputation is permitted, or involved randomization, yielding results that hold only with high probability. The two networks introduced here are the first that are simultaneously deterministic and on-line, and they use two substantially different routing techniques. The performance of their routing algorithms depends on the difficulty of the problem instance, which is  measured by a quantity &lgr; known as the load factor. The pruned butterfly runs in time O(&lgr;log2N), is the number of possible sources and destinations for messages and &lgr; is assumed to be polynomial in N. The sorting fat-tree algorithm runs in O(&lgr; log N + log2 N) time for a restricted class of message sets including partial permutations. Other results of this work include a “flexible” circuit that is area-time optimal across a range of different input sizes and an area-time lower bound for routers based on wire-length arguments.Consider a switching component in a packet-switching network, where messages from several incoming channels arrive and are routed to appropriate outgoing ports according to a service policy. One requirement in the design of such a system is to determine the buffer storage necessary at the input of each channel and the policy for serving these buffers that will prevent buffer overflow and the corresponding loss of messages. In this paper, a class of buffer service policies, called Least Time to Reach Bound (LTRB), is introduced that guarantees no overflow, and for which the buffer size required at each input channel is independent of the number of channels and their relative speeds. Further, the storage requirement is only twice the maximal length of a message in all cases, and as a consequence the class is shown to be optimal in the sense that any nonoverflowing policy requires at least as much storage as LTRB.We propose that the phenomenon of local state may be understood in terms of Strachey's concept of parametric (i.e., uniform) polymorphism. The intuitive basis for our proposal is the following analogy: a non-local procedure is independent of locally-declared variables in the same way that a parametrically polymorphic function is independent of types to which it is instantiated.A connection between parametricity and representational abstraction was first suggested by J.C. Reynolds. Reynolds used logical relations to formalize this connection in languages with type variables and user-defined types. We use relational parametricity to construct a model for an Algol-like language in which interactions between local and non-local entities satisfy certain relational criteria.  Reasoning about local variables essentially involved proving properties of polymorphic functions. The new model supports straightforward validations of all the test equivalences that have been proposed in the literature for local-variable semantics, and encompasses standard methods of reasoning about data representations. It is not known whether our techniques yield fully abstract semantics. A model based on partial equivalence relations on the natural numbers is also briefly examined.
We investigate logical formalization of the effects of actions in the situation calculus. We propose a formal criterion against which to evaluate theories of deterministic actions. We show how the criterion provides us a formal foundation upon which to tackle the frame problem, as well as its variant in the context of concurrent actions. Our main technical contributions are in formulating a wide class of monotonic causal theories that satisfy the criterion, and showing that each such theory can be reformulated succinctly in circumscription.We present a randomized linear-time algorithm to find a minimum spanning tree in a connected graph with edge weights. The algorithm uses random sampling in combination with a recently discovered linear-time algorithm for verifying a minimum spanning tree. Our computational model is a unit-cost random-access machine with the restriction that the only operations allowed on edge weights are binary comparisons.The magic rewriting focuses on relevant data but suffers from additional rules, predicates, and tuples that are generated in search for the relevant data. Reducing the arity of predicates can cut down the number of such rules, predicates, and tuples by an exponential factor. In this paper, we consider a subclass of linear single-IDB programs and show that the magic rewriting can be decomposed in such a way that it is applied to only programs having smaller arities and fewer recursive rules, without losing the binding capacity. The decomposed rewriting is shown to be much more efficient than the standard one and amenable to distributed and parallel environments. The considered subclass significantly generalizes recursions previously proposed for efficient implementation. The decomposed rewriting and the standard generalized magic rewriting are extended to multi-binding queries in such a way that data relevant to one binding is not necessarily considered as relevant to other bindings. The work in this paper shows the use of tuple ID as an important technique in optimizing logic programs.We study the communication complexity of asynchronous distributed algorithms. Such algorithms can generate excessively many messages in the worst case. Nevertheless, we show that, under certain probabilistic assumptions, the expected number of messages generated per time unit is bounded by a polynomial function of the number of processors under a very general model of distributed computation. Furthermore, for constant-degree processor graphs, the expected number of generated messages is only O(nT), where n is the number of processors and T is the running time. We conclude that (under our model) any asynchronous algorithm with good time complexity will also have good communication complexity, on the average.In this paper we consider problems and complexity classes definable by interdependent queries to an oracle in NP. How the queries depend on each other is specified by a directed graph G. We first study the class of problems where G is a general dag and show that this class coincides with   Dp2 .We then consider the class where G is a tree. Our main result states that this class is identical to PNP[O(log n)], the class of problems solvable in polynomial time with a logarithmic number of queries to an oracle in NP. This result has interesting applications in the fields of modal logic and artificial intelligence. In particular, we show that the following problems are all PNP[O(log n)] complete: validity-checking of formulas in Carnap's modal logic, checking whether a formula is almost surely valid over finite structures in modal logics K, T, and S4 (a problem recently considered by Halpern and Kapron [1992]), and checking whether a formula belongs to the stable set of beliefs generated by a propositional theory.We generalize the case of dags to the case where G is a general (possibly cyclic) directed graph of NP-oracle queries and show that this class corresponds to   Pp2 . We show that such  graphs are easily expressible in autoepistemic logic. Finally, we generalize our complexity results to higher classes of the polynomial-time hierarchy.Three temporal logics are introduced that induce on labeled transition systems the same identifications as branching bisimulation, a behavioral equivalence that aims at ignoring invisible transitions while preserving the branching structure of systems. The first logic is an extension of Hennessy-Milner Logic with an “until” operator. The second one is another extension of Hennessy-Milner Logic, which exploits the power of backward modalities. The third logic is CTL* without the next-time operator. A relevant side-effect of the last characterization is that it sets a bridge between the state- and action-based approaches to the semantics of concurrent systems.This paper gives an algorithm for solving linear programming problems. For a problem with n constraints and d variables, the algorithm requires an expectedOd2n+lognOdd/2+O1+Od4nlogn arithmetic operations, asn→∞. The constant factors do not depend on d. Also, an algorithm is given for integer linear programming. Let 4 bound the number of bits required to specify the rational numbers defining an input constraint or the    objective function vector. Let n and d be as before. Then, the algorithm requires expected   O2ddn+8ddnl nnlnn +dod4 lnn  operations on numbers with   do14  bits, as   n→∞ , where the constant factors do not depend on d or4to other convex programming problems. For example, an algorithm for finding the smallest sphere enclosing a set of n points in  Edhas  the same time bound.This paper presents algorithms for routing channels with L≥2 layers. For the unit vertical overlap model, we describe a two-layer channel routing algorithm that uses at most   d+Od   tracks to route two-terminal net problems and   2d+od   tracks to route multiterminal nets. We also show that   d+Wlog d   tracks are required to route two-terminal net problems in the worst case even if arbitrary vertical overlap is allowed. We generalize the algorithm to unrestricted multilayer routing and use only  d/L-1+O d/L+1  tracks for two-terminal net problems (within   within Od/L+ 1tracks of optimaland d/L-2+O d/L+1  tracks for multiterminal net problems   within a factor ofL-1 /L-2times optimal . We demonstrate the generality of our routing strategy by showing that it can be used to duplicate some of the best previous upper bounds for other models (two-layer Manhattan routing and two and three-layer knock-knee routing of two-terminal, two-sided nets), and gives a new upper bound for rotuing with 45-degree diagonal wires.
Abduction is an important form of nonmonotonic reasoning allowing one to find explanations for certain symptoms or manifestations. When the application domain is described by a logical theory, we speak about logic-based abduction. Candidates for abductive explanations are usually subjected to minimality criteria such as subset-minimality, minimal cardinality, minimal weight, or minimality under prioritization of individual hypotheses. This paper presents a comprehensive complexity analysis of relevant decision and search problems related to abduction on propositional theories. Our results indicate that abduction is harder than deduction. In particular, we show that with the most basic forms of abduction the relevant decision problems are complete for complexity classes at the second level of the polynomial hierarchy, while the use of prioritization raises the complexity to the third level in certain cases.We introduce a new subclass of Allen's interval algebra we call “ORD-Horn subclass,” which is a strict superset of the “pointisable subclass.” We prove that reasoning in the ORD-Horn subclass is a polynomial-time problem and show that the path-consistency method is sufficient for deciding satisfiability. Further, using an extensive machine-generated case analysis, we show that the ORD-Horn subclass is a maximal tractable subclass of the full algebra (assuming P   ≠  NP). In fact, it is the unique greatest tractable subclass amongst the subclasses that contain all basic relations.We define the notion of a well-separated pair decomposition of points in d-dimensional space. We then develop efficient sequential and parallel algorithms for computing such a decomposition. We apply the resulting decomposition to the efficient computation of k-nearest neighbors and n-body potential fields.Network throughput can be increased by allowing multipath, adaptive routing. Adaptive routing allows more freedom in the paths taken by messages, spreading load over physical channels more evenly. The flexibility of adaptive routing introduces new possibilities of deadlock. Previous deadlock avoidance schemes in k-ary n-cubes require an exponential number of virtual channels. We describe a family of deadlock-free routing algorithms, called planar-adaptive routing algorithms, that require only a constant number of virtual channels, independent of networks size and dimension. Planar-adaptive routing algorithms reduce the complexity of deadlock prevention by reducing the number of choices at each routing step. In the fault-free case, planar-adaptive networks are guaranteed to be  deadlock-free. In the presence of network faults, the planar-adaptive router can be extended with misrouting to produce a working network which remains provably deadlock free and is provably livelock free. In addition, planar-adaptive networks can simultaneously support both in-order and adaptive, out-of-order packet delivery.Planar-adaptive routing is of practical significance. It provides the simplest known support for deadlock-free adaptive routing in k-ary n-cubes of more than two dimensions (with k>2). Restricting adaptivity reduces the hardware complexity, improving router speed or allowing additional performance-enhancing network features. The structure of planar-adaptive routers is amenable to efficient implementation.Simulation studies show that  planar-adaptive routers can increase the robustness of network throughput for nonuniform communication patterns. Planar-adaptive routers outperform deterministic routers with equal hardware resources. Further, adding virtual lanes to planar-adaptive routers increases this advantage. Comparisons with fully adaptive routers show that planar-adaptive routers, limited adaptive routers, can give superior performance. These results indicate the best way to allocate router resources to combine adaptivity and virtual lanes.Planar-adaptive routers are a special case of limited adaptivity routers. We define a class of adaptive routers with f degrees of routing freedom. This class, termed f-flat adaptive routers, allows a direct cost-performance tradeoff between implementation cost  (speed and silicon area) and routing freedom (channel utilization). For a network of a particular dimension, the cost of adaptivity grows linearly with the routing freedom. However, the rate of growth is a much larger constant for high-dimensional networks. All of the properties proven for planar-adaptive routers, such as deadlock and livelock freedom, also apply to f-flat adaptive routers.Emulators that translate algorithms from the shared-memory model to two different message-passing models are presented. Both are achieved by implementing a wait-free, atomic, single-writer multi-reader register in unreliable, asynchronous networks. The two message-passing models considered are a complete network with processor failures and an arbitrary network with dynamic link failures.These results make it possible to view the shared-memory model as a higher-level language for designing algorithms in asynchronous distributed systems. Any wait-free algorithm based on atomic, single-writer multi-reader registers can be automatically emulated in message-passing systems, provided that at least a majority of the processors are not faulty and remain connected. The overhead introduced by these emulations is polynomial in the number of processors in the system.Immediate new results are obtained by applying the emulators to known shared-memory algorithms. These include, among others, protocols to solve the following problems in the message-passing model in the presence of processor or link failures: multi-writer multi-reader registers, concurrent time-stamp systems, l-exclusion, atomic snapshots, randomized consensus, and implementation of data structures.This paper gives two simple efficient distributed algorithms: one for keeping clocks in a network synchronized and one for allowing new processors to join the network with their clocks synchronized. Assuming a fault-tolerant authentication protocol, the algorithms tolerate both link and processor failures of any type. The algorithm for maintaining synchronization works for arbitrary networks (rather than just completely connected networks) and tolerates any number of processor or communication link faults as long as the correct processors remain connected by fault-free paths. It thus represents an improvement over other clock synchronization algorithms such as those of Lamport and Melliar Smith and Welch and Lynch, although, unlike them, it does require an authentication protocol to handle Byzantine faults. Our algorithm for allowing new processors to join requires that more than half the processors be correct, a requirement that is provably necessary.A simple wait-free construction of 1-writer multireader multivalued atomic variable from multireader regular variables is presented in this paper. A key point of the construction is the use of an elegant forwarding technique to overcome the new-old inversion property inherent in regular variables.Another construction, using a different forwarding technique, is also given. This technique is a refinement of one proposed in the literature.Formal correctness proofs for both the constructions are short and easy to follow.In this paper, we derive bounds on the speedup and efficiency of applications that schedule tasks on a set of parallel processors. We assume that the application runs an algorithm that consists of N iterations and before starting its i+1st iteration, a processor must wait for data (i.e., synchronize) calculated in the ith iteration by a subset of the other processors of the system. Processing times and interconnections between iterations are modeled by random variables with possibly deterministic distributions. Scientific applications consisting of iterations of recursive equations are examples of such applications that can be modeled within this formulation. We consider the efficiency of applications and show that, although efficiency decreases with an increase in the number of processors, it has a nonzero limit when the number of processors increases to infinity. We obtain a lower bound for the efficiency by solving an equation that depends on the distribution of task service times and the expected number of tasks needed to be synchronized. We also show that the lower bound is approached if the topology of the processor graph is ldquo;spread-out,” a notion we define in the paper.In the concurrent language CCS, two programs are considered the same if they are bisimilar. Several years and many researchers have demonstrated that the theory of bisimulation is mathematically appealing and useful in practice. However, bisimulation makes too many distinctions between programs. We consider the problem of adding operations to CCS to make bisimulation fully abstract. We define the class of GSOS operations, generalizing the style and technical advantages of CCS operations. We characterize GSOS congruence in as a bisimulation-like relation called ready-simulation. Bisimulation is strictly finer than ready simulation, and hence not a congruence for any GSOS language.A program correctness checker is an algorithm for checking the output of a computation. That is, given a program and an instance on which the program is run, the checker certifies whether the output of the program on that instance is correct. This paper defines the concept of a program checker. It designs program checkers for a few specific and carefully chosen problems in the class FP of functions computable in polynomial time. Problems in FP for which checkers are presented in this paper include Sorting, Matrix Rank and GCD. It also applies methods of modern cryptography, especially the idea of a probabilistic interactive proof, to the design of program checkers for group theoretic computations.Two structural theorems are proven here. One is a characterization of problems that can be checked. The other theorem establishes equivalence classes of problems such that whenever one problem in a class is checkable, all problems in the class are checkable.
Given an off-line sequence S of n set-manipulation operations, we investigate the parallel complexity of evaluating S (i.e., finding the response to every operation in S and returning the resulting set). We show that the problem of evaluating S is in NC for various combinations of common set-manipulation operations. Once we establish membership in NC (or, if membership in NC is obvious), we develop techniques for improving the time and/or processor complexity.The problem of Verifiable Secret Sharing (VSS) is the following: A dealer, who may be honest or cheating, can share a secret s, among n ≥ 2t + 1 players, where t players at most are cheaters. The sharing process will cause the dealer to commit himself to a secret s. If the dealer is honest, then, during the sharing process, the set of dishonest players will have no information about s. When the secret is reconstructed, at a later time, all honest players will reconstruct s. The solution that is given is a constant round protocol, with polynomial time local computations and polynomial message size. The protocol assumes private communication lines between every two  participants, and a broadcast channel. The protocol achieves the desired properties with an exponentially small probability of error.A new tool, called Information Checking, which provides authentication and is not based on any unproven assumptions, is introduced, and may have wide application elsewhere.For the case in which it is known that the dealer is honest, a simple constant round protocol is proposed, without assuming broadcast.A weak version of secret sharing is defined: Weak Secret Sharing (WSS). WSS has the same properties as VSS for the sharing process. But, during reconstruction, if the dealer is dishonest, then he might obstruct the reconstruction of s. A protocol for WSS is also introduced. This protocol has an  exponentially small probability of error. WSS is an essential building block for VSS. For certain applications, the much simpler WSS protocol suffice.All protocols introduced in this paper are secure in the Information Theoretic sense.Although many closed multiclass queuing networks have a product-form solution, evaluating their performance measures remains nontrivial due to the presence of a normalization constant. We propose the application of Monte Carlo summation in order to determine the normalization constant, throughputs, and gradients of throughputs. A class of importance-sampling functions leads to a decomposition approach, where separate single-class problems are first solved in a setup module, and then the original problem is solved by aggregating the single-class solutions in an execution model. We also consider Monte Carlo methods for evaluating performance measures based on integral representations of the normalization constant; a theory for optimal importance sampling is developed. Computational  examples are given that illustrate that the Monte Carlo methods are robust over a wide range of networks and can rapidly solve networks that cannot be handled by the techniques in the existing literature.Bottom-up evaluation of deductive database programs has the advantage that it avoids repeated computations by storing all intermediate results and replacing recomputation by table lookup. However, in general, storing all intermediate results for the duration of a computation wastes space. In this paper, we propose an evaluation scheme that avoids recomputation, yet for a fairly general class of programs at any given time stores only a small subset of the facts generated. The results constitute a significant first step in compile-time garbage collection for bottom-up evaluation of deductive database programs.Though the declarative semantics of both explicit and nonmonotonic negation in logic programs has been studied extensively, relatively little work has been done on computation and implementation of these semantics. In this paper, we study three different approaches to computing stable models of logic programs based on mixed integer linear programming methods for automated deduction introduced by R. Jeroslow. We subsequently discuss the relative efficiency of these algorithms. The results of experiments with a prototype compiler implemented by us tend to confirm our theoretical discussion. In contrast to resolution, the mixed integer programming methodology is both fully declarative and handles reuse of old computations gracefully.We also introduce, compare, implement, and  experiment with linear constraints corresponding to four semantics for “explicit” negation in logic programs: the four-valued annotated semantics [Blair and Subrahmanian 1989], the Gelfond-Lifschitz semantics [1990], the over-determined models [Grant and Subrahmanian 1989], the Gelfond-Lifschitz semantics [1990], the over-determined models [Grant and Subrahmanian 1990], and the classical logic semantics. Gelfond and Lifschitz[1990] argue for simultaneous use of two modes of negation in logic programs, “classical” and “nonmonotonic,” so we give algorithms for computing “answer sets” for such logic programs too.A class of “modularly stratified” logic programs is defined. Modular stratification generalizes stratification and local stratification, while allowing programs that are not expressible as stratified programs. For modularly stratified programs, the well-founded semantics coincides with the stable model semantics and makes every ground literal true or false. Modularly stratified programs are weakly stratified, but the converse is false. Unlike some weakly stratified programs, modularly stratified programs can be evaluated in a subgoal-at-a time fashion. An extension of top-down methods with memoing that handles this broader class of programs is presented. A technique for rewriting a modularly stratified program for bottom-up evaluation is demonstrated and extended to include magic-set techniques. The rewritten program, when evaluated bottom-up, gives correct answers according to the well-founded semantics, but much more efficiently than computing the complete well-founded model. A one-to-one correspondence between steps of the extended top-down method and steps during the bottom-up evaluation of the magic-rewritten program is exhibited, demonstrating that the complexity of the two methods is the same. Extensions of modular stratification to other operators such as set-grouping and aggregation, which have traditionally been stratified to prevent semantic difficulties, are discussed.Efficient distribution-free learning of Boolean formulas from positive and negative examples is considered. It is shown that classes of formulas that are efficiently learnable from only positive examples or only negative examples have certain closure properties. A new substitution technique is used to show that in the distribution-free case learning DNF (disjunctive normal form formulas) is no harder than learning monotone DNF. We prove that monomials cannot be efficiently learned from negative examples alone, even if the negative examples are uniformly distributed. It is also shown that, if the examples are drawn from uniform distributions, then the class of DNF in which each variable occurs at most once is efficiently weakly learnable (i.e., individual examples are  correctly classified with a probability larger than 1/2 + 1/p, where p is a polynomial in the relevant parameters of the learning problem). We then show an equivalence between the notion of weak learning and the notion of group learning, where a group of examples of polynomial size, either all positive or all negative, must be correctly classified with high probability.
One important facet of common-sense reasoning is the ability to draw default conclusions about the state of the world, so that one can, for example, assume that a given bird flies in the absence of information to  the contrary. A deficiency in the circumscriptive approach to common-sense reasoning has been its difficulties in producing default that Tweety   ≠  Blutto using ordinary circumscription, or conclude by default that a particular bird flies, if some birds are known not to fly. In this paper, we introduce a new form of circumscription, based on homomorphisms between models, that remedies these two problems and still retains the major desirable properties of traditional forms of circumscription.In this paper, we study quantitative as well as qualitative properties of Fork-Join Queuing Networks with Blocking (FJQN/Bs). Specifically, we prove results regarding the equivalence of the behavior of a FJQN/B and that of its duals and a strongly connected marked graph. In addition, we obtain general conditions that must be satisfied by the service times to guarantee the existence of a long-term throughput and its independence on the initial configuration. We also establish conditions under which the reverse of a FJQN/B has the same throughput as the original network. By combining the equivalence result for duals and the reversibility result, we establish a symmetry property for the throughput of a FJQN/B. Last, we establish that the throughput is a concave function of the buffer sizes and the initial marking, provided that the service times are mutually independent random variables belonging to the class of PERT distributions that includes the Erlang distributions. This last result coupled with the symmetry property can be used to identify the initial configuration that maximizes the long-term throughput in closed series-parallel networks.This paper considers the problem of representing stacks with catenation so that any stack, old or new, is available for access or update operations. This problem arises in the implementation of list-based and functional programming languages. A solution is proposed requiring constant time and space for each stack operation except catenation, which requires O(log log k) time and space. Here k is the number of stack operations done before the catenation. All the resource bounds are amortized over the sequence of operations.We present a practical algorithm for finding minimum-length paths between points in the Euclidean plane with (not necessarily convex) polygonal obstacles. Prior to this work, the best known algorithm for finding the shortest path between two points in the plane required &OHgr;(n2 log n) time and O(n2) space, where n denotes the number of obstacle edges. Assuming that a triangulation or a Voronoi diagram for the obstacle space is provided with the input (if is not, either one can be precomputed in O(n log n) time), we present an O(kn) time algorithm, where k denotes the number of  “islands”  (connected components) in the obstacle space. The algorithm uses only O(n) space and, given a source point s, produces an O(n) size data structure such that the distance between s and any other point x in the plane (x) is not necessarily an obstacle vertex or a point on an obstacle edge) can be computed in O(1) time. The algorithm can also be used to compute shortest paths for the movement of a disk (so that optimal movement for arbitrary objects can be computed to the accuracy of enclosing them with the smallest possible disk).We derive a single-exponential time upper bound for finding the shortest path between two points in 3-dimensional Euclidean space with (nonnecessarily convex) polyhedral obstacles. Prior to this work, the best known algorithm required double-exponential time. Given that the problem is known to be PSPACE-hard, the bound we present is essentially the best (in the worst-case sense) that can reasonably be expected.Many fundamental multi-processor coordination problems can be expressed as counting problems: Processes must cooperate to assign successive values from a given range, such as addresses in memory or destinations on an interconnection network. Conventional solutions to these problems perform poorly because of synchronization bottlenecks and high memory contention.Motivated by observations on the behavior of sorting networks, we offer a new approach to solving such problems, by introducing counting networks, a new class of networks that can be used to count. We give two counting network constructions, one of depth log n(1 + log n)/2 using n log (1 + log    n)/4 “gates,” and a second of depth log2 n using n log2 n/2 gates. These networks avoid the sequential bottlenecks inherent to earlier solutions and substantially lower the memory contention.Finally, to show that counting networks are not merely mathematical creatures, we provide experimental evidence that they outperform conventional synchronization techniques under a variety of circumstances.
We are interested in the problem of solving a system<si=ti:1≤i≤n,pj≠qj:1≤j≤m> of equations and disequations, also known as disunification. Solutions to disunification problems are substitutions for the variables of the problem that make the two terms of each equation equal, but leave those of the disequations different. We investigate this in both algebraic and logical contexts where equality is defined by an equational theory and more generally by a definitive clause equality theory E. We show how E-disunification can be reduced to E-unification, that is, solving equations only, and give a disunification algorithm for theories  given a unification algorithm. In fact, this result shows that for theories in which the solutions of all unification problems can also be represented finitely.  We sketch how disunification can be applied to handle negation in logic programming with equality in a similar style to Colmerauer's logic programming with rational trees, and to represent many solutions to AC-unification problems by a few solutions to ACI-disunification problems.We consider the following problem: given a collection of strings s1,…, sm, find the shortest string s such that each si appears as a substring (a consecutive block) of s. Although this problem is known to be NP-hard, a simple greedy procedure appears to do quite well and is routinely used in DNA sequencing and data compression practice, namely: repeatedly merge the pair of (distinct) strings with maximum overlap until only one string remains. Let n denote the length of the optimal superstring. A common conjecture states that the above greedy procedure produces a superstring of length O(n) (in fact, 2n), yet the only previous nontrivial bound known for any polynomial-time algorithm is a recent O(n log n) result.We show that the greedy algorithm does in fact achieve a constant factor approximation, proving an upper bound of 4n. Furthermore, we present a simple modified version of the greedy algorithm that we show produces a superstring of length at most 3n. We also show the superstring problem to be MAXSNP-hard, which implies that a polynomial-time approximation scheme for this problem is unlikely.Three new decomposition methods are developed for the exact analysis of stochastic multi-facility blocking models of the product-form type. The first is a basic decomposition algorithm that reduces the analysis of blocking probabilities to that of two separate subsystems. The second is a generalized M-subsystem decomposition method. The third is a more elaborate and efficient incremental decomposition technique. All of the algorithms exploit the sparsity of locality that can be found in the demand matrix of a system. By reducing the analysis to that of a set of subsystems, the overall dimensionality of the problem is diminished and the computational requirements are reduced significantly. This enables the efficient computation of blocking probabilities in large systems. Several  numerical examples are provided to illustrate the computational savings that can be realized.One of the most important performance measures for computer system designers is system availability. Most often, Markov models are used in representing systems for dependability/availability analysis. Due to complex interactions between components and complex repair policies, the Markov model often has an irregular structure, and closed-form solutions are extremely difficult to obtain. Also, a realistic system model often has an unmanageably large state space and it quickly becomes impractical to even generate the entire transition rate matrix. In this paper, we present a methodology that can (i) bound the system steady state availability and at the same time, (ii) drastically reduce the state space of the model that must be solved. The bounding algorithm is iterative and generates a part of the transition matrix at each step. At each step, tighter bounds on system availability are obtained. The algorithm also allows the size of the submodel, to be solved at each step, to be chosen so as to accommodate memory limitations. This general bounding methodology provides an efficient way to evaluate dependability models with very large state spaces without ever generating the entire transition rate matrix.Text compression methods can be divided into two classes: symbolwise and parsing. Symbolwise methods assign codes to individual symbols, while parsing methods assign codes to groups of consecutive symbols (phrases). The set of phrases available to a parsing method is referred to as a dictionary. The vast majority of parsing methods in the literature use greedy parsing (including nearly all variations of the popular Ziv-Lempel methods). When greedy parsing is used, the coder processes a string from left to right, at each step encoding as many symbols as possible with a phrase from the dictionary. This parsing strategy is not optimal, but an optimal method cannot guarantee a bounded coding delay.An important problem in compression research has been to establish the relationship between symbolwise methods and parsing methods. This paper extends prior work that shows that there are symbolwise methods that simulate a subset of greedy parsing methods. We provide a more general algorithm that takes any nonadaptive greedy parsing method and constructs a symbolwise method that achieves exactly the same compression. Combined with the existence of symbolwise equivalents for two of the most significant adaptive parsing methods, this result gives added weight to the idea that research aimed at increasing compression should concentrate on symbolwise methods, while parsing methods should be chosen for speed or temporary storage considerations.The time complexity of wait-free algorithms in “normal” executions, where no failures occur and processes operate at approximately the same speed, is considered. A lower bound of log n on the time complexity of any wait-free algorithm that achieves approximate agreement among n processes is proved. In contrast, there exists a non-wait-free algorithm that solves this problem in constant time. This implies an &OHgr;(log n) time separation between the wait-free and non-wait-free computation models. On the positive side, we present an O(log n) time wait-free approximate agreement algorithm; the complexity of this algorithm is within a small constant of the lower bound.This paper investigates the computational complexity of planning the motion of a body B in 2-D or 3-D space, so as to avoid collision with moving obstacles of known, easily computed, trajectories. Dynamic movement problems are of fundamental importance to robotics, but their computational complexity has not previously been investigated.We provide evidence that the 3-D dynamic movement problem is intractable even if B has only a constant number of degrees of freedom of movement. In particular, we prove the problem is PSPACE-hard if B is given a velocity modulus bound on its movements and is NP-hard even if B has no velocity modulus bound, where, in both cases, B has 6 degrees of freedom. To prove these results, we use a unique method of simulation of a Turing machine that uses time to encode configurations (whereas previous lower bound proofs in robotic motion planning used the system position to encode configurations and so required unbounded number of degrees of freedom).We also investigate a natural class of dynamic problems that we call asteroid avoidance problems: B, the object we wish to move, is a convex polyhedron that is free to move by translation with bounded velocity modulus, and the polyhedral obstacles have known translational trajectories but cannot rotate. This problem has many applications to robot, automobile, and aircraft collision avoidance. Our main positive results are polynomial time algorithms for the 2-D asteroid avoidance problem, where B is a moving polygon and we assume a constant number of obstacles, as well as single exponential time or polynomial space algorithms for the 3-D asteroid avoidance problem, where B is a convex polyhedron and there are arbitrarily many obstacles. Our techniques for solving these asteroid avoidance problems use “normal path” arguments, which are an intereting generalization of techniques previously used to solve static shortest path problems.We also give some additional positive results for various other dynamic movers problems, and in particular give polynomial time algorithms for the case in which B has no velocity bounds and the movements of obstacles are algebraic in space-time.This paper presents new upper bounds for channel routing of multiterminal nets, which answers the long-standing open question whether or not multiterminal problems really require channels two times wider than 2-terminal problems. We transform any multiterminal problem of density d into a so-called extended simple channel routing problem (ESCRP) of density   3d/2+Odlog d   . We then descibe routing algorithms for solving ESCRPs in three different models. The channel width w is   ≤3d/2+Odlog          d   in the knock-knee and unit-vertical-overlap models, and    w≤3d/2+O dlogd +Of   in the Manhattan model, where f is the flux of the problem. In all three cases, we improve the best-known upper bounds.
The concepts of binary constraint satisfaction problems can be naturally generalized to the relation algebras of Tarski. The concept of path-consistency plays a central role. Algorithms for path-consistency can be implemented on matrices of relations and on matrices of elements from a relation algebra. We give an example of a 4-by-4 matrix of infinite relations on which on iterative local path-consistency algorithm terminates. We give a class of examples over a fixed finite algebra on which all iterative local algorithms, whether parallel or sequential, must take quadratic time. Specific relation algebras arising from interval constraint problems are also studied: the Interval Algebra, the Point Algebra, and the Containment Algebra.The problem of coloring a graph with the minimum number of colors
is well known to be NP-hard, even restricted to
k-colorable graphs for constant
k ≥ 3. This paper explores the
approximation problem of coloring
k-colorable graphs with as few
additional colors as possible in polynomial time, with special focus on
the case of k = 3.The previous best upper bound on the number of colors needed for
coloring 3-colorable n-vertex graphs
in polynomial time was 

on/logn

 colors by Berger and Rompel, improving a bound of


on

 colors by Wigderson. This paper presents an algorithm
to color any 3-colorable graph with 

on3/8polylog
n

 colors, thus breaking an
“O((n1/2-&ogr;(1))
barrier”. The algorithm given here is based on examining
second-order neighborhoods of vertices, rather than just immediate
neighborhoods of vertices as in previous approaches. We extend our
results to improve   the worst-case bounds for coloring
k-colorable graphs for constant
k > 3 as well.We investigate the descriptive succinctness of three fundamental notions for modeling concurrency: nondeterminism and pure parallelism, the two facets of alternation, and bounded cooperative concurrency, whereby a system configuration consists of a bounded number of cooperating states. Our results are couched in the general framework of finite-state automata, but hold for appropriate versions of most concurrent models of computation, such as Petri nets, statecharts or finite-state versions of concurrent programming languages. We exhibit exhaustive sets of upper and lower bounds on the relative succinctness of these features over &Sgr;* and &Sgr;&ohgr;, establishing that:(1) Each of the three features  represents an exponential saving in succinctness of the representation, in a manner that is independent of the other two and additive with respect to them.(2) Of the three, bounded concurrency is the strongest, representing a similar exponential saving even when substituted for each of the others.For example, we prove exponential upper and lower bounds on the simulation of deterministic concurrent automata by AFAs, and triple-exponential bounds on the simulation of alternating concurrent automata by DFAs.This is the second in a series of papers on the inherent power of bounded cooperative concurrency, whereby an automaton can be in some bounded number of states that cooperate in accepting the input. In this paper, we consider pushdown automata. We are interested in differences in power of expression and in exponential (or higher) discrepancies in succinctness between variants of pda's that incorporate nondeterminism (E), pure parallelism (A), and bounded cooperative concurrency (C). Technically, the results are proved for cooperating push-down automata with cooperating states, but they hold for appropriate versions of most concurrent models of computation. We exhibit exhaustive sets of upper and lower bounds on the relative succinctness of these features for three classes of   languages: deterministic context-free, regular, and finite. For example, we show that C represents exponential savings in succinctness in all cases except when both E and A are present (i.e., except for alternating automata), and that E and A represent unlimited savings in succinctness in all cases.We present new procedures for inferring the structure of a finite-state automaton (FSA) from its input/output behavior, using access to the automaton to perform experiments.Our procedures use a new representation for finite automata, based on the notion of equivalence between tests. We call the number of such equivalence classes the diversity of the automaton; the diversity may be as small as the logarithm of the number of states of the automaton. For the special class of permutation automata, we describe an inference procedure that runs in time polynomial in the diversity and log(1/&dgr;), where &dgr; is a given upper bound on the probability that our procedure returns an incorrect result. (Since our procedure uses randomization to perform experiments, there is a certain controllable chance that it will return an erroneous result.) We also discuss techniques for handling more general automata.We present evidence for the practical efficiency of our approach. For example, our procedure is able to infer the structure of an automaton based on Rubik's Cube (which has approximately 1019 states) in about 2 minutes on a DEC MicroVax. This automaton is many orders of magnitude larger than possible with previous techniques, which would require time proportional at least to the number of global states. (Note that in this example, only a small fraction (10-14) of the global states were even visited.)Finally, we present a new procedure for  inferring automata of a special type in which the global state is composed of a vector of binary local state variables, all of which are observable (or visible) to the experimenter. Our inference procedure runs provably in time polynomial in the size of this vector (which happens to be the diversity of the automaton), even though the global state space may be exponentially larger. The procedure plans and executes experiments on the unknown automaton; we show that the number of input symbols given to the automaton during this process is (to within a constant factor) the best possible.
A spanning tree in a graph is the smallest connected spanning subgraph. Given a graph, how does one find the smallest (i.e., least number of edges) 2-connected spanning subgraph (connectivity refers to both edge and vertex connectivity, if not specified)? Unfortunately, the problem is known to be NP-hard.We consider the problem of finding a better approximation to the smallest 2-connected subgraph, by an efficient algorithm. For 2-edge connectivity, our algorithm guarantees a solution that is no more than 3/2 times the optimal. For 2-vertex connectivity, our algorithm guarantees a solution that is no more than 5/3 times the optimal. The previous best approximation factor is 2 for each of these problems. The new algorithms (and their analyses) depend upon a structure called a carving of a graph, which is of independent interest. We show that approximating the optimal solution to within an additive constant is NP-hard as well.We also consider the case where the graph has edge weights. For this case, we show that an approximation factor of 2 is possible in polynomial time for finding a k-edge connected spanning subgraph. This improves an approximation factor of 3 for k = 2, due to Frederickson and Ja´Ja´ [1981], and extends it for any k (with an increased running time though).We describe the application of proof orderings—a technique for reasoning about inference systems-to various rewrite-based theorem-proving methods, including refinements of the standard Knuth-Bendix completion procedure based on critical pair criteria; Huet's procedure for rewriting modulo a congruence; ordered completion (a refutationally complete extension of standard completion); and a proof by consistency procedure for proving inductive theorems.A model that captures communication on asynchronous unidirectional rings is formalized. Our model incorporates both probabilistic and nondeterministic features and is strictly more powerful than a purely probabilistic model. Using this model, a collection of tools are developed that facilitate studying lower bounds on the expected communication complexity of Monte Carlo algorithms for language recognition problems on anonymous asynchronous unidirectional rings. The tools are used to establish tight lower bounds on the expected bit complexity of the Solitude Verification problem that asymptotically match upper bounds for this problem. The bounds demonstrate that, for this problem, the expected bit complexity depends subtly on the processors' knowledge of the size of the ring and on whether or not processor-detectable termination is required.We present a construction of a single-writer, multiple-reader atomic register from single-writer, single-reader atomic registers. The complexity of our construction is asymptotically optimal; O(M2 + MN) shared single-writer, single-reader safe bits are required to construct a single-writer, M-reader, N-bit atomic register.We carry out an analysis of typability of terms in ML. Our main result is that this problem is DEXPTIME-hard, where by DEXPTIME we mean DTIME(2n0(1)). This, together with the known exponential-time algorithm that solves the problem, yields the DEXPTIME-completeness result. This settles an open problem of P. Kanellakis and J. C. Mitchell.Part of our analysis is an algebraic characterization of ML typability in terms of a restricted form of semi-unification, which we identify as acyclic semi-unification. We prove that ML typability and acyclic semi-unification can be reduced to each other in polynomial time. We believe this result is of independent interest.For any fixed dimension d, the
linear programming problem with n
inequality constraints can be solved on a probabilistic CRCW PRAM with
O(n)
processors almost surely in constant time. The algorithm always finds
the correct solution. With
nd/log2d
processors, the probability that the algorithm will not finish within
O(d2log2d
time tends to zero exponentially with
n.
The Fishspear priority queue algorithm is presented and analyzed. Fishspear is comparable to the usual heap algorithm in its worst-case running time, and its relative performance is much better in many common situations. Fishspear also differs from the heap method in that it can be implemented efficiently using sequential storage such as stacks or tapes, making it potentially attractive for implementation of very large queues on paged memory systems.In this paper, we prove the intractability of learning several classes of Boolean functions in the distribution-free model (also called the Probably Approximately Correct or PAC model) of learning from examples. These results are representation independent, in that they hold regardless of the syntactic form in which the learner chooses to represent its hypotheses.Our methods reduce the problems of cracking a number of well-known public-key cryptosystems to the learning problems. We prove that a polynomial-time learning algorithm for Boolean formulae, deterministic finite automata or constant-depth threshold circuits would have dramatic consequences for cryptography and number theory. In particular, such an algorithm could be used to break the RSA cryptosystem, factor Blum integers (composite numbers equivalent to 3 modulo 4), and detect quadratic residues. The results hold even if the learning algorithm is only required to obtain a slight advantage in prediction over random guessing. The techniques used demonstrate an interesting duality between learning and cryptography.We also apply our results to obtain strong intractability results for approximating a generalization of graph coloring.We introduce a temporal logic for the specification of real-time systems. Our logic, TPTL, employs a novel quantifier construct for referencing time: the freeze quantifier binds a variable to the time of the local temporal context.TPTL is both a natural language for specification and a suitable formalism for verification. We present a tableau-based decision procedure and a model-checking algorithm for TPTL. Several generalizations of TPTL are shown to be highly undecidable.
We consider a processor shared M/M/1 queue that can accommodate at
most a finite number 

K
 of customers. Using singular perturbation techniques,
we construct asymptotic approximations to the distribution of a
customer's sojourn time. We assume that 

K
 is large and treat several different cases of the
model parameters and also treat different time scales. Extensive
numerical comparisons are used to back up our asymptotic formulas.
The Concurrency Control (CC) scheme employed can profoundly affect the performance of transaction-processing systems. In this paper, a simple unified approximate analysis methodology to model the effect on system performance of data contention under different CC schemes and for different system structures is developed. This paper concentrates on modeling data contention and then, as others have done in other papers, the solutions of the data contention model are coupled with a standard hardware resource contention model through an iteration. The methodology goes beyond previously published methods for analyzing CC schemes in terms of the generality of CC schemes and system structures that are handled. The methodology is applied to analyze the performance of centralized transaction processing systems using various optimistic- and pessimistic-type CC schemes and for both fixed-length and variable-length transactions. The accuracy of the analysis is demonstrated by comparison with simulations. It is also shown how the methodology can be applied to analyze the performance of distributed transaction-processing systems with replicated data.This paper introduces a general formulation of
atomic snapshot memory, a shared
memory partitioned into words written
(updated) by individual processes, or
instantaneously read (scanned) in its
entirety. This paper presents three wait-free implementations of atomic
snapshot memory. The first implementation in this paper uses unbounded
(integer) fields in these registers, and is particularly easy to
understand. The second implementation uses bounded registers. Its
correctness proof follows the ideas of the unbounded implementation.
Both constructions implement a single-writer snapshot memory, in which
each word may be updated by only one process, from single-writer,
n-reader registers. The third
algorithm implements a  multi-writer snapshot memory from atomic
n-writer,
n-reader registers, again echoing key
ideas from the earlier constructions. All operations require
&THgr;(n2) reads
and writes to the component shared registers in the worst case.We consider logic programs with a single recursive rules, whose right-hand side consists of binary relations forming a chain. We give a complete characterization of all programs of this form that are computable in NC (assuming that P  ≠ ). Our proof uses ideas from automata and language theory, and the combinatorics of strings.What should it mean for an agent to know or believe an assertion is true with probability 9.99? Different papers [2, 6, 15] give different answers, choosing to use quite different probability spaces when computing the probability that an agent assigns to an event. We show that each choice can be understood in terms of a betting game. This betting game itself can be understood in terms of three types of adversaries influencing three different aspects of the game. The first selects the outcome of all nondeterministic choices in the system; the second represents the knowledge of the agent's opponent in the betting game (this is the key place the papers mentioned above differ); and the third is needed in asynchronous systems to choose the time the bet is placed. We illustrate the need for considering all three types of adversaries with a number of examples. Given a class of adversaries, we show how to assign probability spaces to agents in a way most appropriate for that class, where “most appropriate” is made precise in terms of this betting game. We conclude by showing how different assignments of probability spaces (corresponding to different opponents) yield different levels of guarantees in probabilistic coordinated attack.Many nonmonotonic formalism, including default logic, logic programming with stable models, and autoepistemic logic, can be represented faithfully by means of modal nonmonotonic logics in the family proposed by McDermott and Doyle. In this paper properties of logics in this family are thoroughly investigated. We present several results on characterization of expansions. These results are applicable to a wide class of nonmonotonic modal logics. Using these characterization results, algorithms for computing expansions for finite theories are developed. Perhaps the most important finding of this paper is that the structure of the family of modal nonmonotonic logics is much simpler than that of the family of underlying modal (monotonic) logics. Namely, it is often the case that different monotonic modal logics collapse to the same nonmonotonic system. We exhibit four families of logics whose nonmonotonic variants coincide: 5-KD45, TW5-SW5, N-WK, and W5-D4WB. These nonmonotonic logics naturally represent logics related to commonsense reasoning and knowledge representation such as autoepistemic logic, reflexive autoepistemic logic, default logic, and truth maintenance with negation.

Tight bounds are proved for Sort, Merge, Insert, Gcd of integers, Gcd of polynomials, and Rational functions over a finite inputs domain, in a random access machine with arithmetic operations, direct and indirect addressing, unlimited power for answering YES/NO questions, branching, and tables with bounded size. These bounds are also true even if additions, subtractions, multiplications, and divisions of elements by elements of the field are not counted.In a random access machine with finitely many constants and a bounded number of types of instructions, it is proved that the complexity of a function over a countable infinite domain is equal to the complexity of the function in a sufficiently large finite subdomain.This paper is concerned with a game on graphs called graph searching. The object of this game is to clear all edges of a contaminated graph. Clearing is achieved by moving searchers, a kind of token, along the edges of the graph according to clearing rules. Certain search strategies cause edges that have been cleared to become contaminated again. Megiddo et al. [9] conjectured that every graph can be searched using a minimum number of searchers without this recontamination occurring, that is, without clearing any edge twice. In this paper, this conjecture is proved. This places the graph-searching problem in NP, completing the proof by Megiddo et al. that the graph-searching problem is NP-complete. Furthermore, by eliminating the need to consider recontamination, this result simplifies the analysis of searcher requirements with respect to other properties of graphs.A new polynomial time decidable fragment of first order logic is identified, and a general method for using polynomial time inference procedures in knowledge representation systems is presented. The results shown in this paper indicate that a nonstandard “taxonomic” syntax is essential in constructing natural and powerful polynomial time inference procedures. The central role of taxonomic syntax in the polynomial time inference procedures provides technical support for the often-expressed intuition that knowledge is better represented in terms of taxonomic relationships than classical first order formulas. To use the procedures in a knowledge representation system, a “Socratic proof system” is defined, which is complete for first order inference and which can be used as a semi-automated interface to a first order knowledge base.A procedure is given for recognizing sets of inference rules that generate polynomial time decidable inference relations. The procedure can automatically recognize the tractability of the inference rules underlying congruence closure. The recognition of tractability for that particular rule set constitutes mechanical verification of a theorem originally proved independently by Kozen and Shostak. The procedure is algorithmic, rather than heuristic, and the class of automatically recognizable tractable rule sets can be precisely characterized. A series of examples of rule sets whose tractability is nontrivial, yet machine recognizable, is also given. The technical framework developed here is viewed as a first step toward a general theory of tractable inference relations.This paper analytically studies the performance of a synchronous conservative parallel discrete-event simulation protocol. The class of models considered simulates activity in a physical domain, and possesses a limited ability to predict future behavior. Using a stochastic model, it is shown that as the volume of simulation activity in the model increases relative to a fixed architecture, the complexity of the average per-event overhead due to synchronization, event list manipulation, lookahead calculations, and processor idle time approaches the complexity of the average per-event overhead of a serial simulation, sometimes rapidly. The method is therefore within a constant factor of optimal. The result holds for the worst case “fully-connected” communication topology,  where an event in any other portion of the domain can cause an event in any other protion of the domain. Our analysis demonstrates that on large problems—those for which parallel processing is ideally suited— there is often enough parallel workload so that processors are not usually idle. It also demonstrated the viability of the method empirically, showing how good performance is achieved on large problems using a thirty-two node Intel iPSC/2 distributed memory multiprocessor.Time and knowledge are studied in synchronous and asynchronous distributed systems. A large class of problems that can be solved using logical clocks as if they were perfectly synchronized clocks is formally characterized. For the same class of problems, a broadcast primitive that can be used as if it achieves common knowledge is also proposed. Thus, logical clocks and the broadcast primitive simplify the task of designing and verifying distributed algorithms: The designer can assume that processors have access to perfectly synchronized clocks and the ability to achieve common knowledge.Efficient ways of analyzing families of graphs that are generated by a certain type of context-free graph grammars are considered. These graph grammars are called cellular graph grammars. They generate the same graph families as hyperedge replacement systems, but are defined in a way that supports complexity analysis. A characteristic called “finiteness” of graph properties are defined, and combinatorial algorithms are presented for deciding whether a graph language generated by a given cellular graph grammar contains a graph with a given finite graph property. Structural parameters are introduced that bound the complexity of the decision procedure and special cases for which the decision can be made in polynomial time are discussed. Extensions to graph grammars that are not context-free are also given. Our results provide explicit and efficient combinatorial algorithms where, so far, only the existence of algorithms has been shown, or the best known algorithms are highly inefficient.In a priority-based computer system, besides the regular jobs, an additional job (refereed to as job A) is invoked infrequently but requires a significant amount of CPU time. To avoid CPU hogging, job A receives (up to) a fixed amount of CPU time whenever it is served. When the time expires, job A immediately relinquishes the CPU and puts itself to sleep for a period of time. By doing so, jobs with low priority may be processed in a timely manner. When the sleep time is over, job A is awakened and waits to resume service according to its priority. Then, the whole process is repeated until job A service is completed. In this paper, such an execution/sleep (ES) scheduling policy is analyzed for serving job A in a nonpreemptive priority queuing system. The Laplace Transforms are derived for: (i) the conditional response time of job A and (ii) the response time for jobs with priorities higher and lower than job A.This work is motivated by the ES policy in a switching system in which job A is invoked in response to the failure of signaling links. The proposed model is applicable to other real-time computer systems, and the modeling techniques can be applied or generalized to analyzing other scheduling policies in which timers are involved.
In this paper, it is shown that there is an algorithm that, given by finite set E of ground equations, produces a reduced canonical rewriting system R equivalent to E in polynomial time. This algorithm based on congruence closure performs simplification steps guided by a total simplification ordering on ground terms, and it runs in time O(n3).This paper studies the problem of perfectly secure communication in general network in which processors and communication lines may be faulty. Lower bounds are obtained on the connectivity required for successful secure communication. Efficient algorithms are obtained that operate with this connectivity and rely on no complexity-theoretic assumptions. These are the first algorithms for secure communication in a general network to simultaneously achieve the three goals of perfect secrecy, perfect resiliency, and worst-case time linear in the diameter of the network.Consider the problem of generating bitmaps from character shapes given as outlines. The obvious scan-conversion process does not produce acceptable results unless important features such as stem widths are carefully controlled during the scan-conversion process. This paper describes a method for automatically extracting the necessary feature information and generating high-quality bitmaps without resorting to hand editing. Almost all of the work is done in a preprocessing step, the result of which is an intermediate form that can be quickly converted into bitmaps once the font size and device resolution are known.A heuristically defined system of linear equations describes how the ideal outlines should be distorted in order to produce the best possible results when scan converted in a straightforward manner. The Lova´sz basis reduction algorithm then reduces the system of equations to a form that makes it easy to find an approximate solution subject to the constraint that some variables must be integers.The heuristic information is of such a general nature that it applies equally well to Roman fonts and Japanese Kanji.The minimum consistent DFA problem is that of finding a DFA with as few states as possible that is consistent with a given sample (a finite collection of words, each labeled as to whether the DFA found should accept or reject). Assuming that P   ≠  NP, it is shown that for any constant k, no polynomial-time algorithm can be guaranteed to find a consistent DFA with fewer than optk states, where opt is the number of states in the minimum state DFA consistent with the sample. This result holds even if the alphabet is of constant size two, and if the algorithm is allowed to produce an NFA, a regular expression, or a regular grammar that is consistent with the sample. A  similar nonapproximability result is presented     for the problem of finding small consistent linear grammars. For the case of finding minimum consistent DFAs when the alphabet is not of constant size but instead is allowed to vary with the problem specification, the slightly stronger lower bound on approximability of opt(1-&egr;)log logopt is shown for any &egr; > 0.The Edinburgh Logical Framework (LF) provides a means to define (or present) logics. It is based on a general treatment of syntax, rules, and proofs by means of a typed &lgr;-calculus with dependent types. Syntax is treated in a style similar to, but more general than, Martin-Lo¨f's system of arities. The treatment of rules and proofs focuses on his notion of a judgment. Logics are represented in LF via a new principle, the judgments as types principle, whereby each judgment is identified with the type of its proofs. This allows for a smooth treatment of discharge and variable occurence conditions and leads to a uniform treatment of rules and proofs whereby rules are viewed as proofs of higher-order judgments and proof checking is reduced to type checking. The practical benefit of our treatment of formal systems is that logic-independent tools, such as proof editors and proof checkers, can be constructed.A read-once formula is a Boolean formula in which each variable occurs, at most, once. Such formulas are also called &mgr;-formulas or Boolean trees. This paper treats the problem of exactly identifying an unknown read-once formula using specific kinds of queries.The main results are a polynomial-time algorithm for exact identification of monotone read-once formulas using only membership queries, and a polynomial-time algorithm for exact identification of general read-once formulas using equivalence and membership queries (a protocol based on the notion of a minimally adequate teacher [1]). The results of the authors improve on Valiant's previous results for read-once formulas [26]. It is also shown, that no polynomial-time algorithm using only membership queries or only equivalence queries can exactly identify all read-once formulas.
In practice, almost all dynamic systems require decisions to be made on-line, without full knowledge of their future impact on the system. A general model for the processing of sequences of tasks is introduced, and a general on-line decision algorithm is developed. It is shown that, for an important class of special cases, this algorithm is optimal among all on-line algorithms.Specifically, a task system (S,d) for processing sequences of tasks consists of a set S of states and a cost matrix d where d(i, j is the cost of changing from state i to state j (we assume that d satisfies the triangle inequality and all diagonal entries are 0).     The cost of processing a given task depends on the state of the system. A schedule for a sequence T1, T2,…, Tk of tasks is a sequence s1, s2,…,sk of states where si is the state in which Ti is processed; the cost of a schedule is the sum of all task processing costs and the state transition costs incurred.An on-line scheduling algorithm is one that chooses si only knowing    T1T2…Ti. Such an algorithm is w-competitive if, on any input task sequence, its cost is within an additive constant of w times the optimal offline schedule cost. The competitive ratio w(S, d) is the infimum w for which there is a w-competitive on-line scheduling algorithm for (S,d). It is shown that w(S, d) = 2|S|–1 for every task system in which d is symmetric, and w(S, d) =    O(|S|2) for every task system. Finally, randomized on-line scheduling algorithms are introduced. It is shown that for the uniform task system (in which d(i,j) = 1 for all i,j), the expected competitive ratio w¯(S,d) = O(log|S|).Nonoblivious hashing, where information gathered from unsuccessful probes is used to modify subsequent probe strategy, is introduced and used to obtain the following results for static lookup on full tables:
(1) An O(1)-time worst-case scheme that uses only logarithmic additional memory, (and no memory when the domain size is linear in the table size), which improves upon previously linear space requirements.
(2) An almost sure O(1)-time probabilistic worst-case scheme, which uses no additional memory and which improves upon previously logarithmic time requirements.
(3) Enhancements to hashing: (1) and (2) are solved for multikey recors, where search can be performed under  any key  in time O(1); these schemes also permit properties, such as nearest neighbor and rank, to be determined in logarithmic time.(1) An O(1)-time worst-case scheme that uses only logarithmic additional memory, (and no memory when the domain size is linear in the table size), which improves upon previously linear space requirements.(2) An almost sure O(1)-time probabilistic worst-case scheme, which uses no additional memory and which improves upon previously logarithmic time requirements.(3) Enhancements to hashing: (1) and (2) are solved for multikey recors, where search can be performed under  any key  in time O(1); these schemes also permit properties, such as nearest neighbor and rank, to be determined in logarithmic time.The efficiency of data-link protocols for reliable transmission of a sequence of messages over non-FIFO physical channels is discussed. The transmission has to be on-line; i.e., a message cannot be accessed by the transmitting station before the preceding message has been received. Three resources are considered: The number of packets that have to be sent, the number of headers, and the amount of space required by the protocol. Three lower bounds are proved. First, the space required by any protocol for delivering n messages that uses less than n headers cannot be bounded by any function of n. Second, the number of packets that have to be sent by any protocol that uses a fixed number of headers in order to deliver a message is linear  in the number of packets that are delayed on the channel at the time the message is sent. Finally, the notion of a probabilistic physical channel, in which a packet can be delayed on the channel with probability q, is introduced. An exponential lower bound, with overwhelming probability, is proved on the number of packets that have to be sent by any data-link protocol using a fixed number of headers when it is implemented over a probabilistic physical channel.An investigation of interactive proof systems (IPSs) where the verifier is a 2-way probabilistic finite state automaton (2pfa) is initiated. In this model, it is shown:(1) IPSs in which the verifier uses private randomization are strictly more powerful than IPSs in which the random choices of the verifier are made public to the prover.(2) IPSs in which the verifier uses public randomization are strictly more powerful than 2pfa's alone, that is, without a prover.(3) Every language which can be accepted by some deterministic Turing machine in exponential time can be accepted by some IPS.Additional results concern two other classes of verifiers: 2pfa's that halt in polynomial expected time, and 2-way probabilistic pushdown automata that halt in polynomial time. In particular, IPSs with verifiers in the latter class are as powerful as IPSs where verifiers are polynomial-time probabilistic Turing machines. In a companion paper [7], zero knowledge IPSs with 2pfa verifiers are investigated.The zero knowledge properties of interactive proof systems (IPSs) are studied in the case that the verifier is a 2-way probabilistic finite state automaton (2pfa). The following results are proved:(1) There is a language L such that L has an IPS with 2pfa verifiers but L has no zero knowledge IPS with 2pfa verifiers.(2) Consider the class of 2pfa's that are sweeping and that halt in polynomial expected time. There is a language L such that L has a zero knowledge IPS with respect to this class of verifiers, and L cannot be recognized by any verifier in the class on its own.A new definition of zero knowledge is  introduced. This definition captures a concept of “zero knowledge” for IPSs that are used for language recognition.A new algebraic technique for the construction of interactive proof systems is presented. Our technique is used to prove that every language in the polynomial-time hierarchy has an interactive proof system. This technique played a pivotal role in the recent proofs that IP = PSPACE [28] and that MIP = NEXP [4].In this paper, it is proven that when both randomization and interaction are allowed, the proofs that can be verified in polynomial time are exactly those proofs that can be generated with polynomial space.Lund et al. [1] have proved that PH is contained in IP. Shamir [2] improved this technique and proved that PSPACE = IP. In this note, a slightly simplified version of Shamir's proof is presented, using degree reductions instead of simple QBFs.In a distributed system, node failures, network delays, and other unpredictable occurences can result in orphan computations—subcomputations that continue to run but whose results are no longer needed. Several algorithms have been proposed to prevent such computations from seeing inconsistent states of the shared data. In this paper, two such orphan management algorithms are analyzed. The first is an algorithm implemented in the Argus distributed-computing system at MIT, and the second is an algorithm proposed at Carnegie-Mellon. The algorithms are described formally, and complete proofs of their correctness are given.The proofs show that the fundamental concepts underlying the two algorithms are very similar in that each can be regarded as an  implementation of the same high-level algorithm. By exploiting properties of information flow within transaction management systems, the algorithms ensure that orphans only see states of the shared data that they could also see if they were not orphans. When the algorithms are used in combination with any correct concurrency control algorithm, they guarantee that all computations, orphan as well as nonorphan, see consistent states of the shared data.The deBruijn graph Bn is the state diagram for an n-stage binary shift register. It has 2n vertices and 2n + 1 edges. In this papers, it is shown that Bn can be built by appropriately “wiring together“ (i.e., connecting together with extra edges) many isomorphic copies of a fixed graph, which is called a building block for Bn. The efficiency of such a building block is refined as the fraction of the edges of Bn which are present in the copies of the building block. It is then shown, among other things, that for   any &agr; < 1, there exists a graph G which is a building block for Bn of efficiency > &agr; for all sufficiently large n. These results are illustrated by describing how a special hierarchical family of building blocks has been used to construct a very large Viterbi decoder (whose floorplan is the graph B13) which will be used on NASA's Galileo mission.A framework for efficient dataflow analyses of logic programs is investigated. A number of problems arise in this context: aliasing effects can make analysis computationally expensive for sequential logic programming languages; synchronization issues can complicate the analysis of parallel logic programming languages; and finiteness restrictions to guarantee termination can limit the expressive power of such analyses. Our main result is to give a simple characterization of a family of flow analyses where these issues can be ignored without compromising soundness. This results in algorithms that are simple to verify and implement, and efficient in execution. Based on this approach, we describe an efficient algorithm for flow analysis of sequential logic programs, extend this approach to handle parallel executions, and finally describe how infinite chains in the analysis domain can be accommodated without compromising termination.
A high-level, knowledge-based approach for deriving a family of protocols for the sequence transmission problem is presented. The protocols of Aho et al. [2, 3], the Alternating Bit protocol [5], and Stenning's protocol [44] are all instances of one knowledge-based protocol that is derived. The derivation in this paper leads to transparent and uniform correctness proofs for all these protocols.In 1979, Bernhart and Kainen conjectured that graphs of fixed genus g ≥ 1 have unbounded pagenumber. In this paper, it is proven that genus g graphs can be embedded in O(g) pages, thus disproving the conjecture. An &OHgr;(g1/2) lower bound is also derived. The first algorithm in the literature for embedding an arbitrary graph in a book with a non-trivial upper bound on the number of pages is presented. First, the algorithm computes the genus g of a graph using the algorithm of Filotti, Miller, Reif (1979), which is polynomial-time for fixed genus. Second, it applies an optimal-time algorithm for obtaining an O(g)-page book  embedding. Separate book embedding   algorithms are given for the cases of graphs embedded in orientable and nonorientable surfaces. An important aspect of the construction is a new decomposition theorem, of independent interest, for a graph embedded on a surface. Book embedding has application in several areas, two of which are directly related to the results obtained: fault-tolerant VLSI and complexity theory.This paper presents an efficient algorithm to solve one of the task allocation problems. Task assignment in an heterogeneous multiple processors system is investigated. The cost function is formulated in order to measure the intertask communication and processing costs in an uncapacited network. A formulation of the problem in terms of the minimization of a submodular quadratic pseudo-Boolean function with assignment constraints is then presented. The use of a branch-and-bound algorithm using a Lagrangean relaxation of these constraints is proposed. The lower bound is the value of an approximate solution to the Lagrangean dual problem. A zero-duality gap, that is, a saddle point, is characterized by checking the consistency of a pseudo-Boolean equation. A solution is found for  large-scale problems (e.g., 20 processors, 50 tasks, and 200 task communications or 10 processors, 100 tasks, and 300 task communications). Excellent experimental results were obtained which are due to the weak frequency of a duality gap and the efficient characterization of the zero-gap (for practical purposes, this is achieved in linear time). Moreover, from the saddle point, it is possible to derive the optimal task assignment.Dynamic programming solutions to a number of different recurrence equations for sequence comparison and for RNA secondary structure prediction are considered. These recurrences are defined over a number of points that is quadratic in the input size; however only a sparse set matters for the result. Efficient algorithms for these problems are given, when the weight functions used in the recurrences are taken to be linear. The time complexity of the algorithms depends almost linearly on the number of points that need to be considered; when the problems are sparse this results in a substantial speed-up over known algorithms.Dynamic programming solutions to two recurrence equations, used to compute a sequence alignment from a set of matching fragments between two strings, and to predict RNA secondary structure, are considered. These recurrences are defined over a number of points that is quadratic in the input size; however, only a sparse set matters for the result. Efficient algorithms are given for solving these problems, when the cost of a gap in the alignment or a loop in the secondary structure is taken as a convex or concave function of the gap or loop length. The time complexity of our algorithms depends almost linearly on the number of points that need to be considered; when the problems are sparse, this results in a substantial speed-up over known algorithms.The problem of testing membership in aperiodic or “group-free” transformation monoids is the natural counterpart to the well-studied membership problem in permutation groups. The class A of all finite aperiodic monoids and the class G of all finite groups are two examples of varieties, the fundamental complexity units in terms of which finite monoids are classified. The collection of all varieties V forms an infinite lattice under the inclusion ordering, with the subfamily of varieties that are contained in A forming an infinite sublattice. For each V   ⊆   A, the associated problem MEMB(V) of testing membership in transformation monoids that belong to V,   is considered. Remarkably, the computational complexity of each such problem turns out to look familiar. Moreover, only five possibilities occur as V ranges over the whole aperiodic sublattice: With one family of NP-hard exceptions whose exact status is still unresolved, any such MEMB(V) is either PSPACE-complete, NP-complete, P-complete or in AC0. These results thus uncover yet another surprisingly tight link between the theory of monoids and computational complexity theory.Traditional work in inductive inference has been to model a learner receiving data about a function f and trying to learn the function. The data is usually just the values f(0), f(1),…. The scenario is modeled so that the learner is also allowed to ask questions about the data (e.g., (  ∀   &khgr;) [&khgr;> 17 → f(&khgr;) = 0]?). An important parameter is the language that the lerner may use to formulate queries. We show that for most languages a learner can learn more by asking questions than by passively receiving data. Mathematical tools used include the solution to Hilbert's tenth problem, the decidability of Presuburger arithmetic, and &ohgr;-automata.Methods are given for automatically verifying temporal properties of concurrent systems containing an arbitrary number of finite-state processes that communicate using CCS actions. TWo models of systems are considered. Systems in the first model consist of a unique control process and an arbitrary number of user processes with identical definitions. For this model, a decision procedure to check whether all the executions of a process satisfy a given specification is presented. This algorithm runs in time double exponential in the sizes of the control and the user process definitions. It is also proven that it is decidable whether all the fair executions of a process satisfy a given specification. The second model is a special case of the first. In  this model, all the processes have identical definitions. For this model, an efficient decision procedure is presented that checks if every execution of a process satisfies a given temporal logic specification. This algorithm runs in time polynomial in the size of the process definition. It is shown how to verify certain global properties such as mutual exclusion and absence of deadlocks. Finally, it is shown how these decision procedures can be used to reason about certain systems with a communication network.It is proven that monotone circuits computing the perfect matching function on n-vertex graphs require &OHgr;(n) depth. This implies an exponential gap between the depth of monotone and nonmonotone circuits.
An improved and general approach to connected-component labeling of images is presented. The algorithm presented in this paper processes images in predetermined order, which means that the processing order depends only on the image representation scheme and not on specific properties of the image. The algorithm handles a wide variety of image representation schemes (rasters, run lengths, quadrees, bintrees, etc.). How to adapt the standard UNION-FIND algorithm to permit reuse of temporary labels is shown. This is done using a technique called  age balancing, in which, when two labels are merged, the older label becomes the father of the younger label. This technique can be made to coexist with the more conventional rule of weight 
   balancing, in which the label with more descendants becomes the father of the label with fewer descendants. Various image scanning orders are examined and classified. It is also shown that when the algorithm is specialized to a pixel array scanned in raster order, the total processing time is linear in the number of pixels. The linear-time processing time follows from a special property of the UNION-FIND algorithm, which may be of independent interest. This property states that under certain restrictions on the input, UNION-FIND runs in time linear in the number of FIND and UNION operations. Under these restrictions, linear-time performance can be achieved without resorting to the more complicated  Gabow-Tarjan algorithm for disjoint set union.Text compression is often done using a fixed, previously formed dictionary (code book) that expresses which substrings of the text can be replaced by code words. There always exists an optimal solution for text-encoding problem. Due to the long processing times of the various optimal algorithms, several heuristics have been proposed in the literature. In this paper, the worst-case compression gains obtained by the longest match and the greedy heuristics for various types of dictionaries is studied. For general dictionaries, the performance of the heuristics can be almost the weakest possible. In practice, however, the dictionaries have usually properties that lead to a space-optimal or near-space-optimal coding result with the heuristics.Tree pattern matching is a fundamental operation that is used in a number of programming tasks such as mechanical theorem proving, term rewriting, symbolic computation, and nonprocedural programming languages. In this paper, we present new sequential algorithms for nonlinear pattern matching in trees. Our algorithm improves upon know tree pattern matching algorithms in important aspects such as time performance, ease of integration with several reduction strategies and ability to avoid unnecessary computation steps on match attempts that fail. The expected time complexity of our algorithm is linear in the sum of the sizes of the two trees.The problem of generating random, uniformly distributed, binary
trees is considered. A closed formula that counts the number of trees
having a left subtee with 

k-1
 nodes 

k=1,2,...,n
 is found. By inverting the formula, random trees with


n
 nodes are generated according to the appropriate
probability distribution, determining the number of nodes in the left
and right subtrees that can be generated recursively. The procedure is
shown to run in time 

On
, occupying an extra space in the order of


On
.In this paper, it is shown that the method of matings due to
Andrews and Bibel can be extended to (first-order) languages with
equality. A decidable version of
E-unification called
rigid E-unification is introduced,
and it is shown that the method of equational matings remains complete
when used in conjunction with rigid
E-unification. Checking that a family
of mated sets is an equational mating is equivalent to the following
restricted kind of
E-unification.Problem Given 

E&ar;=Ei∣
1≤i≤n
 a family of n
finite sets of equations and 

S=ui,vi
∣1≤i≤n
 a set of n pairs
of terms, is there a substitution 

q
 such that, treating each set 

qEi
 as a set of ground
equations (i.e., holding the variables in 

qEi
 “rigid”), 

qui
, and 

qvi
 are provably equal from 

qEi
 for 

i=1,...,n
?Equivalently, is there a substitution 

q
 such that 

qui
 and 

qvi
 can be shown congruent from 

qEi
 by the congruence closure method for


i=1,...,n
?A substitution 

q
 solving the above problem is called a
rigid 

E&ar;
-unifier of S, and
a pair 

E&ar;,S

 such that S has
some rigid 

E&ar;
-unifier is called an equational
premating. It is show that deciding whether a pair


E&ar;,S

 is an equational premating is an NP-complete
problem.Given a regular expression R of length P and a word A of length N, the membership problem is to determine if A is in the language denoted by R. An O(PN/lgN) time algorithm is presented that is based on a lgN speedup of the standard O(PN) time simulation of R's nonderministic finite automaton on A using a combination of the node-listing and “Four-Russians” paradigms. This result places a new worst-case upper bound on regular expression pattern matching. Moreover, in practice the method provides an implementation that is faster than existing software for small regular expressions.
The main contribution of this work is an O(n log n + k)-time algorithm for computing all k intersections among n line segments in the plane. This time complexity is easily shown to be optimal. Within the same asymptotic cost, our algorithm can also construct the subdivision of the plane defined by the segments and compute which segment (if any) lies right above (or below) each intersection and each endpoint. The algorithm has been implemented and performs very well. The storage requirement is on the order of n + k in the worst case, but it is considerably lower in practice. To analyze the complexity of the algorithm, an amortization argument based on  a new combinatorial theorem on line arrangements is used.A deterministic O(log N)-time algorithm for the problem of routing an aribitrary permutation on an N-processor bounded-degree network with bounded buffers is presented.Unlike all previous deterministic solutions to this problem, our routing scheme does not reduce the routing problem to sorting and does not use the sorting network of Ajtai, et al. [1]. Consequently, the constant in the run time of our routing scheme is substantially smaller, and the network topology is significantly simpler.A domain-independent formula of first-order predicate calculus is a formula whose evaluation in a given interpretation does not change when we add a new constant to the interpretation domain. The formulas used to express queries, integrity constraints or deductive rules in the database field that have an intuitive meaning are domain independent. That is the reason why this class is of great interest in practice. Unfortunately, this class is not decidable, and the problem is to characterize new subclasses, as large as possible, which are decidable. A syntactic characterization of a class of formulas, the Evaluable formulas, which are proved to be domain independent are provided. This class is defined only for function-free formulas. It is also proved that the class of evaluable   formulas contains the other classes of syntactically characterized domain-independent formulas usually found in the literature, namely, range-separable formulas and range-restricted formulas. Finally, it is shown that the expressive power of evaluable formulas is the same as that of domain-independent formulas. That is, each domain-independent formula admits an equivalent evaluable one. An important advantage of this characterization is that, to check if a formula is evaluable, it is not necessary to transform it to a normal form, as is the case for range-restricted formulas.There is a population explosion among the logical systems used in computing science. Examples include first-order logic, equational logic, Horn-clause logic, higher-order logic, infinitary logic, dynamic logic, intuitionistic logic, order-sorted logic, and temporal logic; moreover, there is a tendency for each theorem prover to have its own idiosyncratic logical system. The concept of institution is introduced to formalize the informal notion of “logical system.” The major requirement is that there is a satisfaction relation between models and sentences that is consistent under change of notation. Institutions enable abstracting away from syntactic and semantic detail when working on language structure “in-the-large”; for example, we can   define language features for building large logical system. This applies to both specification languages and programming languages. Institutions also have applications to such areas as database theory and the semantics of artificial and natural languages. A first main result of this paper says that any institution such that signatures (which define notation) can be glued together, also allows gluing together theories (which are just collections of sentences over a fixed signature). A second main result considers when theory structuring is preserved by institution morphisms. A third main result gives conditions under which it is sound to use a theorem prover for one institution on theories from another. A fourth main result shows how to extend institutions so that their theories may include, in addition  to the original sentences, various kinds of constraint that are useful for defining   abstract data types, including both “data” and “hierarchy” constraints. Further results show how to define institutions that allow sentences and constraints from two or more institutions. All our general results apply to such “duplex” and “multiplex” institutions.In this paper, a process algebra that incorporates explicit representations of successful termination, deadlock, and divergence is introduced and its semantic theory is analyzed. Both an operational and a denotational semantics for the language is given and it is shown that they agree. The operational theory is based upon a suitable adaptation of the notion of bisimulation preorder. The denotational semantics for the language is given in terms of the initial continuous algebra that satisfies a set of equations E, CIE. It is shown that CIE is fully abstract with respect to our choice of behavioral preorder. Several results of independent interest are obtained; namely, the finite  approximability of the behavioral preorder and a partial completeness result for the set of equations E with respect to the preorder.In a closed, separable, queuing network model of a computer system, the number of customer classes is an input parameter. The number of classes and the class compositions are assumptions regarding the characteristics of the system's workload. Often, the number of customer classes and their associated device demands are unknown or are unmeasurable parameters of the system. However, when the system is viewed as having a single composite customer class, the aggregate single-class parameters are more easily obtainable.This paper addresses the error made when constructing a single-class model of a multi-class system. It is shown that the single-class model pessimistically bounds, the performance of the multi-class system. Thus, given a multi-class system, the corresponding  single-class model can be constructed with the assurance that the actual system performance is better than that given by the single-class model. In the worst case, it is shown that the throughput given by the single-class model underestimates the actual multi-class throughput by, at most, 50%. Also, lower bounds are provided for the number of necessary customer classes, given observed device utilizations. This information is useful to clustering analysis techniques as well as to analysts who must obtain class-specific device demands.A digital signature scheme is presented, which is based on the existence of any trapdoor permutation. The scheme is secure in the strongest possible natural sense: namely, it is secure against existential forgery under adaptive chosen message attack.

Autoepistemic logic is one of the principal modes of nonmonotonic
reasoning. It unifies several other modes of nonmonotonic reasoning and
has important application in logic programming. In the paper, a theory
of autoepistemic logic is developed. This paper starts with a brief
survey of some of the previously known results. Then, the nature of
nonmonotonicity is studied by investigating how membership of
autoepistemic statements in autoepistemic theories depends on the
underlying objective theory. A notion similar to set-theoretic forcing
is introduced. Expansions of autoepistemic theories are also
investigated. Expansions serve as sets of consequences of an
autoepistemic theory and they can also be used to define semantics for
logic programs with negation. Theories that  have expansions are
characterized, and a normal form that allows the description of all
expansions of a theory is introduced. Our results imply algorithms to
determine whether a theory has a unique expansion. Sufficient conditions
(stratification) that imply existence of a unique expansion are
discussed. The definition of stratified theories is extended and (under
some additional assumptions) efficient algorithms for testing whether a
theory is stratified are proposed. The theorem characterizing expansions
is applied to two classes of theories, K1-theories
and ae-programs. In each case, simple hypergraph characterization of
expansions of theories from each of these classes is given. Finally,
connections with stable model semantics for logic programs with negation
is  discussed. In particular, it is proven that the problem of existence
of stable models is NP-complete.
Certain problems related to the length of cycles and paths modulo a given integer are studied. Linear-time algorithms are presented that determine whether all cycles in an undirected graph are of length P mod Q and whether all paths between two specified nodes are of length P mod Q, for fixed integers P.Q. These results are compared to those for directed graphs.Let S be a set, f: S×S→R+ a bivariate function, and f(x,S) the maximum value of f(x,y) over all elements y∈S. We say that f is decomposable with respect with the maximum if f(x,S) = max    {f(x,S1),f(x,S2),…,f(x,Sk)} for any decomposition S = &mgr;i=1i=kSi. Computing the maximum (minimum) value of a decomposable function is inherent in many problems of computational geometry and robotics. In this paper, a general technique is presented for updating the maximum (minimum) value of a decomposable function as elements are inserted into and deleted from the set S. Our result holds for a    semi-online model of dynamization: When an element is inserted, we are told how long it will stay. Applications of this technique include efficient algorithms for dynamically computing the diameter or closest pair of a set of points, minimum separation among a set of rectangles, smallest distance between a set of points and a set of hyperplanes, and largest or smallest area (perimeter) retangles determined by a set of points. These problems are fundamental to application areas such as robotics, VLSI masking, and optimization.A logic simulator can prove the correctness of a digital circuit if it can be shown that only circuits fulfilling the system specification will produce a particular response to a sequence of simulation commands.This style of verification has advantages over the other proof methods in being readily automated and requiring less attention on the part of the user to the low-level details of the design. It has advantages over other approaches to simulation in providing more reliable results, often at a comparable cost.This paper presents the theoretical foundations of several related approaches to circuit verification based on logic simulation. These approaches exploit the three-valued modeling capability found in most logic simulators, where the third-value X 
 indicates a signal with unknown digital value. Although the circuit verification problem is NP-hard as measured in the size of the circuit description, several techniques can reduce the simulation complexity to a manageable level for many practical circuits.An algebraic framework for the study of recursion has been developed. For immediate linear recursion, a Horn clause is represented by a relational algebra operator. It is shown that the set of all such operators forms a closed semiring. In this formalism, query answering corresponds to solving a linear equation. For the first time, the query answer is able to be expressed in an explicit algebraic form within an algebraic structure. The manipulative power thus afforded has several implications on the implementation of recursive query processing algorithms. Several possible decompositions of a given operator are presented that improve the performance of the algorithms, as well as several transformations that give the ability to take into account any selections or projections that are present in a givin query. In addition, it is shown that mutual linear recursion can also be studied within a closed semiring, by using relation vectors and operator matrices. Regarding nonlinear recursion, it is first shown that Horn clauses always give rise to multilinear recursion, which can always be reduced to bilinear recursion. Bilinear recursion is then shown to form a nonassociative closed semiring. Finally, several sufficient and necessary-and-sufficient conditions for bilinear recursion to be equivalent to a linear one of a specific form are given. One of the sufficient conditions is derived by embedding to bilinear recursion in an algebra.The general problem of parallel (concurrent) processing is investigated from a queuing theoretic point of view.As a basic simple model, consider infinitely many processors that can work simultaneously, and a stream of arriving jobs, each carrying a processing time requirement. Upon arrival, a job is allocated to a processor and starts being executed, unless it is blocked by another one already in the system. Indeed, any job can be randomly blocked by any preceding one, in the sense that it cannot start being processed before the one that blocks it leaves. After execution, the job leaves the system. The arrival times, the processing times and the blocking structures of the jobs form a stationary and ergodic sequence.The random precedence constraints capture the   essential operational characteristic of parallel processing and allow a unified treatment of concurrent processing systems from such diverse areas as parallel computation, database concurrency control, queuing networks, flexible manufacturing systems. The above basic model includes the G/G/1 and G/G/ ∞  queuing systems as special extreme cases.Although there is an infinite number of processors, the precedence constraints induce a queuing phenomenon, which, depending on the loading conditions, can lead to stability or instability of the system.In this paper, the condition for stability of the system is first precisely specified. The asymptotic behavior, at large times, of the quantities associated with the performance of the system is then studied, and the   degree of parallelism, expressed as the asymptotic average number of processors that work concurrently, is computed. Finally, various design and simulation aspects concerning parallel processing systems are considered, and the case of finite number of processors is discussed.The results proved for the basic model are then extended to cover more complex and realistic parallel processing systems, where each job has a random internal structure of subtasks to be executed according to some internal precedence constriants.It is proved that no finite computation tree with operations { +, -, *, /, mod, < } can decide whether the greatest common divisor (gcd) of a and b is one, for all pairs of integers a and b. This settles a problem posed by Gro¨tschel et al. Moreover, if the constants explicitly involved in any operation performed in the tree are restricted to be “0” and “1” (and any other constant must be computed), then we prove an &OHgr;(log log n) lower bound on the depth of any computation tree with operations { +, -, *, /, mod, < } that decides whether the gcd of a and b is one, for all pairs of n-bit integers a     and b.A novel technique for handling the truncation operation is implicit in the proof of this lower bound. In a companion paper, other lower bounds for a large class of problems are proved using a similar technique.Let K(m) denote the smallest number with the property that every m-state finite automaton can be built as a neural net using K(m) or fewer neurons. A counting argument shows that K(m) is at least &OHgr;((m log m)1/3), and a construction shows that K(m) is at most O(m3/4). The counting argument and the construction allow neural nets with arbitrarily complex local structure and thus may require neurons that themselves amount to complicated networks. Mild, and in practical situations almost necessary, constraints on the local   structure of the network give, again by a counting argument and a construction, lower and upper bounds for K(m) that are both linear in m.
A randomized polynomial-time algorithm for approximating the volume of a convex body K in n-dimensional Euclidean space is presented. The proof of correctness of the algorithm relies on recent theory of rapidly mixing Markov chains and isoperimetric inequalities to show that a certain random walk can be used to sample nearly uniformly from within K.The problem of determining shortest paths through a weighted planar polygonal subdivision with n vertices is considered. Distances are measured according to a weighted Euclidean metric: The length of a path is defined to be the weighted sum of (Euclidean) lengths of the subpaths within each region. An algorithm that constructs a (restricted) “shortest path map” with respect to a given source point is presented. The output is a partitioning of each edge of the subdivion into intervals of &egr;-optimality, allowing an &egr;-optimal path to be traced from the source to any query point along any edge. The algorithm runs in worst-case time O(ES) and requires O(E) space, where  E is  the number of “events” in our algorithm and S is the time it takes to run a numerical search procedure. In the worst case, E is bounded above by O(n4) (and we give an &OHgr;(n4) lower bound), but it is likeky that E will be much smaller in practice. We also show that S is bounded by O(n4L), where L is the precision of the problem instance (including the number of bits in the user-specified tolerance &egr;). Again, the value of S should be smaller in practice. The algorithm applies the “continuous  Dijkstra” paradigm  and exploits the fact that shortest paths obey Snell's Law of Refraction at region boundaries, a   local optimaly property of shortest paths that is well known from the analogous optics model. The algorithm generalizes to the multi-source case to compute Voronoi diagrams.Randomized, optimal algorithms to find a partition of the plane induced by a set of algebraic segments of a bounded degree, and a set of linear chains of a bounded degree, are given. This paper also provides a new technique for clipping, called virtual clipping, whose overhead per window W depends logarithmically on the number if intersections between the borders of W and the input segments.  In contrast, the overhead of the conventional clipping technique depends linearly on this number of intersections. As an application of virtual clipping, a new simple and efficient algorithm for plannar point location is given.In many computing applications, there are several equivalent algorithms capable of performing a particular task, and no one is the most efficient under all statistical distributions of the data. In such contexts, a good heuristic is to take a sample of the database and use it to guess which procedure is likely to be the most efficient. This paper defines the very general notion of a differentiable query problem and shows that the ideal sample size for guessing the optimal choice of algorithm is O(N2/3) for all differential problems involving approximately N executing steps.A simple characterization of independent database schemes is proved. An algorithm is given for translating a tableau T, posed as a query on a representative instance, to a union of tableaux that is equivalent to T, but can be applied directly to database relations. The algorithm may take exponential time (in the size of T and the database scheme), and it is applicable only to independent database schemes. If T is a just a projection of a representative instance, then the algorithm has a simpler form (which is still exponential in the worst case) and is polynomial in some cases.An algorithm is presented for generating a succinct encoding of all pairs shortest path information in a directed planar graph G with real-valued edge costs but no negative cycles. The algorithm runs in O(pn) time, where n is the number of vertices in G, and p is the minimum cardinality of a subset of the faces that cover all vertices, taken over all planar embeddings of G. The algorithm is based on a decomposition of the graph into O(pn) outerplanar subgraphs satisfying certain separator properties. Linear-time algorithms are presented for various subproblems including that of finding an appropriate embedding of G and a  corresponding face-on-vertex covering of cardinality   O(p), and of generating all pairs shortest path information in a directed outerplannar graph.The class of Horn clause sets in propositional logic is extended to a larger class for which the satisfiability problem can still be solved by unit resolution in linear time. It is shown that to every arborescence there corresponds a family of extended Horn sets, where ordinary Horn sets correspond to stars with a root at the center. These results derive from a theorem of Chandresekaran that characterizes when an integer solution of a system of inequalities can be found by rounding a real solution in a certain way. A linear-time procedure is provided for identifying “hidden” extended Horn sets (extended Horn but for complementation of variables) that correspond to a specified arborescence. Finally, a way to interpret extended Horn sets in applications is suggested.A technology-independent framework is established for measuring the
switching energy consumed by very
large scale integrated (VLSI) circuits. Techniques are developed for
analyzing functional energy consumption, and for designing
energy-efficient VLSI circuits. A wire (or gate) in a circuit uses
switching energy when it changes state from 1 to 0 or vice versa. This
paper develops the Uniswitch Model
(USM) of energy consumption, which measures the
differences between pairs of states of an embedded
circuit.…
The Patricia trie is a simple modification of a regular trie. By eliminating unary branching nodes, the Patricia achieves better performance than regular tries. However, the question is: how much on the average is the Patricia better? This paper offers a thorough answer to this question by considering some statistics of the number of nodes examined in a successful search and an unsuccessful search in the Patricia tries. It is shown that for the Patricia containing n records the average of the successful search length Sn asymptotically becomes 1/h1 · ln n + O(1), and the variance of Sn is either var Sn = c · ln n + 0(1) for an asymmetric Patricia or var Sn = 0(1) for a symmetric Patricia, where h1 is the entropy of the alphabet over which the Patricia is built and c is an explicit constant. Higher moments of Sn are also assessed. The number of nodes examined in an unsuccessful search Un is studied only for binary symmetric Patricia tries. We prove that the mth moment of the unsuccessful search length EUmn satisfies limn→∞ EUmn/logm2n = 1, and the variance of Un is var Un = 0.87907. These results suggest that Patricia tries are very well balanced trees in the sense that a random shape of Patriciatries resembles the shape of complete trees that are ultimately balanced trees.Henschen and Naqvi described a technique for translating queries on recursively defined relations of a Datalog database into iterative programs that invoke a query processor for conventional select-project-join queries of the relational algebra. Although the technique has been cited as one of the most efficient available, it will in some cases fail to produce all answers defined by the usual semantics for such databases. The technique is reviewed, a recursive query is exhibited where it fails, the cause of failure is noted, and a correction is described. A graphical representation of the computation based on a formal representation of rule expansions is employed.Two different kinds of Byzantine Agreement for distributed systems with processor faults are defined and compared. The first is required when coordinated actions may be performed by each participant at different times. This kind is called Simultaneous Byzantine Agreement (SBA).This paper deals with the number of rounds of message exchange required to reach Byzantine Agreement of either kind (BA). If an algorithm allows its participants to reach Byzantine agreement in every execution in which at most t participants are faulty, then the algorithm is said to tolerate t faults. It is well known that any BA algorithm that tolerates t faults (with t < n - 1 where n denotes  the total  number of processors) must run at least t + 1 rounds in some execution. However, it might be supposed that in executions where the number f of actual faults is small compared to t, the number of rounds could be correspondingly small. A corollary of our first result states that (when t < n - 1) any algorithm for SBA must run t + 1 rounds in some execution where there are no faults. For EBA (with t < n - 1), a lower bound of min(t + 1,f + 2) rounds is proved. Finally, an algorithm for EBA is presented that achieves the lower bound, provided that t is on the order of the square root of the  total number of  processors.This paper examines the unification problem in the class of primal algebras and the varieties they generate. An algebra is called primal if every function on its carrier can be expressed just in terms of the basic operations of the algebra. The two-element Boolean algebra is the simplest nontrivial example: Every truth-function can be realized in terms of the basic connectives, for example, negation and conjunction.It is shown that unification in primal algebras is unitary, that is, if an equation has a solution, it has a single most general one. Two unification algorithms, based on equation-solving techniques for Boolean algebras due to Boole and Lo¨wenheim, are studied in detail. Applications include certain finite Post algebras and matrix rings over   finite fields. The former are algebraic models for many-valued logics, the latter cover in particular modular arithmetic.Then unification is extended from primal algebras to their direct powers, which leads to unitary unification algorithms covering finite Post algebras, finite, semisimple Artinian rings, and finite, semisimple nonabelian groups.Finally the fact that the variety generated by a primal algebra coincides with the class of its subdirect powers is used. This yields unitary unification algorithms for the equational theories of Post algebras and p-rings.A generalization of Horn clauses to a higher-order logic is described and examined as a basis for logic programming. In qualitative terms, these higher-order Horn clauses are obtained from the first-order ones by replacing first-order terms with simply typed &lgr;-terms and by permitting quantification over all occurrences of function symbols and some occurrences of predicate symbols. Several proof-theoretic results concerning these extended clauses are presented. One result shows that although the substitutions for predicate variables can be quite complex in general, the substitutions necessary in the context of higher-order Horn clauses are tightly constrained. This observation is used to show that these higher-order formulas can specify computations in a fashion similar to   first-order Horn clauses. A complete theorem-proving procedure is also described for the extension. This procedure is obtained by interweaving higher-order unification with backchaining and goal reductions, and constitutes a higher-order generalization of SLD-resolution. These results have a practical realization in the higher-order logic programming language called &lgr;Prolog.The reduction algorithm is a technique for improving a decision tree in the abseence of  aproecise cost criterion. The result of applying the algorithm is an irreducible tree that is no less efficient than the original, and may be more efficient. Irreducible trees arise in discrete decision theory as an algebraic form for decision trees. This form has significant computational properties. In fact, every irreducible is optimal with respect to some expected testing cost criterion and is strictly better than any given distinct tree with respect to some criterion.Many irreducibles are decision equivalent to a given tree; onely some of these are reductions of the tree. The reduction algorithm is a particular way of finding one of these. It tends to preserve the  overall structure of the tree by reducing the subtrees first.A bound on the complexity of this algorithm with input tree t is O(hgt9t)2). usize(t) is the uniform size of the tree (the number of leaves less one) and hgt(t) is the height of the tree. This means that decision tree reduction has the same worst-case order of complexity as most heuristic methods for building suboptimal trees. While the purpose of using heuristics is often rather different, such comparisons are an indication of the efficiency of the reduction algorithms.The polynomiality of nonlinear separable convex (concave) optimization problems, on linear constraints with a matrix with “small” subdeterminants, and the polynomiality of such integer problems, provided the inteter linear version of such problems ins polynomial, is proven. This paper presents a general-purpose algorithm for converting procedures that solves linear programming problems. The conversion is polynomial for constraint matrices with polynomially bounded subdeterminants. Among the important corollaries of the algorithm is the extension of the polynomial solvability of integer linear programming problems with totally unimodular constraint matrix, to integer-separable convex programming. An algorithm for finding a &egr;-accurate optimal continuous solution to the  nonlinear problem that is polynomial in log(1/&egr;) and the input size and the largest subdeterminant of the constraint matrix is also presented. These developments are based on proximity results between the continuous and integral optimal solutions for problems with any nonlinear separable convex objective function. The practical feature of our algorithm is that is does not demand an explicit representation of the nonlinear function, only a polynomial number of function evaluations on a prespecified grid.A major component of a parallel machine is its interconnection network (IN), which provides concurrent communication between the processing elements. It is common to use a multistage interconnection network (MIN) that is constructed using crossbar switches and introduces contention not only for destination addresses but also for internal links. Both types of contention are increased when nonlocal communication across a MIN becomes concentrated on a certain destination address, the hot-spot. This paper considers analytical models of asynchronous, circuit-switched INs in which partial paths are held during path building, beginning with a single crossbar and extending recursively to MINs. Since a path must be held between source and destination processors before data  can be transmitted, switching networks are passive resources and queuing networks that include them do not therefore have product-form solutions. Using decomposition techniques, the flow-equivalent server (FES) that represents a bank of devices transmitting through a switching network is determined, under mild approximating assumptions. In the case of a full crossbar, the FES can be solved directly and the result can be applied recursively to model the MIN. Two cases are considered: one in which there is uniform routing and the other where there is a hot-spot at one of the output pins. Validation with respect to simulation for MINs with up to six stages (64-way switching) indicated a high degree of accuracy in the models.
Lower bounds on the complexity of orthogonal range searching in the
static case are established. Specifically, we consider the following
dominance search problem: Given a collection of
n weighted points in
d-space and a query point
q, compute the cumulative weight of
the points dominated (in all coordinates) by
q. It is assumed that the weights are
chosen in a commutative semigroup and that the query time measures only
the number of arithmetic operations needed to compute the answer. It is
proved that if m units of storage are
available, then the query time is at least proportional to (log
n/log(2m/n))d–1
in both the worst and average cases. This lower bound is provably tight
for m =
&OHgr;(n(log
n)
d–1+&egr;) and any fixed &egr;
> 0. A lower bound of &OHgr;(n/log
log
n)d)
on the time required for executing n
inserts and queries is also established.An O(nL)-time algorithm is introduced for constructing an optimal Huffman code for a weighted alphabet of size n, where each code string must have length no greater than L. The algorithm uses O(n) space.The question “Is a given join dependency equivalent to some set of multivalued dependencies?” led to the development of acyclicity theory [1]. The central question of this paper is: “Is a given equality-generating dependency equivalent to a set of functional dependencies?” An algorithm is presented that answers that question in polynomial time without using the chase process and, in the case of a “yes” answer, can be used to find (a cover of) the set of functional dependencies involved. This question is also related to the similar question about join dependencies and multivalued dependencies by proving a result about the hypergraph representation of an egd. It is interesting to note that a minimal representation of an egd must be &bgr;-acyclic for the egd to be equivalent to a set of fd's, in contrast to the jd/mvd case, in which only &agr;-acyclicity is needed. The &bgr;-acyclicity of an egd not necessarily minimal is always sufficient for the egd to be equivalent to a set of fd's as shown. Finally, the algorithm is extended for a single egd to answer the question whether a set of egd's with the same right-hand-side column is equivalent to a set of fd's.A detailed model of a transaction processing system with dynamic locking is developed and analyzed. Transaction classes are distinguished on the basis of the number of data items accessed and the access mode (read-only/update). The performance of the system is affected by transaction blocking and restarts, due to lock conflicts that do not or do cause deadlocks, respectively. The probability of these events is determined by the characteristics of transactions and the database access pattern. Hardware resource contention due to concurrent transaction processing is taken into account by specifying the throughput characteristic of the computer system for processing transactions when there is no data contention. A solution method based on decomposition is developed to analyze the system, and also used as the basis of an iterative scheme with reduced computational cost. The analysis to estimate the probability of lock conflicts and deadlocks is based on the mean number of locks held by transactions. These probabilities are used to derive the state transition probabilities for the Markov chain specifying the transitions among the system states. The decomposition solution method and the associated iterative scheme are shown to be more accurate than previously defined methods for dynamic locking through validation against simulation results. Several important conclusions regarding the behavior of dynamic locking systems are derived from parametric studies.This paper is concerned with the solvability of the problem of processor renaming in unreliable, completely asynchronous distributed systems. Fischer et al. prove in [8] that “nontrivial consensus” cannot be attained in such systems, even when only a single, benign processor failure is possible. In contrast, this paper shows that problems of processor renaming can be solved even in the presence of up to t < n/2 faulty processors, contradicting the widely held belief that no nontrivial problem can be solved in such a system. The problems deal with renaming processors so as to reduce the size of the initial name space. When only uniqueness of the new names is required, we present a lower bound of n + 1 on the size of the new name space, and a renaming algorithm that establishes an upper bound on n + t. If the new names are required also to preserve the original order, a tight bound of 2′(n - t + 1) - 1 is obtained.Reasoning about knowledge seems to play a fundamental role in distributed systems. Indeed, such reasoning is a central part of the informal intuitive arguments used in the design of distributed protocols. Communication in a distributed system can be viewed as the act of transforming the system's state of knowledge. This paper presents a general framework for formalizing and reasoning about knowledge in distributed systems. It is shown that states of knowledge of groups of processors are useful concepts for the design and analysis of distributed protocols. In particular, distributed knowledge corresponds to knowledge that is “distributed” among the members of the group, while common knowledge corresponds to a fact being “publicly known.” The relationship between common knowledge and a variety of desirable actions in a distributed system is illustrated. Furthermore, it is shown that, formally speaking, in practical systems common knowledge cannot be attained. A number of weaker variants of common knowledge that are attainable in many cases of interest are introduced and investigated.Many problems in the area of symbolic computing can be solved by iterative algorithms. Implementations of these algorithms on multiprocessors can be synchronous or asynchronous. Asynchronous implementations are potentially more efficient because synchronization is a major source of performance degradation in most multiprocessor systems.In this paper, sufficient conditions for the convergence of asynchronous iterations to desired solutions are given. The main sufficient condition is shown to be also necessary for the case of finite data domains. The results are applied to prove the convergence of three asynchronous algorithms for the all-pairs shortest path problem, the consistent labeling problem, and a neural net model.In this paper the shortest-path problem in networks in which the delay (or weight) of the edges changes with time according to arbitrary functions is considered. Algorithms for finding the shortest path and minimum delay under various waiting constraints are presented and the properties of the derived path are investigated. It is shown that if departure time from the source node is unrestricted, then a shortest path can be found that is simple and achieves a delay as short as the most unrestricted path. In the case of restricted transit, it is shown that there exist cases in which the minimum delay is finite, but the path that achieves it is infinite.A parallel algorithm for computing the connected components of undirected graphs is presented. Shared memory computation models are assumed. For a graph of e edges and n nodes, the time complexity of the algorithm is &Ogr;(e/p + (n log n)/p + log2n) with p processors. The algorithm can be further refined to yield time complexity &Ogr;(H(e, n, p)/p + (n log n)/(p log(n/p)) + log2n), where H(e, n, p) is very close to &Ogr;(e). These results show that linear speedup can be obtained for up to p ≤ e/log2n processors when e ≥ n log n. Linear speedup can still be achieved with up to p ≤ n&egr; processors, 0 ≤ &egr; < 1, for graphs satisfying e ≥ n log(*)n. Our results can be further improved if a more efficient integer sorting algorithm is available.This paper is concerned with the properties of nonlinear equations associated with the Scheweitzer-Bard (S-B) approximate mean value analysis (MVA) heuristic for closed product-form queuing networks. Three forms of nonlinear S-B approximate MVA equations in multiclass networks are distinguished: Schweitzer, minimal, and the nearly decoupled forms. The approximate MVA equations have enabled us to: (a) derive bounds on the approximate throughput; (b) prove the existence and uniqueness of the S-B throughput solution, and the convergence of the S-B approximation algorithm for a wide class of monotonic, single-class networks; (c) establish the existence of the S-B solution for multiclass, monotonic networks; and (d) prove the asymptotic (i.e., as the number of customers of each class tends to ∞) uniqueness of the S-B throughput solution, and (e) the convergence of the gradient projection and the primal-dual algorithms to solve the asymptotic versions of the minimal, the Schweitzer, and the nearly decoupled forms of MVA equations for multiclass networks with single server and infinite server nodes. The convergence is established by showing that the approximate MVA equations are the gradient vector of a convex function, and by using results from convex programming and the convex duality theory.A technique is developed for establishing lower bounds on the computational complexity of certain natural problems. The results have the form of time-space trade-off and exhibit the power of nondeterminism. In particular, a form of the clique problem is defined, and it is proved that:

a nondeterministic log-space Turing machine solves the problem in linear time, buta nondeterministic log-space Turing machine solves the problem in linear time, butno deterministic machine (in a very general use of this term) with sequential-access input tape and work space n&sgr; solves the problem in time n1+&tgr; if &sgr; + 2&tgr; < 1/2.
Pearl has shown that, in admissible A* tree-searching, the expected number of nodes expanded is bounded above and below by exponential functions of heuristic error. An additional assumption required for the validity of Pearl's argument is given. The assumption's significance and interpretation are discussed.We establish lower bounds on the complexity of orthogonal range reporting in the static case. Given a collection of n points in d-space and a box [a1, b1] X … X [ad, bd], report every point whose ith coordinate lies in [ai, bi], for each i = l, … , d. The collection of points is fixed once and for all and can be preprocessed. The box, on the other hand, constitutes a query that must be answered online. It is shown that on a pointer machine a query time of O(k + polylog(n)), where k is the number of points to be reported, can only be achieved at the expense of &OHgr;(n(log n/log log n)d-1) storage. Interestingly, these bounds are optimal in the pointer machine model, but they can be improved (ever so slightly) on a random access machine. In a companion paper, we address the related problem of adding up weights assigned to the points in the query box.Efficient implementations of Dijkstra's shortest path algorithm are investigated. A new data structure, called the radix heap, is proposed for use in this algorithm. On a network with n vertices, m edges, and nonnegative integer arc costs bounded by C, a one-level form of radix heap gives a time bound for Dijkstra's algorithm of O(m + n log C). A two-level form of radix heap gives a bound of O(m + n log C/log log C). A combination of a radix heap and a previously known data structure called a Fibonacci heap gives a bound of O(m + na @@@@log C). The best previously known bounds are O(m + n log n) using Fibonacci heaps alone and O(m log log C) using the priority queue structure of Van Emde Boas et al. [ 17].A new approach to the analysis of random probing hashing algorithms is presented. The probability-generating function in closed form for the asymptotic cost of insertion via random probing with secondary clustering is derived. For higher-order clustering, it is shown that all the moments of the probability distribution of the insertion cost exist and are asymptotically equal to the corresponding moments of the cost distribution under uniform hashing. The method in this paper also leads to simple derivations for the expected cost of insertion for random probing with secondary and higher-order clustering.This paper concerns the message complexity of broadcast in arbitrary point-to-point communication networks. Broadcast is a task initiated by a single processor that wishes to convey a message to all processors in the network. The widely accepted model of communication networks, in which each processor initially knows the identity of its neighbors but does not know the entire network topology, is assumed. Although it seems obvious that the number of messages required for broadcast in this model equals the number of links, no proof of this basic fact has been given before.It is shown that the message complexity of broadcast depends on the exact complexity measure. If messages of unbounded length are counted at unit cost, then broadcast requires &THgr;(↿V↾) messages, where V is the set of processors in the network. It is proved that, if one counts messages of bounded length, then broadcast requires &THgr;(↿E↾) messages, where E is the set of edges in the network.Assuming an intermediate model in which each vertex knows the topology of the network in radius &rgr; ≥ 1 from itself, matching upper and lower bounds of &THgr;(min{↿E↾, ↿V↾1+&THgr;(l)/&rgr;}) is proved on the number of messages of bounded length required for broadcast. Both the upper and lower bounds hold for both synchronous and asynchronous network models.The same results hold for the construction of spanning trees, and various other global tasks.A replicated data object is a typed object that is stored redundantly at multiple locations in a distributed system. Each of the object's operations has a set of quorums, which are sets of sites whose cooperation is needed to execute that operation. A quorum assignment associates each operation with its set of quorums. An operation's quorums determine its availability, and the constraints governing an object's quorum assignments determine the range of availability properties realizable by replication.In this paper, the restrictions on quorum assignment imposed by three kinds of atomicity mechanisms found in the literature are analyzed: (1) serial schemes, in which replication and atomicity are implemented independently at different levels in the system, (2) static schemes, in which the transaction serialization order is predetermined, and (3) hybrid schemes in which the serialization order emerges dynamically.The following results are derived: (1) Although serial schemes place the strongest restrictions on concurrency, they place the weakest restrictions on availability. (2) Although hybrid and static mechanisms place incomparable restrictions on concurrency, hybrid mechanisms place weaker restrictions on availability. (3) Bounding the maximum depth of transaction nesting strengthens restrictions on concurrency for all classes, but weakens restrictions on availability for hybrid schemes only. Concurrency and availability are best considered as dual properties: A complete analysis of an atomicity mechanism should take both into account.This paper presents a proof system for first-order temporal logic. The system extends the nonclausal resolution method for ordinary first-order logic with equality, to handle quantifiers and temporal operators. Soundness and completeness issues are considered. The use of the system for verifying concurrent programs is discussed and variants of the system for other modal logics are also described.The maximum concurrent flow problem (MCFP) is a multicommodity flow problem in which every pair of entities can send and receive flow concurrently. The ratio of the flow supplied between a pair of entities to the predefined demand for that pair is called throughput and must be the same for all pairs of entities for a concurrent flow. The MCFP objective is to maximize the throughput, subject to the capacity constraints. We develop a fully polynomial-time approximation scheme for the MCFP for the case of arbitrary demands and uniform capacity. Computational results are presented. It is shown that the problem of associating costs (distances) to the edges so as to maximize the minimum-cost of routing the concurrent flow is the dual of the MCFP. A path-cut type duality theorem to expose the combinatorial structure of the MCFP is also derived. Our duality theorems are proved constructively for arbitrary demands and uniform capacity using the algorithm. Applications include packet-switched networks [1, 4, 8], and cluster analysis [16].An axiomatic algebraic calculus of modules is given that is based on the operators combination/union, export, renaming, and taking the visible signature. Four different models of module algebra are discussed and compared.The new class of queuing models, called Synchronized Queuing Networks, is proposed for evaluating the performance of multiprogrammed and multitasked multiprocessor systems, where workloads consists of parallel programs of similar structure and where the scheduling discipline is first-come-first-serve.Pathwise evolution equations are established for these networks that capture the effects of competition for processors and the precedence constraints governing tasks executions.A general expression is deduced for the stability condition of such queuing networks under general statistical assumptions (basically the stationarity and the ergodicity of input sequences), which yields the maximum program throughput of the multiprocessor system, or equivalently, the maximum rate at which programs can be executed or submitted. The proof is based on the ergodic theory of queues.Basic integral equations are also derived for the stationary distribution of important performance criteria such as the workload of the queues and program response times. An iterative numerical schema that converges to this solution is proposed and various upper and lower bounds on moments are derived using stochastic ordering techniques.The probabilistic polynomial-time hierarchy (BPH) is the hierarchy generated by applying the BP-operator to the Meyer-Stockmeyer polynomial-time hierarchy (PH), where the BP-operator is the natural generalization of the probabilistic complexity class BPP. The similarity and difference between the two hierarchies BPH and PH is investigated. Oracles A and B are constructed such that both PH(A) and PH(B) are infinite while BPH(A) is not equal to PH(A) at any level and BPH(B) is identical to PH(B) at every level. Similar separating and collapsing results in the case that PH(A) is finite having exactly k levels are also considered.
The effects of circumscribing first-order formulas are explored from a computational standpoint. First, extending work of V. Lifschitz, it is Shown that the circumscription of any existential first-order formula is equivalent to a first-order formula. After this, it is established that a set of universal Horn clauses has a first-order circumscription if and only if it is bounded (when considered as a logic program); thus it is undecidable to tell whether such formulas have first-order circumscription. Finally, it is shown that there arefirst-order formulas whode circumscription has a coNP-complete model-checking problem.Unary inclusion dependencies are database constraints expressing subset relationships. The decidability of implication for these dependencies together with embedded implicational dependencies, such as functional dependencies, are investigated. As shown by Casanova et al., the unrestricted and finite implication problems are different for the class of functional and unary inclusion dependencies; also, for this class and for any fixed k, finite implication has no k-ary complete axiomatization. For both of these problems, complete axiomatizations and polynomial-time decision procedures are provided: linear time for unrestricted implication and cubic time for finite implication. It follows that functional and unary inclusion dependencies form a semantically natural class of first-order sentences with equality, which although not finitely controllable, is efficiently solvable and docile. Generalizing from these results, it is shown that the interaction between functional
and inclusion dependencies characterizes: (1) unrestricted implication of unary inclusion and all embedded implicational dependencies; (2) finite implication of unary inclusion and all full implicational dependencies; (3) finite implication of unary inclusion and all embedded tuple-generating dependencies. As a direct consequence of this analysis, most of the applications of dependency implication are extended, within polynomial-time, to database design problems involving unary inclusion dependencies. Such examples are tests for lossless joins and tests for complementarity of projective views. Finally, if one additionally requires thatThe fundamental satisfiability problem for word equations has been solved recently by Makanin. However, this algorithm is purely a decision algorithm. The main result of this paper solves the complementary problem of generating the set of all solutions. Specifically, the algorithm in this paper generates, given a word equation, a minimal and complete set of unifiers. It stops if this set is finite.This paper treats languages whose operational semantics is given by a set of rewrite rules. For such languages, it is important to be able to determine that there are enough rules to be able to compute the correct meaning of all expressions, but not so many that the system of rules is inconsistent. A formal framework is developed in which to give a precise treatment of these completeness and soundness issues, which are then investigated in the context of an extended version of the functional programming language FP. The rewrite rules of FP are shown to be sound and complete with respect to three different notions of completeness. The latter half of the paper considers rewrite strategies. In order to implement a language based on rewrite rules, it does not suffice to know that there are “enough” rules in the language; a good strategy for determining the order in which to apply them is also needed. But what is “good”? Corresponding to each notion of completeness, there is a notion of a good rewrite strategy. These notions of goodness are examined and characterized, and examples of a number of natural good strategies are given. Although these results are presented in the context of FP, the techniques (some of which are nontrivial extensions of techniques first used in the context of &lgr;-calculus) should apply well beyond the realm of FP rewriting systems.In this paper, a new asymptotic method is developed for analyzing closed BCMP queuing networks with a single class (chain) consisting of a large number of customers, a single infinite server queue, and a large number of single server queues with fixed (state-independent) service rates. Asymptotic approximations are computed for the normalization constant (partition function) starting directly from a recursion relation of Buzen. The approach of the authors employs the ray method of geometrical optics and the method of matched asymptotic expansions. The method is applicable when the servers have nearly equal relative utilizations or can be divided into classes with nearly equal relative utilizations. Numerical comparisons are given that illustrate the accuracy of the asymptotic approximations.A semantic, or model theoretic, approach is proposed to study the problems P =? NP and NP =? co-NP. This approach seems to avoid the difficulties that recursion-theoretic approaches appear to face in view of the result of Baker et al. on relativizations of the P =? NP question; moreover, semantical methods are often simpler and more powerful than syntactical ones. The connection between the existence of certain partial extensions of nonstandard models of arithmetic and the question NP =? co-NP is discussed. Several problems are stated about nonstandard models, and a possible link between the Davis-Matijasevi@@@@-Putnam-Robinson theorem on Diophantine sets and the NP =? co-NP question is mentioned.
Many real-world applications involve the management of large
amounts of time-dependent information. Temporal database systems
maintain this information in order to support various sorts of inference
(e.g., answering questions involving propositions that are true over
some intervals and false over others). For any given proposition, there
are typically many different occasions on which that proposition becomes
true and persists for some length of time. In this paper, these
occasions are referred to as time tokens. Many routine database
operations must search through the database for time tokens
satisfying certain temporal constraints. To expedite these operations,
this paper describes a set of techniques for organizing temporal
information by exploiting the local and global structure  inherent in a
wide class of temporal reasoning problems. The global structure of time
is exemplified in conventions for partitioning time according to the
calendar and the clock. This global structure is used to partition the
set of time tokens to facilitate retrieval. The local structure of time
is exemplified in the causal relationships between events and the
dependencies between planned activities. This local structure is used as
part of a strategy for reducing the computation required during
constraint propagation. The organizational techniques described in this
paper are quite general, and have been used to support a variety of
powerful inference mechanisms. Integrating these techniques into an
existing temporal database system has increased, by an order of
magnitude or more in most applications, the number of time tokens that
can be efficiently handled.To construct a short tour through points in the plane, the points are sequenced as they appear along a spacefilling curve. This heuristic consists essentially of sorting, so it is easily coded and requires only O(N) memory and O(N log N) operations. Its performance is competitive with that of other fast methods.A periodic sorting network consists of a sequence of identical blocks. In this paper, the periodic balanced sorting network, which consists of log n blocks, is introduced. Each block, called a balanced merging block, merges elements on the even input lines with those on the odd input lines.The periodic balanced sorting network sorts n items in O([log n]2) time using (n/2)(log n)2 comparators. Although these bounds are comparable to many existing sorting networks, the periodic structure enables a hardware implementation consisting of only one block with the output of the block recycled back as input until the output is sorted. An implementation of our network on the shuffle exchange interconnection model in which the direction of the comparators are all identical and fixed is also presented.An operational approach to database specification is proposed and investigated. Valid database states are described as the states resulting from the application of admissible transactions, specified by a transactional schema. The approach is similar in spirit to the modeling of behavior by methods and encapsulation in object-oriented systems. The transactions considered are line programs consisting of insertions, deletions, and modifications, using simple selection conditions. The results concern basic properties of transactional schemas, as well as the connection with traditional constraint schemas. In particular, the expressive power of transactional schemas is characterized. Although it is shown that transaction-based specification and constraint-based specification are incomparable, constraints of practical interest that have corresponding transactional schemas are identified. The preservation of constraints by transactions is also studied.The algebras and query languages for nested relations defined thus far do not allow us to “flatten” a relation scheme by disregarding the internal representation of data. In real life, however, the degree in which the structure of certain information, such as addresses, phone numbers, etc., is taken into account depends on the particular application and may even vary in time. Therefore, an algebra is proposed that does allow us to simplify relations by disregarding the internal structure of a certain class of information. This algebra is based on a careful manipulation of attribute names. Furthermore, the key operator in this algebra, called “copying,” allows us to deal with various other common queries in a very uniform manner, provided these queries are interpreted as operations on classes of semantically equivalent relations rather than individual relations. Finally, it is shown that the proposed algebra is complete in the sense of Bancilhon and Paredaens.The design of systolic algorithms is discussed, that is, algorithms that may efficiently be executed by a synchronous array of cells that perform local communications only. Systolic algorithms are designed through techniques developed in the context of sequential programming. Heuristics are given that guide the programmer in the design of a variety of efficient solutions.Theoretical results are presented on multi-pass (both left-to-right and alternating), multi-sweep, and multi-visit attribute grammars. For each of these, a pure type and a simple type are distinguished: The pure attribute grammars are defined by nondeterministic attribute evaluators, and the simple ones by the corresponding (usual) deterministic evaluators. The time complexity of deciding membership in these classes of attribute grammars is studied. In general, this is harder for the pure classes than for the simple ones, for which it is either polynomial or NP-complete. The expressive power of the eight classes is compared by studying the translations they can compute. It is shown that sweeps are more powerful than passes, and visits are more powerful than sweeps.The conjecture of Fliess concerning commutative context-free languages is disproved using a counterexample.A classical algorithm for finding a minimum-cost circulation consists of repeatedly finding a residual cycle of negative cost and canceling it by pushing enough flow around the cycle to saturate an arc. We show that a judicious choice of cycles for canceling leads to a polynomial bound on the number of iterations in this algorithm. This gives a very simple strongly polynomial algorithm that uses no scaling. A variant of the algorithm that uses dynamic trees runs in &Ogr;(nm(log n)min{log(nC), m log n}) time on a network of n vertices, m arcs, and arc costs of maximum absolute value C. This bound is comparable to those of the fastest previously known algorithms.A new equivalence between concurrent processes is proposed. It generalizes the well-known bisimulation equivalence to take into account the distributed nature of processes. The result is a noninterleaving semantic theory; concurrent processes are differentiated from processes that are non-deterministic but sequential. The new equivalence, together with its observational version, is investigated for a subset of the language CCS, and various algebraic characterizations are obtained.Much complexity-theoretic work on parallelism has focused on the class NC, which is defined in terms of logspace-uniform circuits. Yet P-uniform circuit complexity is in some ways a more natural setting for studying feasible parallelism. In this paper, P-uniform NC (PUNC) is characterized in terms of space-bounded AuxPDAs and alternating Turing Machines with bounded access to the input. The notions of general-purpose and special-purpose computation are considered, and a general-purpose parallel computer for PUNC is presented. It is also shown that NC = PUNC if all tally languages in P are in NC; this implies that the NC = PUNC question and the NC = P question are both instances of the ASPACE(S(n)) = ASPACE,TIME(S(n), S(n)o(1)) question. As a corollary, it follows that NC = PUNC implies PSPACE = DTIME(2no(1)).Valiant's learnability model is extended to learning classes of concepts defined by regions in Euclidean space En. The methods in this paper lead to a unified treatment of some of Valiant's results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned. Using this parameter, the complexity and closure properties of learnable classes are analyzed, and the necessary and sufficient conditions are provided for feasible learnability.
The main result of this paper is an 0([V] x [E]) time algorithm for deciding whether a given graph is a circle graph, that is, the intersection graph of a set of chords on a circle. The algorithm utilizes two new graph-theoretic results, regarding necessary induced subgraphs of graphs having neither articulation points nor similar pairs of vertices. Furthermore, as a substep of the algorithm, it is shown how to find in 0([V] x [E]) time a decomposition of a graph into prime graphs, thereby improving on a result of Cunningham.Using hierarchical definitions, one can describe very large graphs in small space. The blow-up from the length of the hierarchical description to the size of the graph can be as large as exponential. If the efficiency of graph algorithms is measured in terms of the length of the hierarchical description rather than in terms of the graph size, algorithms that do not exploit the hierarchy become hopelessly inefficient. Whether the hierarchy can be exploited to speed up the solution of graph problems depends on the hierarchical graph model. In the literature, hierarchical graph models have been described that allow almost no exploitation of the hierarchy [ 16]. In this paper, a hierarchical graph model that permits taking advantage of the hierarchy is presented. For this model algorithms are given that test planarity of a hierarchically described graph in linear time in the length of the hierarchical description.Two conflicting goals play a crucial role in the design of routing schemes for communication networks. A routing scheme should use paths that are as short as possible for routing messages in the network, while keeping the routing information stored in the processors' local memory as succinct as possible. The efficiency of a routing scheme is measured in terms of its stretch factor-the maximum ratio between the length of a route computed by the scheme and that of a shortest path connecting the same pair of vertices.Most previous work has concentrated on finding good routing schemes (with a small fixed stretch factor) for special classes of network topologies. In this paper the problem for general networks is studied, and the entire range of possible stretch factors is examined. The results exhibit a trade-off between the efficiency of a routing scheme and its space requirements. Almost tight upper and lower bounds for this trade-off are presented. Specifically, it is proved that any routing scheme for general n-vertex networks that achieves a stretch factor k ≥ 1 must use a total of &OHgr;(n1+1/(2k+4)) bits of routing information in the networks. This lower bound is complemented by a family K(k) of hierarchical routing schemes (for every k ≥ l) for unit-cost general networks, which guarantee a stretch factor of O(k), require storing a total of O(k3n1+(1/h)logn)- bits of routing information in the network, name the vertices with O(log2n)-bit names and use O(logn)-bit headers.A new probabilistic failure model for networks of gates is formulated. Although this model has not been used previously, it supports the proofs of both the positive and negative results appearing in the literature. Furthermore, with respect to this new model, the complexity measures of both size and depth are affected by at most constant multiplicative factors when the set of functions that can be computed by gates is changed from one finite and complete basis to another, or when the bound on the failure probability of the gates is changed (within the limits allowed by the basis), or when the bound on the error probability of the network is changed (within the limits allowed by the basis and the failure probability of the gates).The (component) merging problem is a new graph problem. Versions of this problem appear as bottlenecks in various graph algorithms. A new data structure solves this problem efficiently, and two special cases of the problem have even more efficient solutions based on other data structures. The performance of the data structures is sped up by introducing a new algorithmic tool called packets.The algorithms that use these solutions to the component merging problem also exploit new properties of two existing data structures. Specifically, &Bgr;-trees can be used simultaneously as a priority queue and a concatenable queue. Similarly, F-heaps support some kinds of split operations with no loss of efficiency.An immediate application of the solution to the simplest version of the merging problem is an &Ogr;(t(m, n)) algorithm for finding minimum spanning trees in undirected graphs without using F-heaps, where t(m, n) = mlog2log2logdn, the graph has n vertices and m edges, and d = max(m/n, 2). Packets also improve the F-heap minimum spanning tree algorithm, giving the fastest algorithm currently known for this problem.The efficient solutions to the merging problem and the new observation about F-heaps lead to an &Ogr;(n(t(m, n) + nlogn)) algorithm for finding a maximum weighted matching in general graphs. This settles an open problem posed by Tarjan [ 15, p. 123], where the weaker bound of O(nm log (n2/m)) was conjectured.Binary search trees with costs &agr; and &bgr;, respectively, on the left and right edges (lopsided search trees) are considered. The exact shape, minimum worst-case cost, and minimum average cost of lopsided trees of n internal nodes are determined for nonnegative &agr; and &bgr;; the costs are both roughly logp(n + 1) where p is the unique real number in the interval (1. 2] satisfying 1/p&agr; + 1/p&bgr; = 1. Search procedures are given that come within a small additive constant of the lower bounds. Almost-optimum algorithms for the lopsided case of unbounded searching are also obtained. Some extensions to nonconstant costs are briefly sketched.Using simple protocols, it is shown how to achieve consensus in constant expected time, within a variety of fail-stop and omission failure models. Significantly, the strongest models considered are completely asynchronous. All of the results are based on distributively flipping a coin, which is usable by a significant majority of the processors. Finally, a nearly matching lower bound is also given for randomized protocols for consensus.In this paper the class of acyclic fork-join queuing networks that arise in various applications, including parallel processing and flexible manufacturing are studied. In such queuing networks, a fork describes the simultaneous creation of several new customers, which are sent to different queues. The corresponding join occurs when the services of all these new customers are completed. The evolution equations that govern the behavior of such networks are derived. From this, the stability conditions are obtained and upper and lower bounds on the network response times are developed. These bounds are based on various stochastic ordering principles and on the notion of association of random variables.Optimal &OHgr;(log n/log log n) lower bounds on the time for CRCW PRAMS with polynomially bounded numbers of processors or memory cells to compute parity and a number of related problems are proven. A strict time hierarchy of explicit Boolean functions of n bits on such machines that holds up to &Ogr;(log n/log log n) time is also exhibited. That is, for every time bound T within this range a function is exhibited that can be easily computed using polynomial resources in time T but requires more than polynomial resources to be computed in time T - 1. Finally, it is shown that almost all Boolean functions of n bits require log n - log log n + &OHgr;(1) time when the number of processors is at most polynomial in n. The bounds do not place restrictions on the uniformity of the algorithms nor on the instruction sets of the machines.Lower bounds are proven on the parallel-time complexity of several basic functions on the most powerful concurrent-read concurrent-write PRAM with unlimited shared memory and unlimited power of individual processors (denoted by PRIORITY(∞)):

It is proved that with a number of processors polynomial in n, &OHgr; (log n) time is needed for addition, multiplication or bitwise OR of n numbers, when each number has n' bits. Hence even the bit complexity (i.e., the time complexity as a function of the total number of bits in the input) is logarithmic in this case. This improves a beautiful result of Meyer auf der Heide and Wigderson [22]. They proved a log n lower bound using Ramsey-type techniques. Using Ramsey theory, it is possible to get an upper bound on the number of bits in the inputs used. However, for the case of polynomially many processors, this upper bound is more than a polynomial in n.
An &OHgr; (log n) lower bound is given for PRIORITY(∞) with no(1) processors on a function with inputs from {0, 1}, namely for the function ƒ(x1, … , xn,) = &Sgr; nl- 1 xlai where a is fixed and xi &egr; {0, 1}.
Finally, by a new efficient simulation of PRIORITY(∞) by unbounded fan-in circuits, that with less than exponential number of processors, it is proven a PRIORITY(∞) cannot compute PARITY in constant time, and with nO(1) processors &OHgr;(@@@@log n) time is needed. The simulation technique is of independent interest since it can serve as a general tool to translate circuit lower bounds into PRAM lower bounds.

Further, the lower bounds in (1) and (2) remain valid for probabilistic or nondeterministic concurrent-read concurrent-write PRAMS.
It is proved that with a number of processors polynomial in n, &OHgr; (log n) time is needed for addition, multiplication or bitwise OR of n numbers, when each number has n' bits. Hence even the bit complexity (i.e., the time complexity as a function of the total number of bits in the input) is logarithmic in this case. This improves a beautiful result of Meyer auf der Heide and Wigderson [22]. They proved a log n lower bound using Ramsey-type techniques. Using Ramsey theory, it is possible to get an upper bound on the number of bits in the inputs used. However, for the case of polynomially many processors, this upper bound is more than a polynomial in n.An &OHgr; (log n) lower bound is given for PRIORITY(∞) with no(1) processors on a function with inputs from {0, 1}, namely for the function ƒ(x1, … , xn,) = &Sgr; nl- 1 xlai where a is fixed and xi &egr; {0, 1}.Finally, by a new efficient simulation of PRIORITY(∞) by unbounded fan-in circuits, that with less than exponential number of processors, it is proven a PRIORITY(∞) cannot compute PARITY in constant time, and with nO(1) processors &OHgr;(@@@@log n) time is needed. The simulation technique is of independent interest since it can serve as a general tool to translate circuit lower bounds into PRAM lower bounds.Further, the lower bounds in (1) and (2) remain valid for probabilistic or nondeterministic concurrent-read concurrent-write PRAMS.
This paper gives a fast, linear-time algorithm for generating high-quality pixel representations of curved lines. The results are similar to what is achieved by selecting a circle whose diameter is the desired line width, and turning on all pixels covered by the circle as it moves along the desired curve. However, the circle is replaced by a carefully chosen polygon whose deviations from the circle represent subpixel corrections designed to improve the aesthetic qualities of the rasterized curve. For nonsquare pixels, equally good results are obtained when an ellipse is used in place of the circle. The class of polygons involved is introduced, an algorithm for generating them is given, and how to construct the set of pixels covered when such a polygon moves along a curve is shown. The results are analyzed in terms of a mathematical model for the uniformity and accuracy of line width in the rasterized image.Today's standard model for database concurrency control, called serializability theory, represents executions of transactions as partial orders of operations. The theory tells when an execution is serializable, that is, when the set of operations of a transaction execute atomically with respect to those of other transactions. It has been used successfully to prove correctness of most database concurrency control algorithms. Its most serious limitation is its inability to represent nested computations conveniently.This paper presents a more general model that permits nested transactions. In this model, transactions may execute subtransactions, giving rise to tree-structured computations. A serializability theory is developed for this model, which can be used to prove the  correctness of concurrency control algorithms for nested transactions and for multilevel database systems.The theory is based on an abstract model of computation that allows arbitrary operations, and parallel and even nondeterministic programs. Axioms are presented that express the basic properties that programs that manage or access data need to satisfy. We use these axioms to derive proof techniques. One new technique—substitution—shows the equivalence of two executions by substituting one subcomputation by another, usually shallower (i.e., less nested), one. Our proof techniques are illustrated by applying them to several well-known concurrency control problems.It is shown that n + k - O(1) comparisons are necessary, on average, to find the kth smallest of n numbers (k ⪇ n/2). This lower bound matches the behavior of the technique of Floyd and Rivest to within a lower-order term. 7n/4 ± o(n) comparisons, on average, are shown to be necessary and sufficient to find the maximum and median of a set. An upper bound of 9n/4 ± o(n) and a lower bound of 2n - o(n) are shown for the max-min-median problem.It is shown that the external path length of a binary tree is closely related to the ratios of means of certain integers and establish the upper bound    ExternalPathLength≤N log2N+D -log2D -0.6623,    where N denotes the number of external nodes in the tree and    D is the difference in length between a longest and shortest path. Then it is proved that this bound is tight up to an    ON  term if      D≤N . If   D>N ,  we contstruct binary trees whose external path length is at least as large as    Nlog2 N+fN,D D-log2 D-4 , where    fN,D=1/ 1+2D/N  .In 1979, G. K. Manacher showed that the Ford-Johnson sorting algorithm [FJA], acting on t real numbers, can be beaten for an infinite set of values t. These values form a partial cover of constant density not close to 1 over an initial sequence of each band running from uk = ⌊(4/3)2k⌋ to uk+l - 1. This early result depended on showing that the Hwang-Lin merging algorithm [HLA], merging m elements with n, m ≠ n, could be surpassed by cm comparisons, where c is an arbitrary small positive constant.In this paper, it is shown that the FJA can be beaten for a set of integers of asymptotic density 1 under the greatly weakened assumption that the HLA can be surpassed by only (1/2 + &egr;)log m comparisons, with &egr; a small positive constant. The even weaker assumption that no improvement in the HLA exists, but that an isolated value to exists for which the FJA can be surpassed by only (1 + &egr;)log to comparisons yields the same result. Only for a set of “refractory” integers of size about t1/2 in the neighborhood of each uk does the FJA fail to be beaten.All these results depend on a new technique for obtaining optimum sort-merge sequences for best-possible sorting given a merging method. The technique turns out to be amenable to precise asymptotic analysis. When the technique is applied using the most powerful known merging algorithm [Christen's], the density mentioned above is still 1, but islands of refractory points still remain, this time forming sets provably of size &THgr;(log2t) in the neighborhood of each uk.It is shown that if “information theoretic” merging were achievable, the FJA could be beaten for all t > u10 = 1365. From these results and a few others, we adduce evidence in support of our main conjecture: that even optimum combinations of optimum merging and Ford-Johnson sorting will not beat the FJA when t = uk, but will instead produce refractory regions of size &THgr;(log2t) in the neighborhood of each &lgr;.An Information Dispersal Algorithm (IDA) is developed that breaks a file F of length L = ↿ F↾ into n pieces Fi, l ≤ i ≤ n, each of length ↿Fi↾ = L/m, so that every m pieces suffice for reconstructing F. Dispersal and reconstruction are computationally efficient. The sum of the lengths ↿Fi↾ is (n/m) · L. Since n/m can be chosen to be close to l, the IDA is space efficient.  IDA has numerous applications to secure and reliable storage of information in computer networks and even on single disks, to fault-tolerant and efficient transmission of information in networks, and to communications between processors in parallel computers. For the latter problem provably time-efficient and highly fault-tolerant routing on the n-cube is achieved, using just constant size buffers.A general framework is presented for rapidly analyzing tree networks to compute a measure of the centrality or eccentricity of all vertices in the tree. Several problems, which have been previously described in the literature, fit this framework. Some of these problems have no published solution better than performing a separate traversal for each vertex whose eccentricity is calculated. The method presented in this paper performs just two traversals and yields the eccentricities of all vertices in the tree. Natural sufficient conditions for the algorithm to work in linear time on any given problem are stated.The prefix problem consists of computing all the products x0x1 … xj (j = 0, … , N - 1), given a sequence x = (x0, x1, … , xN-1) of elements in a semigroup. In this paper we completely characterize the size-time complexity of computing prefixes with Boolean networks, which are synchronized interconnections of Boolean gates and one-bit storage devices. This complexity crucially depends upon two properties of the underlying semigroup, which we call cycle-freedom (no cycle of length greater than one in the Cayley graph of the semigroup), and memory-induciveness (arbitrarily long products of semigroup elements are true functions of all their factors). A nontrivial characterization is given of non-memory-inducive semigroups as those whose recurrent subsemigroup (formed by the elements with self-loops in the Cayley graph) is the direct product of a left-zero semigroup and a right-zero semigroup. Denoting by S and T size and computation time, respectively, we have S = &THgr;((N/T)log(N/T)) for memory-inducive non-cycle-free semigroups, and S = &THgr;(N/T) for all other semigroups. We have T &egr; [&OHgr;(log N), &Ogr;(N)] for all semigroups, with the exception of those whose recurrent subsemigroup is a right-zero semigroup, for which T &egr; [&OHgr;(1), &Ogr;(N)]. The preceding results are also extended to the VLSI model of computation. Area-time optimal circuits are obtained for both boundary and nonboundary I/O protocols.Inductive inference machines construct programs for total recursive functions given only example values of the functions. Probabilistic inductive inference machines are defined, and for various criteria of successful inference, it is asked whether a probabilistic inductive inference machine can infer larger classes of functions if the inference criterion is relaxed to allow inference with probability at least p, (0 < p < 1) as opposed to requiring certainty. For the most basic criteria of success (EX and BC), it is shown that any class of functions that can be inferred from examples with probability exceeding 1/2 can be inferred deterministically, and that for probabilities p ≤ 1/2 there is a discrete hierarchy of inferability parameterized by p. The power of probabilistic inference strategies is characterized by equating the classes of probabilistically inferable functions with those classes that can be inferred by teams of inductive inference machines (a parallel model of inference), or by a third model called frequency inference.
Modular decomposition is a form of graph decomposition that has been discovered independently by researchers in graph theory, game theory, network theory, and other areas. This paper reduces the time needed to find the modular decomposition of a graph from &OHgr;(n3) to &Ogr;(n2). Together with a new algorithm for transitive orientation given in [21], this leads to fast new algorithms for a number of problems in graph recognition and isomorphism, including recognition of comparability graphs and permutation graphs. The new algorithm works by inserting each vertex successively into the decomposition tree, using &Ogr;(n) time to insert each vertex.A unified framework is developed for the study of asynchronous circuits of both gate and MOS type. A basic network model consisting of a directed graph and a set of vertex excitation functions is introduced. A race analysis model, using three values (0, 1, and x), is developed for studying state transitions in the network. It is shown that the results obtained using this model are equivalent to those using ternary simulation. It is also proved that the set of state variables can be reduced to a minimum size set of feedback variables, and the analysis still yields both the correct state transitions and output hazard information. Finally, it is shown how the general results above are applicable to both gate and MOS circuits.If a relational database is required to satisfy a set of integrity constraints, then when the database is updated, one must ensure that it continues to satisfy the constraints. It is desirable not to have to evaluate each constraint after each update. A method is described that takes a constraint C and a class of updates, and either proves that an update in the class cannot violate C, or produces a formula C' (a complete test) that is satisfied before the update if and only if C would continue to be satisfied were the update to occur. C' is frequently much easier to evaluate than C. In addition, a formula D (a sufficient test) is sometimes produced such that if D is satisfied before the update, then C would continue to be satisfied were the update to occur. The method is proved correct. The method is substantially more general than other reported techniques for this problem. The method has been implemented, and a number of experiments with the implementation are presented.Recursive inference rules arise in recursive definitions in logic programming systems and in database systems with recursive query languages. Let D be a recursive definition of a relation t. D is considered minimal if for any predicate p in a recursive rule in D, p must appear in a recursive rule in any definition of t. It is shown that testing for minimality is, in general, undecidable. However, an efficient algorithm for a useful class of recursive rules is presented, and it is used to transform a recursive definition to a minimal recursive definition. Evaluating the minimized definition avoids redundant computation without the overhead of caching intermediate results and run-time checking for duplicate goals.Jazayeri [J. ACM 28, 4 (Oct. 1981), 715-720] proposes a simpler construction for use in the proof by Jazayeri et al. [Commun. ACM 18, 12 (Dec. 1975), 697-706] that the circularity problem for attribute grammars has inherent exponential time complexity. The simplification introduces a flaw that invalidates the proof. The flaw can be corrected, at the cost of eliminating some of the simplification claimed for the new construction.A new model for dynamic programming and branch and bound algorithms is presented. The model views these algorithms as utilizing computationally feasible dominance relations to infer the orderings of application objects, thereby implicitly enumerating a finite solution space. The formalism is broad enough to apply the computational strategies of dynamic programming and branch and bound to problems with nonassociative objects, and can model both oblivious and nonoblivious algorithms, as well as parallel algorithms. The model is used to classify computations based, in part, on the types of computationally feasible dominances that they employ. It is demonstrated that the model is computationally precise enough to support the derivation of lower bounds on the number of operations required to solve various types of problems.In this paper, efficient algorithms are given for inferring sequences produced by certain pseudo-random number generators. The generators considered are all of the form Xn = &Sgr;kj-l &agr;j&phgr;j(Xo, Xl, . . ., Xn-l) (mod m). In each case, we assume that the functions &phgr;j are known and polynomial time computable, but that the coefficients aj and the modulus m are unknown. Using this general method, specific examples of generators having this form, the linear congruential method, linear congruences with n terms in the recurrence, and quadratic congruences are shown to be cryptographically insecure.Probabilistic algorithms are presented for testing the result of the product of two n-bit integers in O(n) bit operations and for testing the result of the product of two polynomials of degree n over any integral domain in 4n + o(n) algebraic operations with the error probability o(l/n1-&egr;) for any &egr; > 0. The last algorithm does not depend on the constants of the underlying domain.Let Mq(n) denote the number of multiplications required to compute the coefficients of the product of two polynomials of degree n over a q-element field by means of bilinear algorithms. It is shown that Mq(n) ≱ 3n - o(n). In particular, if q/2 < n ⪇ q + 1, we establish the tight bound Mq(n) = 3n + 1  [q/2].The technique we use can be applied to analysis of algorithms for multiplication of polynomials modulo a polynomial as well.Repairable computer systems are considered, the availability behavior of which can be modeled as a homogeneous Markov process. The randomization method is used to calculate various measures over a finite observation period related to availability modeling of these systems. These measures include the distribution of the number of events of a certain type, the distribution of the length of time in a set of states, and the probability of a near-coincident fault. The method is then extended to calculate performability distributions. The method relies on coloring subintervals of the finite observation period based on the particular application, and then calculating the measure of interest using these colored intervals.A new computational algorithm called distribution analysis by chain (DAC) is developed. This algorithm computes joint queue-length distributions for product-form queuing networks with single-server fixed rate, infinite server, and queue-dependent service centers. Joint distributions are essential in problems such as the calculation of availability measures using queuing network models. The algorithm is efficient since the cost to evaluate joint queue-length probabilities is of the same order as the number of these probabilities. This contrasts with the cost of evaluating these probabilities using previous algorithms. The DAC algorithm also computes mean queue lengths and throughputs more efficiently than the recently proposed RECAL and MVAC algorithms. Furthermore, the algorithm is  numerically stable and its recursion is surprisingly simple.
For every choice of positive integers c and k such that k ≥ 3 and c2-k ≥ 0.7, there is a positive number &egr; such that, with probability tending to 1 as n tends to ∞, a randomly chosen family of cn clauses of size k over n variables is unsatisfiable, but every resolution proof of its unsatisfiability must generate at least (1 + &egr;)n clauses.The minimum-weight perfect matching problem for complete graphs of n vertices with edge weights satisfying the triangle inequality is considered. For each nonnegative integer k ≤ log3n, and for any perfect matching algorithm that runs in t(n) time and has an error bound of ƒ(n) times the optimal weight, an O(max{n2, t(3-kn)})-time heuristic algorithm with an error bound of (7/3)k(1 + ƒ(3 kn)) - 1 is given. By the selection of k as appropriate functions of n, heuristics that have better running times and/or error bounds than existing ones are derived.This work describes a large number of constructions for sorting N integers in the range [0, M - 1], for N ≤ M ≤ N2, for the standard VLSI bit model. Among other results, we attain:VLSI sorter constructions that are within a constant factor of optimal size, for all M and almost all running times T.a fundamentally new merging network for sorting numbers in a bit model.new organizational approaches for optimal tuning of merging networks and the proper management of data flow.The problem of connecting a set of terminals that lie on the sides of a rectangle to minimize the total area is discussed. An O(n) algorithm is presented to solve this problem when the set of n terminals is initially sorted. The strategy in this paper is to reduce the problem to several problems such that no matter what instance is started with, at least one of these problems can be solved optimally by a greedy method.Many problems can be modeled as single-server queues with impatient customers. An example is that of the transmission of voice packets over a packet-switched network. If the voice packets do not reach their destination within a certain time interval of their transmission, they are useless to the receiver and considered lost. It is therefore desirable to schedule the customers such that the fraction of customers served within their respective deadlines is maximized. For this measure of performance, it is shown that the shortest time to extinction (STE) policy is optimal for a class of continuous and discrete time nonpreemptive M/G/1 queues that do not allow unforced idle times. When unforced idle times are allowed, the best policies belong to the class of shortest time to extinction with inserted idle time (STEI) policies. An STEI policy requires that the customer closest to his or her deadline be scheduled whenever it schedules a customer. It also has the choice of inserting idle times while the queue is nonempty. It is also shown that the STE policy is optimal for the discrete time G/D/1 queue where all customers receive one unit of service. The paper concludes with a comparison of the expected customer loss using an STE policy with that of the first-come, first-served (FCFS) scheduling policy for one specific queue.The computational capabilities of a system of n indistinguishable (anonymous) processors arranged on a ring in the synchronous and asynchronous models of distributed computation are analyzed. A precise characterization of the functions that can be computed in this setting is given. It is shown that any of these functions can be computed in O(n2) messages in the asynchronous model. This is also proved to be a lower bound for such elementary functions as AND, SUM, and Orientation. In the synchronous model any computable function can be computed in O(n log n) messages. A ring can be oriented and start synchronized within the same bounds.The main contribution of this paper is a new technique for proving lower bounds in the synchronous model. With this technique tight lower bounds of &thgr;(n log n) (for particular n) are proved for XOR, SUM, Orientation, and Start Synchronization. The technique is based on a string-producing mechanism from formal language theory, first introduced by Thue to study square-free words. Two methods for generalizing the synchronous lower bounds to arbitrary ring sizes are presented.Suppose we want to eliminate the local go to statements of a Pascal program by replacing them with multilevel loop exit statements. The standard ground rules for eliminating go to's require that we preserve the flow graph of the program, but they allow us to completely rewrite the control structures that glue together the program's atomic tests and actions. The go to's can be eliminated from a program under those ground rules if and only if the flow graph of that program has the graph-theoretic property named reducibility.This paper considers a stricter set of ground rules, introduced by Peterson, Kasami, and Tokura, which demand that we preserve the program's original control structures, as well as its flow graph, while we eliminate its go to's. In particular, we are allowed to delete the go to statements and the labels that they jump to and to insert various exit statements and labeled repeat-endloop pairs for them to jump out of. But we are forbidden to change the rest of the program text in any way. The critical issue that determines whether go to's can be eliminated under these stricter rules turns out to be the static order of the atomic tests and actions in the program text. This static order can be encoded in the program's flow graph by augmenting it with extra edges. It can then be shown that the reducibility of a program's augmented flow graph, augmenting edges and all, is a necessary and sufficient condition for the eliminability of go to's from that program under the stricter rules.All previously known efficient maximum-flow algorithms work by finding augmenting paths, either one path at a time (as in the original Ford and Fulkerson algorithm) or all shortest-length augmenting paths at once (using the layered network approach of Dinic). An alternative method based on the preflow concept of Karzanov is introduced. A preflow is like a flow, except that the total amount flowing into a vertex is allowed to exceed the total amount flowing out. The method maintains a preflow in the original network and pushes local flow excess toward the sink along what are estimated to be shortest paths. The algorithm and its analysis are simple and intuitive, yet the algorithm runs as fast as any other known method on dense graphs, achieving an O(n3) time bound on an n-vertex graph. By incorporating the dynamic tree data structure of Sleator and Tarjan, we obtain a version of the algorithm running in O(nm log(n2/m)) time on an n-vertex, m-edge graph. This is as fast as any known method for any graph density and faster on graphs of moderate density. The algorithm also admits efficient distributed and parallel implementations. A parallel implementation running in O(n2log n) time using n processors and O(m) space is obtained. This time bound matches that of the Shiloach-Vishkin algorithm, which also uses n processors but requires O(n2) space.Recently a new connection was discovered between the parallel complexity class NC1 and the theory of finite automata in the work of Barrington on bounded width branching programs. There (nonuniform) NC1 was characterized as those languages recognized by a certain nonuniform version of a DFA. Here we extend this characterization to show that the internal structures of NC1 and the class of automata are closely related.In particular, using Thérien's classification of finite monoids, we give new characterizations of the classes AC0, depth-k AC0, and ACC, the last being the AC0 closure of the mod q functions for all constant q. We settle some of the open questions in [3], give a new proof that the dot-depth hierarchy of algebraic automata theory is infinite [8], and offer a new framework for understanding the internal structure of NC1.The nature of programming languages that fail to have a relatively complete proof formalism is discussed. First, it is shown that such failures may be due to the meagerness of the programming language, rather than to the presence of complex control structures as in the cases studied so far. The failure of relative completeness is then derived for two languages with a rich control structure, using simple simulations of general recursive functions by procedure call mechanisms.The computational complexity of learning Boolean concepts from examples is investigated. It is shown for various classes of concept representations that these cannot be learned feasibly in a distribution-free sense unless R = NP. These classes include (a) disjunctions of two monomials, (b) Boolean threshold functions, and (c) Boolean formulas in which each variable occurs at most once. Relationships between learning of heuristics and finding approximate solutions to NP-hard optimization problems are given.For any fixed k, a remarkably simple single-tape Turing machine can simulate k independent counters in real time.
The Church-Rosser theorem is a celebrated metamathematical result on the lambda calculus. We describe a formalization and proof of the Church-Rosser theorem that was carried out with the Boyer-Moore theorem prover. The proof presented in this paper is based on that of Tait and Martin-Löf. The mechanical proof illustrates the effective use of the Boyer-Moore theorem prover in proof checking difficult metamathematical proofs.The computational complexity of constructing the imbeddings of a given graph into surfaces of different genus is not well understood. In this paper, topological methods and a reduction to linear matroid parity are used to develop a polynomial-time algorithm to find a maximum-genus cellular imbedding. This seems to be the first imbedding algorithm for which the running time is not exponential in the genus of the imbedding surface.Efficient decomposition algorithms for the weighted maximum independent set, minimum coloring, and minimum clique cover problems on planar perfect graphs are presented. These planar graphs can also be characterized by the absence of induced odd cycles of length greater than 3 (odd holes). The algorithm in this paper is based on decomposing these graphs into essentially two special classes of inseparable component graphs whose optimization problems are easy to solve, finding the solutions for these components and combining them to form a solution for the original graph. These two classes are (i) planar comparability graphs and (ii) planar line graphs of those planar bipartite graphs whose maximum degrees are no greater than three. The same techniques can be applied to other classes of perfect graphs, provided that efficient algorithms are available for their inseparable component graphs.A probability distribution &mgr; on [0, 1] allows perfect packing if n items of size X1, … , Xn, independent and identically distributed according to &mgr; can be packed in unit size bins in such a way that the expected wasted space is o(n). A large class of distributions that allow perfect packing is exhibited. As a corollary, the intervals [a, b] for which the uniform distribution on [a, b] allows perfect packing are determined.Binary exponential backoff is a randomized protocol for regulating transmissions on a multiple-access broadcast channel. Ethernet, a local-area network, is built upon this protocol. The fundamental theoretical issue is stability: Does the backlog of packets awaiting transmission remain bounded in time, provided the rates of new packet arrivals are small enough? It is assumed n ≥ 2 stations share the channel, each having an infinite buffer where packets accumulate while the station attempts to transmit the first from the buffer. Here, it is established that binary exponential backoff is stable if the sum of the arrival rates is sufficiently small. Detailed results are obtained on which rates lead to stability when n = 2 stations share the channel. In passing, several other results are derived bearing on the efficiency of the conflict resolution process. Simulation results are reported that, in particular, indicate alternative retransmission protocols can significantly improve performance.The problem of realizing an idealized parallel architecture on a (possibly fault-laden) physical architecture is studied. Our formulation performs the mapping in the light of the algorithm that one wants to implement on the idealized architecture. A version of the mapping algorithm suggested by the DIOGENES methodology for designing fault-tolerant VLSI processor arrays is settled definitely. Two quality metrics for mappings are considered, the first embodying an idealized notion of average delay, which relates to power consumption, and the second being the length of the longest run of wire. For the average-delay measure, four algorithms that optimally assign the m vertices of the embedded graph to the n fault-free processors that have been fabricated are presented. The most general algorithm makes no assumptions about the structure of the array or the physical format of the processors; it runs in time O(m · (n - m)2). The other algorithms assume that the processors are laid out in such a way that interprocessor distances obey the triangle equality; they run in times ranging from time O(max{m, n - m} · log min {m, n - m}) for certain array structures, including linear arrays, to time O(max{m, n - m}) for a narrow class of array structures, including pyramid arrays. For the max-wire-run cost measure, it is shown that the problem of finding cost-optimal vertex-to-processor assignments is NP-complete. However, an algorithm is presented that yields, in time O(m · (n - m)2), vertex-to-processor assignments that are within a factor of 3 of optimal (they are optimal when the input graph-embedding is outplanar). This algorithm can easily be converted to one that yields, in time O(m · (n - m)3), vertex-to-processor assignments that are within a factor of 2 of optimal. Finally, an algorithm that yields optimal assignments when the interprocessor distances obey the triangle equality is presented; this algorithm operates in time O(m · (n - m) · log(m · (n - m)) · log M), where M is the largest interprocessor distance.Let G and H be two mesh-connected arrays of processors, where G = g1, X g2 X … X g1, H = h1 x h2 x … x hd, and g1 … g1 ≤ h1 … hd. The problem of simulating G by H is considered and the best possible simulation in terms of the gi's and hi's is characterized by giving such a simulation and proving its optimality in the worst-case sense. Also the same bound on the average cost of encoding the edges of G as distinct paths in H is established.Modular integer exponentiation (given a, e, and m, compute ae mod m) is a fundamental problem in algebraic complexity for which no efficient parallel algorithm is known. Two closely related problems are modular polynomial exponentiation (given a(x), e, and m(x), compute (a(x))e mod m(x)) and polynomial exponentiation (given a(x), e. and t, compute the coefficient of xt in (a(x))e). It is shown that these latter two problems are in NC2 when a(x) and m(x) are polynomials over a finite field whose characteristic is polynomial in the input size.The busy period of order n for a subnetwork, which for large n describes heavy traffic periods of that subnetwork, is described for queuing networks. The mean duration of such busy periods and efficient algorithms for computing these quantities are determined.A new algorithm for the hierarchical aggregation of singularly perturbed finite-state Markov processes is derived. The approach taken bridges the gap between conceptually simple results for a relatively restricted class of processes and the significantly more complex results for the general case. The critical role played by (almost) transient states is exposed, resulting in a straightforward algorithm for the construction of a sequence of aggregate generators associated with various time scales. These generators together provide a uniform asymptotic approximation of the original probability transition function.In this paper, a very simple model of parallel computation is considered, and the question of how restricting the flow of data to be one way compares with two-way flow is studied. It is shown that the one-way version is surprisingly very powerful in that it can solve problems that seemingly require two-way communication. Whether or not one-way communication is strictly weaker than two-way is an open problem, although the conjecture in this paper is in the positive. It is shown, however, that proving this conjecture is at least as hard as some well-known open problems in complexity theory.Recent advances in graph theory and graph algorithms dramatically alter the traditional view of concrete complexity theory, in which a decision problem is generally shown to be in P by producing an efficient algorithm to solve an optimization version of the problem. Nonconstructive tools are now available for classifying problems as decidable in polynomial time by guaranteeing only the existence of polynomial-time decision algorithms. In this paper these new methods are employed to prove membership in P for a number of problems whose complexities are not otherwise known. Powerful consequences of these techniques are pointed out and their utility is illustrated. A type of partially ordered set that supports this general approach is defined and explored.Let M be a parallel RAM with p processors and arithmetic operations addition and subtraction recognizing L ⊂ Nn in T steps. (Inputs for M are given integer by integer, not bit by bit.) Then L can be recognized by a (sequential) linear search algorithm (LSA) in O(n4(log(n) + T + log(p))) steps. Thus many n-dimensional restrictions of NP-complete problems (binary programming, traveling salesman problem, etc.) and even that of the uniquely optimum traveling salesman problem, which is &Dgr;P2-complete, can be solved in polynomial time by an LSA. This result generalizes the construction of a polynomial LSA for the n-dimensional restriction of the knapsack problem previously shown by the author, and destroys the hope of proving nonpolynomial lower bounds on LSAs for any problem that can be recognized by a PRAM as above with 2poly(n) processors in poly(n) time.For on-line random-access machines under logarithmic cost, the simple task of storing arbitrary binary inputs has nonlinear complexity. Even if all kinds of powerful internal operations are admitted and reading of storage locations is free of charge, just successively changing the storage contents for properly storing arbitrary n-bit inputs requires an average cost of order n · log*n.
The problem of computing the Euclidean shortest path between two points in three-dimensional space bounded by a collection of convex and disjoint polyhedral obstacles having n faces altogether is considered. This problem is known to be NP-hard and in exponential time for arbitrarily many obstacles; it can be solved in O(n2log n) time for a single convex polyhedral obstacle and in polynomial time for any fixed number of convex obstacles. In this paper Mount's technique is extended to the case of two convex polyhedral obstacles and an algorithm that solves this problem in time O(n3 · 2O{&agr;(n)4}log n) (where &agr;(n) is the functional inverse of Ackermann's function, and is thus extremely slowly growing) is presented, thus improving significantly Sharir's previous results for this special case. This result is achieved by constructing a new kind of Voronoi diagram, called peeper's Voronoi diagram, which is introduced and analyzed in this paper, and which may be of interest in its own right.The concept of partial synchrony in a distributed system is introduced. Partial synchrony lies between the cases of a synchronous system and an asynchronous system. In a synchronous system, there is a known fixed upper bound &Dgr; on the time required for a message to be sent from one processor to another and a known fixed upper bound &PHgr; on the relative speeds of different processors. In an asynchronous system no fixed upper bounds &Dgr; and &PHgr; exist. In one version of partial synchrony, fixed bounds &Dgr; and &PHgr; exist, but they are not known a priori. The problem is to design protocols that work correctly in the partially synchronous system regardless of the actual values of the bounds &Dgr; and &PHgr;. In another version of partial synchrony, the bounds are known, but are only guaranteed to hold starting at some unknown time T, and protocols must be designed to work correctly regardless of when time T occurs. Fault-tolerant consensus protocols are given for various cases of partial synchrony and various fault models. Lower bounds that show in most cases that our protocols are optimal with respect to the number of faults tolerated are also given. Our consensus protocols for partially synchronous processors use new protocols for fault-tolerant “distributed clocks” that allow partially synchronous processors to reach some approximately common notion of time.Since about 1971, much research has been done on Thue systems that have properties that ensure viable and efficient computation. The strongest of these is the Church-Rosser property, which states that two equivalent strings can each be brought to a unique canonical form by a sequence of length-reducing rules. In this paper three ways in which formal languages can be defined by Thue systems with this property are studied, and some general results about the three families of languages so determined are studied.Considered is the question of whether top-down (Prolog-like) evaluation of a set of logical rules can be guaranteed to terminate. The NAIL! system is designed to process programs consisting of logical rules and to select, for each fragment of the program, the best from among many possible strategies for its evaluation. In the context of such a system, it is essential that termination tests be fast. Thus, the “uniqueness” property of logical rules is introduced. This property is satisfied by many of the common examples of rules and is easily recognized. For rules with this property, a set of inequalities, whose satisfaction is sufficient for termination of the rules, can be generated in polynomial time. Then a polynomial test for satisfaction of constraints generated by this process is given.The minimum-cost flow problem is: Given a network with n vertices and m edges, find a maximum flow of minimum cost. Many network problems are easily reducible to this problem. A polynomial-time algorithm for the problem has been known for some time, but only recently a strongly polynomial algorithm was discovered. In this paper an O(n2(m + n log n)log n) algorithm is designed. The previous best algorithm, due to Fujishige and Orlin, had an O(m2(m + nlogn)logn) time bound. Thus, for dense graphs an improvement of two orders of magnitude is obtained.The algorithm in this paper is based on Fujishige's algorithm (which is based on Tardos's algorithm). Fujishige's algorithm consists of up to m iterations, each consisting of O(m log n) steps. Each step solves a single source shortest path problem with nonnegative edge lengths. This algorithm is modified in order to make an improved analysis possible. The new algorithm may still consist of up to m iterations, and an iteration may still consist of up to O(mlogn) steps, but it can still be shown that the total number of steps is bounded by O(n2logn. The improvement is due to a new technique that relates the time spent to the progress achieved.The random, heuristic search algorithm called simulated annealing is considered for the problem of finding the maximum cardinality matching in a graph. It is shown that neither a basic form of the algorithm, nor any other algorithm in a fairly large related class of algorithms, can find maximum cardinality matchings such that the average time required grows as a polynomial in the number of nodes of the graph. In contrast, it is also shown for arbitrary graphs that a degenerate form of the basic annealing algorithm (obtained by letting “temperature” be a suitably chosen constant) produces matchings with nearly maximum cardinality in polynomial average time.The protection state of a system is defined by the privileges possessed by subjects at a given moment. Operations that change this state are themselves authorized by the current state. This poses a design problem in constructing the initial state so that all derivable states conform to a particular policy. It also raises an analysis problem of characterizing the protection states derivable from a given initial state. A protection model provides a framework for both design and analysis. Design generality and tractable analysis are inherently conflicting goals. Analysis is particularly difficult if creation of subjects is permitted. The schematic protection model resolves this conflict by classifying subjects and objects into protection types. The privileges possessed by a subject consist of a type-determined part specified by a static protection scheme and a dynamic part consisting of tickets (capabilities). It is shown that analysis is tractable for this model provided certain restrictions are imposed on subject creation. A scheme authorizes creation of subjects via a binary relation on subject types. Our principal constraint is that this relation be acyclic, excepting loops that authorize a subject to create subjects of its own type. Our assumptions admit a variety of useful systems.Two mathematical models dealing with optimal placement of directories on disk devices are analyzed. Storage addresses on the disk are approximated by points in the interval [0, 1]. Requests for information on the disk are represented by a sequence of file names. To process a request, a read-write head is first moved to a directory kept on the disk that specifies the address of the file, and then a head is moved to the specified address. The addresses are assumed to be independent and uniform on [0,1].In the first model we consider a system of two heads separated by a fixed distance d and a directory situated at 0 ≤ x ≤ 1. In the second model we consider a system consisting of one head and n ≥ 2 directories at 0 ≤ x1 < x2 < … < xn ≤ 1. For both models we study the problem of finding those values of the parameters that minimize the expected head motion to process a request in statistical equilibrium.Methods are presented for finding reductions between the computations of certain arithmetic functions that preserve asymptotic Boolean complexities (circuit depth or size). They can be used to show, for example, that all nonlinear algebraic functions are as difficult as integer multiplication with respect to circuit size. As a consequence, any lower or upper bound (e.g., O(n log n log log n)) for one of them applies to the whole class. It is also shown that, with respect to depth and size simultaneously, multiplication is reducible to any nonlinear and division to any nonpolynomial algebraic function.Exponential lower bounds on the complexity of computing the clique functions in the Boolean decision-tree model are proved. For one-time-only branching programs, large polynomial lower bounds are proved for k-clique functions if k is fixed, and exponential lower bounds if k increases with n. Finally, the hierarchy of the classes BPd(P) of all sequences of Boolean functions that may be computed by d-times only branching programs of polynomial size is introduced. It is shown constructively that BP1(P) is a proper subset of BP2(P).
Many-sorted unification is considered; that is, unification in the many-sorted free algebras of terms, where variables, as well as the domains and ranges of functions, are restricted to certain subsets of the universe, given as a potentially infinite hierarchy of sorts. It is shown that complete and minimal sets of unifiers may not always exist for many-sorted unification. Conditions for sort hierarchies that are equivalent for the existence of these sets with one, finitely many, or infinitely many elements are presented. It is also proved that being a forest-structured sort hierarchy is a necessary and sufficient criterion for the Robinson Unification Theorem to hold for many-sorted unification. An algorithm for many-sorted unification is given.T. Parsons originally proposed and studied the following pursuit-evasion problem on graphs: Members of a team of searchers traverse the edges of a graph G in pursuit of a fugitive, who moves along the edges of the graph with complete knowledge of the locations of the pursuers. What is the smallest number s(G) of searchers that will suffice for guaranteeing capture of the fugitive? It is shown that determining whether s(G) ≤ K, for a given integer K, is NP-complete for general graphs but can be solved in linear time for trees. We also provide a structural characterization of those graphs G with s(G) ≤ K for K = 1, 2, 3.In this paper, a new method is presented for (i) determining an optimal retry policy and (ii) using retry for fault characterization, which is defined as classification of the fault type and determination of fault durations. First, an optimal retry policy is derived for a given fault characteristic, which determines the maximum allowable retry durations so as to minimize the total task completion time. Then, the combined fault characterization and retry decision, in which the characteristic of a fault is estimated simultaneously with the determination of the optimal retry policy, are carried out. Two solution approaches are developed: one is based on point estimation and the other on Bayes sequential decision analysis.Numerical examples are presented in which all the durations associated with faults (i.e., active, benign, and interfailure durations) have monotone hazard rate functions (e.g., exponential Weibull and gamma distributions). These are standard distributions commonly used for modeling and analyses of faults.A large class of relational database update transactions is investigated with respect to equivalence and optimization. The transactions are straight-line programs with inserts, deletes, and modifications using simple selection conditions. Several basic results are obtained. It is shown that transaction equivalence can be decided in polynomial time. A number of optimality criteria for transactions are then proposed, as well as two normal forms. Polynomial-time algorithms for transaction optimization and normalization are exhibited. Also, an intuitively appealing system of axioms for proving transaction equivalence is introduced. Finally, a simple, natural subclass of transactions, called strongly acyclic, is shown to have particularly desirable properties.Reliable concurrent processing of transactions in a database system is examined. Since serializability, the conventional concurrency control correctness criterion, is not adequate in the presence of common failures, another theory of correctness is proposed, involving the concepts of commit serializability, recoverability, and resiliency.Conjunctive queries are generalized so that inequality comparisons can be made between elements of the query. Algorithms for containment and equivalence of such “inequality queries” are given, under the assumption that the data domains are dense and totally ordered. In general, containment does not imply the existence of homomorphisms (containment mappings), but the homomorphism property does exist for subclasses of inequality queries. A minimization algorithm is defined using the equivalence algorithm. It is first shown that the constants appearing in a query can be divided into “essential” and “nonessential” subgroups. The minimum query can be nondeterministically guessed using only the essential constants of the original query.The following problem is studied: How, and to what extent, can the retrieval speed of external hashing be improved by storing a small amount of extra information in internal storage? Several algorithms that guarantee retrieval in one access are developed and analyzed. In the first part of the paper, a restricted class of algorithms is studied, and a lower bound on the amount of extra storage is derived. An algorithm that achieves this bound, up to a constant difference, is also given. In the second part of the paper a number of restrictions are relaxed and several more practical algorithms are developed and analyzed. The last one, in particular, is very simple and efficient, allowing retrieval in one access using only a fixed number of bits of extra internal storage per bucket. The amount of extra internal storage depends on several factors, but it is typically very small: only a fraction of a bit per record stored. The cost of inserting a record is also analyzed and found to be low. Taking all factors into account, this algorithm is highly competitive for applications requiring very fast retrieval.The first part of the paper shows that previous theoretical work on the semantics of probabilistic programs (Kozen) and on the correctness of performance annotated programs (Ramshaw) can be used to automate the average-case analysis of simple programs containing assignments, conditionals, and loops. A performance compiler has been developed using this theoretical foundation. The compiler is described, and it is shown that special cases of symbolic simplifications of formulas play a major role in rendering the system usable. The performance compiler generates a system of recurrence equations derived from a given program whose efficiency one wishes to analyze. This generation is always possible, but the problem of solving the resulting equations may be complex. The second part of the paper presents an original method that generalizes the previous approach and is applicable to functional programs that make use of recursion and complex data structures. Several examples are presented, including an analysis of binary tree sort. A key feature of the analysis of such programs is that distributions on complex data structures are represented using attributed probabilistic grammars.A product-form queuing network with multiple open and closed chains is considered. Some of the closed chains, which have a single customer each, require allocation of resources in the network so as to maximize a weighted throughput performance criterion. Chains with more than one customer can be decomposed into many chains of one customer each. It is proved that an optimal allocation of resources lies on a vertex (extreme points) of the set of feasible allocations. This considerably reduces the search space for an optimal allocation. Applications of this result in distributed computing are discussed.Algorithms on multivariate polynomials represented by straight-line programs are developed. First, it is shown that most algebraic algorithms can be probabilistically applied to data that are given by a straight-line computation. Testing such rational numeric data for zero, for instance, is facilitated by random evaluations modulo random prime numbers. Then, auxiliary algorithms that determine the coefficients of a multivariate polynomial in a single variable are constructed. The first main result is an algorithm that produces the greatest common divisor of the input polynomials, all in straight-line representation. The second result shows how to find a straight-line program for the reduced numerator and denominator from one for the corresponding rational function. Both the algorithm for that construction and the greatest common divisor algorithm are in random polynomial time for the usual coefficient fields and output a straight-line program, which with controllably high probability correctly determines the requested answer. The running times are polynomial functions in the binary input size, the input degrees as unary numbers, and the logarithm of the inverse of the failure probability. The algorithm for straight-line programs for the numerators and denominators of rational functions implies that every degree-bounded rational function can be computed fast in parallel, that is, in polynomial size and polylogarithmic depth.
With a system of parallel coordinates, objects in RN can be represented with planar “graphs” (i.e., planar diagrams) for arbitrary N [21]. In R2, embedded in the projective plane, parallel coordinates induce a point ← → line duality. This yields a new duality between bounded and unbounded convex sets and hstars (a generalization of hyperbolas), as well as a duality between convex union (convex merge) and intersection. From these results, algorithms for constructing the intersection and convex merge of convex polygons in O(n) time and the convex hull on the plane in O(log n) for real-time and O(n log n) worst-case construction, where n is the total number of points, are derived. By virtue of the duality, these algorithms also apply to polygons whose edges are a certain class of convex curves. These planar constructions are studied prior to exploring generalizations to N-dimensions. The needed results on parallel coordinates are given first.Parallel communication algorithms and networks are central to large-scale parallel computing and, also, data communications. This paper identifies adverse source-destination traffic patterns and proposes a scheme for obtaining relief by means of randomized routing of packets on simple extensions of the well-known omega networks. Valiant and Aleliunas have demonstrated randomized algorithms, for a certain context which we call nonrenewal, that complete the communication task in time O(log N) with overwhelming probability, where N is the number of sources and destinations. Our scheme has advantages because it uses switches of fixed degree, requires no scheduling, and, for the nonrenewal context, is as good in proven performance. The main advantage of our scheme comes when we consider the renewal context in which packets are generated at the sources continually and asynchronously. Our algorithm extends naturally from the nonrenewal context. In the analysis in the renewal context we, first, explicitly identify the maximum traffic intensities in the internal links of the extended omega networks over all source-destination traffic specifications that satisfy loose bounds. Second, the benefits of randomization on the stability of the network are identified. Third, exact results, for certain restricted models for sources and transmission, and approximate analytic results, for quite general models, are derived for the mean delays. These results show that, in the stable regime, the maximum mean time from source to destination is asymptotically proportional to log N. Numerical results are presented.A new one-pass algorithm for constructing dynamic Huffman codes is introduced and analyzed. We also analyze the one-pass algorithm due to Faller, Gallager, and Knuth. In each algorithm, both the sender and the receiver maintain equivalent dynamically varying Huffman trees, and the coding is done in real time. We show that the number of bits used by the new algorithm to encode a message containing t letters is < t bits more than that used by the conventional two-pass Huffman scheme, independent of the alphabet size. This is best possible in the worst case, for any one-pass Huffman method. Tight upper and lower bounds are derived. Empirical tests show that the encodings produced by the new algorithm are shorter than those of the other one-pass algorithm and, except for long messages, are shorter than those of the two-pass method. The new algorithm is well suited for on-line encoding/decoding in data networks and for tile compression.An orthogonal query that asks to aggregate the set of records in k-dimensional box regions is studied, and it is shown that space O(N((log N)/(log log N))k-1) makes possible a combined time complexity O(logkN ) for retrievals, insertions, and deletions.Weighted voting is used as the basis for a replication technique for directories. This technique affords arbitrarily high data availability as well as high concurrency. Efficient algorithms are presented for all of the standard directory operations. A structural property of the replicated directory that permits the construction of an efficient algorithm for deletion is proven. Simulation results are presented and the system is modeled and analyzed. The analysis agrees well with the simulation, and the space and time performance are shown to be good for all configurations of the system.Byzantine Generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. These protocols are run in a system that consists of n processes, t of which are faulty. The protocols are conducted in synchronous rounds of message exchange. It is shown that, in the absence of eavesdropping, without using cryptography, for any &egr; > 0 and t = n/(3 + &egr;), there is a randomized protocol with O(log n) expected number of rounds. If cryptographic methods are allowed, then, for &egr; > 0 and t = n/(2 + &egr;), there is a randomized protocol with O(log n) expected number of rounds. This is an improvement on the lower bound of t + 1 rounds required for deterministic protocols, and on a previous result of t/log n expected number of rounds for randomized noncryptographic protocols.The main result of this paper is a general technique for determining lower bounds on the communication complexity of problems on various distributed computer networks. This general technique is derived by simulating the general network by a linear array and then using a lower bound on the communication complexity of the problem on the linear array. Applications of this technique yield optimal bounds on the communication complexity of merging, ranking, uniqueness, and triangle-detection problems on a ring of processors. Nontrivial near-optimal lower bounds on the communication complexity of distinctness, merging, and ranking on meshes and complete binary trees are also derived.A new summation formula based on the orthogonal property of Walsh functions is devised. Using this formula, the k-dimensional discrepancy of the generalized feedback shift register (GFSR) pseudorandom numbers is derived. The relation between the discrepancy and k-distribution of GFSR sequences is also obtained. Finally the definition of optimal GPSR pseudorandom number generators is introduced.Algorithms are given that compute maximum flows in planar directed networks either in O((log n)3) parallel time using O(n4) processors or O((log n)2) parallel time using O(n6) processors. The resource consumption of these algorithms is dominated by the cost of finding the value of a maximum flow. When such a value is given, or when the computation is on an undirected network, the bound is O((log n)2) time using O(n3) processors. No efficient parallel algorithm is known for the maximum flow problem in general networks.An algorithm is presented to compute the residue of a polynomial over a finite field of degree n modulo a polynomial of degree O(log n) in O(n) algebraic operations. This algorithm can be implemented on a Turing machine. The implementation is based on Turing machine procedure that divides a polynomial of degree n by a sparse polynomial with k nonzero coefficients in O(kn) steps. This algorithm can be adapted to compute the residue of a number of length n modulo a number of length O(log n) in O(n) bit operations.Striking progress has been made recently in obtaining expressions for the sojourn time distribution function (STDF) of a job at a c-server, first-come, first-serve (FCFS) center in a closed, product-form queuing network. These results have more recently been extended, and expressions have been obtained for the joint distribution function (DF) of the sojourn times of a job at a sequence of single-server, FCFS centers lying on an “overtake-free” path. However, these formulas present considerable computational problems in the case of large, closed queuing networks. In this paper, asymptotic techniques developed by Mitra and McKenna for the calculation of the partition function of large, product-form closed queuing networks are applied to the sojourn time problem. Asymptotic expansions are obtained for the STDF of a job at c-server, FCFS center in closed, product-form queuing networks. Similar expansions are obtained for the joint DF of the sojourn times of a job at a sequence of single server, FCFS centers lying on an “overtake-free” path. In addition, integral expressions are obtained for the STDF of a job at a single server, FCPS center in a closed, product-form queuing network in which all the centers are load independent. These integral expressions also yield useful asymptotic expansions. Finally, integral expressions are also obtained for the joint DF of the sojourn times of a job at the centers of an “overtake-free” path in such a network.In connection with the least fixed point operator the following question was raised: Suppose that a first-order formula P(P) is (semantically) monotone in a predicate symbol P on finite structures. Is P(P) necessarily equivalent on finite structures to a first-order formula with only positive occurrences of P? In this paper, this question is answered negatively. Moreover, the counterexample naturally gives a uniform sequence of constant-depth, polynomial-size, monotone Boolean circuits that is not equivalent to any (however nonuniform) sequence of constant-depth, polynomial-size, positive Boolean circuits.According to the definition of satisfaction of Boolean dependencies, Theorem 15 is not true for Boolean dependencies with negation. (A positive Boolean dependency is built using the Boolean connectives ⋏, ⋎, and ↛; a general Boolean dependency (with negation) may use also the Boolean connective ¬.) Actually, the definition of satisfaction is not meaningful for Boolean dependencies with negation, since many are never satisfied. We show how the definition of satisfaction should be changed in order to make Boolean dependencies with negation meaningful and correct the error.We associate with each relation r a set &agr;(r) of truth assignments, as follows. For each pair of distinct tuples of r, the set &agr;(r) contains the truth assignment that maps an attribute A to true if the two tuples are equal on A, and to false if the two tuples have different values for A. A Boolean dependency &sgr; is satisfied by a relation r if &sgr; (i.e., the corresponding Boolean formula) satisfies every truth assignment of &agr;(r).The original definition given in the paper is equivalent to having &agr;(r) also include the truth assignment that is generated by pairs in which both tuples are really the same tuple of r, that is, to having &agr;(r) also always include the truth assignment &tgr; mapping all attributes to true. Under that definition, however, many Boolean dependencies with negation are never satisfied and, hence, are meaningless. More precisely, according to the original definition, a Boolean dependency is satisfied by
This paper describes an O(n3logn) deterministic algorithm and an O(n3) Las Vegas algorithm for testing whether two given trivalent graphs on n vertices are isomorphic. In fact, the algorithms construct the set of all isomorphisms between two such graphs, presenting, in particular, generators for the group of all automorphisms of a trivalent graph. The algorithms are based upon the original polynomial-time solution to these problems by Luks but they introduce numerous speedups. These include improved permutation-group algorithms that exploit the structure of the underlying 2-groups. A remarkable property of the Las Vegas algorithm is that it computes the set of all isomorphisms between two trivalent graphs for the cost of computing only those isomorphisms that map a specified edge to a specified edge.In an instance of size n of the stable marriage problem, each of n men and n women ranks the members of the opposite sex in order of preference. A stable matching is a complete matching of men and women such that no man and woman who are not partners both prefer each other to their actual partners under the matching. It is well known [2] that at least one stable matching exists for every stable marriage instance. However, the classical Gale-Shapley algorithm produces a marriage that greatly favors the men at the expense of the women, or vice versa. The problem arises of finding a stable matching that is optimal under some more equitable or egalitarian criterion of optimality. This problem was posed by Knuth [6] and has remained unsolved for some time. Here, the objective of maximizing the average (or, equivalently, the total) “satisfaction” of all people is used. This objective is achieved when a person's satisfaction is measured by the position of his/her partner in his/her preference list. By exploiting the structure of the set of all stable matchings, and using graph-theoretic methods, an O(n4) algorithm for this problem is derived.The desirability of acyclic database schemes is well argued in [8] and [13]. For schemas described by multivalued dependencies, acyclicity means that the dependencies do not split each other's left-hand sides and do not form intersection anomalies. In a recent work [4] it is argued that real-world database schemes always meet the former requirement, and in [5] it is shown that any given real-world scheme can be made to satisfy also the latter requirement, after being properly extended. However, the method of elimination of intersection anomalies proposed in [5] is intrinsically nondeterministic—an undesirable property for a design tool. In the present work it is shown that this nondeterminism does not, however, affect the final result of the design process. In addition, we present an efficient deterministic algorithm, which is equivalent to the nondeterministic process of [5]. Along the way a study of intersection anomalies, which is interesting in its own right, is performed.Given a finite set of texts S = {w1, … , wk} over some fixed finite alphabet &Sgr;, a complete inverted file for S is an abstract data type that provides the functions find(w), which returns the longest prefix of w that occurs (as a subword of a word) in S; freq(w), which returns the number of times w occurs in S; and locations(w), which returns the set of positions where w occurs in S. A data structure that implements a complete inverted file for S that occupies linear space and can be built in linear time, using the uniform-cost RAM model, is given. Using this data structure, the time for each of the above query functions is optimal. To accomplish this, techniques from the theory of finite automata and the work on suffix trees are used to build a deterministic finite automaton that recognizes the set of all subwords of the set S. This automaton is then annotated with additional information and compacted to facilitate the desired query functions. The result is a data structure that is smaller and more flexible than the suffix tree.In this paper we develop a new data structure for implementing heaps (priority queues). Our structure, Fibonacci heaps (abbreviated F-heaps), extends the binomial queues proposed by Vuillemin and studied further by Brown. F-heaps support arbitrary deletion from an n-item heap in O(log n) amortized time and all other standard heap operations in O(1) amortized time. Using F-heaps we are able to obtain improved running times for several network optimization algorithms. In particular, we obtain the following worst-case bounds, where n is the number of vertices and m the number of edges in the problem graph:

O(n log n + m) for the single-source shortest path problem with nonnegative edge lengths, improved from O(mlog(m/n+2)n);
O(n2log n + nm) for the all-pairs shortest path problem, improved from O(nm log(m/n+2)n);
O(n2log n + nm) for the assignment problem (weighted bipartite matching), improved from O(nmlog(m/n+2)n);
O(m&bgr;(m, n)) for the minimum spanning tree problem, improved from O(mlog log(m/n+2)n); where &bgr;(m, n) = min {i ↿ log(i)n ≤ m/n}. Note that &bgr;(m, n) ≤ log*n if m ≥ n.
Of these results, the improved bound for minimum spanning trees is the most striking, although all the results give asymptotic improvements for graphs of appropriate densities.
O(n log n + m) for the single-source shortest path problem with nonnegative edge lengths, improved from O(mlog(m/n+2)n);O(n2log n + nm) for the all-pairs shortest path problem, improved from O(nm log(m/n+2)n);O(n2log n + nm) for the assignment problem (weighted bipartite matching), improved from O(nmlog(m/n+2)n);O(m&bgr;(m, n)) for the minimum spanning tree problem, improved from O(mlog log(m/n+2)n); where &bgr;(m, n) = min {i ↿ log(i)n ≤ m/n}. Note that &bgr;(m, n) ≤ log*n if m ≥ n.Of these results, the improved bound for minimum spanning trees is the most striking, although all the results give asymptotic improvements for graphs of appropriate densities.Presented are several algorithms whose operations are governed by a principle of failure functions: When searching for an extremal value within a sequence, it suffices to consider only the subsequence of items each of which is the first possible improvement of its predecessor. These algorithms are more efficient than their more traditional counterparts.We present a simple, efficient, and unified solution to the problems of synchronizing, initializing, and integrating clocks for systems with different types of failures: crash, omission, and arbitrary failures with and without message authentication. This is the first known solution that achieves optimal accuracy—the accuracy of synchronized clocks (with respect to real time) is as good as that specified for the underlying hardware clocks. The solution is also optimal with respect to the number of faulty processes that can be tolerated to achieve this accuracy.An algorithm for computing the power series solution of a system of linear equations with components that are dense univariate polynomials over a field is described and analyzed. A method for converting the power series solution to rational form is derived. Theoretical and experimental cost estimates are obtained and used to identify classes of problems for which the power series method outperforms modular methods. Finally, it is shown that the power series method also provides an effective mechanism for solving the problem in which the coefficients of the polynomials are from the ring of integers.In this paper catastrophic behavior found in computer systems is investigated. Deterministic Catastrophe theory is introduced first. Then it is shown how the theory can be applied in a stochastic framework, which is useful for understanding computer system performance models. Computer system models that exhibit stochastic cusp catastrophe behavior are then analyzed. These models include slotted ALOHA, multiprogramming in computer systems, and buffer flow control in computer networks.A rigorous extension of the recent perturbation analysis approach to more general discrete event systems is given. First, a general class of systems and performance measures is defined, and some basic reprsentational and linearity properties are derived. Next a sample gradient of performance with respect to a parameter of the system is defined. Then, for certain parameters of such systems, an infinitesimal perturbation analysis algorithm is derived. It is proved that this algorithm gives exact values for the sample gradients of performance with respect to the parameters, by observing only one sample path of the DEDS. (However, the sample gradient may or may not be a good estimate of the gradient of the performance measure; this point is elaborated in the body of the paper.) The computational complexity of this algorithm is bound to be linear in the number of events. These results offer the potential for very efficient calculation of the gradients—a fact that can be used for design/operation of computer systems, communication networks, manufacturing systems, and many other real-world systems, particularly since restrictive assumptions (e.g., exponential distributions) are not required of the system.If C is a class of sets and A is not in C, then an infinite set H is a proper hard core for A with respect to C, if H ⊆ A and for every C &egr; C such that C ⊆ A, C ⋒ H is finite. It is shown that if C is a countable class of sets of strings that is closed under finite union and finite variation, then every infinite set not in C has a proper hard core with respect to C. In addition, the density of such generalized complexity cores is studied.The equivalence problem for deterministic real-time pushdown automata is shown to be decidable. This result is obtained by showing that Valiant's parallel stacking technique using a replacement function introduced in this paper succeeds for deterministic real-time pushdown automata. Equivalence is also decidable for two deterministic pushdown automata, one of which is real-time.
A graphical representation of quantifier-free predicate calculus formulas in negation normal form and a new rule of inference that employs this representation are introduced. The new rule, path resolution, is an amalgamation of resolution and Prawitz analysis. The goal in the design of path resolution is to retain some of the advantages of both Prawitz analysis and resolution methods, and yet to avoid to some extent their disadvantages.Path resolution allows Prawitz analysis of an arbitrary subgraph of the graph representing a formula. If such a subgraph is not large enough to demonstrate a contradiction, a path resolvent of the subgraph may be generated with respect to the entire graph. This generalizes the notions of large inference present in hyperresolution, clash-resolution, NC-resolution, and UR-resolution. A class of subgraphs is described for which deletion of some of the links resolved upon preserves the spanning property.An O(n3) algorithm for recognizing planar graphs that do not contain induced odd cycles of length greater than 3 (odd holes) is presented. A planar graph with this property satisfies the requirement that its maximum clique size equal the minimum number of colors required for the graph (graphs all of whose induced subgraphs satisfy the latter property are perfect as defined by Berge). The algorithm presented is based on decomposing these graphs into essentially two special classes of inseparable component graphs that are easy to recognize. They are (i) planar comparability graphs and (ii) planar line graphs of those planar bipartite graphs whose maximum degrees are no greater than 3. Composition schemes for generating planar perfect graphs from those basic components are also provided. This decomposition algorithm can also be adapted to solve the corresponding maximum independent set and minimum coloring problems. Finally, the path-parity problem on planar perfect graphs is considered.New, improved algorithms are proposed for regulating access to a multiple-access channel, a common channel shared by many geographically distributed computing stations. A conflict of multiplicity n occurs when n stations transmit simultaneously to the channel. As a result, all stations receive feedback indicating whether n is 0, 1, or ≥2. If n = 1, the transmission succeeds; whereas if n ≥ 2, all the transmissions fail. Algorithms are presented and analyzed that allow the conflicting stations to compute a stochastic estimate n* of n, cooperatively, at small cost, as a function of the feedback elicited during its execution. An algorithm to resolve a conflict among two or more stations controls the retransmissions of the conflicting stations so that each eventually transmits singly to the channel. Combining one of our estimation algorithms with a tree algorithm (of Capetanakis, Hayes, and Tsybakov and Mikhailov) then leads to a hybrid algorithm for conflict resolution. Several efficient combinations are possible, the most efficient of which resolves conflicts about 20 percent faster on average than any of the comparable algorithms reported to date.A new quantitative approach to the problem of reconfiguring a degradable multimodule system is presented. The approach is concerned with both assigning some modules for computation and arranging others for reliability. Conventionally, a fault-tolerant system performs reconfiguration only upon a subsystem failure. Since there exists an inherent trade-off between the computation capacity and fault tolerance of a multimodule computing system, the conventional approach is a passive action and does not yield a configuration that provides an optimal compromise for the trade-off. By using the expected total reward as the optimal criterion, the need and existence of an active reconfiguration strategy, in which the system reconfigures itself on the basis of not only the occurrence of a failure but also the progression of the mission, are shown.Following the problem formulation, some important properties of an optimal reconfiguration strategy, which specify (i) the times at which the system should undergo reconfiguration and (ii) the configurations to which the system should change, are investigated. Then, the optimal reconfiguration problem is converted to integer nonlinear knapsack and fractional programming problems. The algorithms for solving these problems and a demonstrative example are given. Extensions of the optimal reconfiguration problem are also discussed.It has been observed that, for some database schemes, users may have difficulties retrieving correct information, even for simple queries. The problem occurs when some implicit “piece” of information, defined on some subset of a relation scheme, is not explicitly represented in the database state. In this situation, users may be required to know how the state and the constraints interact before they can retrieve the information correctly.In this paper, the formal notion of embedded-completeness is proposed, and it is shown that schemes with this property avoid the problem described above. A polynomial-time algorithm is given to test whether a database scheme is independent and embedded-complete. Under the assumption of independence, it is shown that embedded-complete schemes allow efficient computation of optimal relational algebra expressions equivalent to the X-total projection, for any set of attributes X.This paper investigates the complexity of multioperand residue addition and multiplication implemented by associative table lookup processing. The complexity measure used is the size of the associative memory, that is, the number of matching words in memory. This measure largely depends on the residue recurrencies, or multiplicities, in the addition and multiplication tables module M. The major effort in this work is to evaluate the recurrencies in simultaneous multioperand residue addition and multiplication. The evaluation is simple in case of addition mod M, and also in multiplication mod M if M is prime. To treat the more difficult case of M nonprime, a recursive procedure was developed for computing the 2-operand multiplication recurrencies mod M. The basis of this technique is the precedence relationships associated with a tree representation of the factors of M. It is then shown that the general D-operand multiplication mod M, D > 2 and M nonprime, can be reduced to the 2-operand case by isomorphic transformation. Computation results of 2-operand residue arithmetic operations are provided. Applications to RNS arithmetic implementation are discussed.This paper presents a new approach to the analysis of hashing with linear probing for nonuniformly distributed hashed keys. The use of urn models is avoided. Instead, some facts about empirical processes, which are well known in statistics are used. In particular, an asymptotic formula for the expected probe length for both a successful and an unsuccessful search is obtained. The accuracy of the approximation is confirmed by simulation.In our model, a graph describes a net of processes communicating through ports and, at the same time, its computation history consisting of a partial ordering of events. Stand-alone evolution of processes is specified by context-free productions. From productions and a basic synchronization mechanism, a set of context-sensitive rewriting rules that models the evolution of processes connected to the same ports can be derived. A computation is a sequence of graphs obtained by successive rewritings. The result of a finite computation is its last graph, whereas the result of an infinite computation is the limit, infinite graph defined through a completion technique based on metric spaces. A result characterizes a concurrent computation, since it abstracts from any particular interleaving of concurrent events, while in the meantime providing information about termination, partial or complete deadlocks, and fairness. Not every result is acceptable, however, but only the computations that produce a result no longer rewritable are successful. Infinite successful computations are shown to coincide with weakly fair computations, and a scheduler yielding all and only such computations is defined.In this paper concurrent dynamic logic (CDL) is introduced as an extension of dynamic logic tailored toward handling concurrent programs. Properties of CDL are discussed, both on the propositional and first-order level, and the extension is shown to possess most of the desirable properties of DL. Its relationships with the &mgr;-calculus, game logic, DL with recursive procedures, and PTIME are further explored, revealing natural connections between concurrency, recursion, and alternation.The existence of minimal degrees is investigated for several polynomial reducibilities. It is shown that no set has minimal degree with respect to polynomial many-one or Turing reducibility. This extends a result of Ladner in which only recursive sets are considered. A polynomial reducibility ≤hT is defined. This reducibility is a strengthening of polynomial Turing reducibility, and its properties relate to the P = ? NP question. For this new reducibility, a set of minimal degree is constructed under the assumption that P = NP. However, the set constructed is nonrecursive, and it is shown that no recursive set is of minimal ≤ hT degree.This paper is concerned with the question of the decidability and the complexity of the decision problem for certain fragments of the theory of free term algebras. The existential fragment of the theory of term algebras is shown to be decidable through the presentation of a nondeterministic algorithm, which, given a quantifier-free formula P, constructs a solution for P if it has one and indicates failure if there are no solutions. It is shown that the decision problem is in NP by proving that, if a quantifier-free formula P has a solution, then there is one that can be represented as a dag in space at most cubic in the length of P. The decision problem is shown to be complete for NP by reducing 3-SAT to that problem. Thus it is established that the existential fragment of the theory of pure list structures in the language of NIL, CONS, CAR, CDR, =, ≤ (subexpression) is NP-complete. It is further shown that even a slightly more expressive fragment of the theory of term algebras, the one that allows bounded universal quantifiers, is undecidable.
One of the basic geometric operations involves determining whether a pair of convex objects intersect. This problem is well understood in a model of computation in which the objects are given as input and their intersection is returned as output. For many applications, however, it may be assumed that the objects already exist within the computer and that the only output desired is a single piece of data giving a common point if the objects intersect or reporting no intersection if they are disjoint. For this problem, none of the previous lower bounds are valid and algorithms are proposed requiring sublinear time for their solution in two and three dimensions.A simple extension of the relational model is introduced to study the effects of dynamic constraints on database evolution. Both static and dynamic constraints are used in conjunction with the model. The static constraints considered here are functional dependencies (FDs). The dynamic constraints involve global updates and are restricted to certain analogs of FDs, called “dynamic” FDs. The results concern the effect of the dynamic constraints on the static constraints satisfied by the database in the course of time. The effect of the past history of the database on the static constraints is investigated using the notions of age and age closure. The connection between the static constraints and the potential future evolution of the database is briefly discussed using the notions of survivability and survivability closure.A randomized algorithm that sorts on an N node network with constant valence in O(log N) time is given. More particularly, the algorithm sorts N items on an N-node cube-connected cycles graph, and, for some constant k, for all large enough &agr;, it terminates within k&agr; log N time with probability at least 1 - N-&agr;.Reaching agreement is a primitive of distributed computing. Whereas this poses no problem in an ideal, failure-free environment, it imposes certain constraints on the capabilities of an actual system: A system is viable only if it permits the existence of consensus protocols tolerant to some number of failures. Fischer et al. have shown that in a completely asynchronous model, even one failure cannot be tolerated. In this paper their work is extended: Several critical system parameters, including various synchrony conditions, are identified and how varying these affects the number of faults that can be tolerated is examined. The proofs expose general heuristic principles that explain why consensus is possible in certain models but not possible in others.The problem of electing a leader in a synchronous ring of n processors is considered. Both positive and negative results are obtained. On the one hand, if processor IDS are chosen from some countable set, then there is an algorithm that uses only O(n) messages in the worst case. On the other hand, any algorithm that is restricted to use only comparisons of IDs requires &OHgr;(n log n) messages in the worst case. Alternatively, if the number of rounds is required to be bounded by some t in the worst case, and IDs are chosen from any set having at least ƒ(n, t) elements, for a certain very fast-growing function ƒ, then any algorithm requires &OHgr;(n log n) messages in the worst case.The power of shared-memory in models of parallel computation is studied, and a novel distributed data structure that eliminates the need for shared memory without significantly increasing the run time of the parallel computation is described. More specifically, it is shown how a complete network of processors can deterministically simulate one PRAM step in O(log n/(log log n)2) time when both models use n processors and the size of the PRAM's shared memory is polynomial in n. (The best previously known upper bound was the trivial O(n)). It is established that this upper bound is nearly optimal, and it is proved that an on-line simulation of T PRAM steps by a complete network of processors requires &OHgr;(T(log n/ log log n)) time.A simple consequence of the upper bound is that an Ultracomputer (the currently feasible general-purpose parallel machine) can simulate one step of a PRAM (the most convenient parallel model to program) in O((log n)2log log n) steps.The direct sum of two term rewriting systems is the union of systems having disjoint sets of function symbols. It is shown that if two term rewriting systems both have the Chruch-Rosser property, then the direct sum of these systems also has this property.The problem of scheduling a set of n jobs on m identical machines so as to minimize the makespan time is perhaps the most well-studied problem in the theory of approximation algorithms for NP-hard optimization problems. In this paper the strongest possible type of result for this problem, a polynomial approximation scheme, is presented. More precisely, for each &egr;, an algorithm that runs in time O((n/&egr;)1/&egr;2) and has relative error at most &egr; is given. In addition, more practical algorithms for &egr; = 1/5 + 2-k and &egr; = 1/6 + 2-k, which have running times O(n(k + log n)) and O(n(km4 + log n)) are presented. The techniques of analysis used in proving these results are extremely simple, especially in comparison with the baroque weighting techniques used previously.The scheme is based on a new approach to constructing approximation algorithms, which is called dual approximation algorithms, where the aim is to find superoptimal, but infeasible, solutions, and the performance is measured by the degree of infeasibility allowed. This notion should find wide applicability in its own right and should be considered for any optimization problem where traditional approximation algorithms have been particularly elusive.The ability of the strongest parallel random access machine model WRAM is investigated. In this model different processors may simultaneously try to write into the same cell of the common memory. It has been shown that a parallel RAM without this option (PRAM), even with arbitrarily many processors, can almost never achieve sublogarithmic time. On the contrary, every function with a small domain like binary values in case of Boolean functions can be computed by a WRAM in constant time. The machine makes fast table look-ups using its simultaneous write ability. The main result of this paper implies that in general this is the “only way” to perform such fast computations and that a domain of small size is necessary. Otherwise simultaneous writes do not give an advantage. Functions with large domains for which any change of one of the n arguments also changes the result are considered, and a logarithmic lower time bound for WRAMs is proved. This bound can be achieved by machines that do not perform simultaneous writes. A simple example of such a function is the sum of n natural numbers.Today's concomitant needs for higher computing power and reliability has increased the relevance of multiple-processor fault-tolerant systems. Multiple functional units improve the raw performance (throughput, response time, etc.) of the system, and, as units fail, the system may continue to function albeit with degraded performance. Such systems and other fault-tolerant systems are not adequately characterized by separate performance and reliability measures. A composite measure for the performance and reliability of a fault-tolerant system observed over a finite mission time is analyzed. A Markov chain model is used for system state-space representation, and transient analysis is performed to obtain closed-form solutions for the density and moments of the composite measure. Only failures that cannot be repaired until the end of the mission are modeled. The time spent in a specific system configuration is assumed to be large enough to permit the use of a hierarchical model and static measures to quantify the performance of the system in individual configurations. For a multiple-processor system, where performance measures are usually associated with and aggregated over many jobs, this is tantamount to assuming that the time to process a job is much smaller than the time between failures. An extension of the results to general acyclic Markov chain models is included.Megiddo introduced a technique for using a parallel algorithm for one problem to construct an efficient serial algorithm for a second problem. This paper provides a general method that trims a factor of O(log n) time (or more) for many applications of this technique.Exponential lower bounds are proved for the length-of-resolution refutations of sets of disjunctions constructed from expander graphs, using the method of Tseitin. Since these sets of clauses encode biconditionals, they have short (polynomial-length) refutations in a standard axiomatic formulation of propositional calculus.
Properties are proved about INSTANCE, a theorem prover module that recognizes that a formula is a special case and/or an alphabetic variant of another formula, and about INSURER, another theorem prover module that decomposes a problem, represented by a formula, into independent subproblems, using a conjunction. The main result of INSTANCE is soundness; the main result of INSURER is a maximum decomposition into subproblems (with some provisos). Experimental results show that a connection graph theorem prover extended with these modules is more effective than the resolution-based connection graph theorem prover alone.The problem of recognizing what objects are where in the workspace of a robot can be cast as one of searching for a consistent matching between sensory data elements and equivalent model elements. In principle, this search space is enormous, and to control the potential combinatorial explosion, constraints between the data and model elements are needed. A set of constraints for sparse sensory data that are applicable to a wide variety of sensors are derived, and their characteristics are examined. Known bounds on the complexity of constraint satisfaction problems are used, together with explicit estimates of the effectiveness of the constraints derived for the case of sparse, noisy, three-dimensional sensory data, to obtain general theoretical bounds on the number of interpretations expected to be consistent with the data. It is shown that these bounds are consistent with empirical results reported previously. The results are used to demonstrate the graceful degradation of the recognition technique with the presence of noise in the data, and to predict the number of data points needed, in general, to uniquely determine the object being sensed.It is assumed that long wires represent large capacitive loads, and the effect on the area of a VLSI layout when drivers are introduced along many long wires in the layout is investigated. A layout is presented for which the introduction of standard drivers along long wires squares the area of the layout; it is shown, however, that the increase in area is never greater than the layout's area squared if the driver can be laid out in a square region. This paper also shows an area-time trade-off for the driver of a single long wire of length / by which the area of the driver from &THgr;(l), to &THgr;(lq), q < l, can be reduced if a delay of &THgr;(ll-q) rather than &THgr;(log l) can be tolerated. Tight bounds are also obtained on the worst-case area increase in general layouts having these drivers.It is proved that the production probabilities of a probabilistic context-free grammar may be obtained as the limit of the estimates inferred from an increasing sequence of randomly drawn samples from the language generated by the grammar.The generalized feedback shift register pseudorandom number generators proposed by Lewis and Payne provide a very attractive method of random number generation. Unfortunately, the published initialization procedure can be extremely time consuming. This paper considers an alternative method of initialization based on a natural polynomial representation for the terms of a feedback shift register sequence that results in substantial improvements in the initialization process.An optimal algorithm to perform the parallel QR decomposition of a dense matrix of size N is proposed. It is deduced that the complexity of such a decomposition is asymptotically 2N, when an unlimited number of processors is available.Four semantics for a small programming language involving unbounded (but countable) nondeterminism are provided. These comprise an operational semantics, two state transformation semantics based on the Egli-Milner and Smyth orders, respectively, and a weakest precondition semantics. Their equivalence is proved. A Hoare-like proof system for total correctness is also introduced and its soundness and completeness in an appropriate sense are shown. Finally, the recursion theoretic complexity of the notions introduced is studied. Admission of countable nondeterminism results in a lack of continuity of various semantic functions, and this is shown to be necessary for any semantics satisfying appropriate conditions. In proofs of total correctness, one resorts to the use of (countable) ordinals, and it is shown that all recursive ordinals are needed.RECAL, a Recursion by Chain Algorithm for computing the mean performance measures of product-form multiple-chain closed queuing networks, is presented. It is based on a new recursive expression that relates the normalization constant of a network with r closed routing chains to those of a set of networks having (r - 1) chains. It relies on the artifice of breaking down each chain into constituent subchains that each have a population of one. The time and space requirements of the algorithm are shown to be polynomial in the number of chains. When the network contains many routing chains, the proposed algorithm is substantially more efficient than the convolution or mean value analysis algorithms. The algorithm, therefore, extends the range of queuing networks that can be analyzed efficiently by exact means.A constructive theory of randomness for functions, based on computational complexity, is developed, and a pseudorandom function generator is presented. This generator is a deterministic polynomial-time algorithm that transforms pairs (g, r), where g is any one-way function and r is a random k-bit string, to polynomial-time computable functions ƒr: {1, … , 2k} → {1, … , 2k}. These ƒr's cannot be distinguished from random functions by any probabilistic polynomial-time algorithm that asks and receives the value of a function at arguments of its choice. The result has applications in cryptography, random constructions, and complexity theory.The accessibility problem for linear sequential machines [12] is the problem of deciding whether there is an input x such that on x the machine starting in a given state q1 goes to a given state q2. Harrison shows that this problem is reducible to the following simply stated linear algebra problem, which we call the "orbit problem":Given (n, A, x, y), where n is a natural number and A, x, and y are nxn, nx1, and nx1 matrices of rationals, respectively, decide whether there is a natural number I such that Aix=y.He conjectured that the orbit problem is decidable. No progress was made on the conjecture for ten years until Shank [22] showed that if n is fixed at 2, then the problem is decidable. This paper shows that the orbit problem for general n is decidable and indeed decidable in polynomial time. The orbit problem arises in several contexts; two of these, linear recurrences and the discrete logarithm problem for polynomials, are discussed, and we apply our algorithm for the orbit problem in these contexts.
The concept of a tuple sequence is introduced in order to investigate structure connected with relational model implementation. Analogs are presented for the relational operations of projection, join, and selection, and the decomposition problem for tuple sequences is considered. The lexicographical ordering of tuple sequences is studied via the notion of (lexicographic) index. A sound and complete set of inference rules for indexes is exhibited, and two algorithmic questions related to indexes examined. Finally, indexes and functional dependencies in combination are studied.The desirability of acyclic (conflict-free) schemes is well argued in [8] and [13]. When a scheme is described by multivalued dependencies, acyclicity means that the dependencies do not split each other's left-hand side and do not form intersection anomalies. It is shown that if the second condition fails to hold, the scheme can be amended so that it does hold. The basic step is to add one attribute and some dependencies to resolve one intersection anomaly. This step generates an extension of the given scheme in which the anomaly does not exist. Also, the iterative use of the basic step is analyzed and it is proved that the transformation so defined terminates and removes all intersection anomalies.The basic inference problem is defined as follows: For a finite set X = {xi,  , xn}, we wish to infer properties of elements of X on the basis of sets of "queries" regarding subsets of X. By restricting these queries to statistical queries, the statistical database (SDB) security problem is obtained. The security problem for the SDB is to limit the use of the SDB so that only statistical information is available and no sequence of queries is sufficient to infer protected information about any individual. When such information is obtained the SDB is said to be compromised. In this paper, two applications concerning the security of the SDB are considered: The complexity of these two applications, when the set of queries consists of (a) a single type of SUM query, (b) a single type of MAX/MIN query, (c) mixed types of MAX and MIN queries, (d) mixed types of SUM and MAX/MIN queries, and (e) mixed types of SUM, MAX, and MIN queries, is studied. Efficient algorithms are designed for some of these situations while others are shown to be NP-hard.The notion of sort set is introduced here to formalize the fact that certain database relations can be sorted so that two or more columns are simultaneously listed in order. This notion is shown to be applicable in several ways to enhance the efficiency of an implemented database. A characterization of when order dependency implies the existence of sort sets in a database is presented, along with several corollaries concerning complexity, Armstrong relations, and cliques of certain graphs. Sort-set dependencies are then introduced. A (finite) sound and complete set of inference rules for sort-set dependencies is presented, as well as a proof that there is no such set for functional and sort-set dependencies taken together. Deciding logical implication for sort-set dependencies is proved to be polynomial, but if functional dependencies are included the problem is co-NP-complete. Each set of sort-set and functional dependencies is shown to have an Armstrong relation. A natural generalization of Armstrong relation, here called separator, is given and then used to study the relationship between order and sort-set dependencies.Let Hn be the height of a binary search tree with n nodes constructed by standard insertions from a random permutation of 1, … , n. It is shown that Hn/log n → c = 4.31107 … in probability as n → ∞, where c is the unique solution of c log((2e)/c) = 1, c ≥ 2. Also, for all p > 0, limn→∞E(Hpn)/ logpn = cp. Finally, it is proved that Sn/log n → c* = 0.3733 … , in probability, where c* is defined by c log((2e)/c) = 1, c ≤ 1, and Sn is the saturation level of the same tree, that is, the number of full levels in the tree.This paper considers a variant of the Byzantine Generals problem, in which processes start with arbitrary real values rather than Boolean values or values from some bounded range, and in which approximate, rather than exact, agreement is the desired goal. Algorithms are presented to reach approximate agreement in asynchronous, as well as synchronous systems. The asynchronous agreement algorithm is an interesting contrast to a result of Fischer et al, who show that exact agreement with guaranteed termination is not attainable in an asynchronous system with as few as one faulty process. The algorithms work by successive approximation, with a provable convergence rate that depends on the ratio between the number of faulty processes and the total number of processes. Lower bounds on the convergence rate for algorithms of this form are proved, and the algorithms presented are shown to be optimal.In solving large sparse linear least squares problems A x ≃ b, several different numeric methods involve computing the same upper triangular factor R of A. It is of interest to be able to compute the nonzero structure of R, given only the structure of A. The solution to this problem comes from the theory of matchings in bipartite graphs. The structure of A is modeled with a bipartite graph, and it is shown how the rows and columns of A can be rearranged into a structure from which the structure of its upper triangular factor can be correctly computed. Also, a new method for solving sparse least squares problems, called block back-substitution, is presented. This method assures that no unnecessary space is allocated for fill, and that no unnecessary space is needed for intermediate fill.In this paper a powerful, and yet simple, technique for devising approximation algorithms for a wide variety of NP-complete problems in routing, location, and communication network design is investigated. Each of the algorithms presented here delivers an approximate solution guaranteed to be within a constant factor of the optimal solution. In addition, for several of these problems we can show that unless P = NP, there does not exist a polynomial-time algorithm that has a better performance guarantee.Currently, network codes based on the primal simplex algorithm are believed to be computationally superior to those based on other methods. Some modifications of the out-of-kilter algorithm of Ford and Fulkerson are given, together with proofs of their correctness and computer implementations using appropriate data structures. The computational tests in this paper indicate that the final code based on these modifications is superior to any previously implemented version of this algorithm. Although this code is not competitive with state-of-the-art primal simplex codes, its performance is encouraging, especially in the case of assignment problems.A method is presented for calculating the partition function, and from it, performance measures, for closed Markovian stochastic networks with queuing centers in which the service or processing rate depends on the center's state or load. The analysis on which this method is based is new and a major extension of our earlier work on load-independent queuing networks. The method gives asymptotic expansions for the partition function in powers of 1/N, where N is a parameter that reflects the size of the network. The expansions are particularly useful for large networks with many classes, each class having many customers. The end result is a decomposition by which expansion coefficients are obtained exactly by linear combinations of partition function values of small network constructs called pseudonetworks. Effectively computable bounds are given for errors arising from the use of a finite number of expansion terms. This method is important because load dependence is at once an essential element of sophisticated network models of computers, computer communications, and switching, teletraffic, and manufacturing systems, and the cause of very intensive computations in conventional techniques. With this method, very large load-dependent networks can be analyzed, whereas previously only small networks were computationally tractable.A queuing system with infinitely many servers, and with the following queuing discipline is considered: For any two jobs i and j in the system, such that i arrived later than j, there is a fixed probability p that i will have to wait for j's execution to terminate before i starts executing. This queuing system is a very simple model for database concurrency control via “static” locking, as well as of parallel execution of programs consisting of several interdependent processes. The problem of determining the maximum arrival rate (as a function of p) that can be sustained before this system becomes unstable is studied. It is shown that this rate is inversely proportional to p, and close upper and lower bounds on the constant for the case of deterministic departures are found. The result suggests that the degree of multiprogramming of multiuser databases, or the level of parallelism of concurrent programs, is inversely proportional to the probability of conflict, and that the constant is small and known within a factor of 2. The technique used involves the computation of certain asymptotic parameters of a random infinite directed acyclic graph (dag) that seem of interest by themselves.Questions about the polynomial-time hierarchy are studied. In particular, the questions, “Does the polynomial-time hierarchy collapse?” and “Is the union of the hierarchy equal to PSPACE?” are considered, along with others comparing the union of the hierarchy with certain probabilistic classes. In each case it is shown that the answer is “yes” if and only if for every sparse set S, the answer is “yes” when the classes are relativized to S if and only if there exists a sparse set S such that the answer is “yes” when the classes are relativized to S. Thus, in each case the question is answered if it is answered for any arbitrary sparse oracle set.Long and Selman first proved that the polynomial-time hierarchy collapses if and only if for every sparse set S, the hierarchy relative to S collapses. This result is re-proved here by a different technique.Baker, Gill, and Solovay constructed sparse sets A and B such that P(A) ≠ NP(A) and NP(B) ≠ co-NP(B). In contrast to their results, we prove that P = NP if and only if for every tally language T, P(T) = NP( T), and that NP = co-NP if and only if for every tally language T, NP(T) = co-NP(T). We show that the polynomial hierarchy collapses if and only if there is a sparse set S such that the polynomial hierarchy relative to S collapses. Similar results are obtained for several other complexity classes.
A major event in automated reasoning was the introduction by Robinson of resolution as an inference principle that is complete for the first-order predicate calculus. Here the theory of binary resolution, based strictly on unification, is recast to incorporate the axioms of equality. Equality-based binary resolution is complete without making use of paramodulation and leads to refutations that are less than half as long as standard refutations with the equality axioms. A detailed discussion is given of the first major use of a theorem prover based on this new method.The problem of partitioning a polygonal region into a minimum number of trapezoids with two horizontal sides is discussed. A triangle with a horizontal side is considered to be a trapezoid with two horizontal sides one of which is degenerate. First, a method of achieving a minimum partition is presented. The number M* of the trapezoids in the minimum partition of a polygonal region P is shown to be M* = n + w - h - d - 1, where n, w, and h are the number of vertices, windows (holes), and horizontal edges of P, respectively, and d is the cardinality of a maximum independent set of the straight-lines-in-the-plane graph associated with P. Next, this problem is shown to be polynomially equivalent to the problem of finding a maximum independent set of a straight-lines-in-the-plane graph, and consequently, it is shown to be NP-complete. However, for a polygonal region without windows, an O(n2)-time algorithm for partitioning it into a minimum number of trapezoids is presented. Finally, an O(n log n)-time approximation algorithm with the performance bound 3 is presented.A novel formal theory of concurrent systems that does not assume any atomic operations is introduced. The execution of a concurrent program is modeled as an abstract set of operation executions with two temporal ordering relations: “precedence” and “can causally affect”. A primitive interprocess communication mechanism is then defined. In Part II, the mutual exclusion is expressed precisely in terms of this model, and solutions using the communication mechanism are given.The theory developed in Part I is used to state the mutual exclusion problem and several additional fairness and failure-tolerance requirements. Four “distributed” N-process solutions are given, ranging from a solution requiring only one communication bit per process that permits individual starvation, to one requiring about N! communication bits per process that satisfies every reasonable fairness and failure-tolerance requirement that we can conceive of.A sound and, in certain cases, complete method is described for evaluating queries in relational databases with null values where these nulls represent existing but unknown individuals. The soundness and completeness results are proved relative to a formalization of such databases as suitable theories of first-order logic. Because the algorithm conforms to the relational algebra, it may easily be incorporated into existing relational systems.A precise analysis of partial match retrieval of multidimensional data is presented. The structures considered here are multidimensional search trees (k-d-trees) and digital tries (k-d-tries), as well as structures designed for efficient retrieval of information stored on external devices. The methods used include a detailed study of a differential system around a regular singular point in conjunction with suitable contour integration techniques for the analysis of k-d-trees, and properties of the Mellin integral transform for k-d-tries and extendible cell algorithms.
Two deduction rules are introduced to give streamlined treatment to relations of special importance in an automated theorem-proving system. These rules, the relation replacement and relation matching rules, generalize to an arbitrary binary relation the paramodulation and E-resolution rules, respectively, for equality, and may operate within a nonclausal or clausal system. The new rules depend on an extension of the notion of polarity to apply to subterms as well as to subsentences, with respect to a given binary relation. The rules allow us to eliminate troublesome axioms, such as transitivity and monotonicity, from the system; proofs are shorter and more comprehensible, and the search space is correspondingly deflated.In this paper an O(N log N) algorithm for routing through a rectangle is presented. Consider an n-by-m rectangular grid and a set of N two-terminal nets. A net is a pair of points on the boundary of the rectangle. A layout is a set of edge-disjoint paths, one for each net. Our algorithm constructs a layout, if there is one, in O(N log N) time; this contrasts favorably with the area of the layout that might be as large as N2. The layout constructed can be wired using four layers of interconnect with only O(N) contact cuts. A partial extension to multiterminal nets is also discussed.A high-resolution raster-graphics display is usually combined with processing power and a memory organization that facilitates basic graphics operations. For many applications, including interactive text processing, the ability to quickly move or copy small rectangles of pixels is essential. This paper proposes a novel organization of raster-graphics memory that permits all small rectangles to be moved efficiently. The memory organization is based on a doubly periodic assignment of pixels to M memory chips according to a “Fibonacci” lattice. The memory organization guarantees that, if a rectilinearly oriented rectangle contains fewer than M/ @@@@5 pixels, then all pixels will reside in different memory chips and thus can be accessed simultaneously. Moreover, any M consecutive pixels, arranged either horizontally or vertically, can be accessed simultaneously.We also define a continuous analog of the problem, which can be posed as: “What is the maximum density of a set of points in the plane such that no two points are contained in the interior of a rectilinearly oriented rectangle of unit area?” We show the existence of such a set with density 1/ @@@@5, and prove this is optimal by giving a matching upper bound.Two notions of dependency satisfaction, consistency and completeness, are introduced. Consistency is the natural generalization of weak-instance satisfaction and seems appropriate when only equality-generating dependencies are given, but disagrees with the standard notion in the presence of tuple-generating dependencies. Completeness is based on the intuitive semantics of tuple-generating dependencies but differs from the standard notion for equality-generating dependencies. It is argued that neither approach is the correct one, but rather that they correspond to different policies on constraint enforcement, and each one is appropriate in different circumstances. Consistency and completeness of a state are characterized in terms of the tableau associated with the state and in terms of logical properties of a set of first-order sentences associated with the state. A close relation between the problems of testing for consistency and completeness and of testing implication of dependencies is established, leading to lower and upper bounds for the complexity of consistency and completeness. The possibility of formalizing dependency satisfaction without using a universal relation scheme is examined.Given a nonnegative, irreducible matrix P of spectral radius unity, there exists a positive vector &pgr; such that &pgr; = &pgr;P. If P also happens to be stochastic, then &pgr; gives the stationary distribution of the Markov chain that has state-transition probabilities given by the elements of P. This paper gives an algorithm for computing &pgr; that is particularly well suited for parallel processing. The main attraction of our algorithm is that the timing and sequencing restrictions on individual processors are almost entirely eliminated and, consequently, the necessary coordination between processors is negligible and the enforced idle time is also negligible.Under certain mild and easily satisfied restrictions on P and on the implementation of the algorithm, x(.), the vectors of computed values are proved to converge to within a positive, finite constant of proportionality of &pgr;. It is also proved that a natural measure of the projective distance of x(.) from &pgr; vanishes geometrically fast, and at a rate for which a lower bound is given. We have conducted extensive experiments on random matrices P, and the results show that the improvement over the parallel implementation of the synchronous version of the algorithm is substantial, sometimes exceeding the synchronization penalty to which the latter is always subject.The differences between and appropriateness of branching versus linear time temporal logic for reasoning about concurrent programs are studied. These issues have been previously considered by Lamport. To facilitate a careful examination of these issues, a language, CTL*, in which a universal or existential path quantifier can prefix an arbitrary linear time assertion, is defined. The expressive power of a number of sublanguages is then compared. CTL* is also related to the logics MPL of Abrahamson and PL of Harel, Kozen, and Parikh. The paper concludes with a comparison of the utility of branching and linear time temporal logics.An algorithm for computing bounds on the performance measures of multiple-class, product-form queuing networks is presented. The algorithm offers the user a hierarchy of bounds with differing accuracy levels and computational cost requirements. Unlike previously proposed bounding algorithms, the algorithm is applicable to all of the types of product-form queuing networks that are commonly used in computer system and computer-communication network applications.An explicit steady-state solution is given for any queuing loop made up of two general servers, whose distribution functions have rational Laplace transforms. The solution is in matrix geometric form over a vector space that is itself a direct or Kronecker product of the internal state spaces of the two servers. The algebraic properties of relevant entities in this space are given in an appendix. The closed-form solution yields simple recursive relations that in turn lead to an efficient algorithm for calculating various performance measures such as queue length and throughput. A computational-complexity analysis shows that the algorithm requires at least an order of magnitude less computational effort than any previously reported algorithm.Elementary translations between various kinds of recursive trees are presented. It is shown that trees of either finite or countably infinite branching can be effectively put into one-one correspondence with infinitely branching trees in such a way that the infinite paths of the latter correspond to the “P-abiding” infinite paths of the former. Here P can be any member of a very wide class of properties of infinite paths. For many properties ??, the converse holds too. Two of the applications involve (a) the formulation of large classes of highly undecidable variants of classical computational problems, and in particular, easily describable domino problems that are III11-complete, and (b) the existence of a general method for proving termination of nondeterministic or concurrent programs under any reasonable notion of fairness.
Backtrack search is often used to solve constraint satisfaction problems. A relationship involving the structure of the constraints is described that provides a bound on the backtracking required to advance deeper into the backtrack tree. This analysis leads to upper bounds on the effort required for solution of a class of constraint satisfaction problems. The solutions involve a combination of relaxation preprocessing and backtrack search. The bounds are expressed in terms of the structure of the constraint connections. Specifically, the effort is shown to have a bound exponential in the size of the largest biconnected component of the constraint graph, as opposed to the size of the graph as a whole.A parallel algorithm is presented that accepts as input a graph G and produces a maximal independent set of vertices in G. On a P-RAM without the concurrent write or concurrent read features, the algorithm executes in O((log n)4) time and uses O((n/(log n))3) processors, where n is the number of vertices in G. The algorithm has several novel features that may find other applications. These include the use of balanced incomplete block designs to replace random sampling by deterministic sampling, and the use of a “dynamic pigeonhole principle” that generalizes the conventional pigeonhole principle.A hypergraph formalism is introduced to represent database schemata. In particular, a database schema B, described by one full join dependency and a set of functional dependencies, is represented by a (database) hypergraph H, containing both undirected and directed hyperedges. Undirected hyperedges correspond to the relations in the join dependency, and directed hyperedges correspond to the functional dependencies. In addition, two classes of database hypergraphs are defined: e-acyclic hypergraphs and e-independent hypergraphs. A hypergraph is e-acyclic if it is equivalent to some acyclic hypergraph; it is e-independent if it is equivalent to some independent (i.e., cover-embedding) hypergraph. Furthermore, the closure of a database hypergraph is defined as the extension of the transitive closure of a graph. By using a lower bound and an upper bound of the hypergraph closure (called L-closure and U-closure, respectively), it is proved that two e-acyclic (e-independent) hypergraphs are equivalent if and only if they have the same closure. Moreover, a hypergraph is e-acyclic (e-independent) if and only if its closure is acyclic (independent) and, in most cases, such a recognition can be done in polynomial time. Finally, it is shown how to use the database hypergraph closure to solve some database design problems.The problem of simulating a synchronous network by an asynchronous network is investigated. A new simulation technique, referred to as a synchronizer, which is a new, simple methodology for designing efficient distributed algorithms in asynchronous networks, is proposed. The synchronizer exhibits a trade-off between its communication and time complexities, which is proved to be within a constant factor of the lower bound.A consensus protocol enables a system of n asynchronous processes, some of which are faulty, to reach agreement. There are two kinds of faulty processes: fail-stop processes that can only die and malicious processes that can also send false messages. The class of asynchronous systems with fair schedulers is defined, and consensus protocols that terminate with probability 1 for these systems are investigated. With fail-stop processes, it is shown that ⌈(n + 1)/2⌉ correct processes are necessary and sufficient to reach agreement. In the malicious case, it is shown that ⌈(2n + 1)/3⌉ correct processes are necessary and sufficient to reach agreement. This is contrasted with an earlier result, stating that there is no consensus protocol for the fail-stop case that always terminates within a bounded number of steps, even if only one process can fail. The possibility of reliable broadcast (Byzantine Agreement) in asynchronous systems is also investigated. Asynchronous Byzantine Agreement is defined, and it is shown that ⌈(2n + 1)/3⌉ correct processes are necessary and sufficient to achieve it.In a distributed system, one strategy for achieving mutual exclusion of groups of nodes without communication is to assign to each node a number of votes. Only a group with a majority of votes can execute the critical operations, and mutual exclusion is achieved because at any given time there is at most one such group. A second strategy, which appears to be similar to votes, is to define a priori a set of groups that intersect each other. Any group of nodes that finds itself in this set can perform the restricted operations. In this paper, both of these strategies are studied in detail and it is shown that they are not equivalent in general (although they are in some cases). In doing so, a number of other interesting properties are proved. These properties will be of use to a system designer who is selecting a vote assignment or a set of groups for a specific application.The optimization problem for linear functions on finite languages is studied, and an (almost) complete characterization of those functions for which a primal and a dual greedy algorithm work well with respect to a canonically associated linear programming problem is given. The discussion in this paper is within the framework of ordered languages, and the characterization uses the notion of rank feasibility of a weighting with respect to an ordered language. This yields a common generalization of a sufficient condition, obtained recently by Korte and Lovász for greedoids, and the greedy algorithm for ordered sets in Faigel's paper [6]. Ordered greedoids are considered the appropriate generalization of greedoids, and the connection is established between ordered languages, polygreedoids, and Coxeteroids. Answering a question of Björner, the author shows in particular that a polygreedoid is a Coxeteroid if and only if it is derived from an integral polymatroid.It has been a challenge for mathematicians to confirm theoretically the extremely good performance of simplex-type algorithms for linear programming. In this paper the average number of steps performed by a simplex algorithm, the so-called self-dual method, is analyzed. The algorithm is not started at the traditional point (1, … , l)T, but points of the form (1, &egr;, &egr;2, …)T, with &egr; sufficiently small, are used. The result is better, in two respects, than those of the previous analyses. First, it is shown that the expected number of steps is bounded between two quadratic functions c1(min(m, n))2 and c2(min(m, n))2 of the smaller dimension of the problem. This should be compared with the previous two major results in the field. Borgwardt proves an upper bound of O(n4m1/(n-1)) under a model that implies that the zero vector satisfies all the constraints, and also the algorithm under his consideration solves only problems from that particular subclass. Smale analyzes the self-dual algorithm starting at (1, … , 1)T. He shows that for any fixed m there is a constant c(m) such the expected number of steps is less than c(m)(ln n)m(m+1); Megiddo has shown that, under Smale's model, an upper bound C(m) exists. Thus, for the first time, a polynomial upper bound with no restrictions (except for nondegeneracy) on the problem is proved, and, for the first time, a nontrivial lower bound of precisely the same order of magnitude is established. Both Borgwardt and Smale require the input vectors to be drawn from spherically symmetric distributions. In the model in this paper, invariance is required only under certainA simple model, AT, for nondeterministic machines is presented which is based on certain types of trees. A set of operations, &Sgr;, is defined over AT and it is shown to be completely characterized by a set of inequations over &Sgr;. AT is used to define the denotational semantics of a language for defining nondeterministic machines. The significance of the model is demonstrated by showing that this semantics reflects an intuitive operational semantics of machines based on the idea that machines should only be differentiated if there is some experiment that differentiates between them.The problem of recognizing the language Ln(Ln, k) of solvable Diophantine linear equations with n variables (and solutions from {O, … , k}n) is considered. The languages ∪n&egr;N Ln, ∪n&egr;N Ln, l, the knapsack problem, are NP-complete. The &OHgr;(n2 lower bound for Ln,1 on linear search algorithms due to Dobkin and Lipton is generalized to an &OHgr;(n2log(k + 1)) lower bound for Ln, k. The method of Klein and Meyer auf der Heide is further improved to carry over the &OHgr;(n2) lower bound for Ln, 1 to random access machines (RAMS) in such a way that it holds for a large class of problems and for very small input sets. By this method, lower bounds that depend on the input size, as is necessary for Ln, are proved. Thereby, an &OHgr;(n2log(k + 1)) lower bound is obtained for RAMS recognizing Ln or Ln, k, for inputs from {0, … , (nk)0(n2)}n.Combinatorial techniques for extending lower bound results for decision trees to general types of queries are presented. Problems that are defined by simple inequalities between inputs, called order invariant problems, are considered. A decision tree is called k-bounded if each query depends on at most k variables. No further assumptions on the type of queries are made. It is proved that one can replace the queries of any k-bounded decision tree that solves an order-invariant problem over a large enough input domain with k-bounded queries whose outcome depends only on the relative order of the inputs. As a consequence, all existing lower bounds for comparison-based algorithms are valid for general k-bounded decision trees, where k is a constant.An &OHgr;(n log n) lower bound for the element uniqueness problem and several other problems for any k-bounded decision tree, such that k = O(nc) and c < 1/2 is proved. This lower bound is tight since there exist n1/2-bounded decision trees of complexity O(n) that solve the element-uniqueness problem. All the lower bounds mentioned above are shown to hold for nondeterministic and probabilistic decision trees as well.An algorithm is presented that finds a min-cut linear arrangement of a tree in O(n log n) time. An extension of the algorithm determines the number of pebbles needed to play the black and white pebble game on a tree.
This paper reports several properties of heuristic best-first search strategies whose scoring functions ƒ depend on all the information available from each candidate path, not merely on the current cost g and the estimated completion cost h. It is shown that several known properties of A* retain their form (with the minmax of f playing the role of the optimal cost), which helps establish general tests of admissibility and general conditions for node expansion for these strategies. On the basis of this framework the computational optimality of A*, in the sense of never expanding a node that can be skipped by some other algorithm having access to the same heuristic information that A* uses, is examined. A hierarchy of four optimality types is defined and three classes of algorithms and four domains of problem instances are considered. Computational performances relative to these algorithms and domains are appraised. For each class-domain combination, we then identify the strongest type of optimality that exists and the algorithm for achieving it. The main results of this paper relate to the class of algorithms that, like A*, return optimal solutions (i.e., admissible) when all cost estimates are optimistic (i.e., h ≤ h*). On this class, A* is shown to be not optimal and it is also shown that no optimal algorithm exists, but if the performance tests are confirmed to cases in which the estimates are also consistent, then A* is indeed optimal. Additionally, A* is also shown to be optimal over a subset of the latter class containing all best-first algorithms that are guided by path-dependent evaluation functions.An analysis of structured flowcharts is presented, where size is measured by the number, n, of decision nodes (IF-THEN-ELSE and DO-WHILE nodes). For all classes of structured flowcharts considered, the number of charts is approximately, cn-3/2&ggr;n, for large n, where c>and &ggr; are parameters that depend on the class. It is also shown that most large flowcharts consist of a short sequence of basic charts (IF-THEN-ELSE and DO-WHILE charts). The average length of such sequences is 2.5.In a nonnegative edge-weighted network, the weight of an edge represents the effort required by an attacker to destroy the edge, and the attacker derives a benefit for each new component created by destroying edges. The attacker may want to minimize over subsets of edges the difference between (or the ratio of) the effort incurred and the benefit received. This idea leads to the definition of the “strength” of the network, a measure of the resistance of the network to such attacks. Efficient algorithms for the optimal attack problem, the problem of computing the strength, and the problem of finding a minimum cost “reinforcement” to achieve a desired strength are given. These problems are also solved for a different model, in which the attacker wants to separate vertices from a fixed central vertex.The one-dimensional on-line bin-packing problem is considered, A simple O(1)-space and O(n)-time algorithm, called HARMONICM, is presented. It is shown that this algorithm can achieve a worst-case performance ratio of less than 1.692, which is better than that of the O(n)-space and O(n log n)-time algorithm FIRST FIT. Also shown is that 1.691 … is a lower bound for all 0(1)-space on-line bin-packing algorithms. Finally a revised version of HARMONICM , an O(n)-space and O(n)- time algorithm, is presented and is shown to have a worst-case performance ratio of less than 1.636.A new model of computation for VLSI, based on the assumption that time for propagating information is at least linear in the distance, is proposed. While accommodating for basic laws of physics, the model is designed to be general and technology independent. Thus, from a complexity viewpoint, it is especially suited for deriving lower bounds and trade-offs. New results for a number of problems, including fan-in, transitive functions, matrix multiplication, and sorting are presented. As regards upper bounds, it must be noted that, because of communication costs, the model clearly favors regular and pipelined architectures (e.g., systolic arrays).A problem related to the decentralized control of a multiple access channel is considered: Suppose k stations from an ensemble of n simultaneously transmit to a multiple access channel that provides the feedback 0, 1, or 2+, denoting k = 0, k = 1, or k ≥ 2, respectively. If k = 1, then the transmission succeeds. But if k ≥ 2, as a result of the conflict, none of the transmissions succeed. An algorithm to resolve a conflict determines how to schedule retransmissions so that each of the conflicting stations eventually transmits singly to the channel. In this paper, a general model of deterministic algorithms to resolve conflicts is introduced, and it is established that, for all k and n (2 ≤ k ≤ n), &OHgr;(k(log n)/(log k)) time must elapse in the worst case before all k transmissions succeed.A database is said to allow range restrictions if one may request that only records with some specified field in a specified range be considered when answering a given query. A transformation is presented that enables range restrictions to be added to an arbitrary dynamic data structure on n elements, provided that the problem satisfies a certain decomposability condition and that one is willing to allow increases by a factor of O(log n) in the worst-case time for an operation and in the space used. This is a generalization of a known transformation that works for static structures. This transformation is then used to produce a data structure for range queries in k dimensions with worst-case times of O(logk n) for each insertion, deletion, or query operation.A new performance model for dynamic locking is proposed. It is based on a flow diagram and uses only the steady state average values of the variables. It is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. The analysis is restricted to the case in which all conflicts are resolved by restarts. It has been shown elsewhere that, under certain conditions, this pure restart policy is as good as, if not better than, a policy that uses both blocking and restarts.The analysis is straightforward, and the computational complexity of the solution, given some nonrestrictive approximations, does not depend on the input parameters. The solution is also well defined and well behaved. The model's predictions agree well with simulation results.The model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. It also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. Static locking has higher throughput, but longer response time, than dynamic locking. Replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.The splay tree, a self-adjusting form of binary search tree, is developed and analyzed. The binary search tree is a data structure for representing tables and lists so that accessing, inserting, and deleting items is easy. On an n-node splay tree, all the standard search tree operations have an amortized time bound of O(log n) per operation, where by “amortized time” is meant the time per operation averaged over a worst-case sequence of operations. Thus splay trees are as efficient as balanced trees when total running time is the measure of interest. In addition, for sufficiently long access sequences, splay trees are as efficient, to within a constant factor, as static optimum search trees. The efficiency of splay trees comes not from an explicit structural constraint, as with balanced trees, but from applying a simple restructuring heuristic, called splaying, whenever the tree is accessed. Extensions of splaying give simplified forms of two other data structures: lexicographic or multidimensional search trees and link/cut trees.It was conjectured by J. Ullman that uniform hashing is optimal in its expected retrieval cost among all open-address hashing schemes [4]. In this paper, we show that, for any open-address hashing scheme, the expected cost of retrieving a record from a large table that is &agr;-fraction full is at least (1/&agr;) log (1/(1 - &agr;)) + o(1). This proves Ullman's conjecture to be true in the asymptotic sense.A new algorithm that, for the first time, exploits the rotational geometry of binary trees is developed in order to allow for the lexicographic generation of computer representations of these trees in average time O(1) per tree. “Rotation” codewords for these trees (in average time O(1) per tree) are also generated. It is shown how these codewords relate to lattice paths, and, using this relationship, that n(n - 1)/(n + 2) is the average number of rotations needed to generate a binary tree on n nodes. Finally, a necessary and sufficient condition that a codeword represent a full binary tree (each node has 0 or 2 sons) on n = 2m + 1 nodes is given and how to contract this codeword to obtain the codeword for the binary tree on m nodes for which this full tree is the extended binary tree is shown.Iterative aggregation/disaggregation methods provide an efficient approach for computing the stationary probability vector of nearly uncoupled (also known as nearly completely decomposable) Markov chains. Three such methods that have appeared in the literature recently are considered and their similarities and differences are outlined. Specifically, it is shown that the method of Takahashi corresponds to a modified block Gauss-Seidel step and aggregation, whereas that of Vantilborgh corresponds to a modified block Jacobi step and aggregation. The third method, that of Koury et al., is equivalent to a standard block Gauss-Seidel step and iteration. For each of these methods, a lemma is established, which shows that the unique fixed point of the iterative scheme is the left eigenvector corresponding to the dominant unit eigenvalue of the stochastic transition probability matrix. In addition, conditions are established for the convergence of the first two of these methods; convergence conditions for the third having already been established by Stewart et al. All three methods are shown to have the same asymptotic rate of convergence.This work generalizes decision trees in order to study lower bounds on the running times of algorithms that allow probabilistic, nondeterministic, or alternating control. It is shown that decision trees that are allowed internal randomization (at the expense of introducing a small probability of error) run no faster asymptotically than ordinary decision trees for a collection of natural problems. Two geometric techniques from the literature for proving lower bounds on the time required by ordinary decision trees are shown to be special cases of one unified technique that, in fact, applies to nondeterministic decision trees as well. Finally, it is shown that any lower bound on alternating decision tree time also applies to alternating Turing machine time.The complexity of satisfiability and determination of truth in a particular finite structure are considered for different propositional linear temporal logics. It is shown that these problems are NP-complete for the logic with F and are PSPACE-complete for the logics with F, X, with U, with U, S, X operators and for the extended logic with regular operators given by Wolper.
The problem of synthesizing a procedure from example computations is examined. An algorithm for this task is presented, and its success is considered. To do this, a model of procedures and example computations is introduced, and the class of acceptable examples is defined. The synthesis algorithm is shown to be successful, with respect to the model of procedures and examples, from two perspectives. First, it is shown to be sound, that is, that the procedure synthesized from a set of examples produces the same result as the intended one on the inputs used to generate that set of examples. Second, it is shown to be complete, that is, that for any procedure in the class of procedures, there exists a finite set of examples such that the procedure synthesized behaves as the intended one on all inputs for which the intended one halts.The costs of subsumption algorithms are analyzed by an estimation of the maximal number of unification attempts (worst-case unification complexity) made for deciding whether a clause C subsumes a clause D. For this purpose the clauses C and D are characterized by the following parameters: number of variables in C, number of literals in C, number of literals in D, and maximal length of the literals. The worst-case unification complexity immediately yields a lower bound for the worst-case time complexity.First, two well-known algorithms (Chang-Lee, Stillman) are investigated. Both algorithms are shown to have a very high worst-case time complexity. Then, a new subsumption algorithm is defined, which is based on an analysis of the connection between variables and predicates in C. An upper bound for the worst-case unification complexity of this algorithm, which is much lower than the lower bounds for the two other algorithms, is derived. Examples in which exponential costs are reduced to polynomial costs are discussed. Finally, the asymptotic growth of the worst-case complexity for all discussed algorithms is shown in a table (for several combinations of the parameters).The problem of finding a minimum cardinality feedback vertex set of a directed graph is considered. Of the classic NP-complete problems, this is one of the least understood. Although Karp showed the general problem to be NP-complete, a linear algorithm for its solution on reducible flow graphs was given by Shamir. The class of reducible flow graphs is the only nontrivial class of graphs for which a polynomial-time algorithm to solve this problem is known. The main result of this paper is to present a new class of graphs—the cyclically reducible graphs—for which minimum feedback vertex sets can be found in polynomial time. This class is not restricted to flow graphs, and most small graphs (10 or fewer nodes) fall into this class. The identification of this class is particularly important since there do not exist approximation algorithms for this problem having a provably good worst case performance. Along with the class and a simple polynomial-time algorithm for finding minimum feedback vertex sets of graphs in the class, several related results are presented. It is shown that there is no “forbidden subgraph” characterization of the class and that there is no particular inclusion relationship between this class and the reducible flow graphs. In addition, it is shown that a class of (general) graphs, which are related to the reducible flow graphs, are contained in the cyclically reducible class.Many database systems maintain the consistency of the data by using a locking protocol to restrict access to data items. It has been previously shown that if no information is known about the method of accessing items in the database, then the two-phase protocol is optimal. However, the use of structural information about the database allows development of non-two-phase protocols, called graph protocols, that can potentially increase efficiency. Yannakakis developed a general class of protocols that included many of the graph protocols. Graph protocols either are only usable in certain types of databases or can incur the performance liability of cascading rollback. In this paper, it is demonstrated that if the system has a priori information as to which data items will be locked first by various transactions, a new graph protocol that is outside the previous classes of graph protocols and is applicable to arbitrarily structured databases can be constructed. This new protocol avoids cascading rollback and its accompanying performance degradation, and extends the class of serializable sequences allowed by non-two-phase protocols. This is the first protocol shown to be always as effective as the two-phase protocol, and it can be more effective for certain types of database systems.Parallel algorithms for data compression by textual substitution that are suitable for VLSI implementation are studied. Both “static” and “dynamic” dictionary schemes are considered.The consensus problem involves an asynchronous system of processes, some of which may be unreliable. The problem is for the reliable processes to agree on a binary value. In this paper, it is shown that every protocol for this problem has the possibility of nontermination, even with only one faulty process. By way of contrast, solutions are known for the synchronous case, the “Byzantine Generals” problem.This paper analyzes decomposition properties of a graph that, when they occur, permit a polynomial solution of the traveling salesman problem and a description of the traveling salesman polytope by a system of linear equalities and inequalities. The central notion is that of a 3-edge cutset, namely, a set of 3 edges that, when removed, disconnects the graph. Conversely, our approach can be used to construct classes of graphs for which there exists a polynomial algorithm for the traveling salesman problem. The approach is illustrated on two examples, Halin graphs and prismatic graphs.Criteria for adequacy of a data flow semantics are discussed and Kahn's successful semantics for functional (deterministic) data flow is reviewed. Problems arising from nondeterminism are introduced and the paper's approach to overcoming them is introduced. The approach is based on generalizing the notion of input-output relation, essentially to a partially ordered multiset of input-output histories. The Brock-Ackerman anomalies concerning the input-output relation model of nondeterministic data flow are reviewed, and it is indicated how the proposed approach avoids them. A new anomaly is introduced to motivate the use of multisets. A formal theory of asynchronous processes is then developed. The main result is that the operation of forming a process from a network of component processes is associative. This result shows that the approach is not subject to anomalies such as that of Brock and Ackerman.A distributed computer system that consists of a set of heterogeneous host computers connected in an arbitrary fashion by a communications network is considered. A general model is developed for such a distributed computer system, in which the host computers and the communications network are represented by product-form queuing networks. In this model, a job may be either processed at the host to which it arrives or transferred to another host. In the latter case, a transferred job incurs a communication delay in addition to the queuing delay at the host on which the job is processed. It is assumed that the decision of transferring a job does not depend on the system state, and hence is static in nature. Performance is optimized by determining the load on each host that minimizes the mean job response time. A nonlinear optimization problem is formulated, and the properties of the optimal solution in the special case where the communication delay does not depend on the source-destination pair is shown.Two efficient algorithms that determine the optimal load on each host computer are presented. The first algorithm, called the parametric-study algorithm, generates the optimal solution as a function of the communication time. This algorithm is suited for the study of the effect of the speed of the communications network on the optimal solution. The second algorithm is a single-point algorithm; it yields the optimal solution for given system parameters. Queuing models of host computers, communications networks, and a numerical example are illustrated.Two of the most powerful classes of programs for which interesting decision problems are known to be solvable are the class of finite-memory programs and the class of programs that characterize the Presburger, or semilinear, sets. In this paper, a new class of programs that presents solvable decision problems similar to the other two classes of programs is introduced. However, the programs in the new class are shown to be computationally more powerful (i.e., capable of defining larger sets of input-output relations).A projection of a Boolean function is a function obtained by substituting for each of its variables a variable, the negation of a variable, or a constant. Reducibilities among computational problems under this relation of projection are considered. It is shown that much of what is of everyday relevance in Turing-machine-based complexity theory can be replicated easily and naturally in this elementary framework. Finer distinctions about the computational relationships among natural problems can be made than in previous formulations and some negative results are proved.
Three different approaches to heuristic search in networks are analyzed. In the first approach, as formulated initially by Hart, Nilsson, and Raphael, and later modified by Martelli, the basic idea is to choose for expansion that node for which the evaluation function has a minimum value. A second approach has recently been suggested by Nilsson. In this method, in contrast to the earlier one, a node that is expanded once is not expanded again; instead, a “propagation” of values takes place. The third approach is an adaptation for networks of an AND/OR graph “marking” algorithm, originally due to Martelli and Montanari.Five algorithms are presented. Algorithms A and C illustrate the first approach; PropA and PropC, the second one; and MarkA, the third one. The performances of these algorithms are compared for both admissible and inadmissible heuristics using the following two criteria: (i) cost of the solution found; (ii) time of execution in the worst case, as measured by the number of node expansions (A, C), or node “selections” (PropA, PropC), or arc “markings” (MarkA).The relative merits and demerits of the algorithms are summarized and indications are given regarding which algorithm to use in different situations.Two new marking algorithms for AND/OR graphs called CF and CS are presented. For admissible heuristics CS is not needed, and CF is shown to be preferable to the marking algorithms of Martelli and Montanari. When the heuristic is not admissible, the analysis is carried out with the help of the notion of the first and second discriminants of an AND/OR graph. It is proved that in this case CF can be followed by CS to get optimal solutions, provided the sumcost criterion is used and the first discriminant equals the second. Estimates of time and storage requirements are given. Other cost measures, such as maxcost, are also considered, and a number of interesting open problems are enumerated.Algorithms are described for maintaining clock synchrony in a distributed multiprocess system where each process has its own clock. These algorithms work in the presence of arbitrary clock or process failures, including “two-faced clocks” that present different values to different processes. Two of the algorithms require that fewer than one-third of the processes be faulty. A third algorithm works if fewer than half the processes are faulty, but requires digital signatures.An algebraic foundation of database schema design is presented. A new database operator, namely, disaggregation, is introduced. Beginning with “free” families, repeated applications of disaggregation and three other operators (matching function, Cartesian product, and selection) yield families of increasingly elaborate structure. In particular, families defined by one join dependency and several “embedded” functional dependencies can be obtained in this manner.One aspect of network design is the extent to which memory is shared among the processing elements. In this paper a model with limited sharing (only two processors connected to each memory) is analyzed and its performance compared with the performance of two other models that have appeared in the literature. One of these is a model of multiple processors sharing a single memory; the other model considers a multiprocessor configuration in which each processor has its own dedicated memory. The tasks processed by these networks are described by both time and memory requirements. The largest-memory-first (LMF) scheduling algorithm is employed and its performance with respect to an enumerative optimal scheduling algorithm is bounded. On the basis of this measure we conclude that memory sharing is only desirable on very small networks and is disadvantageous on networks of larger size.A unified and powerful approach is presented for devising polynomial approximation schemes for many strongly NP-complete problems. Such schemes consist of families of approximation algorithms for each desired performance bound on the relative error &egr; > &Ogr;, with running time that is polynomial when &egr; is fixed. Though the polynomiality of these algorithms depends on the degree of approximation &egr; being fixed, they cannot be improved, owing to a negative result stating that there are no fully polynomial approximation schemes for strongly NP-complete problems unless NP = P.The unified technique that is introduced here, referred to as the shifting strategy, is applicable to numerous geometric covering and packing problems. The method of using the technique and how it varies with problem parameters are illustrated. A similar technique, independently devised by B. S. Baker, was shown to be applicable for covering and packing problems on planar graphs.Since a nondeterministic and concurrent program may, in general, communicate repeatedly with its environment, its meaning cannot be presented naturally as an input/output function (as is often done in the denotational approach to semantics). In this paper, an alternative is put forth. First, a definition is given of what it is for two programs or program parts to be equivalent for all observers; then two program parts are said to be observation congruent if they are, in all program contexts, equivalent. The behavior of a program part, that is, its meaning, is defined to be its observation congruence class.The paper demonstrates, for a sequence of simple languages expressing finite (terminating) behaviors, that in each case observation congruence can be axiomatized algebraically. Moreover, with the addition of recursion and another simple extension, the algebraic language described here becomes a calculus for writing and specifying concurrent programs and for proving their properties.An aggregative technique to obtain an improved approximation of the equilibrium vector of a Markov chain with a nearly completely decomposable transition matrix is presented. The technique is demonstrated on a model of a multiprogrammed computer system.Byzantine Agreement has become increasingly important in establishing distributed properties when errors may exist in the systems. Recent polynomial algorithms for reaching Byzantine Agreement provide us with feasible solutions for obtaining coordination and synchronization in distributed systems. In this paper the amount of information exchange necessary to ensure Byzantine Agreement is studied. This is measured by the total number of messages the participating processors have to send in the worst case. In algorithms that use a signature scheme, the number of signatures appended to messages are also counted.First it is shown that &OHgr;(nt) is a lower bound for the number of signatures for any algorithm using authentication, where n denotes the number of processors and t the upper bound on the number of faults the algorithm is supposed to handle. For algorithms that reach Byzantine Agreement without using authentication this is even a lower bound for the total number of messages. If n is large compared to t, these bounds match the upper bounds from previously known algorithms. For the number of messages in the authenticated case we prove the lower bound &OHgr;(n + t2). Finally algorithms that achieve this bound are presented.Nancy Lynch proved that if a decision problem A is not solvable in polynomial time, then there exists an infinite recursive subset X of its domain on which the decision is almost everywhere complex. In this paper, general theorems of this kind that can be applied to several well-known automata-based complexity classes, including a common class of randomized algorithms, are proved.Lengauer and Tarjan proved that the number of black and white pebbles needed to pebble the root of a tree is at least half the number of black pebbles needed to pebble the root. This result is extended to a larger class of acyclic directed graphs including pyramid graphs.The subset sum problem is to decide whether or not the 0-l integer programming problem &Sgr;ni=l aixi = M, ∀I, xI = 0 or 1, has a solution, where the ai and M are given positive integers. This problem is NP-complete, and the difficulty of solving it is the basis of public-key cryptosystems of knapsack type. An algorithm is proposed that searches for a solution when given an instance of the subset sum problem. This algorithm always halts in polynomial time but does not always find a solution when one exists. It converts the problem to one of finding a particular short vector v in a lattice, and then uses a lattice basis reduction algorithm due to A. K. Lenstra, H. W. Lenstra, Jr., and L. Lovasz to attempt to find v. The performance of the proposed algorithm is analyzed. Let the density d of a subset sum problem be defined by d = n/log2(maxi ai). Then for “almost all” problems of density d < 0.645, the vector v we searched for is the shortest nonzero vector in the lattice. For “almost all” problems of density d < 1/n, it is proved that the lattice basis reduction algorithm locates v. Extensive computational tests of the algorithm suggest that it works for densities d < dc(n), where dc(n) is a cutoff value that is substantially larger than 1/n. This method gives a polynomial time attack on knapsack public-key cryptosystems that can be expected to break them if they transmit information at rates below dc(n), as n → ∞.

A mathematical theory for the study of data representation in
databases is introduced and developed. The theory focuses on three
data constructs (collection, composition and classification).
"Formats" with semantically rich yet tractable structure are built
recursively using these constructs. Using formats, we obtain
several nontrivial results concerning notions of relative
information capacity and restructuring of data sets. As such, the
format model provides a new approach for the formal study of the
construction of "user views" and other data manipulations in
databases.The information-based study of the optimal solution of large
linear systems is initiated by studying the case of Krylov
information. Among the algorithms that use Krylov information are
minimal residual, conjugate gradient, Chebyshev, and successive
approximation algorithms. A "sharp" lower bound on the number of
matrix-vector multiplications required to compute an
å-approximation is obtained for any orthogonally invariant
class of matrices. Examples of such classes include many of
practical interest such as symmetric matrices, symmetric positive
definite matrices, and matrices with bounded condition number. It
is shown that the minimal residual algorithm is within at most one
matrix-vector multiplication of the lower bound. A similar result
is obtained for the generalized minimal residual algorithm. The
lower bound is computed for certain classes of orthogonally
invariant matrices. How the lack of certam properties (symmetry,
positive definiteness) increases the lower bound is shown. A
conjecture and a number of open problems are stated.A mathematical model for communicating sequential processes is
given, and a number of its interesting and useful properties are
stated and proved. The possibilities of nondetermimsm are fully
taken into account.An intuitive presentation of the trace method for the abstract
specification of software contains sample specifications, syntactic
and semantic definitions of consistency and totalness, methods for
proving specifications consistent and total, and a comparison of
the method with the algebraic approach to specification. This
intuitive presentation is underpinned by a formal syntax,
semantics, and derivation system for the method. Completeness and
soundness theorems establish the correctness of the derivation
system with respect to the semantics, the coextensiveness of the
syntactic definitions of consistency and totalness with their
semantic counterparts, and the correctness of the proof methods
presented. Areas for future research are discussed.A popular means of increasing the effective rate of main storage
accesses in a large computer is a multiplicity of memory modules
accessible in parallel. Although such an organization usually
achieves a net gain in access rate, it also creates new modes of
congestion at the storage controller. This paper analyzes the
variables that describe such a congestion: queue lengths and
delays. A controller that maintains separate register sets to
accommodate the request queue of each module is considered. The
various processors attached to the storage are assumed to generate,
in each memory cycle, a number of access requests with the same
given distribution. The addresses specified by these requests
(reduced to the module index) are further assumed to follow the
states of a first-order Markov chain. The analysis then becomes one
of a single-server queuing system with constant service time and
indexed batch arrival process. Results are derived for several
descriptors of the congestion and thus of the quality of service
offered by such an organization. The aim throughout is to embody
the results in a form readily suitable for numerical
evaluation.
















In the bin-packing problem a list L of n numbers are to be packed into unit-capacity bins. For any algorithm S, let r(S) be the maximum ratio S(L)/L* for large L*, where S(L) denotes the number of bins used by S and L* denotes the minimum number needed. An on-line &Ogr;(n log n)-time algorithm RFF with r(RFF) = 5/3 and an off-line polynomial-time algorithm RFFD with r(RFFD) ≤ 11/9 - &egr; for some fixed &egr; > 0, are given. These are strictly better, respectively, than two prominent algorithms: the First-Fit (FF), which is on-line with r(FF) = 17/10, and the First-Fit-Decreasing (FFD) with r(FFD) = 11/9. Furthermore, it is shown that any on-line algorithm S must have r(S) ≥ 3/2. The question, “How well can an &ogr;(n log n)-time algorithm perform?” is also discussed. It is shown that in the generalized d-dimensional bin packing, any &ogr;(n log n)-time algorithm S must have r(S) ≥ d.The problem addressed here concerns a set of isolated processors, some unknown subset of which may be faulty, that communicate only by means of two-party messages. Each nonfaulty processor has a private value of information that must be communicated to each other nonfaulty processor. Nonfaulty processors always communicate honestly, whereas faulty processors may lie. The problem is to devise an algorithm in which processors communicate their own values and relay values received from others that allows each nonfaulty processor to infer a value for each other processor. The value inferred for a nonfaulty processor must be that processor's private value, and the value inferred for a faulty one must be consistent with the corresponding value inferred by each other nonfaulty processor.It is shown that the problem is solvable for, and only for, n ≥ 3m + 1, where m is the number of faulty processors and n is the total number. It is also shown that if faulty processors can refuse to pass on information but cannot falsely relay information, the problem is solvable for arbitrary n ≥ m ≥ 0. This weaker assumption can be approximated in practice using cryptographic methods.A class of first-order databases with no function signs is considered. A closed database DB is one for which the only existing individuals are those explicitly referred to in the formulas of DB. Formally, this is expressed by including in DB a domain closure axiom (x)x = c1 ∨···∨ x = cp, where c1,…,cp are all of the constants occurring in DB. It is shown how to completely capture the effects of this axiom by means of suitable generalizations of the projection and division operators of relational algebra, thereby permitting the underlying theorem prover used for query evaluation to ignore this axiom.A database is E-saturated if all of its constants denote distinct individuals. It is shown that such databases circumvent the usual problems associated with equality, which arise in more general databases.Finally, it is proved for Horn databases and positive queries that only definite answers are obtained, and for databases with infinitely many constants that infinitely long indefinite answers can arise.An algorithm is given for deciding whether a functional or a multivalued dependency &sgr; (with a right-hand side Y) is implied by a set of functional and multivalued dependencies &Sgr;. The running time of the algorithm is O(| Y |‖&Sgr;‖), where Y is the number of attributes in Y and ‖&Sgr;‖ is the size of the description of &Sgr;. The problem of constructing the dependency basis of a set of attributes X is also investigated. It is shown that the dependency basis can be found in O(S‖&Sgr;‖) time, where S is the number of sets in the dependency basis. Since functional and multivalued dependencies correspond to a subclass of propositional logic (that can be viewed as a generalization of Horn clauses), the algorithm given is also an efficient inference procedure for this subclass of propositional logic.All known globally convergent iterations for the solution of a nonlinear operator equation ƒ(x) = 0 are either nonstationary or use nonlinear information. It is asked whether there exists a globally convergent stationary iteration which uses linear information. It is proved that even if global convergence is defined in a weak sense, there exists no such iteration for as simple a class of problems as the set of all analytic complex functions having only simple zeros. It is conjectured that even for the class of all real polynomials which have real simple zeros there does not exist a globally convergent stationary iteration using linear information.The problem of determining whether it is possible for a set of “free-running” processes to become deadlocked is considered. It is assumed that any request by a process is immediately granted as long as there are enough free resource units to satisfy the request. The question of whether or not there exists a polynomial algorithm for predicting deadlock in a “claim-limited” serially reusable resource system has been open. An algorithm employing a network flow technique is presented for this purpose. Its running time is bounded by O(mn1.5) if the system consists of n processes sharing m types of serially reusable resources.The cycle time distribution of a cyclic queue with two exponential servers is derived. Results show that when the population size N is large enough, the cycle time distribution is not sensitive to the ratio of service rates and asymptotically approaches an Erlangian distribution. If service rates are identical, however, the cycle time has an exact Erlangian distribution for any N.An algorithm which schedules forests of n tasks on m identical processors in O(n log m) time, offline, is given. The schedules are optimal with respect to finish time and contain at most n - 2 preemptions, a bound which is realized for all n. Also given is a simpler algorithm which runs in O(nm) time on the same problem and can be adapted to give optimal finish time schedules on-line for independent tasks with release times.It is shown that mean queue sizes, mean waiting times, and throughputs in closed multiple-chain queuing networks which have product-form solution can be computed recursively without computing product terms and normalization constants. The resulting computational procedures have improved properties (avoidance of numerical problems and, in some cases, fewer operations) compared to previous algorithms. Furthermore, the new algorithms have a physically meaningful interpretation which provides the basis for heuristic extensions that allow the approximate solution of networks with a very large number of closed chains, and which is shown to be asymptotically valid for large chain populations.A model of a closed queuing network within which customer routing between queues may depend on the state of the network is presented. The routing functions allowed may be rational functions of the queue lengths of various downstream queues which reside within special subnetworks called p-subnetworks. If a network with no state-dependent routing has a product-form joint equilibrium distribution of the queue lengths, then the introduction of these routing functions will preserve the product form of the equilibrium distribution. An example to illustrate the applicability of the model to the problem of analyzing a load balancing strategy is presented. It is also indicated how the parametric analysis of a network with routing functions can be simplified through the analysis of a simpler “equivalent” network.The correctness of semantic-syntax-directed translators (SSDTs) is examined. SSDTs are a generalization of syntax-directed translators in which semantic information is employed to partially direct the translator. Sufficient conditions for an SSDT to be “semantic-preserving,” or “correct,” are presented. A further result shows that unless certain conditions are met, it is undecidable, in general, whether an SSDT is semantic-preserving.The notion of the congruence closure of a relation on a graph is defined and several algorithms for computing it are surveyed. A simple proof is given that the congruence closure algorithm provides a decision procedure for the quantifier-free theory of equality. A decision procedure is then given for the quantifier-free theory of LISP list structure based on the congruence closure algorithm. Both decision procedures determine the satisfiability of a conjunction of literals of length n in average time O(n log n) using the fastest known congruence closure algorithm. It is also shown that if the axiomatization of the theory of list structure is changed slightly, the problem of determining the satisfiability of a conjunction of literals becomes NP-complete. The decision procedures have been implemented in the authors' simplifier for the Stanford Pascal Verifier.Recent developments by Hewitt and others have stimulated interest in message-passing constructs as an alternative to the more conventional applicative semantics on which most current languages are based. The present work illuminates the distinction between applicative and message-passing semantics by means of the &mgr;-calculus, a syntactic model of message-passing systems similar in mechanism to the &lgr;-calculus. Algorithms for the translation of expressions from the &lgr;- to the &mgr;-calculus are presented, and differences between the two approaches are discussed.Message-passing semantics seem particularly applicable to the study of multiprocessing. The &mgr;-calculus, through the mechanism of conduits, provides a simple model for a limited but interesting class of parallel computations. Multiprocessing capabilities of the &mgr;-calculus are illustrated, and multiple-processor implementations are discussed briefly.Lower bounds on the interprocessor communication required for computing a differentiable real-valued function in a distributed network are derived. These bounds are independent of the network interconnection configuration, and they impose no assumptions other than differentiability constraints on the computations performed by individual processors. As a sample application, lower bounds on information transfer in the distributed computation of some-typical matrix operations are exhibited.It is shown that, given an arbitrary GO position on an n × n board, the problem of determining the winner is Pspace hard. New techniques are exploited to overcome the difficulties arising from the planar nature of board games. In particular, it is proved that GO is Pspace hard by reducing a Pspace-complete set, TQBF, to a game called generalized geography, then to a planar version of that game, and finally to GO.







The method of analysis employing decomposition of busy periods
has, in various forms, been applied to the M/G/1 queueing system
under a variety of scheduling rules This paper extends the
technique of decomposition of busy periods in order to deal with
the M/M/c queueing system. Particular attention is given to a
special busy period referred to as a "delay cycle." The delay cycle
commences with a delay period (of general distribution) in which
jobs arrive but are not serviced, at the conclusion of the delay
period, processing begins and continues until the system is empty.
Closed form solutions are obtained for various entities such as
distribution of busy period length and expected waiting time
conditioned on the type of busy period in progress at the tmae of
job arrival. These results are applied and extended to the analysis
of six examples of multiprocessor systems.



A modification of Pohl's bidirectional heuristic search algorithm is described together with a simplified implementation. Theorems are proved about conditions yielding shortest paths. The results are given of a worst-case analysis of different algorithms, suggesting a rank order of their quality. Results that Pohl had obtained with a unidirectional heuristic search algorithm on the 15-puzzle are compared with the results obtained with the new — simplified — algorithm.A point-disjoint path cover of a directed graph is a collection of point-disjoint paths (some paths possibly having zero length) which covers all the points. A path cover which minimizes the number of paths corresponds to an optimal sequence of the steps of a computer program for efficient coding and documentation. The minimization problem for the general directed graph is hard in the sense of being NP-complete. In the case of cycle-free digraphs, however, the problem is polynomial, for it is shown that it can be reduced to the maximum-matching problem. A heuristic given here for finding a near optimal path cover for the general case is based upon applying the maximum-matching algorithm to the subgraphs of an interval decomposition.The use of hashing schemes for storing extendible arrays is investigated. It is shown that extendible hashing schemes whose worst-case access behavior is close to optimal must utilize storage inefficiently; conversely hashing schemes that utilize storage too conservatively are inevitably poor in expected access time. If requirements for the utilization of storage are relaxed slightly, then one can find rather efficient extendible hashing schemes. Specifically, for any dimensionality of arrays, one can find extendible hashing schemes which at once utilize storage well (fewer than 2p storage locations need be set aside for storing arrays having p or fewer positions) and enjoy good access characteristics (expected access time is O(1), and worst-case access time is O(log log p) for p- or fewer-position arrays). Moreover, at the cost of only a modest additive increase in access time, storage demands can be decreased to (1 + &dgr;)p locations for arbitrary &dgr; > 0.This paper presents a queueing model of a multiprogrammed computer system with virtual memory. Two system organizations are considered: (i) all the processes present in the system share primary storage; (ii) processes which have generated a file request (slow I/O) lose their memory space until the I/O is completed. Our model assumes balanced memory allocation among processes, and accounts for the memory sharing effect through the use of lifetime functions. The model explicitly takes into account the fact that, if a written-onto page is to be replaced at the moment of a page fault, it first has to be saved in the secondary memory. An approximate closed form solution is obtained by using an equivalence and decomposition approach. A procedure for evaluating the accuracy of the approximation is presented. The numerical examples illustrate the influence of the system and program behavior parameters taken into account in our model.A queueing-type model is used to analyze the storage requirements of a component of a real-time data entry system. The objectives and criteria of the buffer management procedure are identified and related to the variables of the model. Both infinite and finite buffers are considered. The analysis is done symbolically in part and numerically in part to accommodate input processes that are peculiar to the system. Techniques to obtain overflow probabilities are described in detail. It is shown that creating a pool of storage blocks for all the terminals is a better policy than maintaining a separate buffer for each station. The savings brought about by this policy are remarkably insensitive to the characteristics of the input process.A new property of queueing discipline, station balance, seems to explain why some disciplines yield product form solutions for queues and networks using nonexponential service disciplines and other disciplines do not. A queueing discipline satisfies station balance if rates at which customers receive service at each position of the queue are proportional to the probability that a customer arrives at that position. Station and local balance in queues and networks of queues are investigated. In addition to characterizing local balance and product form, the results of the paper generalize previous results on local balance to arbitrary differentiable service distribution functions.A dominance relation D is a binary relation defined on the set of partial problems generated in a branch-and-bound algorithm, such that PiDPj (where Pi and Pj are partial problems) implies that Pj can be excluded from consideration without loss of optimality of the given problem if Pi has already been generated when Pj is selected for the test. The branch-and-bound computation is usually enhanced by adding the test based on a dominance relation.A dominance relation D′ is said to be stronger than a dominance relation D if PiDPj always implies PiD′Pj. Although it seems obvious that a stronger dominance relation makes the resulting algorithm more efficient, counterexamples can easily be constructed. In this paper, however, four classes of branch-and-bound algorithms are found in which a stronger dominance relation always gives a more efficient algorithm. This indicates that the monotonicity property of dominance relations would be observed in a rather wide class of branch-and-bound algorithms, thus encouraging the designer of a branch-and-bound algorithm to find the strongest possible dominance relation.The finishing time properties of several heuristic algorithms for scheduling n independent tasks on m nonidentical processors are studied. In particular, for m = 2 an n log n time-bounded algorithm is given which generates a schedule having a finishing time of at most (√5 + 1)/2 of the optimal finishing time. A simplified scheduling problem involving identical processors and restricted task sets is shown to be P-complete. However, the LPT algorithm applied to this problem yields schedules which are near optimal for large n.The problem of determining the minimum representation of programs for execution by a computer is considered. The methods of measuring space requirements suggest practical methods for encoding programs and for designing machine languages. An analysis of the operation portion of instructions finds that the 47 operation codes used by a well-known compiler require, on average, fewer than two bits each.This paper considers informally the relationship between computer aided mathematical proof, formal algebraic languages, computation with transcendental numbers, and proof by sampling.The foundations are laid for a theory of multiplicative complexity of algebras and it is shown how “multiplication problems” such as multiplication of matrices, polynomials, quaternions, etc., are instances of this theory. The usefulness of the theory is then demonstrated by utilizing algebraic ideas and results to derive complexity bounds. In particular linear upper and lower bounds for the complexity of certain types of algebras are established.It is shown that every deterministic multitape Turing machine of time complexity t(n) can be simulated by a deterministic Turing machine of tape complexity t(n)/logt(n). Consequently, for tape constructable t(n), the class of languages recognizable by multitape Turing machines of time complexity t(n) is strictly contained in the class of languages recognized by Turing machines of tape complexity t(n). In particular the context-sensitive languages cannot be recognized in linear time by deterministic multitape Turing machines.A simple programming language which corresponds in computational power to the class of generalized sequential machines with final states is defined. It is shown that a variety of questions of practical programming interest about the language are of nondeterministic linear space complexity. Extensions to the language are defined (adding arithmetic and array data structures) and their complexity properties are explored. It is concluded that questions about halting, equivalence, optimization, and so on are intractable even for very simple programming languages.
Algorithms for finding shortest paths are presented which are faster than algorithms previously known on networks which are relatively sparse in arcs. Known results which the results of this paper extend are surveyed briefly and analyzed. A new implementation for priority queues is employed, and a class of “arc set partition” algorithms is introduced. For the single source problem on networks with nonnegative arcs a running time of O(min(n1+1/k + e, n + e) log n)) is achieved, where there are n nodes and e arcs, and k is a fixed integer satisfying k > 0. This bound is O(e) on dense networks. For the single source and all pairs problem on unrestricted networks the running time is O(min(n2+1/k + ne, n2 log n + ne log n).The performance of a disk system is often measured in terms of the length of the waiting line or queue of requests for each of the system's spindles. Thus it is natural to formulate and analyze queueing models of disk systems. While most disk systems have certain characteristics, such as channel interference and concurrent seeks, in common, previous analyses have always been begun from scratch, without exploiting this commonality. We introduce a general queueing model for disk systems, which incorporates the characteristics common to most disk systems, and use it in the approximate analyses of models of the IBM 2314 and 3330 disk systems. Comparisons with simulation statistics show that the approximations made are very good over a wide range of arrival rates and system parameters. We also show how to use the analytic results to investigate performance differences between devices.Muntz and Coffman give a level algorithm that constructs optimal preemptive schedules on identical processors when the task system is a tree or when there are only two processors available. Their algorithm is adapted here to handle processors of different speeds. The new algorithm is optimal for independent tasks on any number of processors and for arbitrary task systems on two processors, but not on three or more processors, even for trees. By taking the algorithm as a heuristic on m processors and using the ratio of the lengths of the constructed and optimal schedules as a measure, an upper bound on its performance is derived in terms of the speeds of the processors. It is further shown that 1.23√m is an upper bound over all possible processor speeds and that the 1.23√m bound can be improved at most by a constant factor, by giving an example of a system for which the bound 0.35√m can be approached asymptotically.A system of rules for transforming programs is described, with the programs in the form of recursion equations. An initially very simple, lucid, and hopefully correct program is transformed into a more efficient one by altering the recursion structure. Illustrative examples of program transformations are given, and a tentative implementation is described. Alternative structures for programs are shown, and a possible initial phase for an automatic or semiautomatic program-manipulation system is indicated.Many apparently divergent approaches to specifying formal semantics of programming languages are applications of initial algebra semantics. In this paper an overview of initial algebra semantics is provided. The major technical feature is an initial continuous algebra which permits unified algebraic treatment of iterative and recursive semantic features in the same framework as more basic operations.Three ACM symposia on Principles of Programming Languages, jointly sponsored by SIGACT and SIGPLAN, have now been held. The first symposium was in October 1973, the second in January 1975, and the third the following year. It is now planned that the symposia be held annually. The fourth symposium is scheduled for January 17-19, 1977, in Los Angeles.Four of the twenty papers presented at the third symposium have been selected for publication in this special section.1 Each paper was refereed in the customary fashion and each of the papers appearing here is a revised or extended version of the original paper or extended abstract included in the Conference Record. The papers address a variety of topics, yet they have in common a blend of both theoretical and pragmatic considerations.Two of the papers discuss techniques for program transformations. (Indeed, program transformation was an important theme at the symposium.)Baker presents an algorithm for transforming a program in which the flow of control may not be represented by structured-programming-style syntax into a computationally equivalent program which is more readable because the iterative and conditional structure is explicit. An implementation of the technique is described in which Fortran programs are transformed to a structured Fortran dialect.The paper by Loveman discusses the transformation of programs to equivalent programs in the same language which require less running time and/or less space. He illustrates a number of these transformations and describes several programming systems in which the source-to-source techniques have been used. He also argues that appropriate source-to-source transformations can be used to transform a program to a form in which a straightforward code generator could produce the sort of code now generated by conventional optimizing compilers. Such a technique would simplify the code generation phase of a compiler and would provide the programmer with a source-level description of the optimized program.A different aspect of code generation is the topic of the paper by Aho, Johnson, and Ullman. They study the problem of generating good machine code for so-called straight line source code which contains common subexpressions (i.e. multiple occurrences of the same variables or expressions). They show that under a variety of assumptions, the generation of optimal code sequences is computationally difficult in a certain sense. An analysis of some heuristics for code generation is given which offers insight into the reasons for the complexity and some approaches to generating reasonable code sequences at an acceptable cost.Unlike the other papers, all of which, in some sense, give methods for creating programs from other programs, the paper by Summers investigates methods for creating programs from examples of their behavior. An implementation is described in which, by exploiting the structure embodied in S-expressions, programs in a subset of LISP are synthesized from pairs of S-expressions.In order to achieve timely publication of a special section such as this one, it was necessary to set deadlines for each stage in the process. The cooperation of the authors, the referees, and the editors in meeting these deadlines is gratefully acknowledged.This paper describes an algorithm which transforms a flowgraph into a program containing control constructs such as if then else statements, repeat (do forever) statements, multilevel break statements (causing jumps out of enclosing repeats), and multilevel next statements (causing jumps to iterations of enclosing repeats). The algorithm can be extended to create other types of control constructs, such as while or until. The program appears natural because the constructs are used according to common programming practices. The algorithm does not copy code, create subroutines, or add new variables. Instead, goto statements are generated when no other available control construct describes the flow of control. The algorithm has been implemented in a program called STRUCT which rewrites Fortran programs using constructs such as while, repeat, and if then else statements. The resulting programs are substantially more readable than their Fortran counterparts.The use of source-to-source program transformations has proved valuable in improving program performance. The concept of program manipulation is elucidated by describing its role in both conventional optimization and high level modification of conditional, looping, and procedure structures. An example program fragment written in an Algol-like language is greatly improved by transformations enabled by a user-provided assertion about a data array. A compilation model based on the use of source-to-source program transformations is used to provide a framework for discussing issues of code generation, compilation of high level languages such as APL, and eliminating overhead commonly associated with modular structured programming. Application of the compilation model to several different languages is discussed.This paper shows the problem of generating optimal code for expressions containing common subexpressions is computationally difficult, even for simple expressions and simple machines. Some heuristics for code generation are given and their worst-case behavior is analyzed. For one register machines, an optimal code generation algorithm is given whose time complexity is linear in the size of an expression and exponential only in the amount of sharing.An automatic programming system, THESYS, for constructing recursive LISP programs from examples of what they do is described. The construction methodology is illustrated as a series of transformations from the set of examples to a program satisfying the examples. The transformations consist of (1) deriving the specific computation associated with a specific example, (2) deriving control flow predicates, and (3) deriving an equivalent program specification in the form of recurrence relations. Equivalence between certain recurrence relations and various program schemata is proved. A detailed description of the construction of four programs is presented to illustrate the application of the methodology.
The concern here is with proof procedures which are generalizations of input or unit deduction. The author's generalizations of input deduction involve lemmas, whereas those of unit deduction involve longer clauses and are akin to Robinson's P1 deduction. Chang's theorem, which establishes the equivalence of input and unit refutation, is extended to these generalizations. Completeness results of Henschen, Wos, and Kuehner for input or unit deduction applied to Horn sets are generalized to apply also to non-Horn sets. A key result is that any unsatisfiable set can be refuted by a lock linear resolution procedure in which the only lemmas are positive clauses composed entirely of instances of a small set of literals which can be specified in advance. In an implementation such lemmas would be generated only infrequently, thus allowing one to periodically gather the lemmas, discard other generated clauses, and restart the proof procedure.Grammar forms are compared for their efficiency in representing languages, as measured by the sizes (i.e. total number of symbols, number of variable occurrences, number of productions, and number of distinct variables) of interpretation grammars. For every regular set, right- and left-linear forms are essentially equal in efficiency. Any form for the regular sets provides, at most, polynomial improvement over right-linear form. Moreover, any polynomial improvement is attained by some such form, at least on certain languages. Greater improvement for some languages is possible using forms expressing larger classes of languages than the regular sets. However, there are some languages for which no improvement over right-linear form is possible.While a similar set of results holds for forms expressing exactly the linear languages, only linear improvement can occur for forms expressing all the context-free languages.A computer solution to the problem of automatic location of objects in digital pictures is presented. A self-scaling local edge detector that can be applied in parallel on a picture is described. Clustering algorithms and sequential boundary following algorithms process the edge data to local images of objects and generate a data structure that represents the imaged objects.It is possible to significantly reduce the average cost of information retrieval from a large shared database by partitioning data items stored within each record into a primary and a secondary record segment. An analytic model, based upon knowledge of data item lengths, transportation costs, and retrieval patterns, is developed to assist an analyst with this assignment problem. The model is generally applicable to environments in which a database resides in secondary storage, and is useful for both uniprogramming and multiprogramming systems. A computationally tractable record design algorithm has been implemented as a Fortran program and applied to numerous problems. Realistic examples are presented which demonstrate a potential for reducing total system cost by more than 65 percent.Iterative methods for the solution of tridiagonal systems are considered, and a new iteration is presented, whose rate of convergence is comparable to that of the optimal two-cyclic Chebyshev iteration but which does not require the calculation of optimal parameters. The convergence rate depends only on the magnitude of the elements of the tridiagonal matrix and not on its dimension or spectrum. The theory also has a natural extension to block tridiagonal systems. Numerical experiments suggest that on a parallel computer this new algorithm is the best of the iterative algorithms considered.A sequencing problem wherein there is a single processor and a finite number of jobs needing service is considered. Each job consists of a sequence of tasks generated probabilistically by a finite state Markov chain. Each state in the Markov chain is identified with a task and has a service-time requirement and a deferral cost, both of which are random variables. The goal is to minimize the expected value of the sum of the weighted finishing times of all the tasks. The sequencing discipline is nonpreemptive. It is shown that there exists an optimal priority sequencing rule based on a rank defined for each task; an efficient algorithm for calculating the rank is given.A linear time algorithm to obtain a minimum finish time schedule for the two-processor open shop together with a polynomial time algorithm to obtain a minimum finish time preemptive schedule for open shops with more than two processors are obtained. It is also shown that the problem of obtaining minimum finish time nonpreemptive schedules when the open shop has more than two processors is NP-complete.This paper deals with a single server serving N priority classes (N being finite or infinite) and working under an FBz regime, namely, one in which the waiting line consists of infinitely many separate queues obeying the FIFO rule. Each priority class is assigned to one of the queues. A customer from the kth priority class (“k-customer”) in the nth queue is eligible for &thgr;n,k time units of service, at the end of which he either departs, because his requirement is satisfied, or joins the tail of the (n + 1)-th queue. When a quantum of service is completed, the server turns to the first customer in the lowest index (highest priority) nonempty queue.The arrival process of k-customers is assumed to be homogeneous Poisson, and their service requirements are independent, generally distributed, random variable. A set of recursive linear equations is derived for the expected flow time of a k-customer whose service requirement is known, and some examples are discussed and presented graphically.This paper corrects some errors in an earlier paper by the second author.It is shown that specifications of program performance can be formally verified. Formal verification techniques, in particular, the method of inductive assertions, can be adapted to show that a program's maximum or mean execution time is correctly described by specifications supplied with the program. To formally establish the mean execution time, branching probabilities are expressed using inductive assertions which involve probability distributions. Verification conditions are formed and proved which establish that if the input distribution is correctly described by the input specifications, then the inductive assertions correctly describe the probability distributions of the data during execution. Once the inductive assertions are shown to be correct, branching probabilities are obtained and mean computation time is computed.A solution to the problem of counting the number of fanout-free Boolean functions of n variables is presented. The relevant properties of fanout-free functions and circuits are summarized. The AND and OR ranks of a fanout-free function are defined. Recursive formulas for determining the number of distinct functions of specified rank are derived. Based on these, expressions are obtained for @@@@D(n), @@@@ND(n), and @@@@(n), which denote the number of degenerate, nondegenerate, and all n-variable fanout-free functions, respectively. Simple nonrecursive bounds on the various @@@@ functions are also computed and are used to determine some asymptotic properties of the @@@@ functions. It is shown that for large n almost all fanout-free functions are nondegenerate, and that almost all unate functions are not fanout-free. The relationship between the fanout-free function enumeration problem and other function enumeration problems in switching theory is discussed.This paper considers a generalization, called the Shannon switching game on vertices, of a familiar board game called Hex. It is shown that determining who wins such a game if each player plays perfectly is very hard; in fact, if this game problem is solvable in polynomial time, then any problem solvable in polynomial space is solvable in polynomial time. This result suggests that the theory of combinational games is difficult.Control structures and data structures are modeled by directed graphs. In the control case nodes represent executable statements and arcs represent possible flow of control; in the data case nodes represent memory locations and arcs represent logical adjacencies in the data structure. Classes of graphs are compared by a relation ≤S.T where G ≤S.T H if G can be embedded in H with at most a T-fold increase in distance between embedded nodes by making at most S “copies” of any node in G. For both control structures and data structures, S and T are interpreted as space and time constants, respectively. Results are presented that establish hierarchies with respect to ≤S.T for (1) data structures, (2) sequential program schemata normal forms, and (3) sequential control structures.Sentences in first-order predicate logic can be usefully interpreted as programs. In this paper the operational and fixpoint semantics of predicate logic programs are defined, and the connections with the proof theory and model theory of logic are investigated. It is concluded that operational semantics is a part of proof theory and that fixpoint semantics is a special case of model-theoretic semantics.
The resolution principle, an automatic inference technique, is studied as a possible decision procedure for certain classes of first-order formulas. It is shown that most previous resolution strategies do not decide satisfiability even for “simple” solvable classes. Two new resolution procedures are described and are shown to be complete (i.e. semidecision procedures) in the general case and, in addition, to be decision procedures for successively wider classes of first-order formulas. These include many previously studied solvable classes. The proofs that a complete resolution procedure will always halt (without producing the empty clause) when applied to satisfiable formulas in certain classes provide new, and in some cases more enlightening, demonstrations of the solvability of these classes. A technique for constructing a model for a formula shown satisfiable in this way is also described.A natural, and readily computable, first guess at a solution to the coin changing problem is the canonical solution. This solution is a special case of the greedy solution which is a reasonable heuristic guess for the knapsack problem. In this paper, efficient tests are given to determine whether all greedy solutions are optimal with respect to a given set of knapsack objects or coin types. These results improve or extend previous tests given in the literature. Both the incomplete and complete cases are considered.Graphs that in a certain precise sense are rich in sets of vertex-disjoint paths are studied. Bounds are obtained on the minimum number of edges in such graphs, and these are used to deduce nonlinear lower bounds on the computational complexity of shifting, merging, and matching problems.A backtracking algorithm for testing a pair of digraphs for isomorphism is presented. The information contained in the distance matrix representation of a graph is used to establish an initial partition of the graph's vertices. This distance matrix information is then applied in a backtracking procedure to reduce the search tree of possible mappings. While the algorithm is not guaranteed to run in polynomial time, it performs efficiently for a large class of graphs.The problem of finding a minimum k-basis of graph G is that of selecting as small a set B of vertices as possible such that every vertex of G is at distance k or less from some vertex in B. Cockayne, Goodman, and Hedetniemi previously developed a linear algorithm to find a minimum 1-basis (a minimum dominating set) when G is a tree. In this paper the k-basis problem is placed in a more general setting, and a linear algorithm is presented that solves the problem for any forest.The expected depth of each key in the set of binary search trees formed from all sequences composed from a multiset {p1 · 1, p2 · 2, p3 · 3, ···, pn · n} is obtained, and hence the expected weight of such trees. The expected number of left-to-right local minima and the expected number of cycles in sequences composed from a multiset are then deduced from these results.A -stable, semi-implicit Runge-Kutta procedures requiring at most one Jacobian evaluation per time step are developed for the approximate numerical integration of stiff systems of ordinary differential equations. A simple procedure for estimating the local truncation error is described and, with the help of this estimate, efficient integration procedures are derived. The algorithms are illustrated by direct application to a particular example.Given a set @@@@ = {T1,T2,···,Tn} of tasks, with each Ti having execution time 1 and a deadline di > 0, and a set of precedence constraints which restrict allowable schedules, the problem of determining whether there exists a schedule using two processors in which each task is completed before its deadline is examined. An efficient algorithm for finding such a schedule, whenever one exists, is given. The algorithm may also be used to find the shortest such schedule. In addition it is shown that the problem of finding a one-processor schedule which minimizes the number of tasks failing to meet their deadlines is NP-complete and, hence, is likely to be computationally intractable.This paper presents a branch and bound method for solving problems in which the objective function is quadratic, the constraints are linear, and some or all variables are required to be integer. The algorithm is obtained by grafting an inverse-basis version of Beale's method onto the Land-Doig procedure. The code has been tested on a computer, and computational results with various strategies of branching are reported.The ordinal regression problem is an extension to the standard multiple regression problem in terms of assuming only ordinal properties for the dependent variable (rank order of preferred brands in a product class, academic ranks for students in a class, etc.) while retaining the interval scale assumption for independent (or predictor) variables. The linear programming formulation for obtaining the regression weights for ordinal regression, developed in an earlier paper, is outlined and computational improvements and alternatives which utilize the special structure of this linear program are developed and compared for their computational efficiency and storage requirements. A procedure which solves the dual of the original linear programming formulation by the dual simplex method with upper bounded variables, in addition to utilizing the special structure of the constraint matrix from the point of view of storage and computation, performs the best in terms of both computational efficiency and storage requirements. Using this special procedure, problems with 100 observations and 4 independent variables take less than 1/2 minute, on an average, on the IBM 360/67. Results also show that the linear programming solution procedure for ordinal regression is valid — the correlation coefficient between “true” and predicted values for the dependent variable was greater than .9 for most of the problems tested.This paper discusses algorithms which transform expression trees into code for register machines. A necessary and sufficient condition for optimality of such an algorithm is derived, which applies to a broad class of machines. A dynamic programming algorithm is then presented which produces optimal code for any machine in this class; this algorithm runs in time linearly proportional to the size of the input.The majority of computers that have been built have performed all computations in devices called accumulators, or registers. In this paper, it is shown that the problem of generating minimal-length code for such machines is hard in a precise sense; specifically it is shown that the problem is NP-complete. The result is true even when the programs being translated are arithmetic expressions. Admittedly, the expressions in question can become complicated.A method is presented for directly transforming an arbitrary LR(k) grammar to an equivalent LR(1) grammar. It is further shown that the method transforms an arbitrary prefix-free LR(k) grammar to an equivalent LR(0) grammar. It is argued that the method is efficient and offers some advantages over traditional “look-ahead” parsing methods. Finally, it is demonstrated that the method can be used to transform an LR(1) grammar to an equivalent SLR(1) grammar, which in turn can be easily transformed to an equivalent (1, 1) bounded right-context grammar.Let E be an arithmetic expression involving n variables, each of which appears just once, and the possible operations of addition, multiplication, and division. Although other cases are considered, when these three operations take unit time the restructuring algorithms presented in this paper yield evaluation times no greater than 2.88 log2n + 1 and 2.08 log2n for general expressions and division-free expressions, respectively. The coefficients are precisely given by 2/log2&agr; ≈ 2.88 and 1/log2&bgr; ≈ 2.08, where &agr; and &bgr; are the positive real roots of the equations z2 = z + 1 and z4 = 2z + 1, respectively. While these times were known to be of order log2n, the best previously known coefficients were 4 and 2.15 for the two cases.The authors conjecture that the present coefficients are the best possible, since they have exhibited expressions which seem to require these times within an additive constant.The paper also gives upper bounds to the restructuring time of a given expression E and to the number of processors required for its parallel evaluation. It is shown that at most O(n1.44) and O(n1.82) operations are needed for restructuring general expressions and division-free expression, respectively.It is pointed out that, since the order of the compiling time is greater than n log n, the numbers of required processors exhibit the same rate of growth in n as the corresponding compiling times.It is shown that the Chinese Postman Problem, although tractable in the totally directed and the totally undirected cases, is NP-complete in the mixed case. A simpler version of the same problem is shown algorithmically equivalent to the max-flow problem with unit edge capacities.For P-complete problems such as traveling salesperson, cycle covers, 0-1 integer programming, multicommodity network flows, quadratic assignment, etc., it is shown that the approximation problem is also P-complete. In contrast with these results, a linear time approximation algorithm for the clustering problem is presented.Let M(m, n) be the minimum number or comparators needed in an (m, n)-merging network. It is shown that M(m, n) ≥ n(lg(m + 1))/2, which implies that Batcher's merging networks are optimal up to a factor of 2 + &egr; for almost all values of m and n. The limit rm = limn→∞ M(m, n)/n is determined to within 1. It is also proved that M(2, n) = [3n/2].
A matching on a graph is a set of edges, no two of which share a vertex. A maximum matching contains the greatest number of edges possible. This paper presents an efficient implementation of Edmonds' algorithm for finding a maximum matching. The computation time is proportional to V3, where V is the number of vertices; previous implementations of Edmonds' algorithm have computation time proportional to V4. The implementation is based on a system of labels that encodes the structure of alternating paths.Two algorithms for computing the multiplication table of a 2-group are described and discussed. One of the algorithms works in an element-by-element fashion; the other works in terms of subgroups generated by initial subsequences of the given sequence of generators. Estimates of computation times are given which show that the second algorithm is much more efficient than the first. It is also shown how the second algorithm can be modified to make it more useful, without using significantly more time.Let ƒ(x) be one of the usual elementary functions (exp, log, artan, sin, cosh, etc.), and let M(n) be the number of single-precision operations required to multiply n-bit integers. It is shown that ƒ(x) can be evaluated, with relative error &Ogr;(2-n), in &Ogr;(M(n)log (n)) operations as n → ∞, for any floating-point number x (with an n-bit fraction) in a suitable finite interval. From the Schönhage-Strassen bound on M(n), it follows that an n-bit approximation to ƒ(x) may be evaluated in &Ogr;(n log2(n) log log(n)) operations. Special cases include the evaluation of constants such as &pgr;, e, and e&pgr;. The algorithms depend on the theory of elliptic integrals, using the arithmetic-geometric mean iteration and ascending Landen transformations.The parallel evaluation of rational expressions is considered. New algorithms which minimize the number of multiplication or division steps are given. They are faster than the usual algorithms when multiplication or division takes more time than addition or subtraction. It is shown, for example, that xn can be evaluated in two steps of parallel division and ⌈log2 n⌉ steps of parallel addition, while the usual algorithm takes ⌈log2 n⌉ steps of parallel multiplication.Lower bounds on the time required are obtained in terms of the degree of the expressions to be evaluated. From these bounds, the algorithms presented in the paper are shown to be asymptotically optimal. Moreover, it is shown that by using parallelism the evaluation of any first-order rational recurrence of degree greater than 1, e.g. yi+1 = 1/2;(yi + a/yi), and any nonlinear polynomial recurrence can be sped up at most by a constant factor, no matter how many processors are used and how large the size of the problem is.A new algorithm is presented for constructing auxiliary digital search trees to aid in exact-match substring searching. This algorithm has the same asymptotic running time bound as previously published algorithms, but is more economical in space. Some implementation considerations are discussed, and new work on the modification of these search trees in response to incremental changes in the strings they index (the update problem) is presented.A statistical model is presented for the investigation of a practical method used in relevance feedback. A necessary and sufficient condition for the two parameters used in this method to define a better query than the original query is given. A region in the plane of the parameters is shown to satisfy the sufficient condition. While the points for producing optimal queries are not exactly located, they are shown to be lying on a finite portion of a hyperbola. Experimental results support some of the theoretical findings.This paper analyzes the distribution of trailing digits (tail end digits) of positive real floating-point numbers represented in arbitrary base &bgr; and randomly chosen from a logarithmic distribution. The analysis shows that the nth digit for n ≥ 2 is actually approximately uniformly distributed. The approximation depends upon both n and the base&bgr;. It becomes better as n increases, and it is exact in the limit as n ⇒ ∞. A table of this distribution is presented for various &bgr; and n, along with a table of the maximum digit by digit deviation &Dgr; of the logarithmic distribution from the uniform distribution. Various asymptotic results for &Dgr; are included. These results have application in resolving open questions of Henrici, of Kaneko and Liu, and of Tsao.An array may be reordered according to a common permutation of the digits of each of its element indices. The digit-reversed reordering which results from common fast Fourier transform (FFT) algorithms is an example. By examination of this class of permutation in detail, very efficient algorithms for transforming very long arrays are developed.An algorithm for factoring a covariance function into its Hurwitz factors, which is based on the Cholesky factors of a certain matrix, was proposed by F.L. Bauer and others. This algorithm bears a close connection to the theory of orthogonal polynomials, and a closer one to the theory of prediction of stationary time series. In this paper these relations are pointed out and then used to advantage to prove the linear convergence of this algorithm.Exact and approximate algorithms are presented for scheduling independent tasks in a multiprocessor environment in which the processors have different speeds. Dynamic programming type algorithms are presented which minimize finish time and weighted mean flow time on two processors. The generalization to m processors is direct. These algorithms have a worst-case complexity which is exponential in the number of tasks. Therefore approximation algorithms of low polynomial complexity are also obtained for the above problems. These algorithms are guaranteed to obtain solutions that are close to the optimal. For the case of minimizing mean flow time on m-processors an algorithm is given whose complexity is O(n log mn).A two-stage queueing network with feedback and a finite intermediate waiting room is studied. The first-stage server is blocked whenever M requests are enqueued in the second stage. The analysis of this system under exponential assumptions is carried out. An algorithm to calculate the stationary state probabilities is given and some special cases are considered.Upper and lower bounds on processor utilization for the queueing model M/G/1/N are derived. The upper bound is equal to the utilization for constant service times and the lower bound is approached when the average number of operations per busy period approaches one. These bounds show that the form of the processing time distribution can have a substantial effect on processor utilization. We show that the utilization will be near the lower bound if there are a large number of short processing times. The variance does not always give an accurate indication of the effect of the distribution on utilization.In this paper the author continues his study of the regenerative method for analyzing simulations of stable stochastic systems. The principal concern is to estimate the quantiles of the stationary distribution of a regenerative process. Markov chains in discrete or continuous time and multiple server queues in light traffic provide concrete examples of regenerative processes to which this technique applies. Approximate confidence intervals for these quantiles are derived from appropriate central limit theorems. The method has been applied to three stochastic simulations, and the numerical results are presented.The time-dependent equations for the M/M/1 queue can be reduced to a single equation for the expected queue size, but the equation is dependent on P0(t), the probability of no jobs in the system. An exact equation for the behavior of P0(t) under special conditions is derived and an approximation relating P0(t) to Q(t), the expected queue size at time t, is derived for the case when the change in queue size is slow compared to the service rate. It is found that the approximation affords a significant improvement over the use of a steady state approximation to the time-dependent queue and is simpler to use than the exact equations.In the past, picture segmentation has been performed by merging small primitive regions or by recursively splitting the whole picture. This paper combines the two approaches with significant increase in processing speed while maintaining small memory requirements. The data structure is described in detail and examples of implementations are given.This paper is concerned with proving properties of programs which use data structures. The goal is to be able to prove that all instances of a class (e.g. as defined in Simula) satisfy some property. A method of proof which achieves this goal, generator induction, is studied and compared to other proof rules and methods: inductive assertions, recursion induction, computation induction, and, in some detail, structural induction. The paper concludes by using generator induction to prove a characteristic property of an implementation of hashtables.
The problem of finding a longest common subsequence of two strings is discussed. This problem arises in data processing applications such as comparing two files and in genetic applications such as studying molecular evolution. The difficulty of computing a longest common subsequence of two strings is examined using the decision tree model of computation, in which vertices represent “equal - unequal” comparisons. It is shown that unless a bound on the total number of distinct symbols is assumed, every solution to the problem can consume an amount of time that is proportional to the product of the lengths of the two strings. A general lower bound as a function of the ratio of alphabet size to string length is derived. The case where comparisons between symbols of the same string are forbidden is also considered and it is shown that this problem is of linear complexity for a two-symbol alphabet and quadratic for an alphabet of three or more symbols.The string editing problem is to determine the distance between two strings as measured by the minimal cost sequence of deletions, insertions, and changes of symbols needed to transform one string into the other. The longest common subsequence problem can be viewed as a special case. Wagner and Fischer proposed an algorithm that runs in time O(nm), where n, m are the lengths of the two strings. In the present paper, it is shown that if the operations on symbols of the strings are restricted to tests of equality, then O(nm) operations are necessary (and sufficient) to compute the distance.A direct, one-step transformation is presented for transforming an arbitrary LR(k) context-free grammar, G, to an LR(1) grammar, G′, which completely covers G. Under additional hypotheses, G′ may be made LR(0).Subgraph isomorphism can be determined by means of a brute-force tree-search enumeration procedure. In this paper a new algorithm is introduced that attains efficiency by inferentially eliminating successor nodes in the tree search. To assess the time actually taken by the new algorithm, subgraph isomorphism, clique detection, graph isomorphism, and directed graph isomorphism experiments have been carried out with random and with various nonrandom graphs.A parallel asynchronous logic-in-memory implementation of a vital part of the algorithm is also described, although this hardware has not actually been built. The hardware implementation would allow very rapid determination of isomorphism.Graph coloring problems, in which one would like to color the vertices of a given graph with a small number of colors so that no two adjacent vertices receive the same color, arise in many applications, including various scheduling and partitioning problems. In this paper the complexity and performance of algorithms which construct such colorings are investigated. For a graph G, let &khgr;(G) denote the minimum possible number of colors required to color G and, for any graph coloring algorithm A, let A(G) denote the number of colors used by A when applied to G. Since the graph coloring problem is known to be “NP-complete,” it is considered unlikely that any efficient algorithm can guarantee A(G) = &khgr;(G) for all input graphs. In this paper it is proved that even coming close to khgr;(G) with a fast algorithm is hard. Specifically, it is shown that if for some constant r < 2 and constant d there exists a polynomial-time algorithm A which guarantees A(G) ≤ r·&khgr;(G) + d, then there also exists a polynomial-time algorithm A which guarantees A(G) = &khgr;(G).An algorithm (FLOW) for finding the shortest distance from a given node S to each node X of a directed graph with nonnegative integer arc lengths less than or equal to WM is presented. FLOW is compared with its best-known competitor, that of Dijkstra and Yen (DFLO). The new algorithm is shown to execute in time of order max (V, E, D), where D is the maximum distance computed in a graph with E edges and V nodes. By counting the number of operands fetched during execution of FLOW and DFLO, an estimate of the running time of each is obtained. This estimate shows that FLOW should execute faster than DFLO when E/V2 = &agr; < .2875, and V ≥ 22 WM/&bgr; + 5/&bgr; + 7.04, where &bgr; = 23 - 80&agr;. FLOW also will solve the all-pairs shortest distance problem, requiring time O(V * max(V, E, D)) for the solution.By defining a suitable algebra for cut sets, it is possible to reduce the problem of enumerating the cut sets between all pairs of nodes in a graph to the problem of solving a system of linear equations. An algorithm for solving this system using Gaussian elimination is presented in this paper. The efficiency of the algorithm depends on the implementation of sum and multiplication. Therefore, some properties of cut sets are investigated, which greatly simplify the implementation of these operations for the case of undirected graphs. The time required by the algorithm is shown to be linear with the number of cut sets for complete graphs. Some experimental results are given, proving that the efficiency of the algorithm increases by increasing the number of pairs of nodes for which the cut sets are computed.Errata are given for “Efficient Planarity Testing” by John Hopcroft and Robert Tarjan [J. ACM 21, 4 (Oct. 1974), 549-568].A great many automatic indexing methods have been implemented and evaluated over the last few years, and automatic procedures comparable in effectiveness to conventional manual ones are now easy to generate. Two drawbacks of the available automatic indexing methods are the absence of reliable linguistic inputs during the indexing process and the lack of formal, analytical proofs concerning the effectiveness of the proposed methods.The precision weighting procedure described in the present study uses relevance criteria to weight the terms occurring in user queries as a function of the balance between relevant and nonrelevant documents in which these terms occur; this approximates a semantic know-how of term importance. Formal mathematical proofs are given under well-defined conditions of the effectiveness of the methodA method is presented for numerically inverting a Laplace transform that requires, in addition to the transform function itself, only sine, cosine, and exponential functions. The method is conceptually much like the method of Dubner and Abate, which approximates the inverse function by means of a Fourier cosine series. The method presented here, however, differs from theirs in two important respects. First of all, the Fourier series contains additional terms involving the sine function selected such that the error in the approximation is less than that of Dubner and Abate and such that the Fourier series approximates the inverse function on an interval of twice the length of the corresponding interval in Dubner and Abate's method. Second, there is incorporated into the method in this paper a transformation of the approximating series into one that converges very rapidly. In test problems using the method it has routinely been possible to evaluate inverse transforms with considerable accuracy over a wide range of values of the independent variable using a relatively few determinations of the Laplace transform itself.A method is presented for numerically inverting a Laplace transform that requires, in addition to the transform function itself, only sine, cosine, and exponential functions. The method is conceptually much like the method of Dubner and Abate, which approximates the inverse function by means of a Fourier cosine series. The method presented here, however, differs from theirs in two important respects. First of all, the Fourier series contains additional terms involving the sine function selected such that the error in the approximation is less than that of Dubner and Abate and such that the Fourier series approximates the inverse function on an interval of twice the length of the corresponding interval in Dubner and Abate's method. Second, there is incorporated into the method in this paper a transformation of the approximating series into one that converges very rapidly. In test problems using the method it has routinely been possible to evaluate inverse transforms with considerable accuracy over a wide range of values of the independent variable using a relatively few determinations of the Laplace transform itself.This paper examines the problem of distributing a set of equal-size records among the sectors of a drum-like storage device in order to exploit known access frequencies and reduce the average access time. A simple catenated search model is defined for which, the problem is shown to be NP-complete. Heuristics are then defined and analyzed in terms of worst-case bounds. It is shown that easily implemented highest-access-frequency-first assignment rules provide an average access time very close to optimal.The following job sequencing problems are studied: (i) single processor job sequencing with deadlines, (ii) job sequencing on m-identical processors to minimize finish time and related problems, (iii) job sequencing on 2-identical processors to minimize weighted mean flow time.Dynamic programming type algorithms are presented to obtain optimal solutions to these problems, and three general techniques are presented to obtain approximate solutions for optimization problems solvable in this way. The techniques are applied to the problems above to obtain polynomial time algorithms that generate “good” approximate solutions.A theoretical justification is given to the empirical observation that in some computing systems with a paged, 2-level storage hierarchy, long-term miss ratio is roughly independent of page size. Let MISS be the expected working-set miss ratio in the independent reference model, with expected working set size CAP pages. Now form blocks, by combining the B pages with the highest probabilities of reference into one block, the B pages with the next-highest probabilities of reference into a second block, and so on. Let MISS* be the expected working-set miss ratio when all data are moved in blocks and when the expected working set size is again CAP pages, that is, CAP/B = C blocks. It is proved that | MISS — MISS* | < (2/C) + (33/C2). Thus, if the expected working-set size (in blocks) is sufficiently large, then the miss ratios in the blocked and unblocked cases are approximately equal. This result is used to argue the approximate independence of miss ratio on page size in more realistic models of page references.The general knapsack problem is known to be NP-complete. In this paper a very special knapsack problem ia studied, namely, one with only two variables. A polynomial-time algorithm is presented and analyzed. However, it remains an open problem that for any fixed n > 2, the knapsack problem with n variables can be solved in polynomial time.Methods are described and results presented for greatly reducing the computation time for long narrow problems of the transportation problem of linear programming. The code builds on known methods with two principal innovations: a substantial reduction in the size of the tree representation of shipments, and a set of methods for calculating improved starting solutions.Kildall has developed data propagation algorithms for code optimization in a general lattice theoretic framework. In another direction, Hecht and Ullman gave a strong upper bound on the number of iterations required for propagation algorithms when the data is represented by bit vectors and depth-first ordering of the flow graph is used. The present paper combines the ideas of these two papers by considering conditions under which the bound of Hecht and Ullman applies to the depth-first version of Kildall's general data propagation algorithm. It is shown that the following condition is necessary and sufficient: Let ƒ and g be any two functions which could be associated with blocks of a flow graph, let x be an arbitrary lattice element, and let 0 be the lattice zero. Then (*) (∀ƒ,g,x) [ƒg(0) ≥ g(0) ∧ ƒ(x) ∧ x]. Then it is shown that several of the particular instances of the techniques Kildall found useful do not meet condition (*).A new algorithm for global flow analysis on reducible graphs is presented. The algorithm is shown to treat a very general class of function spaces. For a graph of e edges, the algorithm has a worst-case time bound of O(e log e) function operations. It is also shown that in programming terms, the number of operations is proportional to e plus the number of exits from program loops. Consequently a restriction to one-entry one-exit control structures guarantees linearity. The algorithm can be extended to yet larger classes of function spaces and graphs by relaxing the time bound. Examples are given of code improvement problems which can be solved using the algorithm.A program scheme which models straight-line code admitting structured variables such as arrays, lists, and queues is considered. A set of expressions is associated with a program reflecting the input-output transformations. A basic set of axioms is given and program equivalence is defined in terms of expression equivalence. Program transformations are then defined such that two programs are equivalent if and only if one program can be transformed to the other via the transformations. An application of these results to code optimization is then discussed.


A generalized multi-entrance and multipriority M/G/1
time-sharing system is dealt with. The system maintains many
separate queues, each identified by two integers, the priority
level and the entry level The arrival process of users is a
homogenous Poisson process, while service requirements are
identically distributed and have a finite second moment. Upon
arrival a user joins one of the levels, through the entry queue of
this level. In the (n, k)-th queue, where n is the
priority level and k is the entry level, a user is eligible
to a (finite or infinite) quantum of service. If the service
requirements of the user are satisfied during the quantum, the user
departs, and otherwise he is trans- ferred to the end of the (n
+ 1, k)-th queue for additional service. When a quantum of
service is completed, the highest priority nonempty level is chosen
to be served next; within this level the queues are scanned
according to the priority of their entry level, and the user at the
head of the highest priority nonempty queue is chosen to be served.
In such a priority discipline, preferred users always get an
improved service, though the service of all users is degraded in
proportion to their service requirements. Expected flow times and
expected number of waiting users are derived and then specialized
to the head-of-the-line M/G/1 priority discipline (in which quanta
have infinite length and service is uninterrupted) and to the
FBn time-sharing system. Finally, the generalized
multientrance and multipriority time-sharing discipline is
(numerically) compared with several other time-sharing systems.A new treatment of the boundary conditions of diffusion
approximations for interconnected queueing systems is presented.
The results have applications to the study of the performance of
multiple-resource computer systems. In this approximation method,
additional equations to represent the behavior of the queues when
they are empty are introduced. This reduces the dependence of the
model on heavy traffic assumptions and yields certain results which
would be expected from queueing or renewal theory. The accuracy of
the approach is evaluated by comparison with certain known exact or
numerical results.

A parsing method for strict deterministic grammars is presented and a technique for using it to parse any deterministic language is indicated. An important characterization of the trees of strict deterministic grammars is established. This is used to prove iteration theorems for (strict) deterministic languages, and hence proving that certain sets are not in these families becomes comparatively straightforward. It is shown that every strict deterministic grammar is LR(0) and that any strict deterministic grammar is equivalent to a bounded right context (1, 0) grammar. Thus rigorous proofs that the families of deterministic, LR (k), and bounded right context languages are coextensive are presented for the first time.This paper describes an efficient algorithm to determine whether an arbitrary graph G can be embedded in the plane. The algorithm may be viewed as an iterative version of a method originally proposed by Auslander and Parter and correctly formulated by Goldstein. The algorithm used depth-first search and has O(V) time and space bounds, where V is the number of vertices in G. An ALGOL implementation of the algorithm succesfully tested graphs with as many as 900 vertices in less than 12 seconds.Given a graph H with E edges and N nodes, a graph G is sought such that H is the line graph of G, if G exists. The algorithm does this within the order of E steps, in fact in E + O(N) steps. This algorithm is optimal in its complexity.A search procedure is given which will determine whether Hamilton paths or circuits exist in a given graph, and will find one or all of them. A combined procedure is given for both directed and undirected graphs. The search consists of creating partial paths and making deductions which determine whether each partial path is a section of any Hamilton path whatever, and which direct the extension of the partial paths.An algorithm combining Gaussian elimination with the modified Gram-Schmidt (MGS) procedure is given for solving the linear least squares problem. The method is based on the operational efficiency of Gaussian elimination for LU decompositions and the numerical stability of MGS for unitary decompositions and is designed for slightly overdetermined linear systems.From Richardson's undecidability results, it is shown that the predictive “there exists a real number r such that G(r) = 0” is recursively undecidable for G(x) in a class of functions which involves polynomials and the sine function. The deduction follows that the convergence of a class of improper integrals is recursively undecidable.The key concepts for this automated theorem-proving paper are those of Horn set and strictly-unit refutation. A Horn set is a set of clauses such that none of its members contains more than one positive literal. A strictly-unit refutation is a proof by contradiction in which no step is justified by applying a rule of inference to a set of clauses all of which contain more than one literal. Horn sets occur in many fields of mathematics such as the theory of groups, rings, Moufang loops, and Henkin models. The usual translation into first-order predicate calculus of the axioms of these and many other fields yields a set of Horn clauses. The striking feature of the Horn property for finite sets of clauses is that its presence or absence can be determined by inspection. Thus, the determination of the applicability of the theorems and procedures of this paper is immediate.In Theorem 1 it is proved that, if S is an unsatisfiable Horn set, there exists a strictly-unit refutation of S employing binary resolution alone, thus eliminating the need for factoring; moreover, one of the immediate ancestors of each step of the refutation is in fact a positive unit clause. A theorem similar to Theorem 1 for paramodulation-based inference systems is proven in Theorem 3 but with the inclusion of factoring as an inference rule. In Section 3 two reduction procedures are discussed. For the first, Chang's splitting, a rule is provided to guide both the choice of clauses and the way in which to split. The second reduction procedure enables one to refute a Horn set by refuting but one of a corresponding family of simpler subproblems.A deductive system is described which combines aspects of resolution (e.g. unification and the use of Skolem functions) with that of natural deduction and whose performance compares favorably with the best predicate calculus theorem provers.To prove really difficult theorems, resolution principle programs need to make better inferences and to make them faster. An approach is presented for taking advantage of the structure of some special theories. These are theories with simplifiers, commutativity, and associativity, which are valuable concepts to build in, since they so frequently occur in important theories, for example, number theory (plus and times) and set theory (union and intersection). The object of the approach is to build in such concepts in a (refutation) complete, valid, efficient (in time) manner by means of a “natural” notation and/or new inference rules. Some of the many simplifiers that can be built in are axioms for (left and right) identities, inverses, and multiplication by zero.As for results, commutativity is built in by a straightforward modification to the unification (matching) algorithm. The results for simplifiers and associativity are more complicated. These theoretical results can be combined with one another and/or extended to either C-linear refutation completeness or theories with partial ordering, total ordering, or sets. How these results can serve as the basis of practical computer programs is discussed.The problem is to calculate a simple zero of a nonlinear function ƒ by iteration. There is exhibited a family of iterations of order 2n-1 which use n evaluations of ƒ and no derivative evaluations, as well as a second family of iterations of order 2n-1 based on n — 1 evaluations of ƒ and one of ƒ′. In particular, with four evaluations an iteration of eighth order is constructed. The best previous result for four evaluations was fifth order.It is proved that the optimal order of one general class of multipoint iterations is 2n-1 and that an upper bound on the order of a multipoint iteration based on n evaluations of ƒ (no derivatives) is 2n.It is conjectured that a multipoint iteration without memory based on n evaluations has optimal order 2n-1.Arrays are among the best understood and most widely used data structures. Yet even now, there are no satisfactory techniques for handling algorithms involving extendible arrays (where, e.g., rows and/or columns can be appended dynamically). In this paper, the problem of allocating storage for extendible arrays is examined in the light of the author's earlier work on data graphs and addressing schemes. A formal analog of the assertion that simplicity of array extension precludes simplicity of traversal (marching along rows/columns) is proved. Two strategies for constructing extendible realizations of arrays are formulated, and certain inherent limitations of such realizations are established.The central notion in a replacement system is one of a transformation on a set of objects. Starting with a given object, in one “move” it is possible to reach one of a set of objects. An object from which no move is possible is called irreducible. A replacement system is Church-Rosser if starting with any object a unique irreducible object is reached. A generalization of the above notion is a replacement system (S, ⇒, ≡), where S is a set of objects, ⇒ is a transformation, and ≡ is an equivalence relation on S. A replacement system is Church-Rosser if starting with objects equivalent under ≡, equivalent irreducible objects are reached. Necessary and sufficient conditions are determined that simplify the task of testing if a replacement system is Church-Rosser. Attention will be paid to showing that a replacement system (S, ⇒, ≡) is Church-Rosser using information about parts of the system, i.e. considering cases where ⇒ is ⇒1 ∪ ⇒2, or ≡ is (≡1 ∪ ≡2)*.The fundamentals of Post algebras are presented and Post and Boolean functions are examined. A functional representation is developed that facilitates the comparison of Post and Boolean algebras. Based on this representation, relationships between finite, higher-order (that is, more than 2-valued) Boolean algebras and functions in these algebras and finite, higher-order Post algebras and their corresponding functions are develop.
The set covering problem is considered and an efficient procedure for finding an irredundant cover is presented. For an m × n cover table, the execution time of the procedure is, in the worst case, proportional to mn. Methods are suggested for obtaining alternate irredundant covers based on an initially obtained irredundant cover. The basic cost-independent algorithm is heuristically extended to consider costs so that a reduced-cost irredundant cover can be obtained. A summary of some computational experience is presented, which indicates that the procedure is fast and applicable to large problems.The binary relation is often a useful mathematical structure for representing simple relationships whose essence is a directed connection. To better aid in interpreting or storing a binary relation we suggest a diclique decomposition. A diclique of a binary relation R is defined as an ordered pair (I, O) such that I × O ⊆ R and (I, O) is maximal. In this paper, an algorithm is described for determining the dicliques of a binary relation; it is proved that the set of such dicliques has a nice algebraic structure. The algebraic structure is used to show how dicliques can be coalesced, the relationship between cliques and dicliques is discussed, and an algorithm for determining cliques from dicliques is described.It is established that if G is a reducible flow graph, then edge (n, m) is backward (a back latch) if and only if either n = m or m dominates n in G. Thus, the backward edges of a reducible flow graph are unique.Further characterizations of reducibility are presented. In particular, the following are equivalent: (a) G = (N, E, n0) is reducible. (b) The “dag” of G is unique. (A dag of a flow graph G is a maximal acyclic flow graph which is a subgraph of G.) (c) E can be partitioned into two sets E1 and E2 such that E1 forms a dag D of G and each (n, m) in E2 has n = m or m dominates n in G. (d) Same as (c), except each (n, m) in E2 has n = m or m dominates n in D. (e) Same as (c), except E2 is the back edge set of a depth-first spanning tree for G. (f) Every cycle of G has a node which dominates the other nodes of the cycle.Finally, it is shown that there is a “natural” single-entry loop associated with each backward edge of a reducible flow graph.The efficiency of an information storage technique based on binary comparisons is analyzed. Generating functions are applied to finding the mean and variance of the number of comparisons needed to retrieve one item from a store of n items. Surprisingly, the variance approaches 7 - 2/3&pgr;2 for large n.Christofides' algorithm for finding the chromatic number of a graph is improved both in speed and memory space by using a depth-first search rule to search for a shortest path in a reduced subgraph tree.This paper deals with a combinatorial minimization problem arising from studies on multimodule memory organizations. Instead of searching for an optimum solution, a particular solution is proposed and it is demonstrated that it is close to optimum. Lower bounds for the objective functions are obtained and compared with the corresponding values of the particular solution. The maximum percentage deviation of this solution from optimum is also established.An attempt is made to apply information-theoretic computational complexity to meta-mathematics. The paper studies the number of bits of instructions that must be given to a computer for it to perform finite and infinite tasks, and also the time it takes the computer to perform these tasks. This is applied to measuring the difficulty of proving a given set of theorems, in terms of the number of bits of axioms that are assumed, and the size of the proofs needed to deduce the theorems from the axioms.Let h be a recursive function. A partial recursive function &psgr; is i.o. (infinitely often) h-complex if every program for &psgr; requires more than h(&khgr;) steps to compute &psgr;(&khgr;) for infinitely many inputs &khgr;. A more stringent notion is that of &psgr; being a.e. (almost everywhere) h-complex: &psgr; is a.e. h-complex if every program for &psgr; requires more than h(&khgr;) steps to compute &psgr;(&khgr;) for all but finitely many inputs &khgr;.These two definitions of h-complex functions do not yield the same theorems. Although it is possible to prove of every i.o. h-complex recursive function that it is i.o. h-complex, it is not possible to prove of every a.e. h-complex recursive function that it is a.e. h-complex. Similarly, recursive functions not i.o. h-complex can be proven to be such, but recursive functions not a.e. h-complex cannot be so proven.The construction of almost everywhere complex recursive functions appears much more difficult than the construction of infinitely often complex recursive functions. There have been found no “natural” examples of recursive functions requiring more than polynomial time for all but finitely many inputs. It is shown that from a single example of a moderately a.e. complex recursive function, one can obtain a.e. very complex recursive functions.The general problem of finding minimal programs realizing given “program descriptions” is considered, where program descriptions may be of finite or infinite length and may specify arbitrary program properties. The problem of finding minimal programs consistent with finite or infinite input-output lists is a special case (for infinite input-output lists, this is a variant of E. M. Gold's function identification problem). Although most program minimization problems are not recursively solvable, they are found to be no more difficult than the problem of deciding whether any given program realizes any given description, or the problem of enumerating programs in order of nondecreasing length (whichever is harder). This result is formulated in terms of k-limiting recursive predicates and functionals, defined by repeated application of Gold's limit operator. A simple consequence is that the program minimization problem is limiting recursively solvable for finite input-output lists and 2-limiting recursively solvable for infinite input-output lists, with weak assumptions about the measure of program size. Gold regarded limiting function identification (more generally, “black box” identification) as a model of inductive thought. Intuitively, iterated limiting identification might be regarded as higher-order inductive inference performed collectively by an ever-growing community of lower order inductive inference machines.A phrase structure grammar is called context-limited if there exists a partial ordering on its alphabet such that any letter on the left of any production is less than some letter on the right of the same production. It is proved that context-limited grammars are equivalent to context-free grammars, the equivalence including ambiguity. The notion of ambiguity in phrase structure grammars is discussed, and a new formal model for ambiguity, based on directed plane graphs with labeled edges, is outlined and compared with other models.The problem of computing a desired function value to within a prescribed tolerance can be formulated in the following two distinct ways: Formulation I: Given x and ∈ > 0, compute f(x) to within ∈. Formulation II: Given only that x is in a closed interval X, compute a subinterval of the image, f(X) = {f(x) : x ∈ X}. The first formulation is applicable when x is known to arbitrary accuracy. The second formulation is applicable when x is known only to a limited accuracy, in which case the tolerance is prescribed albeit indirectly by the interval X, and one must be satisfied with all or part of the set f(X) of possible function values.Elsewhere the author has presented an efficient solution to Formulation I for any rational f and many nonrational f. B. A. Chartres has presented an efficient solution to Formulation II for a very restricted class of rational f and for a few nonrational f.In this paper a solution to Formulation II for the arbitrary nonconstant rational f is presented. By bounding df/dx away from zero over some subset of X, it is shown how to reduce Formulation II to Formulation I, yielding the solution given here.In generalizing to vector-valued functions f, Chartres has solved Formulation II only for rational f which satisfy a linear system of equations, while this paper presents a solution for arbitrary non-degenerate rational vector-valued f.Quite often explicit information about the behavior of a queue over a fairly short period is wanted. This requires solving the nonequilibrium solution of the queue-length distribution, which is usually quite difficult mathematically. The first half of Part II shows how the diffusion process approximation can be used to answer this question. A transient solution is obtained for a cyclic queueing model using the technique of eigenfunction expansion. The second half of Part II applies the earlier results of Part I to modeling and performance problems of a typical multiprogrammed computer system. Such performance measures as utilization, throughput, response time and its distribution, etc., are discussed in some detail.A communication system consisting of a number of buffered input terminals connected to a computer by a single channel is analyzed. The terminals are polled in sequence and the data is removed from the terminal's buffer. When the buffer has been emptied, the channel, for an interval of randomly determined length, is used for system overhead and/or to transmit data to the terminals. The system then continues with a poll of the next terminal. The stationary distributions of the length of the waiting line and the queueing delay are calculated for the case of identically distributed input processes.The amount of store necessary to operate a dynamic storage allocation system, subject to certain constraints, with no risk of breakdown due to storage fragmentation, is considered. Upper and lower bounds are given for this amount of store, both of them stronger than those established earlier. The lower bound is the exact solution of a related problem concerning allocation of blocks whose size is always a power of 2.It is shown that the multisalesmen problem can be solved by solving the standard traveling salesman problem on an expanded graph. The expanded graph has m — 1 more nodes than the original graph where m is the number of salesmen available at the base.An algorithm is proposed for the bounded variable pure integer programming problem which treats general integer variables directly in an implicit enumeration procedure closely related to that advanced by Balas and Geoffrion for binary programming problems. Means of obtaining near optimum solutions through a slight modification of the algorithm are discussed. Techniques which use bounds on variables to improve algorithmic efficiency are developed and examined computationally. Further computational results indicate that direct treatment of general integer variables is significantly more effective than binary expansion.New lower bounds on the minimal condition numbers of a matrix with respect to both one-sided and two-sided scaling by diagonal matrices are obtained. These bounds improve certain results obtained by F. L. Bauer.
Previous studies of heuristic search techniques have usually considered situations for which the search space could be represented as a tree, which limits the applicability of the results obtained. Here Kowalski is followed and a more general formulation in terms of derivation graph is offered; the graphs correspond rather naturally to the search problems arising in automatic theoremproving and other areas. We consider a family of search procedures controlled by evaluation functions of a very general sort, having the form ƒDgr;(x, Lk), where Lk is that portion of the graph generated thus far by the procedure, and the node x is a candidate for incorporation into Lk. Completeness and minimality results are obtained for a number of procedures in this family, including methods analogous to those of Moore, Dijkstra, and Pohl.A procedure is defined for deriving from any statement S an infinite sequence of statements S0, S1, S2, S3, ··· such that: (a) if there exists an i such that Si is unsatisfiable, then S is unsatisfiable; (b) if S is unsatisfiable, then there exists an i such that Si is unsatisfiable; (c) for all i the Herbrand universe of Si is finite; hence, for each i the satisfiability of Si is decidable. The new algorithms are then based on the idea of generating successive Si in the sequence and testing each Si for satisfiability. Each element in the class of new algorithms is complete.It is shown that arithmetic expressions with n ≥ 1 variables and constants; operations of addition, multiplication, and division; and any depth of parenthesis nesting can be evaluated in time 4 log2n + 10(n - 1)/p using p ≥ 1 processors which can independently perform arithmetic operations in unit time. This bound is within a constant factor of the best possible. A sharper result is given for expressions without the division operation, and the question of numerical stability is discussed.In this paper it is shown that whatever the length function employed, the problem of finding the shortest program for a decision table with two (or more) entries is not recursively solvable (whereas for decision tables with a single entry the problem is solvable for some length functions and unsolvable for others). Moreover, it is shown that there is a pair of finite sets of programs and a single entry E such that the shortest program for the decision table formed by adding a single additional entry to E is in all cases in one of the two sets, but it is undecidable in which. Some consequences of these results are then presented, such as showing that for a wide range of restrictions the results remain true, even when the repertoire of possible programs for a decision table is narrowed by only considering programs which meet certain restrictions.A solution is presented for the following problem: Determine a procedure that produces, for each full trio L of context-free languages (more generally, each trio of r.e. languages), a family of context-free (phrase structure) grammars which (a) defines L, (b) is simple enough for practical and theoretical purposes, and (c) in most cases is a subfamily of a well-known family of context-free (phrase structure) grammars for L if such a well-known family exists. (A full trio (trio) is defined to be a family of languages closed under homomorphism (&egr;-free homomorphism), inverse homomorphism, and intersection with regular sets.)The key notion in the paper is that of a grammar schema. With each grammar schema there is associated a family of interpretations. In turn, each interpretation of a grammar schema gives rise to a phrase structure grammar. Given a full trio (trio) L of context-free (r.e.) languages, one constructs a grammar schema whose interpretations (&egr;-limited interpretations) then give rise to the desired family of grammars for L.A modification of linked lists is presented which permits searching almost as efficiently as a pure binary search. The method depends on using consecutive memory locations for consecutive list elements whenever possible.Some elementary mathematical properties of term matching document retrieval systems are developed. These properties are used as a basis for a new file organization technique. Some of the advantages of this new method are (1) the key-to-address transformation is easily determined; (2) the documentary information is stored only once in the file; (3) the file organization allows the use of various matching functions and thresholds; and (4) the dimensionality of the transform is easily expanded to accommodate various sized data bases.We consider a set of static files or inventories, each consisting of the same number of entries, each entry a binary word of the same fixed length selected (with replacement) from the set of all binary sequences of that length, and the entries in each file sorted into lexical order. We also consider several retrieval questions of interest for each such file. One is to find the value of the jth entry, another to find the number of entries of value less than k.When a binary representation of such a file is stored in computer memory and an algorithm or machine which knows only the file parameters (i.e. number of entries, number of possible values per entry) accesses some of the stored bits to answer a retrieval question, the number of bits stored and the number of bits accessed per retrieval question are two cost measures for the storage and retrieval task which have been used by Minsky and Papert. Bits stored depends on the representation chosen: bits accessed also depends on the retrieval question asked and on the algorithm used.We give firm lower bounds to minimax measures of bits stored and bits accessed for each of four retrieval questions, and construct representations and algorithms for a bit-addressable machine which come within factors of two or three of attaining all four bounds at once for files of any size. All four factors approach one for large enough files.An algorithm is proposed for calculating the eigenvectors of a diagonally dominant matrix all of whose elements are known to high relative accuracy. Eigenvectors corresponding to pathologically close eigenvalues are treated by computing the invariant subspace that they span. If the off-diagonal elements of the matrix are sufficiently small, the method is superior to standard techniques, and indeed it may produce a complete set of eigenvectors with an amount of work proportional to the square of the order of the matrix. An analysis is given of the effects of perturbations in the matrix on the eigenvectors.The solution of a set of m linear equations with a non-Hermitian Toeplitz associated matrix is considered. Presently available fast algorithms solve this set with 4m2 “operations” (an “operation” is defined here as a set of one addition and one multiplication). An improved algorithm requiring only 3m2 “operations” is presented.Given r numbers s1, ···, sr, algorithms are investigated for finding all possible combinations of these numbers which sum to M. This problem is a particular instance of the 0-1 unidimensional knapsack problem. All of the usual algorithms for this problem are investigated in terms of both asymptotic computing times and storage requirements, as well as average computing times. We develop a technique which improves all of the dynamic programming methods by a square root factor. Empirical studies indicate this new algorithm to be generally superior to all previously known algorithms. We then show how this improvement can be incorporated into the more general 0-1 knapsack problem obtaining a square root improvement in the asymptotic behavior. A new branch and search algorithm that is significantly faster than the Greenberg and Hegerich algorithm is also presented. The results of extensive empirical studies comparing these knapsack algorithms are givenThe mth degree Bernstein polynomial approximation to a function ƒ defined over [0, 1] is ∑m&mgr;=0 ƒ(&mgr;/m)&pgr;&mgr;(s), where the weights &pgr;&mgr;(s) are binomial density functions. The Bernstein approximations inherit many of the global characteristics of ƒ, like monotonicity and convexity, and they always are at least as “smooth” as ƒ, where “smooth” refers to the number of undulations, the total variation, and the differentiability class of ƒ. Historically, their relatively slow convergence in the L∞-norm has tended to discourage their use in practical applications. However, in a large class of problems the smoothness of an approximating function is of greater importance than closeness of fit. This is especially true in connection with problems of computer-aided geometric design of curves and surfaces where aesthetic criteria and the intrinsic properties of shape are major considerations. For this latter class of problems, P. Bézier of Renault has successfully exploited the properties of parametric Bernstein polynomials. The purpose of this paper is to analyze the Bézier techniques and to explore various extensions and generalizations. In a sequel, the authors consider the extension of the results contained herein to free-form curve and surface design using polynomial splines. These B-spline methods have several advantages over the techniques described in the present paper.Chebyshev approximation on an interval and closed subsets by a Haar subspace are considered. The closeness of best approximations on subsets to the best approximation on the interval is examined. It is shown that under favorable conditions the difference is O((density of the subset)2), making it unnecessary to use very large finite subsets to get good approximations on the interval.In a 1967 publication, D. P. Gaver studied a probabilistic model of a multiprogramming computer system. His results have been utilized recently by a number of authors. However, we have observed that Gaver's results contain inconsistencies. These inconsistencies are discussed in detail and a correction suggested and verified through an independent derivation.The practical value of queueing theory in engineering applications such as in computer modeling has been limited, since the interest in mathematical tractability has almost always led to an oversimplified model. The diffusion process approximation is an attempt to break away from the vogue in queueing theory.The present paper introduces a vector-valued normal process and its diffusion equation in order to obtain an approximate solution to the joint distribution of queue lengths in a general network of queues. In this model, queueing processes of various service stations which interact with each other are approximated by a vector-valued Wiener process with some appropriate boundary conditions. Some numerical examples are presented and compared with Monte Carlo simulation results. A companion paper, Part II, discusses transient solutions via the diffusion approximation.Considerable effort has been invested in devising and analyzing sequencing rules for multiprogrammed or time-shared systems. A much studied discipline of this kind is the so-called system with feedback to lower priority queues. This discipline contains many parameters, in general, which must be fixed in order to achieve the desired waiting-time performance of the discipline. In this paper the problem of synthesizing a system of the above type is solved, by setting parameter values so that prespecified waiting time criteria are satisfied, assuming Poisson arrival and general service time parameters are known.A loop system with N buffered terminals sharing a common time-multiplexed channel is studied. The service discipline is prescribed by a permutation &phgr; = (&phgr;(1), ···, &phgr;(N)) which gives the relative ranking of the terminals. Data from the ith terminal may be buffered at an intermediate terminal—its transmission to the CPU interrupted—if there is a conflict with data from a terminal with higher ranking. It is shown how such systems may be analyzed and how the system performance, as measured by average response time, may be improved by imposing a suitable priority discipline.
A statistical model is presented which is useful in the solution of a Fredholm integral equation of the first kind and equivalent to one proposed by Strand and Westwater. The model and the related problem presented here are familiar to statisticians from the study of regression analysis and are essentially, “(GLM): Find the best linear unbiased estimate of &bgr; given the observation y which satisfies y = H&bgr; + e, e distributed as N (0, &Ggr;).” Here y, &bgr; c are vectors, H is an (m + n) × k matrix, and &Ggr; is a certain (m + n) × (m + n) positive definite, known matrix. The main content of the paper is that (GLM) provides an equivalent way of considering the problem of Strand and Westwater and is to be preferred by virtue of the rich store of results available for the study of (GLM) and its intrinsic geometric nature.Efficiently computable a posteriori error bounds are attained by using a posteriori models for bounding roundoff errors in the basic floating-point operations. Forward error bounds are found for inner product and polynomial evaluations. An analysis of the Crout algorithm in solving systems of linear algebraic equations leads to sharper backward a posteriori bounds. The results in the analysis of the iterative refinement give bounds useful in estimating the rate of convergence. Some numerical experiments are included.Compute-output processing times are determined for n-segment jobs that are preloaded into main storage and processed with overlap. A queueing model with tandem servers is utilized for the performance analysis. In particular, the solution presented involves determination of the transient response for a batched arrival of n segments to be processed through two stages of tandem service with unlimited output buffering. The performance results provide insight into conditions arising in systems consisting of a single CPU and I/O channel with overlap capabilities. Two cases, single-segment overlap and unlimited overlap, are considered. Segmental compute and output (or input) service times are taken to be exponentially distributed; however, the approach is not limited to the exponential case if service is independent. The ratio of mean output time to mean compute time is varied to explore the full range between compute-bound and output-bound extremes. Final results are presented as relative gain over sequential processing.The topic of this paper is a probabilistic analysis of demand paging algorithms for storage hierarchies. Two aspects of algorithm performance are studied under the assumption that the sequence of page requests is statistically independent: the page fault probability for a fixed memory size and the variation of performance with memory. Performance bounds are obtained which are independent of the page request probabilities. It is shown that simple algorithms exist which yield fault probabilities close to optimal with only a modest increase in memory.A cost is defined for demand paging algorithms with respect to a formal stochastic model of program behavior. This cost is shown to exist under rather general assumptions, and a computational procedure is given which makes it possible to determine the optimal cost and optimal policy for moderate size programs, when the formal model is known and not time dependent. In this latter case it is shown that these computational procedures may be extended to larger programs to obtain arbitrarily close approximations to their optimal policies. In previous models either unwarranted information is assumed beyond the formal model, or the complete stochastic nature of the model is not taken into account.A class of demand paging algorithms for some two-level memory hierarchies is analyzed. The typical memory hierarchy is comprised of the core and a backing device. A distance matrix characterizes the properties of the latter device. The sequence of address references directed to the hierarchy by the CPU and channels is modeled as a Markov process. A compact expression for the mean time required to satisfy the page demands is derived and this expression provides the basis for some optimization problems concerning partitionings and rearrangements of pages in the backing device. In connection with these problems, a class of random processes is defined in terms of an ordering property of a joint probability matrix which is central to memory hierarchies. Three results are given on the ordering property, its relation specifically to partitionings inherent in hierarchies and the problem of optimal rearrangements. Finally, for such a class of ordered processes, certain results due to the author are specialized to yield the solution to the problem of optimal rearrangement of pages on an assembly of magnetic bubble loops.An analytic model of a single processor scheduling problem is investigated. The scheduling objective is to minimize the total loss incurred by a finite number of initially available requests when each request has an associated linear loss function. The assumptions of the model are that preemption is allowed with negligible loss of processor time, and that the distribution of actual service times is known for each class of requests. A request is associated with a class by any of its characteristics except its actual service time. A contrived example demonstrates that one reasonable scheduling rule does not always minimize expected total loss. The major results of the paper are the definition of a new scheduling rule based on the known service time distributions, and the proof that expected total loss is always minimized by using this new rule. Brief consideration is given to generalizations of the model in which new requests arrive randomly, and preemption requires a non-negligible amount of processor time.Many compilers for higher order languages attempt to translate the source code into “good” object code. Cocke and Schwartz have described an algorithm for discovering when the computation of an expression is redundant (common), and when it can be moved to a less frequently executed region of the program. The present paper includes a tutorial presentation of their basic methods, along with a number of improvements and extensions. These include simplification of the solution method, to save a pass; extension of it to treat the safety constraint, handle multi-entry regions directly, detect additional commonality and code motion after unsafe code motion, and decide where moved code should be put; and combination of the algorithms for commonality and the dead condition, making use of important work of Ken Kennedy.The methods here applied to collecting information for use in code optimization include general algorithms for solving a set of linear equations in Boolean algebra. The algorithms are most useful when the coefficient matrix is sparse.A technique is introduced for analyzing simulations of stochastic systems in the steady state. From the viewpoint of classical statistics, questions of simulation run duration and of starting and stopping simulations are addressed. This is possible because of the existence of a random grouping of observations which produces independent identically distributed blocks from the start of the simulation. The analysis is presented in the context of the general multiserver queue, with arbitrarily distributed interarrival and service times. In this case, it is the busy period structure of the system which produces the grouping mentioned above. Numerical illustrations are given for the M/M/1 queue. Statistical methods are employed so as to obtain confidence intervals for a variety of parameters of interest, such as the expected value of the stationary customer waiting time, the expected value of a function of the stationary waiting time, the expected number of customers served and length of a busy cycle, the tail of the stationary waiting time distribution, and the standard deviation of the stationary waiting time. Consideration is also given to determining system sensitivity to errors and uncertainty in the input parameters.A technique for simulating GI/G/s queues is shown to apply to simulations of discrete and continuous-time Markov chains. It is possible to address questions of simulation run duration and of starting and stopping simulations because of the existence of a random grouping of observations which produces independent identically distributed blocks from the start of the simulation. This grouping allows confidence intervals to be obtained for a general function of the steady-state distribution of the Markov chain. The technique is illustrated with simulation of an (s, S) inventory model in discrete time and the classical repairman problem in continuous time. Consideration is also given to determining system sensitivity to errors and uncertainty in the input parameters.The model elimination (ME) and resolution algorithms for mechanical theorem-proving were implemented so as to maximize shared features. The identical data structures and large amount of common programming permit meaningful comparisons when the two programs are run on standard problems. ME does better on some classes of problems, and resolution better on others. The depth-first search strategy used in this ME implementation affects the performance profoundly. Other novel features in the implementation are new control parameters to govern extensions, and modified rules for generating and rejecting chains. The resolution program incorporates unit preference and set-of-support. An appendix reproduces the steps of a machine-derived ME refutation.Branch-and-bound implicit enumeration algorithms for permutation problems (discrete optimization problems where the set of feasible solutions is the permutation group Sn) are characterized in terms of a sextuple (Bp S,E,D,L,U), where (1) Bp is the branching rule for permutation problems, (2) S is the next node selection rule, (3) E is the set of node elimination rules, (4) D is the node dominance function, (5) L is the node lower-bound cost function, and (6) U is an upper-bound solution cost. A general algorithm based on this characterization is presented and the dependence of the computational requirements on the choice of algorithm parameters, S, E, D, L, and U is investigated theoretically. The results verify some intuitive notions but disprove others.Let r be the total number of cycles required to complete a compromise merge of a given number of initial strings. Define row vectors mr-j and dj whose components represent the number and length respectively of strings at the end of the jth cycle of the merge. It is shown in this paper that there are asymptotic approximations to these vectors, which enables one to compute their respective components directly. Consequently, the number of cycles r can be computed directly, as in the case of the balanced merge.A family of new algorithms is given for evaluating the first m derivatives of a polynomial. In particular, it is shown that all derivatives may be evaluated in 3n - 2 multiplications. The best previous result required 1/2n(n + 1) multiplications. Some optimality results are presented.The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of “edit operations” needed to change the one string into the other. The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings. Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings.

A reasonably efficient procedure for testing pairs of directed graphs for isomorphism is important in information retrieval and other application fields in which structured data have to be matched. One such procedure, a backtrack procedure based on a representation of directed graphs by linear formulas, is described. A procedure for finding a partial subdigraph of a digraph that is isomorphic to a given digraph is also described.The class of rooted trees or arborescences used for modeling hierarchical classification and indexing procedures is considered. An entropy function measuring the complexity of the paths from the root to the terminal vertices is defined. Upper and lower bounds are found for the values of this function, and it is shown to be additive with respect to a tree product operation defined here. The results are extended to the case when the terminal vertices are weighted, and the path entropy function is compared to the information function defined by C. Picard for questionnaires.An assertion that Dijkstra's algorithm for shortest paths (adapted to allow arcs of negative weight) runs in O(n3) steps is disproved by showing a set of networks which take O(n2n) steps.An error in Yen's algorithm is pointed out, and an alternative is offered which will produce correct results.A bound on the relative error in floating-point addition using a single-precision accumulator with guard digits is derived. It is shown that even with a single guard digit, the accuracy can be almost as good as that using a double-precision accumulator. A statistical model for the roundoff error in double-precision multiplication and addition is also derived. The model is confirmed by experimental measurements.This paper deals with a technique for proving that certain problems of numerical analysis are numerically unsolvable. So that only methods which are natural for dealing with analytic problems may be presented, notions from recursive function theory have been avoided. Instead, the number of necessary function evaluations is taken as the measure of computational complexity. The role of topological concepts in the study of computability is examined. Last, a topological result is used to prove that a simple initial-value problem is numerically unsolvable.An algorithm of W. F. Trench is generalized to apply to the inversion of block matrices of Toeplitz form.This paper deals with a single-server station (a computer) where each customer's demand comprises an independent random number of jobs (programs). Under certain assumptions, two cyclic disciplines are mathematically analyzed: (a) continuous job service—a round-robin discipline where the quantum's length is distributed as the service requirement of a job; (b) intermittent job service—a double round-robin discipline—in the first instance in terms of the jobs within the customer's demand, and in the second in terms of the customer himself.The problem considered is how to place records on a secondary storage device to minimize average retrieval time, based on a knowledge of the probability for accessing the records. Theorems are presented for two limiting cases. A numerical example for an intermediate case is also given.It is the object of this paper to study the topological properties of finite graphs that can be embedded in the n-dimensional integral lattice (denoted Nn). The basic notion of deletability of a node is first introduced. A node is deletable with respect to a graph if certain computable conditions are satisfied on its neighborhood. An equivalence relation on graphs called reducibility and denoted by “∼” is then defined in terms of deletability, and it is shown that (a) most important topological properties of the graph (homotogy, homology, and cohomology groups) are ∼-invariants; (b) for graphs embedded in N3, different knot types belong to different ∼-equivalence classes; (c) it is decidable whether two graphs are reducible to each other in N2 but this problem is undecidable in Nn for n ≥ 4. Finally, it is shown that two different methods of approximating an n-dimensional closed manifold with boundary by a graph of the type studied in this paper lead to graphs whose corresponding homology groups are isomorphic.The generalized feedback shift register pseudorandom number algorithm has several advantages over all other pseudorandom number generators. These advantages are: (1) it produces multidimensional pseudorandom numbers; (2) it has an arbitrarily long period independent of the word size of the computer on which it is implemented; (3) it is faster than other pseudorandom number generators; (4) the “same” floating-point pseudorandom number sequence is obtained on any machine, that is, the high order mantissa bits of each pseudorandom number agree on all machines— examples are given for IBM 360, Sperry-Rand-Univac 1108, Control Data 6000, and Hewlett-Packard 2100 series computers; (5) it can be coded in compiler languages (it is portable); (6) the algorithm is easily implemented in microcode and has been programmed for an Interdata computer.The theoretical limitations on the orders of equidistribution attainable by Tausworthe sequences are derived from first principles and are stated in the form of a criterion to be achieved. A second criterion, extending these limitations to multidimensional uniformity, is also defined. A sequence possessing both properties is said to be asymptotically random as no other sequence of the same period could be more random in these respects.An algorithm is presented which, for any Tausworthe sequence based on a primitive trinomial over GF(2), establishes how closely or otherwise the conditions necessary for the criteria are achieved. Given that the necessary conditions are achieved, the conditions sufficient for the first criterion are derived from Galois theory and always apply. For the second criterion, however, the period must be prime.An asymptotically random 23-bit number sequence of astronomic period, 2607 - 1, is presented. An initialization program is required to provide 607 starting values, after which the sequence can be generated with a three-term recurrence of the Fibonacci type. In addition to possessing the theoretically demonstrable randomness properties associated with Tausworthe sequences, the sequence possesses equidistribution and multidimensional uniformity properties vastly in excess of anything that has yet been shown for conventional congruentially generated sequences. It is shown that, for samples of a size it is practicable to generate, there can exist no purely empirical test of the sequence as it stands capable of distinguishing between it and an ∞-distributed sequence. Bounds for local nonrandomness in respect of runs above (below) the mean and runs of equal numbers are established theoretically.The claimed randomness properties do not necessarily extend to subsequences, though it is not yet known which particular subsequences are at fault.Accordingly, the sequence is at present suggested only for simulations with no fixed dimensionality requirements.This paper gives an algorithm for efficiently ordering the terms and factors of a set (or Boolean) expression so that the work required for the symbolic expansion of the expression according to the distributive law is minimized. Formulas are given for computing the measure of work associated with any ordering of an expression. It is shown from these formulas that reordering the factors of the intersections can possibly reduce the cost of expansion, but this cost is invariant with respect to the ordering of the terms of the unions. A simple ordering algorithm is given for quickly determining an optimal (not necessarily unique) ordering of an intersection and a union. When this algorithm is applied to all intersections and unions in an expression, the resulting order is shown to minimize the total work over all possible orderings. Thus it is easy to establish an optimal ordering for an expression and estimate the machine time for its symbolic expansion before doing the expansion.A class of (monadic) functional schemas which properly includes “Ianov” flowchart schemas is defined. It is shown that the termination, divergence, and freedom problems for functional schemas are decidable. Although it is possible to translate a large class of non-free functional schemas into equivalent free functional schemas, it is shown that in general this cannot be done. It is also shown that the equivalence problem for free functional schemas is decidable. Most of the results are obtained from well-known results in formal languages and automata theory.The purpose of this work is to find a method for building loopless algorithms for listing combinatorial items, like partitions, permutations, combinations. Gray code, etc. Algorithms for the above sequence are detailed in full.The phenomenon of maximal parallelism is investigated in the framework of a class of parallel program schemata. Part I presents the basic properties of this model. Two types of equivalence relation on computations are presented, to each of which there corresponds a concept of determinacy and equivalence for schemata. The correspondence between these relations is shown and related to other properties of schemata. Then the concept of maximal parallelism using one of the relations as a basis is investigated. A partial order on schemata is defined which relates their inherent parallelism. The results presented are especially concerned with schemata which are maximal with respect to this order, i.e. maximally parallel schemata. Several other properties are presented and shown to be equivalent to the property of maximal parallelism. It is then shown that for any schema of a certain class, there exists a unique equivalent schema which is maximally parallel. We call such a schema the “closure” of the original schema.
Computational experience with a modified linear programming method for the inequality or equality set covering problem (i.e. minimize cx subject to Ex ≥ e or Ex = e, xi = 0 or 1, where E is a zero-one matrix, e is a column of ones, and c is a nonnegative integral row) is presented. The zero-one composition of the constraint matrix and the right-hand side of ones suggested an algorithm in which dual simplex iterations are performed whenever unit pivots are available and Gomory all integer cuts are adjoined when they are not. Applications to enumerative and heuristic schemes are also discussed.A computer code for the transportation problem that is even more efficient than the primal-dual method is developed. The code uses the well-known (primal) MODI method and is developed by a benefit-cost investigation of the possible strategies for finding an initial solution, choosing the pivot element, finding the stepping-stone tour, etc. A modified Row Minimum Start Rule, the Row Most Negative Rule for choice of pivot, and a modified form of the Predecessor Index Method for locating stepping-stone tours were found to perform best among the strategies examined. Efficient methods are devised for the relabeling that is involved in moving from one solution to another. The 1971 version of this transportation code solves both 100 × 100 assignment and transportation problems in about 1.9 seconds on the Univac 1108 Computer, which is approximately the same time as that required by the Hungarian method for 100 × 100 assignment problems.An investigation of the effect on mean solution time of the number of significant digits used for the parameters of the problem indicates that the cost parameters have a more significant effect than the rim parameters and that the solution time “saturates” as the number of significant digits is increased. The Minimum Cost Effect, i.e. the fact that total solution time asymptotically tends to the time for finding the initial solution as the problem size is increased (keeping the number of significant digits for the cost entries constant), is illustrated and explained. Detailed breakup of the solution times for both transportation and assignment problems of different sizes is provided. The paper concludes with a study of rectangular shaped problems.A general theory of canonical precedence analysis is defined and studied. The familiar types of precedence analysis such as operator precedence or simple precedence occur as special cases of this theory. Among the theoretical results obtained are a characterization of the structure of precedence relations and the relation of canonical precedence schemes to operator sets.The transition diagram systems first introduced by Conway are formalized in terms of a restricted deterministic pushdown acceptor (DPDA) called a nested DPDA. It is then established that the class of nested DPDA's is capable of accepting all deterministic context-free languages. The proof of this involves demonstrating that left recursion can be eliminated from deterministic (or LR(k)) grammars without destroying the deterministic property. Using various structural properties of nested DPDA's, one can then establish equivalence results for certain classes of deterministic languages.Many experts in mechanized text processing now agree that useful automatic language analysis procedures are largely unavailable and that the existing linguistic methodologies generally produce disappointing results. An attempt is made in the present study to identify those automatic procedures which appear most effective as a replacement for the missing language analysis.A series of computer experiments is described, designed to simulate a conventional document retrieval environment. It is found that a simple duplication, by automatic means, of the standard, manual document indexing and retrieval operations will not produce acceptable output results. New mechanized approaches to document handling are proposed, including document ranking methods, automatic dictionary and word list generation, and user feedback searches. It is shown that the fully automatic methodology is superior in effectiveness to the conventional procedures in normal use.A direct (noniterative) method for solving some singular systems of equations arising from finite difference approximations to partial differential equations is developed. The Moore-Penrose generalized inverse of some large tensor product matrices is expressed in terms of smaller matrices. Some techniques are given to improve computational efficiency.Covering algorithms utilized in methods of search for finding zeros of complex polynomials are investigated from both a deterministic and a probabilistic point of view. q-coverings are found to be optimal, and numerical examples of this and other types of coverings are given.Finite-precision interval arithmetic evaluation of a function ƒ of n variables at an n-dimensional rectangle T which is the Cartesian product of intervals yields an interval which is denoted by F(T). Correspondingly, finite-precision real arithmetic evaluation of ƒ at the midpoint m(T) of T yields a number which is denoted by f(m(T)) &Egr; F(T). Often, f(m(T)) is surprisingly close to m(F(T)). The purpose of this note is to provide some insight into this phenomenon by examining the case of infinite precision and rational functions. It is shown that if the gradient of ƒ is nonzero at a fixed point t &Egr; T, then as the maximum edge length w(T) of T approaches zero, [m(F(T)) - ƒ(m(T))]/w(F(T)) = O(w(T)), where F(T) and ƒ(m(T)) denote the infinite-precision results corresponding to F(T) and f(m(T)), respectively. More precise results are derived when ƒ is one of +, -, ×, or /.A lower bound for the number of additions necessary to compute a family of linear functions by a linear algorithm is given when an upper bound c can be assigned to the modulus of the complex numbers involved in the computation. In the case of the fast Fourier transform, the lower bound is (n/2) log2n when c = 1.The following two important aspects of multilevel static memory management are treated: (1) memory loading, and (2) memory sizing. An algorithm is developed for the optimal loading of program and data files in various memory levels of given sizes. The selection of optimal sizes from a given set of finite alternatives is done through a cyclic queueing model, which recognizes the effects of queues and waiting times and helps in deriving a balanced system.The work described in a previous paper, “Efficient Exercising of Switching Elements in Nets of Identical Gates,” is continued. Sections 1, 3, and 4 of the previous paper are prerequisites to the current work.Exercising, or “testing,” the gates of a net is equivalent to performing fault diagnosis in the case in which each fault is detectable. Where F is any set of switching functions, this paper establishes a necessary and sufficient condition on an arbitrary function set G such that all trees composed only of functions from F will accept G; choosing G as the “testing” functions P(T, r) of the previous paper, this is the condition under which all such trees can be tested by r (or fewer) patterns.The case of “stuck-at” faults is then investigated at length. It is shown that: (a) all trees can be tested for stuck-at faults by three patterns, and nearly all by two patterns; (b) for some interesting classes of functions, all nets can be tested by two patterns—in general, many nets can; (c) for any set of functions which are functionally complete and suitably closed under constant inputs, there are nets which require arbitrarily many patterns. The latter result suggests that nets requiring arbitrarily many patterns exist for nearly all interesting choices of functions and of faults.For “input dependence” tests, a result is cited to show that economical testing of nets of nand (nor) gates of mixed sizes is possible.Finally, for the case of exhaustive testing, experience suggests that economical tests exist for most nets composed from functions of F wherever F is a bounded set—i.e. a set of functions for which there is a fixed upper bound on the number of patterns required to test any tree. The (individually) bounded functions are completely characterized; then the bounded sets of symmetric functions are completely characterized. The latter prove to be those sets for which: (a) each function is individually bounded (i.e. not “and,” “or,” or constant); and (b) F contains at most one of the following: (1) “not,” (2) a “nand” function, (3) a “nor” function. Thus bounded sets of functions are the rule, not the exception, and economical testing appears to be generally possible.A generalization of the resolution method for higher order logic is presented. The languages acceptable for the method are phrased in a theory of types of order w (all finite types)—including the &lgr;-operator, propositional functors, and quantifiers. The resolution method is, of course, a machine-oriented theorem search procedure based on refutation. In order to make this method suitable for higher order logic, it was necessary to overcome two sorts of difficulties. The first is that the unifying substitution procedure—an essential feature of the classic first-order resolution—must be generalized (it is noted that for the higher order unification the proper notion of substitution will include &lgr;-normalization). A general unification algorithm is produced and proved to be complete for second-order languages. The second difficulty arises because in higher order languages, semantic intent is essentially more “interwoven” in formulas than in first-order languages. Whereas quantifiers could be eliminated immediately in first-order resolution, their elimination must be deferred in the higher order case. The generalized resolution procedure which the author produces thus incorporates quantifier elimination along with the familiar features of unification and tautological reduction. It is established that the author's generalized resolution procedure is complete with respect to a natural notion of validity based on Henkin's general validity for type theory. Finally, there are presented examples of the application of the method to number theory and set theory.
Two upper bounds for the total path length of binary trees are obtained. One is for node-trees, and bounds the internal (or root-to-node) path length; the other is for leaf-trees, and bounds the external (or root-to-leaf) path length. These bounds involve a quantity called the balance, which allows the bounds to adapt from the n log n behavior of a completely balanced tree to the n2 behavior of a most skewed tree. These bounds are illustrated for the case of Fibonacci trees.Consider the set of multistep formulas ∑l-1jmn-k &agr;ijxmn+j - h ∑l-1jmn-k&bgr;ijxmn+j = 0, i = 1, ···, l, where xmn+j = ymn+j for j= -k, ···, -1 and xn = ƒn = ƒ(xn , tn). These formulas are solved simultaneously for the xmn+j with j = 0, ···, l - 1 in terms of the xmn+j with j = -k, ··· , - 1, which are assumed to be known.Then ymn+j is defined to be xmn+j for j = 0, ··· , m - 1. For j = m, ··· , l - 1, xmn+j is discarded. The set of y's generated in this manner for successive values of n provide an approximate solution of the initial value problem: y = ƒ(y, t), y(t0) = y0. It is conjectured that if the method, which is referred to as the composite multistep method, is A-stable, then its maximum order is 2l. In addition to noting that the conjecture conforms to Dahlquist's bound of 2 for l = 1, the conjecture is verified for k = 1. A third-order A-stable method with m = l = 2 is given as an example, and numerical results established in applying a fourth-order A-stable method with m = 1 and l = 2 are described. A-stable methods with m = l offer the promise of high order and a minimum of function evaluations—evaluation of ƒ(y, t) at solution points. Furthermore, the prospect that such methods might exist with k = 1—only one past point—means that step-size control can be easily implementedTridiagonal linear systems of equations can be solved on conventional serial machines in a time proportional to N, where N is the number of equations. The conventional algorithms do not lend themselves directly to parallel computation on computers of the ILLIAC IV class, in the sense that they appear to be inherently serial. An efficient parallel algorithm is presented in which computation time grows as log2 N. The algorithm is based on recursive doubling solutions of linear recurrence relations, and can be used to solve recurrence relations of all orders.A combinatorial problem arising from the analysis of a model of interleaved memory systems is studied. The performance measure whose calculation defines this problem is based on the distribution of the number of modules in operation during a memory cycle, assuming saturated demand and an arbitrary but fixed number of modules.In general terms the problem is as follows. Suppose we have a Markov chain of n states numbered 0, 1, ···, n - 1. For each i assume that the one-step transition probability from state i to state (i + 1) mod n is given by the parameter &agr; and from state i to any other state is &bgr; = (1 - &agr;)/(n - 1). Given an initial state, the problem is to find the expected number of states through which the system passes before returning to a state previously entered. The principal result of the paper is a recursive procedure for computing this expected number of states. The complexity of the procedure is seen to be small enough to enable practical numerical studies of interleaved memory systems.The problem of multiprogram scheduling on a single processor is studied from the viewpoint of the characteristics peculiar to the program functions that need guaranteed service. It is shown that an optimum fixed priority scheduler possesses an upper bound to processor utilization which may be as low as 70 percent for large task sets. It is also shown that full processor utilization can be achieved by dynamically assigning priorities on the basis of their current deadlines. A combination of these two scheduling techniques is also discussed.A particular decision-theoretic approach to the problem of detecting straight edges and lines in pictures is discussed. A model is proposed of the appearance of scenes consisting of prismatic solids, taking into account blurring, noise, and smooth variations in intensity over faces. A suboptimal statistical decision procedure is developed for the identification of a line within a narrow band in the field of view, given an array of intensity values from within the band. The performance of this procedure is illustrated and discussed.Characterizations of digital “simple arcs” and “simple closed curves” are given. In particular, it is shown that the following are equivalent for sets S having more than four points: (1) S is a simple curve; (2) S is connected and each point of S has exactly two neighbors in S; (3) S is connected, has exactly one hole, and has no deletable points. It follows that if a “shrinking” algorithm is applied to a connected S that has exactly one hole, it shrinks to a simple curve.The problem of finding a minimum number of patterns to exercise the logic elements of a combinational switching net is investigated. Throughout, the word “testing” refers to exercising of this kind; or, equivalently, to fault diagnosis where each line of the net can be directly observed. Any set of permanent faults can be selected to test against, examples of which range from “stuck-at” faults (allowing the most economical test) to “any possible fault” (requiring the most complete test).The method used depends upon exact structural analysis rather than upon search algorithms or random pattern generation. The types of results presented appear to be fundamentally new. In particular, the maximum number of patterns required to test any one of an infinite class of nets is frequently found to be finite and extremely small. For example, any nontrivial connected tree of 2-input nand gates can be tested for “any possible fault” by exactly five patterns—no more and no less.The method in brief: Given a set F of switching functions and a set of required inputs for each (collectively denoted T), a “testing” function is defined for each element of F for each positive integer r. If the lines of a net can be mapped to the domain of the testing functions P(T, r) so that the gates perform consistent with these functions, we say the net “accepts” P(T, r)—and then r patterns are sufficient to test the net for T.Only nets in which each logic element is intended to realize the same switching function are discussed here. Trees (nets without fanout) are studied first, and the conditions under which a tree of identical gates “accepts” a partial function on an arbitrary domain is established. Then the common symmetric switching functions are separately investigated to find for each a minimum value of r such that all trees composed solely of the function accept P(T, r) (for various T). In most cases, as in the example given, the number of patterns required to test any such tree is extremely low.The conditions under which all nets (nontrees included) accept a set of partial functions with arbitrary domain are then established. These conditions are rarely met in practice, even where F consists of a single function. However, many subclasses of nets can be identified which require only a few patterns at most (depending on the function and the class of faults selected). These subclasses often contain nets of arbitrary size and complexity, and frequently consist of exactly those nets for which a related graph can be “colored” (i.e., h-node colored for some particular h) in the classical graph-theoretic sense. For example, any net of 2-input nand gates can be tested by five patterns if one of its related graphs is 2-colorable and another one is 3-colorable (!).The detailed results and methods used to obtain them are summarized, and in conclusion coloring problems and test construction are commented upon.In connection with the development of an actual question-answering system, the Relational Data File of The Rand Corporation, a class of formulas of the predicate calculus, the definite formulas of Kuhns, was proposed as the class of symbolic representations of the “reasonable” questions to put to this retrieval system. Roughly speaking, the definite formulas are those formulas F such that the set of true substitution instances of F in a finite interpretation I are preserved on passage to a certain special extension I′ of I. The proper formulas are those definite formulas which are supposedly especially suitable for machine processing. It has previously been shown by the author that the decision problem for the class of definite formulas is recursively unsolvable. In this paper, however, it is shown that the decision problem for various classes of proper formulas is solvable. Thus, for each of these classes there is a mechanical procedure to decide of an arbitrary formula whether it is a member of the class. It follows that for each of these classes there is no effective transformation 
@@@@ which takes an arbitrary definite formula F into a proper equivalent @@@@(F). In addition, it is shown that the decision problems for a number of classes of formulas which bear a structural similarity to any class of proper formulas are recursively unsolvable.An improved procedure for resolution theorem proving, called Z-resolution, is described. The basic idea of Z-resolution is to “compile” some of the axioms in a deductive problem. This means to automatically transform the selected axioms into a computer program which carries out the inference rules indicated by the axioms. This is done automatically by another program called the specializer. The advantage of doing this is that the compiled axioms run faster, just as a compiled program runs faster than an interpreted program.A proof is given that the inference rule used in Z-resolution is complete, provided that the axioms “compiled” have certain properties.Suppose we are given two disjoint linearly ordered subsets A and B of a linearly ordered set C, say A = {a1 < a2 < ··· < am} and B = {b1 < b2 < ··· < bn}. The problem is to determine the linear ordering of their union (i.e. to merge A and B) by means of a sequence of pairwise comparisons between an element of A and an element of B (which we refer to in the paper as the (m, n) problem). Given any algorithm s to solve the (m, n) problem, we are interested in the maximum number of comparisons Ks(m, n) required under all possible orderings of A ∪ B. An algorithm s is said to be minimax if Ks(m, n) = K(m, n) where K(m, n) = mins Ks(m, n)It is a rather difficult task to determine K(m, n) in general. In this study the authors are only concerned with the minimax over a particular class of merging algorithms. This class includes the tape merge algorithm, the simple binary algorithm, and the generalized binary algorithm.Subtree replacement systems form a broad class of tree-manipulating systems. Systems with the “Church-Rosser property” are appropriate for evaluation or translation processes: the end result of a complete sequence of applications of the rules does not depend on the order in which the rules were applied. Theoretical and practical advantages of such flexibility are sketched. Values or meanings for trees can be defined by simple mathematical systems and then computed by the cheapest available algorithm, however intricate that algorithm may be.We derive sufficient conditions for the Church-Rosser property and discuss their applications to recursive definitions, to the lambda calculus, and to parallel programming. Only the first application is treated in detail. We extend McCarthy's recursive calculus by allowing a choice between call-by-value and call-by-name. We show that recursively defined functions are single-valued despite the nondeterminism of the evaluation algorithm. We also show that these functions solve their defining equations in a “canonical” manner.








Several graph theoretic cluster techniques aimed at the automatic generation of thesauri for information retrieval systems are explored. Experimental cluster analysis is performed on a sample corpus of 2267 documents. A term-term similarity matrix is constructed for the 3950 unique terms used to index the documents. Various threshold values, T, are applied to the similarity matrix to provide a series of binary threshold matrices. The corresponding graph of each binary threshold matrix is used to obtain the term clusters.Three definitions of a cluster are analyzed: (1) the connected components of the threshold matrix; (2) the maximal complete subgraphs of the connected components of the threshold matrix; (3) clusters of the maximal complete subgraphs of the threshold matrix, as described by Gotlieb and Kumar.Algorithms are described and analyzed for obtaining each cluster type. The algorithms are designed to be useful for large document and index collections. Two algorithms have been tested that find maximal complete subgraphs. An algorithm developed by Bierstone offers a significant time improvement over one suggested by Bonner.For threshold levels T ≥ 0.6, basically the same clusters are developed regardless of the cluster definition used. In such situations one need only find the connected components of the graph to develop the clusters.A new mathematical method is developed for interpolation from a given set of data points in a plane and for fitting a smooth curve to the points. This method is devised in such a way that the resultant curve will pass through the given points and will appear smooth and natural. It is based on a piecewise function composed of a set of polynomials, each of degree three, at most, and applicable to successive intervals of the given points. In this method, the slope of the curve is determined at each given point locally, and each polynomial representing a portion of the curve between a pair of given points is determined by the coordinates of and the slopes at the points. Comparison indicates that the curve obtained by this new method is closer to a manually drawn curve than those drawn by other mathematical methods.A definition is given of computer interval arithmetic suitable for implementation on a digital computer. Some computational properties and simplifications are derived. An ALGOL code segment is proved to be a correct implementation of the definition on a specified machine environment.A family of fifth-order pseudo-Runge-Kutta methods for the numerical solution of systems of ordinary differential equations is presented. A procedure for determining an “optimal” set of parameters is given, and several examples are considered. The principal advantage of these methods is that, for a fixed stepsize, they require two less function evaluations at each step than do the corresponding fifth-order Runge-Kutta methods. Their principal disadvantage is that they are not self-starting; they require two initial values. Numerically, pseudo-Runge-Kutta and Runge-Kutta methods seem to be comparable.The method of reducing a Fredholm integral equation of the second kind to a matrix equation and then inverting the matrix is well suited to a machine computation. The number of the dimension of the matrix equation is desired to be large for a well approximated solution. However, much computing time may be required to invert the matrix or the number of the elements of the matrix may exceed the memory capacity. A successive iterative method is suggested to avoid these difficulties.The fast Fourier transform (FFT) is an algorithm to compute the discrete Fourier coefficients with a substantial time saving over conventional methods. The finite word length used in the computer causes an error in computing the Fourier coefficients. This paper derives explicit expressions for the mean square error in the FFT when floating-point arithmetics are used. Upper and lower bounds for the total relative mean square error are given. The theoretical results are in good agreement with the actual error observed by taking the FFT of data sequences.A procedure using the minimax polynomial fit of degree n on the set of extrema of the Chebyshev polynomial Tn+1 is examined for its effectiveness in generating near-minimax fits on [-1, 1]. It is compared to Powell's results for interpolation at the zeros of Tn.Given N approximations to the zeros of an Nth-degree polynomial, N circular regions in the complex z-plane are determined whose union contains all the zeros, and each connected component of this union consisting of K such circular regions contains exactly K zeros. The bounds for the zeros provided by these circular regions are not excessively pessimistic; that is, whenever the approximations are sufficiently well separated and sufficiently close to the zeros of this polynomial, the radii of these circular regions are shown to overestimate the errors by at most a modest factor simply related to the configuration of the approximations. A few numerical examples are included.A general formulation of discrete deterministic dynamic programming is given. This definition is obtained formally by derivation of a simplified algorithm from a general algorithm, and gives simultaneously the class of concurrent problems.The utilization of space and the running speed of the buddy system are considered Equations are derived that give various statistical properties of the buddy system. For the bottom level with Poisson requests and exponential service times the expected amount of space wasted by pairing full cells with empty cells is about 0.513 &rgr;1/2 and the mean time between requests from the bottom level to the next level is about 1.880 &rgr;1/2 &lgr;-1, where &rgr; is the mean number of blocks in use on the bottom level and &lgr;-1 is the mean time between requests for blocks on the bottom level. The results of a number of simulations of the buddy system are also given and compared with the analytical studies.A resolution in which one of the two parent clauses is a unit clause is called a unit resolution, whereas a resolution in which one of the two parent clauses is an original input clause is called an input resolution. A unit (input) proof is a deduction of the empty clause □ such that every resolution in the deduction is a unit (input) resolution. It is proved in the paper that a set S of clauses containing its unit factors has a unit proof if and only if S has an input proof. A LISP program implementing unit resolution is described and results of experiments are given.A definition is given of the efficiency of an algorithm considered as a whole. This immediately raises the question of whether it is possible to find the most efficient or “optimum” algorithm. It is shown that an optimization problem of this kind is effectively solvable if and only if the set of arguments with which one is concerned is a finite one. Next, conditions under which an optimum algorithm does or does not exist are considered, and a limiting recursive process for finding it when it does is produced. Finally, some observations are made about the best space-time measure for algorithms which can be expected in certain cases. The results and proofs are couched in terms of Turing machines but may be adapted without difficulty to apply to other infinite digital machines such as the extensions of actual computers obtained by adding an infinite memory, or to the computations involved in other theoretical formulations of partial recursive functions such as those provided by Kleene's systems of equations or the URM's of Shepherdson and Sturgis.The problem of evaluating arithmetic expressions on a machine with N ≥ 1 general purpose registers is considered. It is initially assumed that no algebraic laws apply to the operators and operands in the expression. An algorithm for evaluation of expressions under this assumption is proposed, and it is shown to take the shortest possible number of instructions. It is then assumed that certain operators are commutative or both commutative and associative. In this case a procedure is given for finding an expression equivalent to a given one and having the shortest possible evaluation sequence. It is then shown that the algorithms presented here also minimize the number of storage references in the evaluation.Many problems, some of them quite meaningful, have been proved to be recursively unsolvable for programs in general. The paper is directed toward a class of programs where many decision problems are solvable. The equivalence problem has been proved to be unsolvable for the class L2 of loop programs defining the class of elementary functions. A solution is given for the class L1 defining the class of simple functions. Further, a set of other decision problems not directly connected with the equivalence problem is investigated. These problems are found again to be unsolvable for the class L2; but as before, a solution is given for the class L1. It is concluded, therefore, that there is a barrier of unsolvability between the classes L1 and L2.
Two algorithms for solving free boundary problems in two dimensions are described. The algorithms use the method of finite differences and are automated versions of methods due to Southwell. The algorithms have been implemented as a general program FREEBOUN, and the numerical results that were obtained using this program are discussed.The Fredholm integral equation where the kernel is semidegenerate has many applications. The solution of this integral equation may be studied as a function of the upper limit of integration x, while t remains fixed. It is shown that the solution satisfies an initial-value problem. This reformulation is well suited to numerical solution by analog and digital computers.The present paper is one of a series on initial-value methods for Fredholm integral equations. Its considerations are of practical significance since an arbitrary kernel may be approximated by a degenerate kernel to a desired degree of accuracy using standard techniques. Furthermore, the important cases in which the kernel is a Green's function and in which the integral equation is a Volterra equation are both covered by this treatment.The nonlinear interpolation of functions of very many variables is discussed. Deterministic termwise assessment of a prohibitively large number of terms naturally leads to a choice of random sampling from these numerous terms. After introduction of an appropriate higher order interpolation formula, a working algorithm is established by the Monte Carlo method. Numerical examples are also given.The optimization of memory hierarchy involves the selection of types and sizes of memory devices such that the average access time to an information block is a minimum for a particular cost constraint. It is assumed that the frequency of usage of the information is known a priori. In this paper the optimization theory for a single task or program is reviewed and it is extended to a general case in multiprogramming when a number of tasks are executed concurrently. Another important extension treats the case when memories are available only in indivisible modules. Comparisons with conventional methods of solution as well as computational experience on the multiprogrammed and modular cases are given.A syntax-directed picture analysis system based on a formal picture description scheme is described. The system accepts a description of a set of pictures in terms of a grammar generating strings in a picture description language; the grammar is explicitly used to direct the analysis or parse, and to control the calls on pattern classification routines for primitive picture components. Pictures are represented by directed graphs with labeled edges, where the edges denote elementary picture components and the graph connectivity mirrors the picture component connectivity; blank and don't care “patterns” allow the description of simple relations between visible patterns. The bulk of the paper is concerned with the picture parsing algorithm which is an n-dimensional analog of a classical top-down string parser, and an application of an implemented system to the analysis of spark chamber film. The potential benefits of this approach, as demonstrated by the application, include ease of implementation and modification of picture processing systems, and simplification of the pattern recognition problem by automatically taking advantage of contextual information.A syntax-directed picture analysis system based on a formal picture description scheme is described. The system accepts a description of a set of pictures in terms of a grammar generating strings in a picture description language; the grammar is explicitly used to direct the analysis or parse, and to control the calls on pattern classification routines for primitive picture components. Pictures are represented by directed graphs with labeled edges, where the edges denote elementary picture components and the graph connectivity mirrors the picture component connectivity; blank and don't care “patterns” allow the description of simple relations between visible patterns. The bulk of the paper is concerned with the picture parsing algorithm which is an n-dimensional analog of a classical top-down string parser, and an application of an implemented system to the analysis of spark chamber film. The potential benefits of this approach, as demonstrated by the application, include ease of implementation and modification of picture processing systems, and simplification of the pattern recognition problem by automatically taking advantage of contextual information.The information-gathering aspect of sorting is considered from a theoretical viewpoint. A large class, R, of sorting algorithms is defined, based on the idea of information use. Properties of this algorithm class are developed, and it is noted that several well-known sorting algorithms are closely related to algorithms in R. The Binary Tree Sort is shown to be in R and to have unique properties in this class. A vector is defined which characterizes the information-gathering efficiency of the algorithms of R. Finally, a more general class of algorithms is defined, and some of the definitions extended to this class. Two intriguing conjectures are given which appear to require graph theory or combinatorial topology for their solution.The methods currently in use and previously proposed for the choice of a root in minimal storage tree sorting are in reality methods for making inefficient statistical estimates of the median of the sequence to be sorted. By making efficient use of the information in a random sample chosen during input of the sequence to be sorted, significant improvements over ordinary minimal storage tree sorting can be made.A procedure is proposed which is a generalization of minimal storage tree sorting and which has the following three properties: (a) There is a significant improvement (over ordinary minimal storage tree sorting) in the expected number of comparisons required to sort the input sequence. (b) The procedure is statistically insensitive to bias in the input sequence. (c) The expected number of comparisons required by the procedure approaches (slowly) the information-theoretic lower bound on the number of comparisons required. The procedure is, therefore, “asymptotically optimal.”It is shown that, owing to certain restrictions placed upon the set of admissible structures, some previous solutions have not characterized trees in which expected search time is minimized. The more general problem is shown to be a special case of a coding problem, which was previously formulated and solved as a linear integer programming problem, and in the special case of equally probable key requests is found to be solvable almost by inspection. Some remarks are given regarding the possibility of realizing a shorter computational procedure than would be expected from an integer programming algorithm, along with a comparison of results from the present method with those of the previous.A sequential network is said to be controllable if there exists at least one integer k such that it is possible to transition between any pair of arbitrary states (S&agr;, S&bgr;) in exactly k steps. In this paper, necessary and sufficient conditions are given for a nonlinear sequential network to be controllable. Strong connectedness is a necessary condition for controllability. It is shown that the existence of two cycles C1 and C2 on a strongly connected sequential network, whose cycle lengths L1 and L2 are relatively prime, is both necessary and sufficient for controllability. Simple test procedures are also developed which determine if a sequential network is controllable and which determine the transition sequences.A new technique is given for establishing the completeness of resolution-based deductive systems for first-order logic (with or without equality) and several new completeness results are proved using this technique. The technique leads to very simple and clear completeness proofs and can be used to establish the completeness of most resolution-based deductive systems reported in the literature. The main new result obtained by means of this technique is that a linear format for resolution with merging and set of support and with several further restrictions is a complete deductive system for the first-order predicate calculus.The resolution principle is an inference rule for quantifier-free first-order predicate calculus. In the past, the completeness theorems for resolution and its refinements have been stated and proved for finite sets of clauses. It is easy (by Gödel's Compactness Theorem) and of practical interest to extend them to countable sets, thus allowing schemata representing denumerably many axioms. In addition, some theorems similar to Craig's Interpolation Theorem are proved for deduction by resolution. In propositional calculus, the theorem proved is stronger, whereas in predicate calculus the theorems proved are in some ways stronger and in some ways weaker than Craig's theorem. These interpolation theorems suggest procedures which could be embodied in computer programs for automatic proof finding and consequence finding.Directed graphs having logical control associated with each vertex have been introduced as models of computational tasks for automatic assignment and sequencing on parallel processor systems. A brief review of their properties is given. A procedure to test the “legality” of graphs in this class is described, and leads to algorithms for counting the number of all possible executions (AND-type subgraphs), and for evaluating the probability of ever reaching a given vertex in the graph. Numerical results are given for some example graphs.The problems of convergence, correctness, and equivalence of computer programs can be formulated by means of the satisfiability or validity of certain first-order formulas. An algorithm is presented for constructing such formulas for functional programs, i.e. programs defined by LISP-like conditional recursive expressions.


A formal model of a problem is developed and its relationship to the General Problem Solver (GPS) is discussed. Before GPS can work on a problem it must be given differences, a difference-ordering, and a table of connections, in addition to the specifications of a problem. Formal definitions of this additional information are given, and sufficient conditions for the success of GPS are derived. These conditions point out the utility of differences and a difference-ordering that yield a “triangular” table of connections. Several different formulations of the Tower of Hanoi are given to illustrate the formal concepts. The use of subproblems in narrowing search is discussed.An algorithm for the determination of the skeleton of a polygonal figure is presented. The propagation of the figure contour is simulated analytically. All skeleton branch points are obtained, in an order which depends on their distance from the contour, together with the equations of the skeleton branches connecting them. The computing time for polygons with few concave vertices is roughly proportional to the number of sides.It is shown that every two-way (deterministic) stack automaton language is accepted by a two-way (deterministic) stack automaton which for each input has a bound on the length of a valid computation. As a consequence, two-way deterministic stack languages are closed under complementation.When the algorithms of J. T. Welch, Jr. were implemented it was discovered that they did not perform as described. The generation of all cycles from a basis is faulty. The generation of the basis is apparently correct. A modified version of Welch's Algorithm 3 is presented. The reasons for modifying Welch's algorithms are presented with examples.The construction of a hierarchy of indexes (the indexed sequential access method) is one means of providing rapid random access to sequential files. An examination is made of the consequences of partially or completely replacing one or more index levels by linear interpolation procedures. For all possible configurations of the several types of key distributions investigated, linear interpolation on the average provides significant performance improvements. Typically, the two accesses required to obtain track index and data are reduced to 1.1 to 1.7 accesses per record. Extremely unusual key distribution will, however, raise the number of accesses required above 2.A lower bound for the departure from normality of an n X n matrix A is given. Furthermore, various inequalities are obtained for certain condition numbers associated with the reduction of A to its Jordan canonical form.A finite difference scheme for a linear, second-order third boundary value problem in ordinary differential equations is described. The discretization error is found to be O(h4).The algorithm of W. F. Trench for the inversion of Toeplitz matrices is presented with a detailed proof for the case of non-Hermitian matrices. The only condition necessary to insure the validity of the algorithm is that all principal minors be nonzero.A major limitation for time-sharing systems is the time delay encountered in transferring records between central “fast” memory and peripheral memory devices. In this paper the transfer characteristics of disk storage devices are considered. Expected seek time and expected rotational latency are taken as measures of performance for the disk. The following aspects of disk files and their behavior are considered: the speed profile of the positioning mechanism and its effect on seek time; effects of the probability distribution of information stored on tracks; track overflow of records; dynamic queuing strategies; reduction of rotation time by buffered read techniques; and strategies for using multiple-arm devices.Successive modifications of Church's definition of a random sequence are considered in terms of their relative position in the Ritchie hierarchy of Kalmar elementary functions. A general result is derived governing the classification of Church random sequences in subrecursive hierarchies which include the elementary functions, such as the Grzegorczyk and Kleene subrecursive hierarchies.The paper deals with computer time-sharing disciplines in which external priorities are introduced. For a computer system under a time-sharing discipline, the following priority disciplines are discussed: (a) head-of-the-line; (b) preemptive repeat; and (c) mixed preemptive strategy. All models in question assume that customers arrive according to homogeneous Poisson processes, and that service times are mutually independent exponentially distributed random variables. Results are given in terms of steady-state expectations.

Many problems in artificial intelligence involve the searching of large trees of alternative possibilities—for example, game-playing and theorem-proving. The problem of efficiently searching large trees is discussed. A new method called “dynamic ordering” is described, and the older minimax and Alpha-Beta procedures are described for comparison purposes. Performance figures are given for six variations of the game of kalah. A quantity called “depth ratio” is derived which is a measure of the efficiency of a search procedure. A theoretical limit of efficiency is calculated and it is shown experimentally that the dynamic ordering procedure approaches that limit.This paper is a continuation of the studies of Fleck, Weeg, and others concerning the theory of automorphisms of ordinary automata and of the work of Gil pertaining to time varying automata. A certain restricted class of time-varying automata, namely the class of polyadic automata, is investigated in detail. The results of an investigation of relationships concerning the group of automorphisms, the polyadic group of defined polyadic automata and the structure of the polyadic automation and the ordinary automata associated and with the polyadic automaton is presented.It is shown that a short proof of the equivalence of star-free and group-free regular events is possible if one is willing to appeal to the Krohn-Rhodes machine decomposition theorem.Let s and t be states of a finite (deterministic) automaton A. t can be reached from s if there is a tape x such that, if A is in state s and receives x, A goes to state t.We consider (1) automata in which the initial state can be reached from any final state, and (2) automata which can be brought to a known state from any unknown state by applying a predetermined tape x.Necessary and sufficient conditions for an ultimate definite automaton to have the former property are obtained. Every ultimate definite automaton A is shown to have the latter property; and a characterization is obtained for all tapes which bring A to a known state. Finally, those ultimate definite automata having the former property are shown to be precisely those which can be brought to the start state from any state by some tape x.Winograd has considered the time necessary to perform numerical addition and multiplication and to perform group multiplication by means of logical circuits consisting of elements each having a limited number of input lines and unit delay in computing their outputs. In this paper the same model as he employed is adopted, but a new lower bound is derived for group multiplication—the same as Winograd's for an Abelian group but in general stronger. Also a circuit is given to compute the multiplication which, in contrast to Winograd's, can be used for non-Abelian groups. When the group of interest is Abelian the circuit is at least as fast as his. By paralleling his method of application of his Abelian group circuit, it is possible also to lower the time necessary for numerical addition and multiplication.This paper is concerned with the relationship of the termination problem for programs and abstract programs to the validity of certain formulas in the first-order predicate calculus. By exploiting this relationship, subclasses of abstract programs for which the termination problem is decidable can be isolated. Moreover, known proof procedures for the first-order predicate calculus (e.g. resolution) can be applied to prove the termination of both programs and abstract programs. The correctness and equivalence problems of abstract programs are shown to be reducible to the termination problem.A direct and self-contained proof is given of the inherent ambiguity of the context-free language L = {aibici ∣ i,j > 1} ∪ {aibici ∣ i,j > 1}, which is the solution to an open problem pointed out by Ginsburg.A notation for specifying translation networks is analyzed and shown to be a special case of a notation for specifying functions. Cancellation of domains and ranges and associativity is derived more simply than in a previous paper by Sklansky, Finkelstein, and Russell.This paper describes new methods of automatically extracting documents for screening purposes, i.e. the computer selection of sentences having the greatest potential for conveying to the reader the substance of the document. While previous work has focused on one component of sentence significance, namely, the presence of high-frequency content words (key words), the methods described here also treat three additional components: pragmatic words (cue words); title and heading words; and structural indicators (sentence location).The research has resulted in an operating system and a research methodology. The extracting system is parameterized to control and vary the influence of the above four components. The research methodology includes procedures for the compilation of the required dictionaries, the setting of the control parameters, and the comparative evaluation of the automatic extracts with manually produced extracts. The results indicate that the three newly proposed components dominate the frequency component in the production of better extracts.A computer-oriented method is developed for determining relative minima of functions of several variables. No derivatives (or approximations) are required and the process always converges to a relative minimum no matter which initial point is used. Numerical examples using test functions suggested in the literature are included to illustrate the effectiveness of the algorithms. Modifications can easily be incorporated which permit the inclusion of constraints or integer-valued variables.The Dahlquist stability analysis for ordinary differential equations is extended to the case of Volterra integro-differential equations. Thus the standard multistep methods can be generalized to furnish algorithms for solving integro-differential equations. Special starting procedures are discussed, and some numerical examples are presented.The inversion of nonsingular matrices is considered. A method is developed which starts with an arbitrary partitioning of the given matrix. The separate submatrices are grouped into sets determined by the nonzero entries of some appropriate group, G, of permutation matrices. The group structure of G then establishes a sequence of operations on these sets of submatrices from which the corresponding representation of the inverse is obtained.Whether the method described is to be preferred to, say, Gauss's algorithm will depend on the capabilities that are required by other parts of the algorithm that is to be implemented in the special-purpose parallel computer. The basic speed, measured by the count of parallel multiplications and divisions, is comparable to that obtained with Gauss's algorithm and is slightly better under certain conditions. The principal difference is that this method uses primarily matrix multiplication, whereas Gauss's algorithm uses primarily row combinations. When the special-purpose computer under design must supply this capability anyway, the method developed here should be considered.Application of the process is limited to matrices for which we can set up a partitioning such that we can guarantee, a priori, that certain of the submatrices are nonsingular. Hence the method is not useful for arbitrary nonsingular matrices. However, it can be applied to certain important classes of matrices, notably those that are “dominated by the diagonal.” Noise covariance matrices are of this type; therefore the method can be applied to them. The inversion of a noise covariance matrix is required in some problems of optimal prediction and control. It is for applications of this sort that the method seems particularly attractive.A time-sharing queue serving a finite number of customers is described. It is assumed that both the service time and the time elapsing between termination of service and the next arrival of the same customer at the queue (service station) are exponential. The model was studied by Krishnamoorthi and Wood, but their results are not in complete agreement with the results of this paper. In addition, some new results are presented in terms of steady-state expectations.A class of formulas of the first-order predicate calculus, the definite formulas has recently been proposed as the formal representation of the “reasonable” questions to put to a computer in the context of an actual data retrieval system, the Relational Data File of Levien and Maron. It is shown here that the decision problem for the class of definite formulas is recursively unsolvable. Hence there is no algorithm to decide whether a given formula is definite.An attempt is made to show that there is much work in pure recursion theory which implicitly treats computational complexity of algorithmic devices which enumerate sets. The emphasis is on obtaining results which are independent of the particular model one uses for the enumeration technique and which can be obtained easily from known results and known proofs in pure recursion theory.First, it is shown that it is usually impossible to define operators on sets by examining the structure of the enumerating devices unless the same operator can be defined merely by examining the behavior of the devices. However, an example is given of an operator which can be defined by examining the structure but which cannot be obtained merely by examining the behavior.Next, an example is given of a set which cannot be enumerated quickly because there is no way of quickly obtaining large parts of it (perhaps with extraneous elements). By way of contrast, sets are constructed whose elements can be obtained rapidly in conjunction with the enumeration of a second set, but which themselves cannot be enumerated rapidly because there is no easy way to eliminate the members of the second set.Finally, it is shown how some of the elementary parts of the Hartmanis-Stearns theory can be obtained in a general setting.
When the Journal of the Association for Computing Machinery was first issued in 1954, the intention was to make it into the definitive journal in the computer area. This in many ways it has become. Even in some of the applied areas—for example, in information retrieval—many of the articles consistently referred to over the years, and thus considered of fundamental importance, have appeared in the ACM Journal.A number of observations and comments are directed toward suggesting that more than the usual engineering flavor be given to computer science. The engineering aspect is important because most present difficulties in this field do not involve the theoretical question of whether certain things can be done, but rather the practical question of how can they be accomplished well and simply.The teaching of computer science could be made more effective by various alterations, for example, the inclusion of a laboratory course in programming, the requirement for a strong minor in something other than mathematics, and more practical coding and less abstract theory, as well as more seriousness and less game playing.MERCURY is a computer-aided system for the selective distribution of Bell Telephone Laboratories technical reports to employees. MERCURY is based on the idea that the job of distribution should be divided between the author and the reader with each performing that part of the job which he does best. The author is asked to describe the interests of the readers to whom his report should be sent. The reader is asked to described the reports that he wishes to receive. The descriptions are made from a small structured vocabulary. The matching of report to reader and the printing of address labels is done by computer.The fifth in a series of experiments in semi-automated mathematics is described. These experiments culminated in large complex computer programs which allow a mathematician to prove mathematical theorems on an man/machine basis. SAM V, the fifth program, is oriented primarily toward the development of efficient automatic techniques for handling some of the more basic processes of mathematical deduction, and toward the realization of efficient real-time interaction between man and machine through the use of cathode-ray tube displays. SAM V's most notable success is the solution of an open problem in lattice theory.Recursive formulas for the numerical evaluation of the real convolution integral are derived for the case in which the impulse response is given analytically. These formulas require considerably less computation time and memory space than the general time series formulas and can be effectively applied for digital simulation of continuous physical systems.Properly scheduling the usage of input output devices is an important aspect of the design of modern multiprogramming systems featuring a paged environment. In this paper magnetic drums in the role of auxiliary memories are studied in the context of these systems. It is the nature of the drum, its usage by the system, and the organization of information on the drum are discussed in the light of current system designs. Mathematical models are then defined such that two extremes in scheduling disciplines are represented in a system in which page requests are assumed to arrive singly and at random. The analysis leads to results for a measure of drum utilization, a generating function for the queue length probabilities in equilibrium, the mean queue length, and the mean waiting time. Finally, the significance of the results is discussed along with some examples.Decompositions of regular events into star events, i.e. events of the form W = V*, are studied. Mathematically, the structure of a star event is that of a monoid. First it is shown that every regular event contains a finite number of maximal star events, which are shown to be regular and can be effectively computed. Necessary and sufficient conditions for a regular event to be the union of its maximal star events are found. Next, star events are factored out from arbitrary events, yielding the form W - V*T. For each W there exists a unique largest V* and a unique smallest T; an algorithm for finding suitable regular expressions for V and T is developed. Finally, an open problem of Paz and Peleg is answered: Every regular event is decomposable as a finite product of star events and prime events.An attempt is made to carry out a program (outlined in a previous paper) for defining the concept of a random or patternless, finite binary sequence, and for subsequently defining a random or patternless, infinite binary sequence to be a sequence whose initial segments are all random or patternless finite binary sequences. A definition based on the bounded-transfer Turing machine is given detailed study, but insufficient understanding of this computing machine precludes a complete treatment. A computing machine is introduced which avoids these difficulties.Some general results about hierarchies of undecidable problems in automata theory are given, and studies are described which show how properties of sets accepted by automata (i.e. languages) change from decidable to undecidable problems as the computational power of the automata is increased. This work also yields unified techniques which characterize for different languages large classes of undecidable problems.Classes of tape-bounded Turing machines similar to the on-line and off-line Turing machines, but without the restrictions that each machine halt and be deterministic, are studied. It is shown that the lower bounds on tape complexity of [1] depend on neither the halting assumption nor determinism. The existence of a dense hierarchy of complexity classes likewise does not depend on the halting assumption, and it is shown that below log n tape complexity there exists a dense hierarchy of complexity classes for two-way nondeterministic devices. It is also shown that the complexity classes of one-way, nondeterministic machines below linear large complexity are not closed under complementation and are larger that the corresponding deterministic complexity class.A one-dimensional array of finite-state machines is being considered as a model for sequence replication. The authors consider the initial state of the first k machines in the array as representing the sequence of k symbols to be replicated along the array. A construction scheme is developed which allows for such replication to take place. It is also shown that the speed of replication approaches synchronous speed.
PATRICIA is an algorithm which provides a flexible means of storing, indexing, and retrieving information in a large file, which is economical of index space and of reindexing time. It does not require rearrangement of text or index as new material is added. It requires a minimum restriction of format of text and of keys; it is extremely flexible in the variety of keys it will respond to. It retrieves information in response to keys furnished by the user with a quantity of computation which has a bound which depends linearly on the length of keys and the number of their proper occurrences and is otherwise independent of the size of the library. It has been implemented in several variations as FORTRAN programs for the CDC-3600, utilizing disk file storage of text. It has been applied to several large information-retrieval problems and will be applied to others.Memory utilization and retrieval time from direct access inverted files are investigated as a function of the data base, the demands on it, and a parameter which the system designer may control. An analysis of the effects of data base characteristics and data base usage is also made for a linked list structure.Time-shared processing systems (e.g. communication or computer systems) are studied by considering priority disciplines operating in a stochastic queueing environment. Results are obtained for the average time spent in the system, conditioned on the length of required service (e.g. message lenght or number of computations). No chage is made for swap time, and the results hold only for Markov assumptions for the arrival and service processes.Two distinct feedback models with a single quantum-controlled service are considered. The first is a round-robin (RR) system in which the service facility processes each customer for a maximum of q sec. If the customer's service is completed during this quantum, he leaves the system; otherwise he returns to the end of the queue to await another quantum of service. The second is a feedback (FBN) system with N queues in which a new arrival joins the tail of the first queue. The server gives service to a customer from the nth queue only if all lower numbered queues are empty. When taken from the nth queue, a customer is given q sec of service. If this completes his processing requirement he leaves the system; otherwise he joins the tail of the (n + 1)-st queue (n = 1, 2, · · ·, N - 1). The limiting case of N → ∞ is also treated. Both models are therefore quantum-controlled, and involve feedback to the tail of some queue, thus providing rapid service for customers with short service-time requirements. The interesting limiting case in which q → 0 (a “processor-shared” model) is also examined. Comparison is made with the first-come-first-served system and also the shortest-job-first discipline. Finally the FB∞ system is generalized to include (priority) inputs at each of the queues in the system.In this paper the use of the techniques of queueing theory in analyzing the performance of a mass storage device in a real-time environment is demonstrated; concern is with the tradeoff experienced in practice between throughput of a stochastic service device and the response time for each service request. For concreteness, the analysis is applied to the IBM 2314 disk storage facility. The results are presented in a series of graphs showing the file system response time versus the throughput for several distributions of record length and arm movement. The queueing model and the theoretical tools used are described in sufficient detail to permit the reader to apply the techniques to other systems. In particular, any disk whose seek time characteristic can be approximated by a piecewise linear continuous function may be analyzed by the methods presented.A model for parallel computations is given as a directed graph in which nodes represent elementary operations, and branches, data channels. The problem considered is the determination of an admissible schedule for such a computation; i.e. for each node determine a sequence of times at which the node initiates its operation. These times must be such that each node, upon initiation, is assured of having the necessary data upon which to operate. Necessary and sufficient conditions that a schedule be admissible are given. The computation rate of a given admissible schedule is defined and is shown to have a limiting value 1/&pgr; where &pgr; is a parameter dependent upon the cycles in the graph. Thus, the computation cannot proceed at a rate exceeding 1/&pgr;. For &ggr; ≥ &pgr;, the class of all periodic admissible schedules with period &ggr; is characterized by the solution space of a certain system of linear inequalities. In particular, then, the maximum computation rate of 1/&pgr; is attainable under a periodic admissible schedule with period &pgr;. A class of all-integer admissible schedules is given. Finally, an algorithm is given for the determination of the number of initiations of each node in the graph defining a parallel computation.An example for a system of difference equations is given in detail.The problem of obtaining the skeleton of a digitized figure is reduced to an optimal policy problem. A hierarchy of methods of defining the skeleton is proposed; in the more complicated ones, the skeleton is relatively invariant under rotation. Two algorithms for computing the skeleton are defined, and the corresponding computer programs are compared. A criterion is proposed for determining the most significant skeleton points.A formalism is defined in which solutions to theorem-proving and similar problems take the form of a sequence of transformations of states into other states. The data used to solve a problem then consists of a set of rewriting rules which defines the allowable transformations. A method for selecting “useful” transformations, i.e. those which will most probably lead to a solution, is developed.Two problem-solving processes based on the above are defined; one, called the FORTRAN Deductive System (FDS) is shown to be more powerful than the other. A class of problems to which FDS will always find solutions is constructed. Examples of solutions found by a computer implementation of FDS are given and, in some cases, are compared with human performance.Finally, FDS is compared with some of the better-known problem-solving systems.A new type of grammar for generating formal languages, called an indexed grammar, is presented. An indexed grammar is an extension of a context-free grammar, and the class of languages generated by indexed grammars has closure properties and decidability results similar to those for context-free languages. The class of languages generated by indexed grammars properly includes all context-free languages and is a proper subset of the class of context-sensitive languages. Several subclasses of indexed grammars generate interesting classes of languages.An operator precedence language which is not real-time definable by any multitple Turing machine is constructed. This strengthens previous results about the existence of unambiguous context-free languages (CFL's) which are not real-time definable. In contrast, a family of CFL's of increasing degree of inherent ambiguity, each real-time definable by a one-tape Turing machine, is exhibited. In fact, an unboundedly ambiguous CFL, also real-time definable by a one-tape Turing machine, is presented. These results are the basis for the assertion that real-time definability of a CFL is independent from each of the structural properties considered.The following theorem is a refinement of an unsolvability result due to E. Post: For any recursively enumerable degree D of recursive unsolvability there is a recursive class of sequences (of the same length) of nonempty words on an alphabet A such that the Post correspondence decision problem for that class is of degree D.This theorem is proved and then applied to obtain degree analogues of the ambiguity problem and the common program problem for the class of context-free grammars.In this paper it is proved that regular events are representable in the cellular model proposed by John von Neumann, and a construction of a finite-state cellular automaton is presented. The procedure was motivated primarily by McNaughton's construction of an automaton using slow but well-timed elements.Practically, predictor-corrector methods for the solution of differential equations are widely used. In these numerical methods, the question of stability is most important. For a single differential equation Crane and Lambert (1962) studies the stability of a fourth-order generalized corrector formula; Chase (1962) and subsequently Emanuel (1963) demonstrated for particular methods that the overall integration procedure does not generally possess the same stability properties as the corrector except when the step of integration tends to zero. In the present paper it is shown that it is possible to develop a necessary and sufficient condition for the stability of general predictor-corrector methods for the solution of systems of differential equations.The theory of generalized multistep methods using an off-grid point is extended to the special second-order equation y″ = f(x, y). New high-order methods for solving this equation, based on quasi-Hermite interpolating polynomials, are shown to exist, as well as new explicit generalized methods for a first-order equation. Some results in the theory of quasi-Hermite interpolation are given, and results of computations of an unperturbed orbit trajectory are presented.
The necessity for swapping in the operation of modern time-sharing systems constitutes the major reason for the latter's inefficiency compared to batch-processing systems. Time-sharing algorithms are discussed which are designed primarily for the reduction of swapping without intolerable changes in the waiting time distributions. A particular class of such algorithms in which conventional procedures are modified by making the quantum allocation dependent on input activity is given a more detailed treatment. In particular, queueing models corresponding to these algorithms are devised and then analyzed for the purpose of obtaining the mean waiting times conditioned on the service required. These results are then compared to those obtained for the conventional models and the comparison subsequently measured against the swapping requirements of the two classes of algorithms.As a step toward the solution of the placement problem in engineering design, a procedure has been developed for detecting intersections of convex regions in 3-space by means of a pseudocharacteristic function. The mathematical techniques underlying the procedure are discussed, and a system of programs embodying these techniques is described. As a special case a solution is given for the hidden-line problem in graphic display.A refinement of the resolution method for mechanical theorem proving is presented. A resolvent C of clauses A and B is called a merge if literals from A and B merge together to form some literal of C. It is shown that the resolution method remains complete if it is required that two noninitial clauses which are not merges never be resolved with one another. It is also shown that this strategy can be combined with the set-of-support strategy.A study of the problem of recognizing the set of primes by automata is presented. A simple algebraic condition is derived which shows that neither the set of primes nor any infinite subset of the set of primes can be accepted by a pushdown or finite automaton.In view of this result an interesting open problem is to determine the “weakest” automaton which can accept the set of primes. It is shown that the linearly bounded automaton can accept the set of primes, and it is conjectured that no automaton whose memory grows less rapidly can recognize the set of primes. One of the results shows that if this conjecture is true, it cannot be proved by the use of arguments about the distribution of primes, as described by the Prime Number Theorem. Some relations are established between two classical conjectures in number theory and the minimal rate of memory growth of automata which can recognize the set of primes.In 1962, Nordsieck presented a series of numerical methods for solving ordinary differential equations which rely on Taylor's series, but which are equivalent to the correctors in the Adams methods. In a method of Nordsieck, simplifications, such as in the often desirable process of changing step size, are made, since it is unnecessary to store more than one previous step. In 1964, Gragg and Stetter published a series of numerical methods for solving ordinary differential equations which rely on very accurate correctors, using a “nonstep” point within the interval of integration. The method in the present paper is equivalent, for uniform step size, to one of these very accurate correctors but is in the form of Nordsieck. The strengths and weaknesses of the method are discussed.The problem to be solved is the computation of the inverse of a modified symmetric matrix if the inverse of the original symmetric matrix is already known. By “modified” it is meant that the two symmetric matrices differ from each other in two symmetric elements or columns and rows. For both cases new identities and ALGOL algorithms are derived.It is shown that the equivalence problem for A-free nondeterministic generalized machines is unsolvable, and it is observed that this result implies the unsolvability of the equality problem for c-finite languages.It is shown that if a language L is recognized by a (nondeterministic) single-tape Turing machine of time complexity T(n), then L is recognized by a (nondeterministic) offline Turing machine of tape complexity T1/2(n). If T(n) ≥ n2;, L is recognized by a (nondeterministic) single-tape Turing machine of tape complexity T1/2(n). If a language L is recognized by a (nondeterministic) offline Turing machine of time complexity (T(n), then L is recognized by a (nondeterministic) offline Turing machine of tape complexity (T(n) log n)1/2 and by a (nondeterministic) single-tape Turing machine of that tape complexity if T (n) ≥ n2/log n.A device is presented which has its memory organized as a linear list, a type of storage equivalent to having two pushdown stores. Attention is then focused on the nondeterministic automaton (called an lsa) which results when the input is read one-way and the device operates in real-time. The set of words (called a language) accepted by an lsa is extensively studied. In particular, several characterizations and closure properties of languages are given.An algorithm is described that will recognize, and fully analyze, strings of unbounded length, using the rewriting rules of any context-free grammar. It uses a finite random access store, three pushdown tapes, and a counter. It imposes no restrictions on the grammar defined by the rewriting rules, excepting only that it be a context-free phrase structure grammar. The analysis printed out is a linearized form of the structural description tree (or trees, in an ambiguous case) of the input string. A proof that the analyzer will always stop in a finite time is provided. The upper bound on the running time increases exponentially with input string length.A transduction is a mapping from one set of sequences to another. A syntax-directed transduction is a particular type of transduction which is defined on the grammar of a context-free language and which is meant to be a model of part of the translation process used in many compilers. The transduction is considered from an automata theory viewpoint as specifying the input-output relation of a machine. Special consideration is given to machines called translators which both transduce and recognize. In particular, some special conditions are investigated under which syntax-directed translations can be performed on (deterministic) pushdown machines. In addition, some time bounds for translations on Turing machines are derived.
A formalism for representing sequences or networks of program translations and compiler translations is described. The formalism is shown to be capable of (a) concisely and clearly representing the overall tasks of assemblers and preprocessors, (b) checking for similarities or equivalences among translation networks, and (c) reducing translation networks to smaller entities.The structuring of algorithms suitable for execution on parallel processors is discussed. Two examples of such algorithms are given. The first example exhibits a restructuring of Bellman's dynamic programming technique; the second presents a method of parsing MAD-type statements in parallel.A perspective transformation is developed whereby an object in space as viewed from an arbitrary point can be projected into a plane and plotted. Families of curves which can be used to define such an object are discussed and examples are given. An algorithm which eliminates the plotting of hidden portions of the object is discussed.This paper describes a mathematical model for the study of contour-line data. Formal definitions are given for the various classes of contour lines found on a contour map. The concept of cliff lines is introduced and the properties of both contour lines and cliff lines are investigated. The objective of the paper is to lay a foundation for the development of algorithms that will facilitate the digital computer solutions of problems involving contour-line data.A simplex type algorithm is presented which deals uniformly with (a) ordinary linear programming problems, (b) problems with upper bounded variables, and (c) problems with convex piecewise linear objective functions, e.g., absolute value terms. Problems of types (b) and (c) can be solved by suitable transformations into ordinary linear programming forms, but are handled by the unified algorithm without such transformations. Comparative computer runs indicate that direct solution by the unified algorithm is considerably more efficient than conversion into ordinary linear programming form followed by use of a regular simplex routine. Computer tests also show that the algorithm offers a worthwhile alternative to the use of artificial variables as a starting procedure for ordinary linear programming problems.A proof procedure based on a theorem of Herbrand and utilizing the matching technique of Prawitz is presented. In general, Herbrand-type proof procedures proceed by generating over increasing numbers of candidates for the truth-functionally contradictory statement the procedures seek. A trial is successful when some candidate is in fact a contradictory statement. In procedures to date the number of candidates developed before a contradictory statement is found (if one is found) varies roughly exponentially with the size of the contradictory statement. (“Size” might be measured by the number of clauses in the conjunctive normal form of the contradictory statement.) Although basically subject to the same rate of growth, the procedure introduced here attempts to drastically trim the number of candidates at an intermediate level of development. This is done by retaining beyond a certain level only candidates already “partially contradictory.” The major task usually is finding the partially contradictory sets. However, the number of candidate sets required to find these subsets of the contradictory set is generally much smaller than the number required to find the full contradictory set.A modified version of the Fast Fourier Transform is developed and described. This version is well adapted for use in a special-purpose computer designed for the purpose. It is shown that only three operators are needed. One operator replaces successive pairs of data points by their sums and differences. The second operator performs a fixed permutation which is an ideal shuffle of the data. The third operator permits the multiplication of a selected subset of the data by a common complex multiplier.If, as seems reasonable, the slowest operation is the complex multiplications required, then, for reasonably sized date sets—e.g. 512 complex numbers—parallelization by the method developed should allow an increase of speed over the serial use of the Fast Fourier Transform by about two orders of magnitude.It is suggested that a machine to realize the speed improvement indicated is quite feasible.The analysis is based on the use of the Kronecker product of matrices. It is suggested that this form is of general use in the development and classification of various modifications and extensions of the algorithm.A unified derivation is presented of the quasi-Newton methods for solving systems of nonlinear equations. The general algorithm contains, as special cases, all of the previously proposed quasi-Newton methods.A solution of n matrix equations is formulated by using existing theory of pseudo inverses of matrices. Techniques for solving recursively such systems, techniques for determining the rank of matrices, techniques for testing whether or not a vector is a null vector of a given matrix, and techniques for testing whether or not a constant is an eigenvalue are indicated.It is well known that real variable analysis is nonconstructive. For example, although it is asserted that every bounded monotone sequence converges to a limit, there is no algorithm for obtaining this limit.This paper presents a constructive analysis which is restricted to a countable set of numbers, the field of computable numbers. These numbers are defined in a new way by employing the concept of “programmable functions.” The resultant analysis differs from real analysis in many important respects.Two negative results concerning the so-called acceptable sets of numbers are extended to the case or arbitrary context-free languages with the help of conventional analytic techniques.The concept of a pair algebra is extended so that it can be defined between similar relational systems. It is shown that when the relational systems under consideration are lattices a generalized pair algebra specializes to a pair algebra. Closure properties of generalized pair algebra are investigated and their applications to automata theory are considered.Four types of balloon automata (defined by one- or two-way input and deterministic or nondeterministic finite control) and closed classes of balloon automata were previously defined by the authors. A set of closed classes, one for each of the four types, is called a family if the classes use their infinite storage in the same way. The recursiveness and solvability of the emptiness problem for closed classes are investigated in the present paper. It is shown that in many cases the solvability of one of these questions for one closed class in a family implies that some other question are solvable for the closed classes of that family.The quantitative aspects of one-tape Turing machine computations are considered. It is shown, for instance, that there exists a sharp time bound which must be reached for the recognition of nonregular sets of sequences. It is shown that the computation time can be used to characterize the complexity of recursive sets of sequences, and several results are obtained about this classification. These results are then applied to the recognition speed of context-free languages and it is shown, among other things, that it is recursively undecidable how much time is required to recognize a nonregular context-free language on a one-tape Turing machine. Several unsolved problems are discussed.
Reminiscences on the early developments leading to large scale electronic computers show that it took much longer than was expected for the first of the more ambitious and fully engineered computers to be completed and prove themselves in practical operation. Comments on the present computer field assess the needs for future development.Automatic indexing methods are evaluated and design criteria for modern information systems are derived.A programming language for the IBM 360 computers and aspects of its implementation are described. The language, called PL360, provides the facilities of a symbolic machine language, but displays a structure defined by a recursive syntax. PL360 was designed to improve the readability of programs which must take into account specific characteristics and limitations of a particular computer. It represents an attempt to further the state of the art of programming by encouraging and even forcing the programmer to improve his style of exposition and his principles and discipline in program organization. Because of its inherent simplicity, the language is particularly well suited for tutorial purposes.The attempt to present a computer as a systematically organized entity is also hoped to be of interest to designers of future computers.An approximate but fairly rapid method for solving integer linear programming problems is presented, which utilizes, in part, some of the philosophy of “direct search” methods. The method is divided into phases which can be applied in order and has the desirable characteristic that a best feasible solution is always available. Numerical results are presented for a number of test problems. Some possible extensions and improvements are also presented.The heuristic program discussed searches for a constructive proof or disproof of a given proposition. It uses a search procedure which efficiently selects the seemingly best proposition to work on next. This program is multipurpose in that the domains it can handle are varied.As an initial experiment, the program was given the task of searching for proofs and disproofs of propositions about kalah end games. Kalah is a two-person game. In another experiment the program, after some modifications, played the game of kalah. This program was compared with another tree-searching procedure, the Alpha-Beta minimax procedure; the results have been encouraging since the program is fast and efficient. Its greatest usefulness is in solving large problems. It is hoped that this program has added one more step toward the goal of eventually obtaining computer programs which can solve intellectually difficult problems.A method for the numerical solution of a Fredholm integral equation of the first kind is derived and illustrated. The method employs an a priori constraint vector together with covariances of both the constraint vector and the measurement errors. The method automatically incorporates an optimum amount of smoothing in the sense of maximum-likelihood estimation. The problem of obtaining optimum basis vectors is discussed. The trace of the covariance matrix of the error in the solution is used to estimate the accuracy of the results. This trace is used to derive a quality criterion for a set of measurements and a given set of constraint statistics. Examples are given in which the behavior of the solution as obtained from a specific integral equation is studied by the use of random input errors to simulate measurement errors and statistical sampling. The quality criterion and behavior of the trace of the error covariance matrix for various bases is also illustrated for the equation being considered.In this paper the problem of readily determining the inverse Laplace transform numerically by a method which meets the efficiency requirements of automatic digital computation is discussed. Because the result inverse function is given as a Fourier cosine series, the procedure requires only about ten FORTRAN statements. Furthermore, it does not require the use of involved algorithms for the generation of any special functions, but uses only cosines and exponentials.The basis of the method hinges on the fact that in evaluating the inverse Laplace transform integral there exists a freedom in choosing the contour of integration. Given certain restrictions, the contour may be any vertical line in the right-half plane. Specifying a line, the integral to be evaluated is essentially a Fourier integral. However, the method is concerned with determining the proper line, so that when the integral (along this line) is approximated, the error is as small as desired by virtue of having chosen the particular contour.A formal theory is described which incorporates the “assignment” function a(i, k, &xgr;) and the “contents” function c(i, &xgr;). The axioms of the theory are shown to comprise a complete and consistent set.Arbitrary finite automata are decomposed into their major substructures, the primaries. Several characterizations of homomorphisms, endomorphisms, isomorphisms, and automorphisms of arbitrary finite automata are presented via reduction to the primaries of the automata. Various characterizations of these transition-preserving functions on singly generated automata are presented and are used as a basis for the reduction. Estimates on the number of functions of each type are given.In this paper the construction of a switching network capable of n!-permutation of its n input terminals to its n output terminals is described. The building blocks for this network are binary cells capable of permuting their two input terminals to their two output terminals.The number of cells used by the network is <n · log2 n - n + 1> = ∑n k=1 2 k>. It could be argued that for such a network this number of cells is a lower bound, by noting that binary decision trees in the network can resolve individual terminal assignments only and not the partitioning of the permutation set itself which requires only 2 n!> = <∑n k=1 log2 k> binary decisions.An algorithm is also given for the setting of the binary cells in the network according to any specified permutation.
The techniques of automatic programming are useful for constructive proofs in automata theory. A formal definition of an elementary programming language for a stack automaton is given, and it is shown how this may be readily adapted to other classes of automata. The second part of this paper shows how this programming language can be applied to automata theory, as we prove there are non-context-sensitive languages accepted by a stack automaton.Programs to solve combinatorial search problems may often be simply written by using multiple-valued functions. Such programs, although impossible to execute directly on conventional computers, may be converted in a mechanical way into conventional backtracking programs. The process is illustrated with algorithms to find all solutions to the eight queens problem on the chessboard, and to find all simple cycles in a network.The closure properties of the class of languages defined by real-time, online, multi-tape Turing machines are proved. The results obtained are, for the most part, negative and, as one would expect, asymmetric. It is shown that the results remain valid for a broad class of real-time devices. Finally, the position of the class of real-time definable languages in the “classical” linguistic hierarchy is established.The purpose of this note is to show that it is recursively undecidable how much tape is required by a Turing machine to recognize nonregular context-free languages.Given a minimal sequential machine M and a positive integer T, it is desired to partition the state set of M into T classes, say S0, S1, · · ·, ST-1, such that all states in S1, under all possible inputs, pass into states in Si+1 (mod T). If such a T-partition exists, M can be realized by means of periodically-varying logic, which often results in the saving of memory elements. The period of M is defined as the greatest common divisor of all cycle lengths of M—a quantity which can be readily evaluated since it depends only on a finite set of independent loops exhibited by the state graph.The main result is that a T-partition exists for M if and only if T is a divisor of the period of M. For every such T, an algorithm is given for constructing the corresponding partition. If M is not required to be minimal, it is shown (constructively) that a T-partition exists for every T.A Post machine is a Turing machine which cannot both write and move on the same machine step. It is shown that the halting problem for the class of 2-state Post machines is solvable. Thus, there can be no universal 2-state Post machine. This is in contrast with the result of Shannon that there exist universal 2-state Turing machines when the machines are capable of both writing and moving on the same step.If, in a sequential machine, the output upon application of an input character depends only upon the current state of the machine and not upon the input character, the machine is called input-independent. Two states, p and q, of an input-independent machine are compatible if and only if the output strings from the machine starting in initial state, p, are the same as the output strings from the machine with initial state, q, for all input strings to the machine in both states.The minimal length of tape which tests compatibility of states in an input-independent sequential machine with n states is (n2 - 2n)/4 + 1 if n is even, and (n2 - 2n + 1)/4 if n is odd. There are machines with incompatible states such that no tapes of length less than the given bounds will detect the incompatibility.The theory of J. A. Robinson's resolution principle, an inference rule for first-order predicate calculus, is unified and extended. A theorem-proving computer program based on the new theory is proposed and the proposed semantic resolution program is compared with hyper-resolution and set-of-support resolution programs. Renamable and semantic resolution are defined and shown to be identical. Given a model M, semantic resolution is the resolution of a latent clash in which each “electron” is at least sometimes false under M; the nucleus is at least sometimes true under M.The completeness theorem for semantic resolution and all previous completeness theorems for resolution (including ordinary, hyper-, and set-of-support resolution) can be derived from a slightly more general form of the following theorem. If U is a finite, truth-functionally unsatisfiable set of nonempty clauses and if M is a ground model, then there exists an unresolved maximal semantic clash [E1, E2, · · ·, Eq, C] with nucleus C such that any set containing C and one or more of the electrons E1, E2, · · ·, Eq is an unresolved semantic clash in U.In many fields of mathematics the richness of the underlying axiom set leads to the establishment of a number of very general equalities. For example, it is easy to prove that in groups (x-1)-1 = x and that in rings -x · - y = x · y. In the presence of such an equality, each new inference made during a proof search by a theorem-proving program may immediately yield a set of very closely related inferences. If, for example, b·a = c is inferred in the presence of (x-1)-1 = x, substitution immediately yields obviously related inferences such as (b-1)-1 · a = c. Retention of many members of each such set of inferences has seriously impeded the effectiveness of automatic theorem proving. Similar to the gain made by discarding instances of inferences already present is that made by discarding instances of repeated application of a given equality. The latter is achieved by use of demodulation. Its definition, evidence of its value, and a related rule of inference are given. In addition a number of concepts are defined the implementation of which reduces both the number and sensitivity to choice of parameters governing the theorem-proving procedures.Such phrases as “information flow” may be purely metaphorical, or may refer to porterage and storage of physical documents, transmission of signals, power required for signaling, Shannon's Selective Information, changes in the state of one's personal knowledge, propagation of announcements concerning messages, social increase of awareness, propagation of or reaction to imperatives, and so on. These matters are distinct and must be distinguished. Then conditions must be stated under which one can validly speak of and measure the appropriate flow. In this paper it is shown that within the field of Notification (mention and delivery of recorded messages to users) there are twenty basic activities formed by choosing triads from the six variables, Message, Code, Channel, Source, Destination, and Designation.“Flow” has meaning only when two such triads have two variables in common, forming a tetrad. Then flow or correspondence between any pair of variables is inextricable from a conjugate flow or correspondence between the other pair. Between any pair of endpoints there are six possible distinct types of flow, according to which two of the remaining four variables are directly used to achieve the flow.An algorithm for deriving the primary sequence of a protein or RNA is presented. The data is in the form of short sequences of letters which must be fitted together to form the unknown complete sequence. A computer program for carrying out the steps is described, with an example. It is shown that the algorithm cannot make an error and empirical results are given which illustrate the successful use of the algorithm in reconstructing complete sequences known to be solvable.Theoretical methods, based on a priori pointwise bounds, for approximating solutions of many elliptic and parabolic initial and/or boundary value problems, have been developed in recent years. These methods, however, are relatively unknown to potential users since applications of the methods have not appeared in the literature. In this paper their usefulness is illustrated by employing some of the author's theoretical results as a basis for the construction of a digital program to compute an approximate solution of an initial boundary value problem for the heat equation.Given the number of words of computer storage required by the individual tests in a limited-entry decision table, it is sometimes desirable to find an equivalent computer program with minimum total storage requirement. In this paper an algorithm is developed to do this. The rules in the decision table are grouped into action sets, so that several rules with the same actions need not be distinguished. Moreover, if certain combinations of conditions can be excluded from consideration, the algorithm will take advantage of this extra information. The algorithm is initially developed for computer programs possessing a treelike form and then extended to a wider class of programs. The algorithm can be combined with one which finds an equivalent computer program with minimum average processing time, and thus used to find an equivalent computer program which minimizes a cost function which is nondecreasing in both average processing time and total storage requirement.Two general methods of matrix inversion, Gauss's algorithm and the method of bordering, are analyzed from the viewpoint of their adaptability for parallel computation. The analysis is not based on any specific type of parallel processor; its purpose is rather to see if parallel capabilities could be used effectively in matrix inversion.It is shown that both methods are indeed able to make effective use of parallel capability. With reasonable assumptions on the parallelism that is available, the speeds of the two methods are roughly comparable. The two methods, however, make use of different kinds of parallelism.To implement Gauss's algorithm we would like to have (a) parallel transfer capability for n numbers, if the matrix is n X n, (b) the capability for parallel multiplication of the accessed numbers by a common multiplier, and (c) parallel additive read-in capability. For the method of bordering, we need, primarily, the capability of forming the Euclidean inner product of two n-dimensional real vectors. The latter seems somewhat harder to implement, but, because it is an operation that is fundamental to linear algebra in general, it is one that might be made available for other purposes. If so, then the method of bordering becomes of interest.A technique for computing the fixed-point probability vector of an orgodic (cyclic) transition matrix P is developed. The technique utilizes generalized matrix inversion in a scheme which only necessitates calculation of the probability fixed point of a transition matrix having smaller dimensions than P.In a previous paper the authors suggested that the accurate correctors proposed by Gragg and Stetter for solving ordinary differential equations should be accompanied by similar predictors. In each method in that paper the corrector and one of the predictors use one “nonstop” point within the interval of integration. In the present paper a corrector is dealt with in which two “nonstep” points are used, and in which, to some degree, the authors have “balanced” the contributions of the errors in the predictors to the total local truncation error, a technique due to Butcher.Pseudo-random number generators of the power residue (sometimes called congruential or multiplicative) type are discussed and results of statistical tests performed on specific examples of this type are presented. Tests were patterned after the methods of MacLaren and Marsaglia (M&M).The main result presented is the discovery of several power residue generators which performed well in these tests. This is important because, of all the generators using standard methods (including power residue) that were tested by M&M, none gave satisfactory results.The overall results here provide further evidence for their conclusion that the types of tests usually encountered in the literature do not provide an adequate index of the behavior of n-tuples of consecutively generated numbers. In any Monte Carlo or simulation problem where n supposedly independent random numbers are required at each step, this behavior is likely to be important.Finally, since the tests presented here differ in certain details from those of M&M, some of their generators were retested as a check. A cross-check shows that results are compatible; in particular, if a generator failed one of their tests badly, it also failed the present author's corresponding test badly.The time required to perform multiplication is investigated. A lower bound on the time required to perform multiplication, as well as multiplication modulo N, is derived and it is shown that these lower bounds can be approached. Then a lower bound on the amount of time required to perform the most significant part of multiplication (⌞xy/N⌟) is derived.
A probabilistic model is developed for a multiprogramming computer configuration, i.e., one in which several program segments are simultaneously in main memory (core). The model relates speed and number of input-output devices, core size, and central processor speed to central processor and system productivity. Incorporated in the model are parameters describing the statistical variability of input-output and central processor activities. Thus the model permits comparisons between systems loaded with different mixtures of job types (“scientific” vs. “business” applications). Numerical comparisons of various systems are provided.A model for multiprocessor control is considered in which jobs are broken into various pieces, called tasks. Tasks are executed by single processing units. In this paper the structure controlling the assignment of tasks to processors is the task list, which orders all tasks according to servicing priority. When a processors becomes free, it simply picks up the highest priority task that is executable at that moment.The job and its component tasks are imagined to be interacting with an environment consisting of a set of rigid timing constraints. Such constraints are of two types, called start-times and deadlines. The interaction is specified by requiring that certain distinguished tasks conform directly to one or more of these constraints. Tasks conforming to a start-time cannot begin until the start-time has passed, and tasks conforming to a deadline cannot proceed beyond the deadline. In our model, all tasks have known maximum run-times, but in any particular job execution, task run-times are unknown.It is shown that despite the simplicity of this control scheme some peculiar phenomena result. Most of these phenomena were first noticed by P. Richards in 1960. A simulation study (Appendix I) reveals that they may be very common in practice. In the present paper and a companion paper by R. L. Graham [Bell Syst. Tech. J. 45 (1966), 1563-1581] a coherent theory of task-list control is developed, in which the nature of these peculiarities is brought under systematic study. A number of potentially useful results are derived.A regular event W is a star event if there exists another event V such that W = V*. In that case, V is called a root of W. It is shown that every star event has a unique minimum root, which is contained in every other root. An algorithm for finding the minimum root of a regular event is presented, and the root is shown to be regular. The results have applications to languages, codes, canonical forms for regular expressions, simplification of expressions, decomposition of sequential machines, and semigroup theory.Any sequential machine M represents a function fM from input sequences to output symbols. A function f is representable if some finite-state sequential machine represents it. The function fM is called an n-th order approximation to a given function f if fM is equal to f for all input sequences of length less than or equal to n. It is proved that, for an arbitrary nonrepresentable function f, there are infinitely many n such that any sequential machine representing an nth order approximation to f has more than n/2 + 1 states. An analogous result is obtained for two-way sequential machines and, using these and related results, lower bounds are obtained for two-way sequential machines and, using these and related results, lower bounds are obtained on the amount of work tape required online and offline Turing machines that compute nonrepresentable functions.A decision procedure is given which determines whether the languages defined by two parenthesis grammars are equal.The relationship between the set of productions of a context-free grammar and the corresponding set of defining equations is first pointed out. The closure operation on a matrix of strings is defined and this concept is used to formalize the solution to a set of linear equations. A procedure is then given for rewriting a context-free grammar in Greibach normal form, where the replacements string of each production begins with a terminal symbol. An additional procedure is given for rewriting the grammar so that each replacement string both begins and ends with a terminal symbol. Neither procedure requires the evaluation of regular begins and ends with a terminal symbol. Neither procedure requires the evaluation of regular expressions over the total vocabulary of the grammar, as is required by Greibach's procedure.The computer analysis of factorial experiments is discussed in terms of a new generalized matrix notation which highlights the special properties of factorial designs. A method of calculating an analysis of a variance suitable for programming as a library subroutine is described. An orthogonal transformation is used to obtain components of variance based on single degrees of freedom. The transformation is done sequentially so that no additional storage for intermediate results is required. The analysis of variance table is obtained by summation of the components according to a simple indexing procedure. An algorithm for the calculation of sums and means over all combinations of factor levels is proposed.A d-dimensional circuit code of spread s (also called SIBs, code or circuit code of minimum distance s) is a simple circuit Q in the graph of the d-dimensional cube [0, 1]d such that any two vertices of Q differing in exactly r coordinates, with r < s, can be joined by a path formed from r edges of Q. Such codes are designed to introduce error detection into certain analog-to-digital conversion systems. Longer codes correspond to increased accuracy of the system, and hence there is interest in determining the maximum length C(d, s) of d-dimensional circuit codes of spread s. In the present study the author contributes to this problem for even values of s by describing a method of combining a code of spread s with a suitably related code of spread s-1 so as to produce a longer code of spread s.It is shown in this paper that the stable feedback shift registers, when classified according to Hamming weight (the number of fundamental product terms in expanded sum of products form), are binomially distributed, i.e., are (2n - n - 1 w) stable feedback shift registers of order n with Hamming weight equal to w. Using this relationship, a recursive algorithm is established which will generate all stable feedback shift registers of order n. Formulas are also given for determining the number of stable feedback shift registers which have j + 1 starting states and j + 1 branch states, 0 ≤ j ≤ 2n-1 - 1.Some new theorems generalizing a result of Oettli and Prager are applied to the a posteriori analysis of the compatibility of a computed solution to the uncertain data of a linear system (or of a polynomial equation).The Method of Lines, a numerical technique commonly used for solving partial differential equations on analog computers, is used to attain digital computer solutions of such equations. An extensive theoretical development is presented that establishes convergence and stability for one-dimensional parabolic equations with Dirichlet boundary conditions. A new modification of the method, using noncentral differences, is shown to be much faster, in terms of computer time, than conventional grid methods, for two examples.A set equations in the quantities ai(p), where i = 1, 2, · · ·, m and p ranges over a set R of lattice points in n-space, is called a system of uniform recurrence equations if the following property holds: If p and q are in R and w is an integer n-vector, then ai(p) depends directly on aj(p - w) if and only if ai(q) depends directly on aj(q - w). Finite-difference approximations to systems of partial differential equations typically lead to such recurrence equations. The structure of such a system is specified by a dependence graph G having m vertices, in which the directed edges are labeled with integer n-vectors. For certain choices of the set R, necessary and sufficient conditions on G are given for the existence of a schedule to compute all the quantities ai(p) explicitly from their defining equations. Properties of such schedules, such as the degree to which computation can proceed “in parallel,” are characterized. These characterizations depend on a certain iterative decomposition of a dependence graph into subgraphs. Analogous results concerning implicit schedules are also given.The purpose of this study is to illustrate the application of a parallel network processing computing system to an important class of problems in hydrodynamics. The computing system selected for this study is a prototype of the SOLOMON parallel processing system (cited as SOLOMON II) which was developed at the Westinghouse Defense and Space Center, Baltimore, Maryland.Emphasis is placed on the problem of numerical weather prediction mainly because of the large data storage and manipulation required, plus the extensive numerical analysis that is involved. The mathematical basis for numerical weather forecasting lies in the principles of conservation of mass, momentum, and energy. From these principles, research meteorologists have devised equations which express the physical laws governing the atmosphere. These basic equations are nonlinear partial differential equations and are well suited for solution by the SOLOMON II system. The computational method is to replace the region of interest by a grid system and replace the continuous equations by their finite difference approximations. In this representation, the programmer can take maximum advantage of mode control, neighboring connections, variable geometry, and simultaneous operation of a network of processing elements.Another topic discussed is the numerical solution of elliptic partial differential equations by the parallel processing network. It is felt by the authors that a parallel processing system of this type will offer a significant increase in computational speed over that of a sequentially organized computing system in the field of fluid dynamics as well as in other scientific fields.
This paper is a survey of research on microcellular techniques. Of particular interest are those techniques that are appropriate for realization by modern batch-fabrication processes, since the rapid emergence of reliable and economical batch-fabricated components represents probably the most important current trend in the field of digital circuits.First the manufacturing methods for batch-fabricated components are reviewed, and the advantages to be realized from the application of the principles of cellular logic design are discussed. Also two categorizations of cellular arrays are made in terms of the complexity of each cell (only low-complexity cells are considered) and in terms of the various application areas.After a survey of very early techniques that can be viewed as exemplifying cellular approaches, modern-day cellular arrays are discussed on the basis of whether they are fixed cell-function arrays or variable cell-function arrays. In the fixed cell-function arrays the switching function produced by each cell is fixed; the cell parameters are used only in the modification of the interconnection structure. Several versions of NOR gate arrays, majority gate arrays, adder arrays, and others are reviewed in terms of synthesis techniques and array growth rates.Similarly, the current status of research is summarized in variable cell-function arrays, where not only the interconnection structure but also the function produced by each cell is determined by parameter selection. These arrays include various general function cascades, outpoint arrays, and cobweb arrays, for example. Again, the various cell types that have been considered are pointed out, as well as synthesis procedures and growth rates appropriate for them.Finally, several areas requiring further research effort are summarized. These include the need for more realistic measures of array growth rates, the need for synthesis techniques for multiple-function arrays and programmable arrays, and the need for fault-avoidance algorithms in integrated structures.Time-shared computer (or processing) facilities are treated as stochastic queueing systems under priority service disciplines, and the performance measure of these systems is taken to be the average time spent in the system. Models are analyzed in which time-shared computer usage is obtained by giving each request a fixed quantum Q of time on the processor, after which the request is placed at the end of a queue of other requests; the queue of requests is constantly cycled, giving each user Q seconds on the machine per cycle. The case for which Q → 0 (a processor-shared model) is then analyzed using methods from queueing theory. A general time-shared facility is then considered in which priority groups are introduced. Specifically, the pth priority group is given gpQ seconds in the processor each time around. Letting Q → 0 gives results for the priority processor-shared system. These disciplines are compared with the first-come-first-served disciplines. The systems considered provide the two basic features desired in any time-shared system, namely, rapid service for short jobs and the virtual appearance of a (fractional capacity) processor available on a full-time basis. No charge is made for swap time, thus providing results for “ideal” systems. The results hold only for Poisson arrivals and geometric (or exponential) service time distributions.A mathematical derivation of expected response time is presented for selected cyclic and priority scheduling disciplines, thereby demonstrating analytic techniques which may be utilized to evaluate such servicing doctrines. To illustrate the constant time quantum (round-robin) results, a hypothetical is defined and resolved.Upper bounds for the error probability of a Bayes decision function are derived in terms of the differences among the probability distributions of the features used in character recognition. Applications to feature selection and error reduction are discussed. It is shown that if a sufficient number of well-selected features is used, the error probability can be made arbitrarily small.This paper concerns itself with the modeling of computations and systems and the generation of a priori estimates of expected computation time for given problems on given processing systems. In particular, methods are discussed for determining the probabilities of reaching vertices in a graph model of computations.An iterative array is defined as an infinite set of identical blocks, interconnected in a regular manner. Each block has inputs and outputs, with internal connections from certain inputs to certain outputs. This paper is concerned with the problem of determining, for a given structure internal to each block, the existence of a given path. The path is specified by the relative positions in the array of its endpoints, and the algorithm presented decides whether such a path is possible in the given array. A special case of the general procedure, which tests for a closed path, has been successfully programmed.The method of steepest descent is applied in a convergent procedure to determine the zeros of polynomials having either real or complex coefficients. By expressing the polynomials in terms of the Siljak functions, the methods are readily programmed on a digital computer. The significance of the procedures is that their application is straightforward, and not only is convergence rapid in the region of a zero but convergence is guaranteed independent of the initial values.Iterative refinement reduces the roundoff errors in the computed solution to a system of linear equations. Only one step requires higher precision arithmetic. If sufficiently high precision is used, the final result is shown to be very accurate.The number of steps required to compute a function depends, in general, on the type of computer that is used, on the choice of computer program, and on the input-output code. Nevertheless, the results obtained in this paper are so general as to be nearly independent of these considerations.A function is exhibited that requires an enormous number of steps to be computed, yet has a “nearly quickest” program: Any other program for this function, no matter how ingeniously designed it may be, takes practically as many steps as this nearly quickest program.A different function is exhibited with the property that no matter how fast a program may be for computing this function another program exists for computing the function very much faster.A method of generating pseudo-random uniform numbers based on the combination of two congruential generators is described. It retains two of the desirable features of congruential generators, namely, the long cycle and the case of implementation on a digital computer. Furthermore, unlike the method of combining congruential generators recently proposed by MacLaren and Marsaglia, it does not require the retention in computer memory of a table of generated numbers. The generator gave completely satisfactory results on a fairly stringent series of statistical tests.For a system of nonlinear equations satisfying certain conditions it is shown that a solution exists and is unique. An iterative method that starts with an initial arbitrary approximation is presented.A procedure is described to test the individual components of the numerical solution to a system of differential equations for relative instability. Although the test is based on the theory for linear differential equations, it has also worked for nonlinear problems. Extra computation is negligible; the test involves checking the signs of previously computed numbersA simple “mechanical” procedure is described for checking equality of regular expressions. The procedure, based on the work of A. Salomaa, uses derivatives of regular expressions and transition graphs.Given a regular expression R, a corresponding transition graph is constructed. It is used to generate a finite set of left-linear equations which characterize R. Two regular events R and S are equal if and only if each constant term in the set of left-linear equations formed for the pair (R S) is (&phgr; &phgr;) or (^ ^).The procedure does not involve any computations with or transformations of regular expressions and is especially appropriate for the use of a computer.A boundary value problem for the quasi-linear elliptic equation (xx/q2s)x + (xy/q2s)y = 0, where q2 = xx2 + xy2, 0 ≤ s < 1/2, is solved numerically, and the numerical process is analyzed mathematically.An extension of the work initiated by Quine in reducing an arbitrary Boolean truth function to its minimal form is presented. Apart from the unique parts of the form, the entire class of nonunique forms is discussed. The portion of the truth table that is left uncovered by the unique parts of the solution is partitioned into topologically invariant components of which it is the direct sum. Each component may be covered independently of the others. The generation of the set of coverings of a component is developed around a central theorem: A union of cells, all basic to a particular vertex, contains no further cells basic to that vertex. A proof of the theorem is given. The components are components in the topological sense and are preserved under changes of representation. The discussion focuses on the general, unrefinable structure of a Boolean function, as opposed to practical means for calculating its minimal coverings.If an automaton is strongly connected, all of its automorphisms are regular permutations. It is proved that given any two groups G and H of regular permutations on finite sets A and B, respectively, there exists strongly connected automata @@@@ and @@@@ such that G and H are the automorphisms groups of @@@@ and @@@@, @@@@ × @@@@ is strongly connected and the automorphism group of @@@@ × @@@@ is G × H. Also it is proved that the reduced semigroup of an automaton is a regular group of permutations iff the automorphism group of @@@@ is regular and @@@@ is strongly connected. Using this result we construct examples where the automorphism groups have the above property for all strongly connected automata on A and B, and other examples where the automorphism group of @@@@ × @@@@ properly contains G × H.A number of operations which either preserve sets accepted by one-way stack automata or preserve sets accepted by deterministic one-way stack automata are presented. For example, sequential transduction preserves the former; set complementation, the latter. Several solvability questions are also considered.
Many sequences are most efficiently generated on a digital computer with a sieving procedure in which one represents in the main memory of the machine a set of elements known to contain the desired sequence and then systematically sieves out elements not in the desired sequence. In this expository paper, the technical aspects of programming such sieves are discussed. Special attention is given to the most efficient methods of representing sets in the main memory of the machine as well as the programming difficulties encountered when sieving on these sets. The paper concludes with a discussion of four examples in which sieving procedures were employed.A basic hypothesis is stated about the contextual and co-occurrence properties of synonymous words. On the basis of this hypothesis, several statistics are derived for use in discriminating between pairs of words which are synonymous and pairs of words which are nonsynonymous. The discriminating power of these statistics is tested on a corpus consisting of titles of physics theses. The tests indicate that two of the derived statistics have relatively high discriminating power. The results are interpreted and the possibility of obtaining better discriminating power is discussed.A program is described which enables a digital computer to perform the formal algebraic manipulations and differentiations required to test a set of algebraic partial differential equations for consistency, and, if the set is consistent, to reduce the equations in it to such an extent that the nature of the arbitrary functions uniquely generating all local, analytic solutions is apparent upon inspection. The computer performs these operations on polynomials in a nonnumerical sense, treating variables and derivatives of variables precisely as they are treated in the symbolic operations of algebra and calculus. The language which permits the computer to deal with variables and polynomials in this fashion is described. The general mathematical problem of testing and reduction of sets of algebraic partial differential equations is briefly described together with the techniques available for resolving it as adapted to this computer language. A brief description of the FORTRAN program itself is also given. The motivation for the development of this program was the study of the Einstein gravitational equations in general relativity. It has been shown that these equations can be invariantly reduced to a finite number of sets of algebraic partial differential equations, and the problem of testing and reducing these sets led to this program. The program reported on in this paper has been successfully tested for comparatively simple problems, some of which are described here, but must be further developed for the relativity applications.An error analysis of direct methods (i.e., Gaussian elimination or triangular factorization) of solving simultaneous linear algebraic equations is performed in the backward mode, in which the computational errors are expressed as perturbations on the data. Bounds are found for perturbations on the coefficients of the equations, leaving the right-hand sides unchanged. These bounds can be evaluated concurrently with the computation itself, with only a small increase in computing effort. Because they use information obtained during the solution process, these bounds avoid exaggerating the magnitude of the error, and so are also useful as error estimates.A modification of Davidon's method for the unconstrained minimization of a function of several variables is proposed in which the gradient vector is approximated by differences. The step sizes for the differencing are calculated from information available in the course of the minimization and are chosen to approximately balance off the effects of truncation error and cancellation error. Numerical results and comparisons with other methods are given.To obtain high-order integration methods for ordinary differential equatic which combine to some extent the advantage of Runge-Kutta methods on one hand and line multistep methods on the other, the use of “modified multistep” or “hybrid” method has been proposed by various researchers. In this paper formulas are derived for method which use one extra intermediate point than in the previously published methods so that there are analogues of the fourth-order Runge-Kutta method. A five-stage method of order 7 is already given.A method of analysis of uniform random number generators is developed, applicable to almost all practical methods of generation. The method is that of Fourier analysis of the output sequences of such generators. With this tool it is possible to understand and predict relevant statistical properties of such generators and compare and evaluate such methods. Many such analyses and comparisons have been carried out. The performance of these methods as implemented on differing computers is also studied. The main practical conclusions of the study are: (a) Such a priori analysis and prediction of statistical behavior of uniform random number generators is feasible. (b) The commonly used multiplicative congruence method of generation is satisfactory with careful choice of the multiplier for computers with an adequate (≥ ∼ 35-bit) word length. (c) Further work may be necessary on generators to be used on machines of shorter word length.This paper discusses a set of polynomials, {&phgr;r(s)}, orthogonal over a discrete range, with binomial distribution, b(s; n, p), as the weighting function. Two recurrence relations are derived. One expresses &phgr;r in terms of &phgr;r-1 and &Dgr;&phgr;r-1, while the other relates &phgr;r with &phgr;r-1 and &phgr;r-2. It is shown that these polynomials are solutions of a finite difference equation. Also considered are two special cases. The first is the set of Hermite polynomials derived as a limiting case of the binomial-weighted orthogonal polynomials. The second deals with the Poisson distribution used as the weighting function.Let @@@@ be an integral domain, P(@@@@) the integral domain of polynomials over @@@@. Let P, Q ∈ P(@@@@) with m @@@@ deg (P) ≥ n = deg (Q) > 0. Let M be the matrix whose determinant defines the resultant of P and Q. Let Mij be the submatrix of M obtained by deleting the last j rows of P coefficients, the last j rows of Q coefficients and the last 2j+1 columns, excepting column m — n — i — j (0 ≤ i ≤ j < n). The polynomial Rj(x) = ∑ii=0 det (Mij)xi is the j-t subresultant of P and Q, R0 being the resultant. 
If b = £(Q), the leading coefficient of Q, then exist uniquely R, S ∈ P(@@@@) such that bm-n+1 P = QS + R with deg (R) < n; define R(P, Q) = R.  Define Pi ∈ P(F), F the quotient field of @@@@, inductively: P1 = P, P2 = Q, P3 = RP1, 
P2 Pi-2 = R(Pi, Pi+1)/c&dgr;i-1+1i for i ≥ 2 and ni+1 > 0, where ci = £(Pi), ni = deg (Pi) and &dgr;i = ni — ni+1. P1, P2, …, Pk, for k ≥ 3, is called a reduced polynomial remainder sequence. Some of the main results are: (1) Pi ∈  P(@@@@) for 1 ≤ i ≤ k; (2) Pk = ± AkRnk-1-1, when Ak = &Pgr;k-2i-2c&dgr;i-1(&dgr;i-1)i; (3) c&dgr;k-1-1k Pk = ±Ak+1Rnk; (4) Rj = 0 for nk < j < nk-1 — 1. Taking @@@@ to be the integers I, or Pr(I), these results provide new algorithms for computing resultant or greatest common divisors of univariate or multivariate polynomials. Theoretical analysis and extensive testing on a high-speed computer show the new g.c.d. algorithm to be faster than known algorithms by a large factor. When applied to bivariate polynomials, for example this factor grows rapidly with the degree and exceeds 100 in practical cases.A new class of multistep formulas with variable coefficients is derived for the numerical integration of the ordinary differential equation y′ = f(x, y) whose theoretical solution possesses a singularity. A first numerical solution is obtained along with estimates for the behavior of the singularity, and these estimates are then used to obtain further numerical solutions.In April, 1964, Gragg and Stetter published a set of numerical methods for solving y′ = f(x, y) which rely on some very accurate correctors, using a “nonstep” point within the interval of integration. However, the present authors found that this accuracy is largely lost unless one also uses accurate predictors, also using a “nonstep” point within an interval of integration, of the same order of accuracy as that of the correctors. Numerical examples indicate that on the average those differential equations tested can be solved by the method of the sixth order proposed in this paper in about two-thirds the time required by an Adams method of the same order.Let N(T) be the normal system of Post which corresponds to the Thue system, T, as in Martin Davis, Computability and Unsolvability (McGraw-Hill, New York, 1958), pp. 98-100. It is proved that for any recursively enumerable degree of unsolvability, D, there exists a normal system, NT(D), such that the decision problem for NT(D) is of degree D. Define a generalized normal system as a normal system without initial assertion. For such a GN the decision problem is to determine for any enunciations A and B whether or not A and B are equivalent in GN. Thus the generalized system corresponds more naturally to algebraic problems. It is proved that for any recursively enumerable degree of unsolvability, D, there exists a generalized normal system, GNT(D), such that the decision problem for GNT(D), such that the decision problem for GNT(D) is of degree D.Compilation consists of two parts, recognition and translation. A mathematical model is presented which embodies salient features of many modern compiling techniques. The model, called the stack automaton, has the desirable feature of being deterministic in nature. This deterministic device is generalized to a nondeterministic device (nondeterministic stack automaton) and particular instances of this more general device are noted. Sets accepted by nondeterministic stack automata are recursive. Each set accepted by a deterministic linear bounded automaton is accepted by some nonerasing stack automaton. Each context-sensitive language is accepted by some (deterministic) stack automaton.
A formal system is presented for the differentiation of mathematical formulas which can be implemented on a digital computer. The system departs in concept from others in that the programming language is nonprocedural, permitting the statement of the problem for the machine to more closely resemble the corresponding statement for the human. Although limited ostensibly to the narrow and well-defined problem of mechanizing differentiation, many of the ideas and techniques presented can be applied in the broader context of the deeper problem of mechanizing mathematics. They should also be useful in programming theory, where the fundamental problem continues to be the narrowing of the gap between a natural language statement of a problem and a formal language statement acceptable to a computer.The Romberg method of numerical integration is extended to multidimensional integration. The elements of the mth column of the Romberg array are shown to approximate up to 0(h2m+2) provided the integrand has 2m+2 bounded partial derivatives in each variable.With the availability of high speed electronic computers it is now quite convenient to devise statistical experiments for the purpose of estimating certain mathematical constants and functions. The paper contains statistical formulas for estimating such mathematical constants and functions as &pgr;, C (Euler's constant), e, &PSgr;(1)(1), &Ggr;(x), In x, B(x, y), arctan x, &PSgr;(p) and polygamma functions. Statistical estimates of these quantities may be used to construct desired confidence intervals for these parameters. Although numerical techniques are available to approximate these mathematical quantities very satisfactorily, a statistical approach to these problems seems to deserve mention in the scientific literature. Numerical illustrations are given in the paper which also give some indication of the effect of pseudorandom numbers on the final results. Statistical procedures, considered in the paper, make extensive use of a high speed electronic computer which may help to develop a positive attitude among theoreticians, promising students of mathematics and others toward the role of computers in the ever-expanding scientific work.The occurrence pattern of clusterings of keys is analyzed by use of the generating function. Probabilities of occurrences of several clustered patterns are given as the coefficients of generating functions. Moreover, the result when a key-to-address transformation of aK + b/mod c is performed on clustered keys is briefly discussed.It has long been known that increasing the number of tapes used by a Turing machine does not provide the ability to compute any new functions. On the other hand, the use of extra tapes does make it possible to speed up the computation of certain functions. It is known that a square factor is sometimes required for a one-tape machine to behave as a two-tape machine and that a square factor is always sufficient.The purpose of this paper is to show that, if a given function requires computation time T for a k-tape realization, then it requires at most computation time T log T for a two-tape realization. The proof of this fact is constructive; given any k-tape machine, it is shown how to design an equivalent two-tape machine that operates within the stated time bounds. In addition to being interesting in its own right, the trade-off relation between number of tapes and speed of computation can be used in a diagonalization argument to show that if T(n) and U(n) are two time functions such that inf T(n) log T(n) ÷ U(n) = 0 then there exists a function that can be computed within the time bound U(n) but not within the time bound T(n).The use of Turing machines for calculating finite binary sequences is studied from the point of view of information theory and the theory of recursive functions. Various results are obtained concerning the number of instructions in programs. A modified form of Turing machine is studied from the same point of view. An application to the problem of defining a patternless sequence is proposed in terms of the concepts here developed.In this report, certain properties of context-free (CF or type 2) grammars are investigated, like that of Chomsky. In particular, questions regarding structure, possible ambiguity and relationship to finite automata are considered. The following results are presented:

The language generated by a context-free grammmar is linear in a sense that is defined precisely.
The requirement of unambiguity—that every sentence has a unique phrase structure—weakens the grammar in the sense that there exists a CF language that cannot be generated unambiguously by a CF grammar.
The result that not every CF language is a finite automaton (FA) language is improved in the following way. There exists a CF language L such that for any L′ ⊆ L, if L′ is FA, an L″ ⊆ L can be found such that L″ is also FA, L′ ⊆ L″ and L″ contains infinitely many sentences not in L′.
A type of grammar is defined that is intermediate between type 1 and type 2 grammars. It is shown that this type of grammar is essentially stronger than type 2 grammars and has the advantage over type 1 grammars that the phrase structure of a grammatical sentence is unique, once the derivation is given.
The language generated by a context-free grammmar is linear in a sense that is defined precisely.The requirement of unambiguity—that every sentence has a unique phrase structure—weakens the grammar in the sense that there exists a CF language that cannot be generated unambiguously by a CF grammar.The result that not every CF language is a finite automaton (FA) language is improved in the following way. There exists a CF language L such that for any L′ ⊆ L, if L′ is FA, an L″ ⊆ L can be found such that L″ is also FA, L′ ⊆ L″ and L″ contains infinitely many sentences not in L′.A type of grammar is defined that is intermediate between type 1 and type 2 grammars. It is shown that this type of grammar is essentially stronger than type 2 grammars and has the advantage over type 1 grammars that the phrase structure of a grammatical sentence is unique, once the derivation is given.The problem of whether a given context-free language is linear is shown to be recursively undecidable.Call a (context-free) language unambiguous if it is not inherently ambiguous. In the absence of evidence to the contrary, the suspicion has arisen that the unambiguous languages might be precisely those languages with context-free complements. The two theorems presented in this paper lay the suspicion to rest by providing (1) an inherently ambiguous language with context-free complement and (2) an unambiguous language without context-free complement. This establishes the independence of inherent ambiguity from complementedness among the context-free languages.A natural problem, related to the (known unsolvable) halting problem for Post normal systems, arises when one considers, for a given Post normal system, whether the system halts (eventually) on every word over its alphabet. For polygenic systems, this problem is shown to be recursively unsolvable by constructing a reduction from the “Domino Problem.”A substitution rule for obtaining discrete analogs for continuous filters is given. Transient responses of filters related by the rule are in exact agreement. Steady-state responses are in close agreement.An economical approach is described for estimating power spectra from sampled data through the application of Z-transform theory. The method consists of computing a weighting sequence whose frequency characteristics approximate a one-third octave bandwidth filter, and applying these coefficients recursively to the digitized data record. Filtering is followed by a variance calculation and a division by the appropriate filter bandwidth. A specific example of power spectra computed in the usual manner (Fourier transformation of the autocorrelation function) and power spectra computed by the method in this paper demonstrates the practicability of the technique. The most significant advantage is the economical aspect. It is shown that owing to the variable bandwidth and the small number of filtering coefficients, the savings that may be realized by the employment of this technique, in lieu of the autocorrelation transformation approach, may be quite considerable, depending on the record length and the number of lag products.The object of the work described was to develop a procedure for the allocation of logic to cards, boards, platters and so on which utilize modular circuit design and etched wiring techniques.A chain of logic elements is a set of interconnected (logic) elements two of which connect to, at least, the remaining elements of this set. Every logic diagram has a characteristic set of chains. A subset of these chains is selected to initiate the determination of the locations of each element on a card. The elements of a chain are assigned to one row or column of platter locations. The positions of the chains in relation to one another are determined by such criteria as total wire length reduction, proximity requirements between gates based on electrical considerations and the proximity requirements between elements and pins based on timing considerations.The effectiveness of the automatic procedure was tested during the design of small cards (mounting 16 modules, up to 64 gates). In these circumstances human experience and insight are at their best. The layouts of experienced technicians and engineers were compared with the layouts produced by the program. Significant similarities between the determinations were found in all cases tested.
The concept of time-shared computer operations is briefly described and a model of a time-sharing system is proposed, based on the assumption that both interarrival and service times possess an exponential distribution. Although the process described by this model is non-Markovian, an imbedded Markov chain is analyzed by exploiting the fact that the instants of completion of a “quantum” of service are regeneration points. It is shown that user congestion possesses a limiting distribution, and the method of generating functions is used to derive this distribution. The concept of cycle time is discussed and two measures of cycle time developed for a scheduling discipline employing a single queue. Finally, a number of numerical examples are presented to illustrate the effect of the system parameters upon user congestion, system response time and computer efficiency.This paper begins with a brief description of desicion tables, and then presents a discussion of alternative expressions for them as sequential testing procedures for computer implementation and as Boolean functions. An algorithm is developed which, in a finite number of steps, will convert any given limited entry decision table into an “optimal” computer program, one with minimum average processing time. The algorithm is more general than procedures previously developed and guarantees optimality of the resultant computer program. Previous procedures required two distinct steps and gave no assurance of overall optimality. Computer implementation of the algorithm is also discussed.From an arbitrary Turing machine, Z, a monogenic Post normal system, v(Z), is constructed. It is then shown not only that the halting problem for Z is reducible to that of v(Z) but also that the halting problem for v(Z) is reducible to that of Z. Since these two halting problems are of the same degree, the halting problem for the normal system can have an arbitrary (recursively enumerable) degree of undecidability.Various elementary operations are studied to find whether they preserve on ambiguity and inherent ambiguity of language (“language” means “context-free language”) The following results are established:

If L is an unambiguous language and S is a generalized sequential machine, then (a) S(L) is an unambiguous language if S is one-to-one on L, and (b) S-1(L) is an unambiguous language.
Inherent ambiguity is preserved by every generalized sequential machine which is one-to-one on the set of all words.
The product (either left or right) of a language and a word preserves both unambiguity and inherent ambiguity.
Neither unambiguity nor inherent ambiguity is preserved by any of the following language preserving operations: (a) one state complete sequential machine; (b) product by a two-element set; (c) Init(L) = [u ≠ dur in L for some v]; (d) Subw(L) = [w ≠ durr in L for some u, v].
If L is an unambiguous language and S is a generalized sequential machine, then (a) S(L) is an unambiguous language if S is one-to-one on L, and (b) S-1(L) is an unambiguous language.Inherent ambiguity is preserved by every generalized sequential machine which is one-to-one on the set of all words.The product (either left or right) of a language and a word preserves both unambiguity and inherent ambiguity.Neither unambiguity nor inherent ambiguity is preserved by any of the following language preserving operations: (a) one state complete sequential machine; (b) product by a two-element set; (c) Init(L) = [u ≠ dur in L for some v]; (d) Subw(L) = [w ≠ durr in L for some u, v].Multiwrite (selective writing in words which satisfied a previous search) is shown to permit parallel evaluation of any Boolean function (including arithmetic) in a search memory. A general method is given which expresses the Boolean function as a sum of products, searches for these products one at a time and OR8 the results of these searches. An example applying the process to arithmetic given.Some new predictors are presented for use with the Adams-Moulton correctors of orders 4 through 8. The resulting algorithms have better stability characteristics than the usual ones which employ the Adams Bashforth predictors and at the same time require no additional storage. The regions of absolute and relative stability for the methods mentioned above and for the iterated Adams-Moulton correctors are given. Results obtained using the various methods are compared.Recent developments in computer design and error analysis have made feasible the use of variable precision arithmetic and the preparation of programs that automatically determine their own precision requirements. Such programs enable the user to specify the accuracy he wants, and yield answers guaranteed to lie within the bounds prescribed. A class of such programs, called “contracting error programs,” is defined in which the precision is determined by prescribing error bounds on the data. A variant of interval arithmetic is defined which enables a limited class of algorithms to be programmed as contracting error programs. A contracting error program for the solution of simultaneous linear equations is described, demonstrating the application of the idea to a wider class of problems.A method is proposed for internal sorting which has the property that the expected time required per record, to sort n records with random keys, is a bounded function of n. Moreover the storage required in addition to that for the records is only 2n address fields. It appears that the method may be a good choice when the number of records to be sorted is of the order of 10,000 or more.The method consists of partially sorting the records by a radix sort and completing the sort by sifting. In the radix sort, a list type of structure is used to present the distribution of the records.Conventional shift counters are shown to be a special case of the general class of counters that use single-ended information on all but one bit position of JK flip-flops. The resultant class of counters preserves the autocorrelation characteristics necessary for pseudo-random number generation while allowing a saving in the physical mechanization of a given characteristic equation. A companion matrix in canonical form is developed that yields a direct mapping from the characteristic equation to the requisite logic on each flip-flop.A method is described for the numerical inversion of Laplace transforms, in which the inverse is obtained as an expansion in terms of orthonormal Laguerre functions. In order for this to be accomplished, the given Laplace transform is expanded in terms of the Laplace transforms of the orthonormal Laguerre functions. The latter expansion is then reduced to a cosine series whose approximate expansion coefficients are obtained by means of trigonometric interpolation. The computational steps have been arranged to facilitate automatic digital computation, and numerical illustrations have been given.Gregory's formula for numerically integrating a function is one of the most promising formulas for use in a computer library. This paper shows how Gregory's formula can be generalized, and examines special cases which have a number of very favorable properties for library use.The “reverse order law” related to ordinary inverses of matrix products, i.e., (AB)-1 = B-1A-1, is generally not transferable to the generalized inverse. There are, however, applications in which the reverse order law related to the generalized inverse reveals interesting properties in certain classes of matrices.In this paper, some necessary and sufficient conditions for the reverse order property to hold are given.The design of an abstract machine with a recursive function programming language which avoids the predicate type of conditional is described. It is shown that trough the adoption of list processing techniques it has been possible to construct a simple simulator for the machine in FORTRAN. A program for the machine which causes it to perform symbolic differentiation with some algebraic manipulation of the expressions concerned is given as an example of the type of computations which may be performed.
Results are obtained for a model of many processors operating in series. The results are obtained directly by recognizing that a sequential processing may be viewed as a cyclic queue. Exact results are given for two sequential processing stages with a buffer storage of arbitrary size between the stages, and approximate results for the case of 2M (M an integer) stages. The analysis is good only for exponentially distributed computation times.BE VISION is a package of FORTRAN programs for drawing orthographic views of combinations of plane and quadric surfaces. As input, the package takes rectangular coordinate equations specifying the surfaces plus a three-angle specification of the viewing direction. Output is a drawing on the Stromberg Carlson 4020 Microfilm Recorder. Many views of one scene may be obtained simply by changing the viewpoint.The various subroutines of the package and their functions are described in this paper. It also gives numerous examples of pictures that were produced by BE VISION. The package has been in use since April 1964.A method of finding every cycle of an undirected linear graph by computation, rather than search, is presented. The method consists of three algorithms. The first produces a fundamental set of cycles from which all others can be generated. The second groups these cycles according to the nonseparable subgraphs of the original graph, and produces an ordering among groups that satisfies a condition required for the third algorithm. The third algorithm generates all and only cycles of the graph, without duplicates.Discrete sequential systems, such as sampled data systems, discrete Markov processes, linear shift register generators, computer programs, sequential code generators, and prefixed comma-free codes, can be represented and studied in a uniform manner by directed graphs and their generating functions. In this paper the properties of the generating functions are examined from certain connectivity considerations of these graphs.A one-normal system is a Post production system on a finite alphabet {s1, s2, · · ·, s&sgr;} with productions siP → PEij, where i ranges over a subset of {1, 2, · · ·, &sgr;} and, for fixed i, j takes on the values 1, 2, · · ·, ni. The following derivability problem is shown to be solvable for each such system: Given two words P and Q, decide whether Q can be derived from P by successive applications of the production rules. The result was proved by Hao Wang for the monogenic case (i.e., when each ni = 1).A computer is a set M (the memory), a set B, a class of maps S: M → B, known as states, and a class @@@@ of maps T: @@@@ → @@@@, known as instructions. Each instruction I has an input region IR(I), an output region OR(I), and affected regions AR(M′, I), for M′ ⊆ IR(I). For example, let I be the instruction (CLA Y) on the IBM 7094. If L is the location counter and AC is the accumulator, then IR(I) = Y ∪ L and OR(I) = AC ∪ L; if M′ is the address portion of Y, then AR(M′, I) is the address portion of AC. The fundamental properties of all these notions are derived, and computers are related to other models, such as sequential machines. The existence problem (how arbitrarily the input, output and affected regions of an instruction can be specified) is fully settled for countable memory M.The problem of deciding predicates concerned with m-tuples of words by means of 2-way multitape finite automata is considered. In general, more than m tapes are used to decide a predicate concerned with m-tuples of words. In such a way, a necessary and sufficient condition for a predicate to be decidable by a 2-way multitape finite automaton is obtained. The relation between the ability of Turing machines and that of 2-way multitape finite automata is also discussed.The circumstances and methods of the synthesis of linear digital recursive filters which are both stable and physically realizable are described. It is shown that any amplitude requency transfer function expressible as an even trigonometric rational polynomial can be synthesized by a real stable linear digital recursive filter. The degree of the corresponding difference equation is twice the degree of the denominator of the rational trigonometric polynomial.A class of even rational trigonometric functions which exhibit pointwise convergence to the deal rectangular low or high-pass amplitude frequency transfer function is chosen. A member of this class is shown to approximate more closely the ideal rectangular filter than does the corresponding classical continuous Butterworth filter. This class of filters is then mechanized. The phase and unit-impulse response functions are calculated for the corresponding difference equations of degrees 2 and 4.When is a set A of positive integers, represented as binary numbers, “regular” in the sense that it is a set of sequences that can be recognized by a finite-state machine? Let &pgr; A(n) be the number of members of A less than the integer n. It is shown that the asymptotic behavior of &pgr; A(n) is subject to severe restraints if A is regular. These constraints are violated by many important natural numerical sets whose distribution functions can be calculated, at least asymptotically. These include the set P of prime numbers for which &pgr; P(n) @@@@ n/log n for large n, the set of integers A(k) of the form nk for which &pgr; A(k)n) @@@@ nP/k, and many others. The technique cannot, however, yield a decision procedure for regularity since for every infinite regular set A there is a nonregular set A′ for which | &pgr; A(n) — &pgr; A′(n) | ≤ 1, so that the asymptotic behaviors of the two distribution functions are essentially identical.Linear initial value problems, particularly involving first order differential equations, can be transformed into systems of higher order and treated as boundary value problems. The type of difference equations used to replace the associated second order boundary value problem are yn - 2yn+1 + yn+2 = h2 ∑ &bgr;iy″n+i + h3; ∑ &dgr;iy‴n+i + · · ·, n = 1, 2, · · ·, N - 1 and - yN + yN+1 = h(b0y′N + b1y′N+1) + h2;(c0y″N + c1y″N+1) + · · ·.Numerical techniques referred to as M1, M2, and M3 have been developed in which error is O(h4), O(h6) and O(h8), respectively. Experimental results have been given to demonstrate the usefulness of method M3 over  M1 or M2.Conformal maps of simply connected regions onto the interior and exterior of the unit circle can be computed from complex polynomials orthonormal over the boundary of the given region. As an example of this method, maps of ellipses, rectangles, limacons and ovals of Cassini were computed on the IBM 7070. The results indicate that, in general, the method gives good accuracy, but roundoff error can become very serious, indicating the desirability of computing with greater precision. For fourfold symmetric regions, the algorithm can be simplified to decrease significantly the amount of computation resquired to achieve the same accuracy as that achieved by the algorithm for the general case.Some problems arising in multicategory (many pattern types) pattern recognition are treated mathematically, and formulas are derived which describe some inherent limitations associated therewith. The principal results concern the “dimensional” and “correlational” effects and their degradation of a multimeasurement recognition system.
The problem studied is the effect of a time-sharing environment on the structure of programs and on the addressing strategies which may be employed in the hardware. An account is given of some very recent developments toward reduction in the system overhead needed to facilitate time-sharing. One hardware-software scheme designed to implement this reduction is described in some detail.An automatic programming system for the M-20 computer at the Computing Centre of the Siberian Division of the USSR Academy of Sciences has been developed. The translator is a compiler which accepts source programs written in ALPHA language (hardware representation of which input language is an extension of ALGOL 60). The language extensions include the ability to handle complex quantities and the ability to consider variables as matrices or vectors. Certain language limitations are imposed concerning recursion and the correspondence between parameters in procedure definitions and calls. The translator itself consists of two phases. The first phase contains 14 blocks and translates a source program to an intermediate language. The second phase which consists of 10 blocks converts the internal language to machine code. The principal objective in designing the compiler was the production of fast, compact object code. Techniques for such achievement are detailed. Since the compiler has been operational for some time, comparative figures are given for a series of programs as compiled by the ALPHA translator and as handwritten by programmers. Further figures compare ALPHA with TA-1 and TA-2 (earlier compilers written for the same computer). A special debugging system operates in conjunction with the ALPHA translator.Various classes of machines incorporating parallelism are considered. A general class of large-scale multiprocessors is outlined, and some problems of hardware and software implementation for computers of this class are discussed.This paper deals with the synthesis of sequential machines (without a distinguished initial state) which satisfies a specified list of input sequences and corresponding output sequences. Readily testable necessary and sufficient conditions are given for such a list to result in a realizable machine, and an algorithm is formulated for constructing the machine when these conditions are fulfilled.A procedure for index register allocation is described. The rules of this procedure are shown to yield an optimal allocation for “straight line” programs.Four principal results about ambiguity in languages (i.e., context free languages) are proved. It is first shown that the problem of determining whether an arbitrary language is inherently ambiguous is recursively unsolvable. Then a decision procedure is presented for determining whether an arbitrary bounded grammar is ambiguous. Next, a necessary and sufficient algebraic condition is given for a bounded language to be inherently ambiguous. Finally, it is shown that no language contained in w1*w2*, each w1 a word, is inherently ambiguous.The analytic grammar (a model which provides a rigorous description of syntactic analysis) is presented, and some of its fundamental properties are shown. Various submodels are discussed and equivalences among these are noted.An analytic grammar incorporates a set P of syntactic productions, and also a scan @@@@. At each successive “rewriting” in the analysis of a string x, @@@@ computes a subset of productions applicable to x (i.e., which may be used to “rewrite” x) from the set of productions which are potentially applicable to x. Thus each scan determines a class of grammars.It is shown that all analytic languages are recursive, and conversely, all recursive sets are analytic languages. All phrase structure grammars are analytic grammars. A simple sufficient condition is shown under which an analytic grammar provides unique analyses for all strings.Particularly relevant to syntactic analysis of algorithmic languages (i.e., languages which are used to specify computing algorithms) are the “leftmost” scans, each of which chooses a certain “leftmost” production. Conditions which provide equivalences among these scans are noted.Maehly's second method is a general algorithm for finding the best Chebyshev approximation to a continuous function on a finite interval. This paper examines the convergence properties of Maehly's second method, a modification, and the more commonly used algorithm of Remez, using both analytical and numerical results.A third order two step method and a fourth order two step method for the numerical solution of the vector initial value problem dy ÷ dx=F(y), y(a) = n can be defined by making evaluations of F similar to those found in a classical Runge-Kutta formula. These two step methods are different from classical Runge-Kutta methods in that evaluations of F made at the previous point are used along with those made at the current point in order to obtain the solution at the next point. If the stepsize is fixed, this use of previous computations makes it possible to obtain the solution at the next point by evaluating F two or three times for the third or fourth order method, respectively.These methods are consistent with the initial value problem and are shown to be convergent with its unique solution under certain restrictions. The local truncation error terms are given. Finally, a few numerical results are presented.An iterative method is described for computing optimum values of a set of parameters for the best agreement in a least squares between a given set of (experimental) values and corresponding calculated (theoretical) values where these may be dependent non-linearly on the parameters. The method is for use when (a) the amount of work in the calculation of the theoretical values is large as compared with the amount required to solve a set of linear equations in the same number of unknowns; and (b) the amount of work in calculating the first order partial derivatives of all the theoretical values with respect to one parameter would be even greater, so that using finite difference approximations to the partial derivatives is preferable. In addition to setting up and solving the normal equations at each step, the latent vectors of their matrix and estimates of the errors on the current parameter values are computed and used. Also the known physical limits of the range of each parameter are used. The procedure has second order convergence.A simple and yet powerful method of solving two of the more common numerical problems is heuristically derived and briefly discussed. The method makes possible the efficient solution of the zeros of a complex function, either transcendental or algebraic, of a complex variable. In addition, it is applicable to the computation of eigen values of a general matrix in which the parameter may appear in any elements of the matrix in a basically unrestricted way. The method is related to the classical secant and regula falsi methods for the finding of real zeros of a real function. Numerical examples of the method applied to several pathological matrices are presented.The feasibility of Monte Carlo linear extrapolation of multivariable functions is discussed. The method considered is a modified version of the Monte Carlo method for linear interpolation. An effective truncation procedure is introduced, which not only economizes the machine time to a greet extent, but also serves for the purpose of variance reduction. A few examples were tested, all of which have shown that the method is promising.Formulas and algorithms have recently been given for calculating the number of symmetry types of functions, networks and automata under various transformation groups. In almost all cases, the computations involved are quite difficult and require the use of a digital computer. In this paper, asymptotic estimates are given for these numbers, which are trivial to compute and which are very accurate in most cases even for small values of the parameters.The theory of finite automata is closely linked with the theory of Kleene's regular expressions. In this paper, two formal systems for the algebraic transformation of regular expressions are developed. Both systems are consistent and complete; i.e., the set of equations derivable within the system equals the set of equations between two regular expressions denoting the same event. One of the systems is based upon the uniqueness of the solution of certain regular expression equations, whereas some facts concerning the representation theory of regular events are used in connection with the other.The purpose is to investigate the input structure of automata which have a group-like character. The class of perfect automata investigated by Fleck in [2] and Weeg in [6] is a proper subclass of those considered here.

Methods are described for the derivation of minimax and near-minimax polynomial approximations. For minimax approximations techniques are considered for both analytically defined functions and functions defined by a table of values. For near-minimax approximations methods of determining the coefficients of the Fourier-Chebyshev expansion are first described. These consist of the rearrangement of the coefficients of a power polynomial, and also direct determination of the coefficients from the integral which defines them, or the differential equation which defines the function. Finally there is given a convenient modification of an interpolation scheme which finds coefficients of a near-minimax approximation without requiring numerical integration or the numerical solution of a system of equations.For the computer to be a tool of the general scientist rather than of the computer specialist it will have to conform to the characteristics of interpersonal communications, since these are rooted in the characteristics of human intelligence. It must not require the scientist to use a language which is formal or which requires special study. It must, like a human assistant, share with each particular employer the labor of establishing a common language and of formulating clear instructions. This shared labor must take the form of a dialog in which the computer asks questions on obscure portions of its instructions. The computer must accept as answers either paraphrases or explanations. It should not need to repeat identical or similar questions. It should choose fundamental or revealing questions; if necessary an employer can encourage this by giving fundamental answers to superficial questions.The method of least squares is often used to determine the arbitrary constants in an empirical formula or approximating function. If such a function is not linear in its unknowns, approximate values must first be obtained. Typically they are obtained graphically or as the result of physical supposition or similar experiment. This paper describes a program written to find first approximations for certain oscilloscope traces by: (1) displaying the trace on a cathode-ray tube (CRT) and, (2) imposing upon it a computer generated curve which (3) employs parameters arbitrarily supplied by an operator in a quest for visual coincidence.While the technique proved successful in this application, a greater value may lie in its having indicated a mechanism for further automation. In its present form the method might best be utilized as a tool for studying the behavior of selected approximating functions.A storage or processing device with a fixed number of cells in considered. If a chance-mechanism selects which of M persons is to transmit an element of message to the device, the probability is found that the device will be filled without a complete message having been introduced into it. This probability should be useful in determining the “optimum” number of persons who can use the device. The problem is treated as one in an M-dimensional random walk.The latent class structure is modified by requiring the number of latent classes to be equal to the number of keywords. This modification changes the latent structure to one that consists of matrices that are both symmetric and positive definite, therefore making a “computer solution” both possible and practical. The modified latent structure is derived, the numerical analysis considerations for a computer solution are presented, and an example illustrates the use of the latent class structure in both file organization and retrieval.The first problem in a two level Boolean minimization is the determination of the prime k-cubes. This paper is concerned with the estimation of the statistical complexity of some well-known algorithms which solve this problem. Formulas are given for the average number of comparison operations among k-cubes occurring in Quine's method and in Mc-Cluskey's method; these quantities provide indications of the average execution time of computer programs based on the corresponding algorithms. Numerical values are given and commented on.Formulas are also obtained for the variance of the number of k-cubes and the variance of the number of cubes of a Boolean function; in fact the calculation of these quantities is strictly related to that of the average number of comparison operations among k-cubes.These variances give an idea of the probable error made by using the corresponding average values (obtained in a previous paper by the authors) to make forecasts. It turns out that this error is quite small.The equation M′ = Vdi-1 Mi where M′ is supposed to be given is discussed. First it is proved that there is a necessary and sufficient condition for the existence of a solution and, simultaneously, that under such condition, M′ will itself be a solution. However, the main aim is precisely to present an algorithm which gives the so-called minimal solutions: Boolean matrices M satisfying the equation with the least possible number of unity entries. This is proved by the last theorem.The subject may be studied in the context of graph theory and it seems of interest in various fields of operational research.An algorithm is presented for finding a solution, and the value of a solution, to n + 1 linear equations in n unknowns. The arrangement of the computation makes it convenient for use in computing Chebyshev-type approximations by polynomials. The algorithm is particularly efficient if only the value of a solution is desired.A construction is given of a one-dimensional iterative of finite-state sequential machines, which can generate in real time the binary sequence representing the set of prime numbers.Pinning down the maximum of a function in one or more variable is a basic computing problem. Without further a priori information regarding the nature of the function, of course, the problem is not feasible for computation. Thus if the function is permitted to oscillate infinitely often then no number of evaluations can give information regarding its maximum.J. Kiefer, in his paper, treats the one-dimensional problem and shows that the correct a priori assumption regarding the function is that of “unimodality.” He then gives the complete and exact optimal procedure within this framework.In this paper is given what the author believes is the correct a priori background for functions of several variables. (This is analogous to unimodality in one dimension.) However, there is not obtained the exactness of Kiefer's result, but rather the determination of the correct procedures to within “order of magnitude.”New classes of events and finite automata which generalize the noninitial definite class are introduced. These events, called “ultimate definite” (u.d.), “reverse u.d.” and “symmetric definite,” are investigated as to algebraic and closure properties. Effective decision procedures whereby it can be decided whether a given finite automata defines such an event are given and unique canonical representations for these events are derived.Incompletely specified (Mealy type) sequential machines and their generalizations to infinite machines are discussed. Convenient algebraic techniques for the study of such machines are introduced. These techniques are based on binary relations and on an extension of the usual concept of homomorphism to multivalued mappings. The first part of the paper gives a short, unified introduction to the algebraic theory of partial automata. The second part unifies and extends the algebraic approach to generalized cascade decompositions which combine conventional decomposition techniques with state-splitting procedures. The relationship between such generalized decompositions and state reductions of automata is investigated.Several devices with two input lines and one output line are introduced. These devices are viewed as transformations which operate on pairs of (ALGOL-like) languages. Among the results proved are the following: (i) a pair consisting of a language and a regular set is transformed into a language; (ii) let (V, W) be a pair consisting of a language and a regular set. Then the set of those words w1, for which there exists a word w2 in V so that (w1, w2) is mapped into W, is a language.




Information Retrieval systems may be classified either as Document Retrieval systems or Fact Retrieval systems. It is contended that at least some of the latter will require the capability for performing logical deductions among natural language sentences. The problem of developing systems of logical inference for natural languages is discussed, and an example of such an analysis of a sublanguage of English is presented. An experimental Fact Retrieval system which incorporates this analysis has been programmed for the IBM 7090 computer, and its main algorithms are stated.This study reports the results of a series of experiments in the techniques of automatic document classification. Two different classification schedules are compared along with two methods of automatically classifying documents into categories. It is concluded that, while there is no significant difference in the predictive efficiency between the Bayesian and the Factor Score methods, automatic document classification is enhanced by the use of a factor-analytically-derived classification schedule. Approximately 55 percent of the document were automatically and correctly classified.The purpose of this paper is to consider the limiting conditions associated with the handling of many unrelated input data sources by a digital computer which utilizes binary internal logic. Equations are derived which relate the number of distinct input sources to their frequency through inequalities which specify the limiting conditions, i.e. the maximum number of data sources for a given input data frequency. Inequalities are derived for both serial and parallel input data configurations. The limiting conditions for real-time queuing are derived.The modification of the above conditions when program operations are taken into account is examined, and the information content associated with the limiting conditions is discussed. The absolute limit on computer capacity is derived by considering the limit on the information which can be stored in a given computer. Finally, theoretical conclusions are applied to an exemplary model.The first section of the paper contains a brief description of the well-known technique of using a stack, or pushdown store, to re-order the operators of an arithmetic expression, as defined in ALGOL 60, in order to transform the expression into Reverse Polish parenthesis-free form. It is shown that improvements to this Reverse Polish form can be made quite simply, by extending the use of the stack to include information about the operands of the expression. Firstly, information gained from the declarations of the operands can be used to control the generation of real-integer conversion instructions. Secondly, operators whose operands are numerical constants can be computed during translation, using the partially generated Reverse Polish object program as a second stack.The evaluation of a function of one argument is a standard computational task. When an unnormalized number representation is used, it is appropriate that function evaluation to subject to certain “adjustment” criteria, defined independently of the computing method. In this paper some such criteria are developed, and their application described.In particular, consideration is given to questions of the extent to which general principles apply (i.e. for large classes of functions of a single argument) and the manner in which particular properties of the functions involved (e.g. the functions evaluated by standard library subroutines) may be invoked. The analysis, although framed in terms of unnormalized arithmetic, gives insight into the general nature of significance propagation through function evaluation, and provides a means for analyzing calculations carried out in the usual normalized arithmetic, particularly in the case where an “index of significance” is employed.The order p which is obtainable with a stable k-step method in the numerical solution of y′ = f(x, y) is limited to p = k + 1 by the theorems of Dahlquist. In the present paper the customary schemes are modified by including the value of the derivative at one “nonstep point;” as usual, this value is gained from an explicit predictor. It is shown that the order of these generalized predictor-corrector methods is not subject to the above restrictions; stable k-step schemes with p = 2k + 2 have been constructed for k ≤ 4. Furthermore it is proved that methods of order p actually converge like hp uniformly in a given interval of integration. Numerical examples give some first evidence of the power of the new methods.The problem of defining a smooth surface through an array of points in space is well known. Several methods of solution have been proposed. Generally, these restrict the set of points to be one-to-one defined over a planar rectangular grid (X, Y-plane). Then a set of functions Z = F(X, Y) is determined, each of which represents a surface segment of the composite smooth surface. In this paper, these ideas are generalized to include a much broader class of permissible point array distributions: namely (1) the point arrangement (ordering) is topologically equivalent to a planar rectangular grid, (2) the resulting solution is a smooth composite of parametric surface segments, i.e. each surface piece is represented by a vector (point)-valued function. The solution here presented is readily applicable to a variety of problems, such as closed surface body definitions and pressure envelope surface definitions. The technique has been used successfully in these areas and others, such as numerical control milling, Newtonian impact and boundary layer.A study is made of the step-by-step integration of ordinary differential equations of the first order by means of formulas obtained from the Gregory-Newton backward interpolating formula. Tables of relevant constants are presented.A method for computing an analysis of variance from an algebraic specification of the statistical model is described. The model is written in a form consistent with usual statistical notation but also suitable for computer input and logical manipulation. Calculations necessary to obtain the analysis of variance are then determined by the model. An outline of the computational procedure is given.The aim of the research reported is to put the laboratory application of closed-loop digital computer systems to experimental control and analysis in biology and behavioral science on a formal basis, using special concepts of programming to quantitatively control different parameters of variation in sensory feedback of specific response systems. The theory is unconventional in that the computer and the techniques of closed-loop programming are designed to control time delays, space displacements, kinetic modulations, and informational and symbolic transformations in the sensory feedback that is generated by movements and/or physiological actions of the living subject.The present experiment illustrates application of the system and the methods of closed-loop perturbation programming to study of delayed auditory feedback of speech. The subject's speech is transduced, amplified, converted by an analog-to-digital converter, programmed for delays, deconverted by a digital-to-analog converter, filtered, amplified, and then used to activate protected earphones on the speaker's ears. The computer is also used to introduce many special perturbation of the auditory feedback of speech in real time.
Ianov has defined a formal abstraction of the notion of program which represents the sequential and control properties of a program but suppresses the details of the operations. For these schemata he defines a notion corresponding to computation and defines equivalence of schemata in terms of it. He then gives a decision procedure for equivalence of schemata, and a deductive formalism for generating schemata equivalent to a given one. The present paper is intended, first as an exposition of Ianov's results and simplification of his method, and second to point out certain generalizations and extensions of it. We define a somewhat generalized version of the notion of schema, in a language similar to that used in finite automata theory, and present a simple algorithm for the equivalence problem solved by Ianov. We also point out that the same problem for an extended notion of schema, considered rather briefly by Ianov, is just the equivalence problem for finite automata, which has been solved, although the decision procedure is rather long for practical use. A simple procedure for generating all schemata equivalent to a given schema is also indicated.Tape sets and a tape set product are defined, and relationships are found between a table of products of tape sets and elements in the operation-preserving group of a finite automaton.By a simple direct construction it is shown that computations done by Turing machines can be duplicated by a very simple symbol manipulation process. The process is described by a simple form of Post canonical system with some very strong restrictions.This system is monogenic: each formula (string of symbols) of the system can be affected by one and only one production (rule of inference) to yield a unique result. Accordingly, if we begin with a single axiom (initial string) the system generates a simply ordered sequence of formulas, and this operation of a monogenic system brings to mind the idea of a machine.The Post canonical system is further restricted to the “Tag” variety, described briefly below. It was shown in [1] that Tag systems are equivalent to Turing machines. The proof in [1] is very complicated and uses lemmas concerned with a variety of two-tape nonwriting Turing machines. The proof here avoids these otherwise interesting machines and strengthens the main result; obtaining the theorem with a best possible deletion number P = 2.Also, the representation of the Turing machine in the present system has a lower degree of exponentiation, which may be of significance in applications.These systems seem to be of value in establishing unsolvability of combinatorial problems.SNOBOL is a programming language for the manipulation of strings of symbols. A statement in the SNOBOL language consists of a rule that operates on symbolically named strings. The basic operations are string formation, pattern matching and replacement. Facilities for integer arithmetic, indirect referencing, and input-output are included. In the design of the language, emphasis has been placed on a format that is simple and intuitive. SNOBOL has been implemented for the IBM 7090.Random number generators of the mixed congruential type have recently been proposed. They appear to have some advantages over those of the multiplicative type, except that their statistical behavior is unsatisfactory in some cases. It is shown theoretically that a certain class of these mixed generators should be expected to fail statistical tests for randomness. Extensive testing confirms this hypothesis and makes possible a more precise definition of the unsatisfactory class. It is concluded that the advantages of mixed generators can be realized only in special circumstances. On machines with relatively short multiplication times the multiplicative generators are to be preferred.Two pseudo-random number generators are considered, the multiplicative congruential method and the mixed congruential method. Some properties of the generated sequences are derived, and several algorithms are developed for the evaluation of xi = @@@@(i) and i = f-1 (xi is the ith element of a pseudo-random number sequence.The algorithms described in this paper are essentially Jacobi-like iterative procedures employing Householder orthogonal similarity transformations and Jacobi orthogonal similarity transformations to reduce a real symmetrix matrix to diagonal form. The convergence of the first class of algorithms depends upon the fact that the algebraic value of one diagonal element is increased at each step in the iteration and the convergence of the second class of algorithms depends upon the fact that the absolute value of one off-diagonal element is increased at each step in the iteration. Then it is shown how it is possible to combine one algorithm from each class together into a “mixed” strategy for diagonalizing a real symmetric matrix.The generalized Stirling numbers of the first kind are defined, certain of their basic properties are discussed, and tables are given for the square grid k = 0(1)10 and j = 0(1)10 with l = -10(1)10.This paper describes a procedure for producing scales for use in automatic plotting of data. The scale and associated origin of the plot produced by this procedure are “round numbers.” The procedure itself is independent of the range of the data and so no a priori bounds on the data are needed.This paper is concerned with the study of static hazards in combinational switching circuits by means of a suitable ternary switching algebra. Techniques for hazard detection and elimination are developed which are analogous to the Huffman-McCluskey procedures. However, gate and series-parallel contact networks are treated by algebraic methods exclusively, whereas a topological approach is applied to non-series-parallel contact networks only. Moreover, the paper derives necessary and sufficient conditions for a ternary function to adequately describe the steady-state and static hazard behavior of a combinational network. The sufficiency of these conditions is proved constructively leading to a method for the synthesis of combinational networks containing static hazards as specified. The section on non-series-parallel contact networks also includes a brief discussion of the applicability of lattice matrix theory to hazard detection. Finally, hazard prevention in contact networks by suitable contact sequencing techniques is discussed and a ternary map method for the synthesis of such networks is explained.An algorithm is presented which differs from previous methods by being more adapted for searching over wide frequency ranges and which permits the estimation of spectral density as a function of time as well as of frequency. The algorithm may offer economy, especially on smaller computers. The algorithm is based on the analogy between a broadly tuned filter and a truncated cross correlation.




The use of iterative procedures for interpolation is well-known. In this paper an iterative procedure, that may be used to compute values of derivatives and definite integrals, is derived. The procedure may also be used to compute the result of applying any linear operator to a function. The data used are a set of point values, at any reasonable set of abscissae. Within certain limitations, values of the derivatives may also be used. Worked examples are given to demonstrate the use of the procedure in three simple problems.This paper persues a discussion of certain algebraic properties of automata and their relationship to the structure (i.e., properties of the next state function) of automata. The device which is used for this study is the association of a group with each automaton. We introduce functions on automata and study the group of an automaton, a representation for the group elements and the direct product of automata. Finally, for a certain class of automata a necessary and sufficient condition, in terms of the group of the automaton, is given for insuring that an automaton can be represented as a direct product.Backus [1] has developed an elegant method of defining well-formed formulas for computer languages such as ALGOL. It consists of (our notation is slightly different from that of Backus):

A finite alphabet: a1, a2, …, at;
Predicates: P1, P2, …, P@@@@;
Productions, either of the form (a) aj ∈ Pi; or of the form (b) Pi2Pi1 … Pit → Pj.A finite alphabet: a1, a2, …, at;Predicates: P1, P2, …, P@@@@;Productions, either of the form (a) aj ∈ Pi; or of the form (b) Pi2Pi1 … Pit → Pj.A word is a finite sequence of letters from the alphabet. Then IIIa states that certain words (containing only one letter) belong initially to some of the predicates, and IIIb states that if words W1, W2, …, Wt belong to the predicates Pi1, Pi2, …, Pit respectively, then the concatenation W1W2 … Wt belongs to Pj. We call this a Backus system.A simple example of such a system is: Alphabet: a, b; Predicates: P, Q, R; Productions: a ∈ P, b ∈ Q, PQ → R, QP → R; RR → R, PRQ → R, QRP → R. Then P and Q contain only the words a and b, respectively, while R contains all words which have the same number of a's and b's.In the above example, abab belongs to R and can be produced in two ways. Namely, as ab ∈ R and RR → R, abab ∈ R; also as ba ∈ R and PRQ → R, abab ∈ R. We call a Backus system ambiguous if one of its predicates contains a word which can be produced in more than one way. As, in practice, the meaning of a word is determined by the way it is produced, an ambiguous Backus System must be avoided.As the following example illustrates, ALGOL 60 [3] is ambiguous: if B ∧ C then for I: = 1 step 1 until N do if D ∨ E then A[I] : = 0 else K : = K + 1; K : = K - 1 In fact, both for I := 1 step 1 until N do if D ∨ E then A[I] := 0 and for I := 1 step 1 until N do if D ∨ E then A[I] := 0 else K := K + 1 are valid for statements of ALGOL 60. Combining the first with if B ∧ C then … else K = K + 1; or the second with if B ∧ C then … gives rise to the above example, and these two methods of construction correspond to the two possible meanings of the example.D. Dahm and H. Trotter, in a private communication, have raised the question: “Does there exist an algorithm to determine whether a Backus system is ambiguous?” We call this the ambiguity problem. The purpose of this paper is to show that no such algorithm exist, i.e., that the ambiguity problem is unsolvable.We first define a normal system. It consists of:
A finite alphabet: a1, a2, …, at;
A finite collection of ordered pairs: (g1, g1), (g2, g2), …, (gr, gr), where the gi and gi are words.
An axiom A which is some fixed word.A finite alphabet: a1, a2, …, at;A finite collection of ordered pairs: (g1, g1), (g2, g2), …, (gr, gr), where the gi and gi are words.An axiom A which is some fixed word.If U and V are words, we say U → V if U is of the form gP and V is of the form Pg where (g,g) is one of the ordered pairs. We also write, in this case, giP → Pgi. Also, if U1, U2, …, Un are words with Ui → Ui+1, 1 ≦ i ≦ n-1, then U1 → Un, and we say Un is derived from U1. The words which may be derived from the axiom A are called theorems.A normal system is called undecidable if there does not exist an algorithm for determining whether a word is a theorem of the system. It is implicit in [2, sec. 6.5] that there exists an undecidable normal system, which we denote by NS, with the property that in each ordered pair (g, g), the words g and g have no common letters.LEMMA. If U and V are words of NS, then U → V, if and only if there exists indices j1, j2, …, jm such that Ugj1gj2 … gjm = gj1gj2 … gjmV.PROOF. Suppose the equality holds. As gj1 and gj1 have no common letters, U is of the form gj1R1 ; let U1 = R1gj1. Then we have U → U1 and U1gj2 … gjm = gj2gj3 … gjmV.Proceeding inductively, we obtain a sequence of words, U, U1, U2, …, Um = V with U → U1 → … → Um ; hence U → V. Conversely, if U → V, then there exist words U0, U1, …, Um with U0 = U and Um = V, and indices j1, j2, …, jm such that Ui-1gji = gjiUi, 1 ≦ i ≦ m. Then U0gj1 = gj1 U1 or U0gj1gj2 = gj1U1gj2 = gj1gj2U2. By induction the proof is complete.THEOREM. The ambiguity problem is unsolvable.PROOF. We describe certain predicates and Backus systems; to save space we omit the formal definitions. It is easy to construct predicates and systems with the required properties. We use as alphabet the alphabet a1, a2, …, at of NS and in addition the letters b1, b2, …, br, one for each ordered pair (gi, gi) of NS. If A is the axiom of NS, form the predicate P which contains all words of the form bjmbjm-1 … bj1Agj1gj2 … gjm; if W is any word on the alphabet a1, a2, …, ar, let Qw be the predicate containing all words of the form bjmbjm-1 … bj1gj1gj2 … gjmW.It is possible to construct the predicates P and Qw so that there is no ambiguity in their definition, and we assume that this is done. Then form the Backus system Bw which contains the predicates P, Qw, and Sw, where Sw is defined by P → Sw and Qw → Sw.Now, in order for Bw to be ambiguous, Bw must contain a predicate which contains a word which comes about in two ways. The predicates P and Qw, and all predicates used in their definition, do not have this property. Thus Bw is ambiguous if and only if Sw contains a word which comes about in two ways. From the definition of Sw, it is clear that Bw is ambiguous if and only if P and Qw have a word in common. Observing the form of the words in P and Qw we see that Bw is ambiguous if and only if there exists indices j1, j2, …, jm such that bjm … bj1Agj1 … gjm = bjm … bj1gj1 … gjmW. By the lemma, this is true if and only if A → W. Thus if the ambiguity problem for Backus systems were solvable, then the decision problem for NS would be solvable, which is not the case. Hence the ambiguity problem is unsolvable.The underlying logical structure of parallel-search memories is described; the characteristic operation of three major types is displayed in the execution of searches based on equality; and algorithms are presented for searches based on other specifications including maximum, miniTnum, greater than, less than, nearest to, between limits, and ordering (sorting). It is shown that there is a hierarchy of dependency among these algorithms, that they appear in pairs with each member of a pair belonging to one or the other of two distinct classes, and that every type of search can be executed within each class.The application of digital computers to the tasks of document classification, storage and retrieval holds considerable promise for solving the so-called “library problem.” Due to the high-speed and data handling characteristics of digital computers, a number of different approaches to the “library problem” have been placed in operation [4]. Although existing systems are rather rudimentary when compared with the ultimate goal of an automated library, progress towards that goal has been made in several areas: the organization of a mass of documents through automatic indexing schemes; the retrieval from a mass of documents of only those documents related to an information request made by a user of the library. A high proportion of existing document retrieval systems is based upon the author's background and skill rather than upon a mathematical model. Although allowing considerable success in the initial stages of development, the heuristic approach has a limited potential unless an underlying mathematical rationale can be found. Therefore, the present paper proposes an information retrieval based upon Lazarsfeld's latent class analysis [11], which has mathematical foundations. Although latent class analysis was developed by Lazarsfeld [11] to analyze questionnaires, the similarity of this task and document classification suggests that the mathematical rationale for the former could also provide a useful theoretical basis for the latter.Because the number of words contained in even a moderately sized report can exceed the capacity of most computers, some form of data reduction is a necessity. The reduction usually results in one of three levels of abstraction: abstracts of documents, key or topical words which convey the meaning of the document or abstract, and indices or tags based upon key words which are then assigned to the document. In general, indexing systems either assign key words to the document or use several key words to assign tags or indices to the documents. The key words or tags then serve as basic information for a retrieval system. Until a radical change in the data handling characteristics of computers is made, it would appear likely that key words or tags will continue to serve as the raw data for information retrieval systems. Although considerable uniformity exists in basic data introduced into an automated library, many different approaches exist as to the subsequent processing of the data. Several papers are reviewed below, which illustrate some of the considerations that enter into the development of an information retrieval system.Maron and Kuhns [8] have developed the “probabilistic indexing” scheme, which reduces the number of documents searched yet increases the retrieval of appropriate documents. In this approach, a large mass of source documents was read by human reviewers and key words were selected. The key words were then pooled into a few well-defined categories. However, any given key word could appear in more than one category. The resulting categories were then assigned meaningful labels or tags which constituted an index term list. The source documents were then re-inspected and the appropriate tag or tags assigned to the document.Document retrieval using the probabilistic indexing scheme is accomplished by presenting the computer with a series of tags and a value of a relevance number below which documents are not of sufficient importance to be retrieved. The tags locate the document, and the value of the corresponding relevance number compared to the lower bound value determines if the document should be retrieved.The high degree of dependence of the probabilistic indexing scheme upon human reviewers greatly reduces the efficiency of the method. If the number of documents, key words and tags were large, a human reviewer would not be able to maintain a consistent frame of reference when assigning tags and relevance numbers. The unique contribution of the probabilistic indexing scheme, however, is the use of relevance numbers in conjunction with the indices. The number provides a basis for determining the relevance of the stored documents to the indexed terms used by the requester of information.Stiles [10] had also reported the use of an association factor to accompany the index terms assigned to a document. The factor used expressed the discrepancy of the observed joint occurrence from the expected joint occurrence of an index pair, assuming independence. The association factor employed was the &khgr;2 value obtained from a two-fold contingency table involving the pair of index terms. A correlation coefficient, such as tetracortic r which expresses the correlation within the two-fold table, rather than a chi-square value expressing lack of independence would have been more appropriate in the present context. Stiles [10], however, reports that the use of the association factor was found to improve document retrieval.A more intensive study of the inter-relationships among words within a document was performed by Doyle [2]. The joint occurrences of word pairs in a body of 600 documents served as the basic data of the study. Two types of word correlations were found to exist within word pairs: adjacent correlations, resulting from words which appeared in pairs due to the nature of our language; and proximal correlation, due to words which are logically related but appear at non-adjacent positions within a document. The statistical effects of these two correlations were denoted by language redundancy and reality redundancy. In addition, a third type of redundancy, documentation redundancy resulted when more than one document could be classified by a given set of key words. The effect of language redundancy can be reduced by pooling adjacent key words and treating the pair as a single key word, thus eliminating the redundancy. Documentation redundancy would be reduced by pooling similar documents and assigning a single label to the batch, thus eliminating unnecessary duplication of effort. Reality redundancy, however, is the result of the author's cognitive processes, and the degree to which the literature researcher can duplicate this redundancy determines how successfully the original document can be retrieved. This study indicates that an important function in an information retrieval system is machinery for reducing the effects of language and documentation redundancy so that important relationships are not obscured.The results of the three studies reviewed above indicated document retrieval can be improved if the documents are surveyed for document redundancy and if the relationships among the key words are filtered to remove language redundancy. In addition, the use of a relevance number relating the document and key words appears to increase the efficiency of document retrieval.



This paper1 compares the notions of Turing machine, finite automaton and neural net. A new notation is introduced to replace net diagrams. “Equivalence” theorems are proved for nets with receptors, and finite automata; and for nets with receptors and effectors, and Turing machines. These theorems are discussed in relation to papers of Copi, Elgot and Wright; Rabin and Scott; and McCulloch and Pitts. It is shown that sets of positive integers “accepted” by finite automata are recursive; and a strengthened form of a theorem of Kleene is proved.From the very founding of switching theory by Claude E. Shannon, the tree circuit has been a useful instrument in the design of logic networks. Besides being a valuable practical addition to the designer's “tool kit”, it has been a theoretical asset in the study of circuit complexity and the establishment of general bounds on the relative costs of switching networks. The object of this paper is to expand the practical and theoretical scope of the tree circuit. The expansion is effected by the formulation of a generalized tree circuit, which in actuality is a set of circuits having basic tree circuit characteristics.Several methods for simplifying switching circuits using “don't-care” conditions are suggested. One of the methods uses the following procedure: Let the given circuit be represented by the truth function F. Let the given “don't-care” conditions be denoted byD = 0. (For example, if x1 = 1 and x2 = 0 represent the combination of input signals which never takes place, then D = x1x2.) Generate all the irredundant disjunctive and conjunctive forms which are equivalent toF wheneverD = 0. From these forms, the simplest ones may then be chosen according to a given measure of simplicity. They correspond to the simplest two-level AND/OR or OR/AND switching circuits which, under the “don't-care” conditions, perform the same function as the given circuit. Several methods for generating the equivalent irredundant forms are suggested. They are generalizations of those due to Quine, Ghazala and Mott, and may be programmed for use on computers. Alternative simplification methods are also given.An automatic sequencing procedure for assigning sets of instructions to predesignated autonomous units of a multiprocessor is described. The procedure is based upon an assignment matrix developed from a precedence matrix. By associating a column vector whose elements are the operation time of each instruction set with the assignment matrix, numerical computation is made possible. A topological index, the precedence number, stating the position of each instruction set in relation to the last set in its path is contained in a second column vector. A transfer matrix in conjunction with an automatically derived path table is employed in a multipath program.Six generalized rules are derived for the procedure which proceeds directly to a solution without obtaining and testing all permutations of the sets of instructions, and seeks to establish a sequence of operations to minimize the total operation time while satisfying all precedence restrictions. Since the procedure does not require looking at the last instruction set before releasing the first sets for assignment to units, computer processing may start after the first assignment period is completed with processing and subsequent sequencing taking place concurrently.The procedure is readily adaptable to computer operation and automatic development of the assignment matrix is described. A flow chart of the procedure and its use for solution of a problem is presented. The solution obtained is guaranteed to be feasible although it is not necessarily unique. In the examples tested, an optimum or near optimum solution has been obtained. Computational experience with the procedure in complex problems is required to determine its effectiveness in such cases.This study investigated various techniques for systematically abbreviating English words and names. Most of the attention was given to the techniques which could be mechanized with a digital device such as a general purpose digital computer. Particular attention was paid to techniques that could process incoming information without prior knowledge of its existence (i.e., no table lookups). Thirteen basic techniques and their modifications are described. In addition, most of the techniques were tested on a sample of several thousand subject words and several thousand proper names in order to provide a quantitative measure of comparison.Procedures are given to convert any regular expression into a state diagram description and neural net realization of a machine that recognizes the regular set of sequences described by the given expression. It is established that any regular expression with a finite number of connectives describes a regular set of sequences that can be recognized by a finite state machine. All the procedures given are guaranteed to terminate in a finite number of steps, and a generalized computer program can be written to handle the entire conversion. An incidental result of the theory is the design of multiple output sequential machines. The potential usefulness of regular expressions and a long neglected form of a state diagram are demonstrated.Let ƒ (x) be a real-valued function defined and continuous on [- 1, +1] and let T(x) = a0/2 + ∑∞k=1 akTk (x) be the Fourier-Tschebycheff expansion for ƒ (x), that is, T k (x) = cos kl&thgr;, ak = 2/&pgr; ∫&pgr;0 ƒ (cos &thgr;) cos k&thgr; d&thgr; where x = cos &thgr;. If Pn is the class of real polynomials in x of degree less than or equal to n, we let Mn = infpePn sup-1≤x≤1 | ƒ (x) - p (x) |. Mn is called the best Tschebycheff approximation to ƒ (x) by a polynomial of degree less than or equal to n.A rule of thumb in computing is that | Mn/an+1 | → 1 as n → ∞. In other words, the best Tschebycheff approximation to ƒ (x) of degree n is asymptotically equal to the (n + 1)th coefficient in the Tschebycheff expansion. It is the purpose of this note to give upper and lower bounds for Mn in terms of the coefficients {ak}, which will enable us to make precise statements about the validity of this asymptotic result. It has already been observed by Bernstein [1, p. 115] that under appropriate hypotheses on ƒ(x) there exists a subsequence {kj} for which | Mkj/akj+1 | → 1. We show here that if {akj} is the subsequence of {ak} consisting of the nonzero coefficients, then a sufficient condition for limj→∞ | Mkj-1/akj | = 1 (1) is for lim j→∞akj+1 | = 0. If this latter condition holds ƒ (z) = 1/2 a0 + ∑∞n=1 anTn (z) is an entire function. An easy example shows that (1) is not valid, however, for all entire functions.We note first some easily derived bounds for Mn. If the series T (x) converges to ƒ (x), then Mn ≤ sup-1≤x≤1 | ƒ (x) - 1/2 a0 - ∑nk=1 akTk(x) | ≤ ∑∞k=n+1 | ak |.Therefore, if ∑∞k=1 | ak | < ∞, ∑∞k=n+1 | ak | is an upper bound for Mn. Again, if the series T(x) converges, a lower bound for Mn is given by La Vallee Poussin [2, p. 107], i.e., Mn ≧ | an+1 + a3(n+1) + a5(n+1) + ··· |.(2) Another lower bound for Mn in terms of the coefficients an may also be obtained. From the fact that the nth partial sum of the Fourier cosine series gives the best least squares approximation to ƒ (cos &thgr;) we have 1/&pgr; ∫&pgr;0 ƒ(cos &thgr;) - a0/2 - ∑n1 ak cos k&thgr;]2 d&thgr; ≤ Mn2. But by Parseval's theorem 1/&pgr; ∫&pgr;0 ƒ(cos &thgr;) - a0/2 - ∑n1 ak cos k&thgr;]2 d&thgr; = 1/2 ∑∞n=1 ak2. Therefore, Mn ≧ { 1/2 ∑∞n+1ak2}1/2 and, in particular, Mn ≧ 1/√2 | am | for m > n. (3) We now can prove the following result.THEOREM. Let {akj} be the subsequence of the {ak} consisting of all the nonzero coefficeints.If for all j ≧ j0 | akj+1/akj | ≤ &rgr; < 1, then for j ≧ j0, max (1/√2, 1 - 2&rgr;/1 - &rgr;) ≤ Mkj-1/| akj |) ≤ 1/1 - &rgr; (4) and 1/√2 ≤ Mkj/| akj+1 | ≤ 1/1 - &rgr;. (5)PROOF. If for all j ≧ j0, |akj+1/akj| ≤ &rgr; < 1, then ∑∞k=1|ak | < ∞. Therefore, for j ≧ j0, Mkj-1 ≤ ∑∞n=kj | an | = ∑∞ n=j|akn | ≤ | akj | (1 + &rgr; + &rgr;2 + ··· = | akj | 1/1&rgr; Similarly Mkj ≤ | akj+1 | (1/1 - &rgr;). An application of (3) proves that Mkj/| akj+1 | ≧ 1/ √2, establishing (5).On the other hand by (2), Mkj-1 ≧ | akj + a3kj + ··· | ≧ | akj | - | a3kj | ··· ≧ | akj | {1 - &rgr; - &rgr;2 - ···} = | akj | 1 - 2&rgr;/1 - &rgr;. Therefore, by (3), Mkj-1/| akj | ≧ max (1/√2, 1 - 2&rgr;/1 - &rgr;) and the theorem is proved.COROLLARY. If limj→∞ | akj+1/akj | = 0, then limj→∞ | Mkj-1/akj | = 1. REMARK. Suppose an ≠ 0 for all n. Then if limj→∞ | an+1/an | = 0, f (z) ≡ 1/2a0 + ∑ ∞n=1 anTn (z) is an entire function.In light of the above corollary, one might conjecture that for any entire function such that an ≠ 0, limn→∞ | Mn-1/an | = 1. This, however, is false as the following simple argument shows. Certainly for such entire functions Mn ≧ Mn+1 > 0. Therefore limn→∞ Mn+1/Mn ≤ 1. But, Mn+1/Mn = Mn+1/an+2 . an+2/an+1 . an+1/Mn. Therefore, if limn→∞ | Mn-1/an | = 1, limn→∞ | Mn+1/Mn | = 1 limn → | an+2/an+1 |.But one can easily construct examples of entire functions such that limn→∞ | an+2/an+1 | ≧ k > 1; e.g., let a2n = (2n)-2n+1, a2n+1 = k (2n)-2n+1.



The literature concerned with methods for finding the minimal form of a truth function is, by now, quite extensive. This article extends this knowledge by introducing an algorithm whereby all calculations are performed on decimal numbers obtained from binary-decimal conversion of the terms of the Boolean function. Several computational aids are presented for the purpose of adapting this algorithm to the solution of large-scale problems on a digital computer.It has been observed by many people that a striking number of quite diverse mathematical problems can be formulated as problems in integer programming, that is, linear programming problems in which some or all of the variables are required to assume integral values. This fact is rendered quite interesting by recent research on such problems, notably by R. E. Gomory [2, 3], which gives promise of yielding efficient computational techniques for their solution. The present paper provides yet another example of the versatility of integer programming as a mathematical modeling device by representing a generalization of the well-known “Travelling Salesman Problem” in integer programming terms. The authors have developed several such models, of which the one presented here is the most efficient in terms of generality, number of variables, and number of constraints. This model is due to the second author [4] and was presented briefly at the Symposium on Combinatorial Problems held at Princeton University, April 1960, sponsored by SIAM and IBM. The problem treated is: (1) A salesman is required to visit each of n cities, indexed by 1, ··· , n. He leaves from a “base city” indexed by 0, visits each of the n other cities exactly once, and returns to city 0. During his travels he must return to 0 exactly t times, including his final return (here t may be allowed to vary), and he must visit no more than p cities in one tour. (By a tour we mean a succession of visits to cities without stopping at city 0.) It is required to find such an itinerary which minimizes the total distance traveled by the salesman.Note that if t is fixed, then for the problem to have a solution we must have tp ≧ n. For t = 1, p ≧ n, we have the standard traveling salesman problem.Let dij (i ≠ j = 0, 1, ··· , n) be the distance covered in traveling from city i to city j. The following integer programming problem will be shown to be equivalent to (1): (2) Minimize the linear form ∑0≦i≠j≦n∑ dijxij over the set determined by the relations ∑ni=0i≠j xij = 1 (j = 1, ··· , n) ∑nj=0j≠i xij = 1 (i = 1, ··· , n) ui - uj + pxij ≦ p - 1 (1 ≦ i ≠ j ≦ n) where the xij are non-negative integers and the ui (i = 1, …, n) are arbitrary real numbers. (We shall see that it is permissible to restrict the ui to be non-negative integers as well.)If t is fixed it is necessary to add the additional relation: ∑nu=1 xi0 = t Note that the constraints require that xij = 0 or 1, so that a natural correspondence between these two problems exists if the xij are interpreted as follows: The salesman proceeds from city i to city j if and only if xij = 1. Under this correspondence the form to be minimized in (2) is the total distance to be traveled by the salesman in (1), so the burden of proof is to show that the two feasible sets correspond; i.e., a feasible solution to (2) has xij which do define a legitimate itinerary in (1), and, conversely a legitimate itinerary in (1) defines xij, which, together with appropriate ui, satisfy the constraints of (2).Consider a feasible solution to (2).The number of returns to city 0 is given by ∑ni=1 xi0. The constraints of the form ∑ xij = 1, all xij non-negative integers, represent the conditions that each city (other than zero) is visited exactly once. The ui play a role similar to node potentials in a network and the inequalities involving them serve to eliminate tours that do not begin and end at city 0 and tours that visit more than p cities. Consider any xr0r1 = 1 (r1 ≠ 0). There exists a unique r2 such that xr1r2 = 1. Unless r2 = 0, there is a unique r3 with xr2r3 = 1. We proceed in this fashion until some rj = 0. This must happen since the alternative is that at some point we reach an rk = rj, j + 1 < k. Since none of the r's are zero we have uri - uri + 1 + pxriri + 1 ≦ p - 1 or uri - uri + 1 ≦ - 1. Summing from i = j to k - 1, we have urj - urk = 0 ≦ j + 1 - k, which is a contradiction. Thus all tours include city 0. It remains to observe that no tours is of length greater than p. Suppose such a tour exists, x0r1 , xr1r2 , ··· , xrprp+1 = 1 with all ri ≠ 0. Then, as before, ur1 - urp+1 ≦ - p or urp+1 - ur1 ≧ p.But we have urp+1 - ur1 + pxrp+1r1 ≦ p - 1 or urp+1 - ur1 ≦ p (1 - xrp+1r1) - 1 ≦ p - 1, which is a contradiction.Conversely, if the xij correspond to a legitimate itinerary, it is clear that the ui can be adjusted so that ui = j if city i is the jth city visited in the tour which includes city i, for we then have ui - uj = - 1 if xij = 1, and always ui - uj ≦ p - 1.The above integer program involves n2 + n constraints (if t is not fixed) in n2 + 2n variables. Since the inequality form of constraint is fundamental for integer programming calculations, one may eliminate 2n variables, say the xi0 and x0j, by means of the equation constraints and produce an equivalent problem with n2 + n inequalities and n2 variables.The currently known integer programming procedures are sufficiently regular in their behavior to cast doubt on the heuristic value of machine experiments with our model. However, it seems appropriate to report the results of the five machine experiments we have conducted so far. The solution procedure used was the all-integer algorithm of R. E. Gomory [3] without the ranking procedure he describes.The first three experiments were simple model verification tests on a four-city standard traveling salesman problem with distance matrix [ 20 23 4 30 7 27 25 5 25 3 21 26 ]The first experiment was with a model, now obsolete, using roughly twice as many constraints and variables as the current model (for this problem, 28 constraints in 21 variables). The machine was halted after 4000 pivot steps had failed to produce a solution.The second experiment used the earlier model with the xi0 and x0j eliminated, resulting in a 28-constraint, 15-variable problem. Here the machine produced the optimal solution in 41 pivot steps.The third experiment used the current formulation with the xi0 and x0j eliminated, yielding 13 constraints and 9 variables. The optimal solution was reached in 7 pivot steps.The fourth and fifth experiments were used on a standard ten-city problem, due to Barachet, solved by Dantzig, Johnson and Fulkerson [1]. The current formulation was used, yielding 91 constraints in 81 variables. The fifth problem differed from the fourth only in that the ordering of the rows was altered to attempt to introduce more favorable pivot choices. In each case the machine was stopped after over 250 pivot steps had failed to produce the solution. In each case the last 100 pivot steps had failed to change the value of the objective function.It seems hopeful that more efficient integer programming procedures now under development will yield a satisfactory algorithmic solution to the traveling salesman problem, when applied to this model. In any case, the model serves to illustrate how problems of this sort may be succinctly formulated in integer programming terms.Numerous formulas are available for the computation of the Gamma function [1, 2]. The purpose of this note is to indicate the value of a well-known method that is easily extended for higher accuracy requirements.Using the recursion formula for the Gamma function, &Ggr;(x + 1) = x&Ggr;(x), (1) and Stirling's asymptotic expansion for ln &Ggr;(x) [3], we have ln &Ggr;(x) ∼ (x - 1/2) ln x - x + 1/2 ln 2&pgr; + ∑Nr=1 Cr/x2r-1. (2) It follows that, if k and N are appropriately selected positive integers, &Ggr;(x + 1) can be represented by &Ggr;(x + 1) ∼ √2&pgr; exp (x + k - 1/2) ln (x + k) - (x + k) exp ∑Nr=1 Cr/(x + k)2r-1/(x + 1)(x + 2) ··· (x + k - 1) (3) where Cr = (- 1)r-1 Br/(2r - 1)(2r), Br being the Bernoulli numbers [4]. These coefficients have been published by Uhler [5].Requiring the range 0 ≦ x ≦ 1 is no restriction since, if necessary, &Ggr;(x + 1) can be generated for other arguments using (1).For a given N, the error in (2) can be estimated from |&egr;| < |CN+1|/x2N+1. (4)The curves of Figure 1 show contours of constant error bound as a function of N and x. These curves represent single and double-precision floating-arithmetic requirements of &egr; < 5·10-9 and &egr; < 5·10-17. For a given N, k is defined as the minimum integral x greater than or equal to those on the curves. Then N and k can be chosen to minimize round-off and computing time.For N and k equal to 4, formula (3) yields &Ggr;(x + 1) ∼ &radic2&pgr; exp (x + 4 - 1/2) ln (x + 4) - (x + 4) exp ∑4r=1Cr/(x + 4)2r-1/(x + 1)(x + 2)(x + 3). (5)A similar expression suitable for double precision results for N = 8 and k = 9.The exponents in (5) are split to reduce roundoff. Various algebraic manipulations might result in a further reduction of roundoff.For each item to be sorted by address calculation, a location in the file is determined by a linear formula. It is placed there if the location is empty. If there is an item at the specified location, a search is made to find the closest empty space to this spot. The item at the specified location, together with adjacent items, is moved by a process similar to musical chairs, so that the item to be filed can be entered in its proper order in the file. A generalized flowchart for computer address calculation sorting is presented here. A mathematical analysis using average expectation follows. Formulas are derived to determine the number of computer operations required. Further formulas are derived which determine the time required for an address calculation sort in terms of specific computer orders. Several examples are given. A sorting problem solved elsewhere in the literature by an empirical method is solved by the formulas developed here to demonstrate their practical application.
The hope that mathematical methods employed in the investigation of formal logic would lead to purely computational methods for obtaining mathematical theorems goes back to Leibniz and has been revived by Peano around the turn of the century and by Hilbert's school in the 1920's. Hilbert, noting that all of classical mathematics could be formalized within quantification theory, declared that the problem of finding an algorithm for determining whether or not a given formula of quantification theory is valid was the central problem of mathematical logic. And indeed, at one time it seemed as if investigations of this “decision” problem were on the verge of success. However, it was shown by Church and by Turing that such an algorithm can not exist. This result led to considerable pessimism regarding the possibility of using modern digital computers in deciding significant mathematical questions. However, recently there has been a revival of interest in the whole question. Specifically, it has been realized that while no decision procedure exists for quantification theory there are many proof procedures available—that is, uniform procedures which will ultimately locate a proof for any formula of quantification theory which is valid but which will usually involve seeking “forever” in the case of a formula which is not valid—and that some of these proof procedures could well turn out to be feasible for use with modern computing machinery.Hao Wang [9] and P. C. Gilmore [3] have each produced working programs which employ proof procedures in quantification theory. Gilmore's program employs a form of a basic theorem of mathematical logic due to Herbrand, and Wang's makes use of a formulation of quantification theory related to those studied by Gentzen. However, both programs encounter decisive difficulties with any but the simplest formulas of quantification theory, in connection with methods of doing propositional calculus. Wang's program, because of its use of Gentzen-like methods, involves exponentiation on the total number of truth-functional connectives, whereas Gilmore's program, using normal forms, involves exponentiation on the number of clauses present. Both methods are superior in many cases to truth table methods which involve exponentiation on the total number of variables present, and represent important initial contributions, but both run into difficulty with some fairly simple examples.In the present paper, a uniform proof procedure for quantification theory is given which is feasible for use with some rather complicated formulas and which does not ordinarily lead to exponentiation. The superiority of the present procedure over those previously available is indicated in part by the fact that a formula on which Gilmore's routine for the IBM 704 causes the machine to computer for 21 minutes without obtaining a result was worked successfully by hand computation using the present method in 30 minutes. Cf. §6, below.It should be mentioned that, before it can be hoped to employ proof procedures for quantification theory in obtaining proofs of theorems belonging to “genuine” mathematics, finite axiomatizations, which are “short,” must be obtained for various branches of mathematics. This last question will not be pursued further here; cf., however, Davis and Putnam [2], where one solution to this problem is given for eleThis paper reports on a novel technique for literature indexing and searching in a mechanized library system. The notion of relevance is taken as the key concept in the theory of information retrieval and a comparative concept of relevance is explicated in terms of the theory of probability. The resulting technique called “Probabilistic Indexing,” allows a computing machine, given a request for information, to make a statistical inference and derive a number (called the “relevance number”) for each document, which is a measure of the probability that the document will satisfy the given request. The result of a search is an ordered list of those documents which satisfy the request ranked according to their probable relevance.The paper goes on to show that whereas in a conventional library system the cross-referencing (“see” and “see also”) is based solely on the “semantical closeness” between index terms, statistical measures of closeness between index terms can be defined and computed. Thus, given an arbitrary request consisting of one (or many) index term(s), a machine can elaborate on it to increase the probability of selecting relevant documents that would not otherwise have been selected.Finally, the paper suggests an interpretation of the whole library problem as one where the request is considered as a clue on the basis of which the library system makes a concatenated statistical inference in order to provide as an output an ordered list of those documents which most probably satisfy the information needs of the user.One of the outstanding problems in the theory of time series analysis is the distribution problem in spectral analysis for small samples. When the number of observations is sufficiently large for the Central Limit Theorem to be applicable, the normal approximation can be used to advantage. In recent years, work has aimed at the discovery of more generally applicable approximate methods and of more rational criteria for the sample size at which the large sample theory becomes useful; the state of the art is summarized in two recent papers by Grenander, Pollak and Slepian [1] and Freiberger and Grenander [2].Consider a sample x = (x1, x2, ··· , xn) of successive values taken from a discrete-valued stationary time series ··· , y-1 , y0 , y1 , ··· , of normally distributed random variables with mean zero and covariance matrix R with elements r&ngr;&mgr; = E(x&ngr;x&mgr;).For stationarity we have: ri,i+&ngr;≡ r&ngr; = E(yiyi+&ngr;), i, &ngr; = 0, ±1, ±2, ···. The Fourier-Stieltjes representation of the covariances is r&ngr; = 1/2&pgr; ∫&pgr;&pgr; ei&lgr;&ngr; dF(&lgr;) (1) where the spectral distribution function F(&lgr;) is bounded and nondecreasing and can, if it is absolutely continuous, be expressed in terms of a spectral density ƒ(&lgr;): F(&lgr;) = ∫&lgr;&pgr;ƒ(&lgr;) (1) The practical determination of ƒ(&lgr;) from observations of the process is effected by the introduction of a quadratic form Q = ∑n&ngr;,&lgr;=1w&ngr;-&mgr;x&ngr;x&mgr; which is taken as an estimate for ƒ(&lgr;); the coefficients w&ngr; can be written in terms of a spectral weight function or “spectral window” w(&lgr;): w&ngr; = 1/2&pgr; ∫&pgr;-&pgr; x(&lgr;)ei&ngr;&lgr; d&lgr;. (4) For details, see the cited references.In order to obtain confidence limits for these estimates Q of the spectral density ƒ(&lgr;), it is important to have methods for computing the distribution of Q. This paper deals with one such method which has proved very efficient for digital computer application.It is well known and shown, for instance, by H. Cramèr [3, p. 118], that the characteristic function of (3), where w&ngr;&mgr; ≡ w&ngr;-&mgr; are elements of a non-negative definite symmetric matrix W, is given by &phgr;(z) = EeizQ = | I - 2izRW | -1/2 = ∏nj=1 (1 - 2i&lgr;jz)-1/2 (5) where the &lgr;j are the n eigenvalues of the matrix product RW. Although RW is not necessarily a symmetric matrix, both R and W are symmetric and non-negative definite, so that all the &lgr;j are real and non-negative. The frequency function g(x) of Q then follows from Fourier's inversion formula: g(x) = 1/2&pgr; ∫∞∞ e-ixz ∫nj=1 (1 - 2iz&lgr;j)-1/2 dz. (6)Several ways have been suggested for evaluating g (x).Slepian [4] obtains a sum of finite integrals by deforming the contour of integration into a set of circles enclosing pairs of the branch points zj = -i/(2&lgr;j) of (6), and collapsing the circles. This method, which was also used in [2], works well when the eigenvalues cluster toward zero, but not otherwise.A method of repeated convolution of the frequency functions of its individual terms to obtain the frequency function of a quadratic form was developed in [5] and programmed for the IBM 650. It was found to be slowly convergent in most cases.Taking the logarithmic derivative of (5) and applying the inverse Fourier transform yields the following singular integral equation for g(x): xg(x) = ∫x0 g(x - y)h(y) dy (7) where h(x) = 1/2 ∑n&ngr;=1 e-i/2&lgr;&ngr;. This observation forms the basis of an ingeneous method [1] for a computational scheme to derive the distribution of Q. The difficulties of obtaining initial values, associated with the high order zero of the frequency function g(x) at x = 0, are solved and the solution of (7) is discussed in detail in [1]. This method is suitable only for large-scale computers of the order of the IBM 704, and becomes, as does the method in [4], slowly convergent when the eigenvalues are densely spaced.Gurland [6] expanded the frequency function in terms of a Laguerre series and ([7]) presented a convergent series for the distribution fucntion using Laguerre polynomials. This method, which was found to converge slowly, formed the basis for the following procedure which has proved fast, accurate and reliable for a variety of problems. Essentially, the Laguerre expansion is now taken around Rice's approximation [8, p. 99], which is a type III distribution with appropriately chosen parameters.The following definition of the Laguerre polynomials Ln(&agr;) (x) = ∑n&ngr;=0 (n+&agr;n-&ngr;)(-x)&ngr;/&ngr;! (8) satisfies the orthogonality relations ∫∞0e-x x&agr;Ln(&agr;) (x) Lm(&agr;) (x) dx = {&Ggr;(&agr; =1)(n+&agr;n); m = nm ≠ n (See Szegö [9]).Replacing x by &lgr;x gives ∫∞0 e-&lgr;xx&agr;L(&agr;)n(&Ggr;&agr;) dx={&Ggr;(&agr;+1/&lgr;&agr;+1) (n+&agr;n);m = nm ≠ n (9) which has the desired weight. The frequency function may now be expanded in a modified Laguerre series g(x) = Kx&agr;e-&lgr;x[c0 + c1L1(&agr;) (&lgr;x) + c2L2(&agr;) (&lgr;x) + …] (10) where K is chosen so that the weight integrates to one: ∫∞0Kx&agr;e-&lgr;x dx = 1, ∴ K = &lgr;&agr;+1/&Ggr;(&agr; + 1).(11) Multiplying both sides of (10) by Ln(&agr;) (&lgr;x) and integrating from 0 to ∞, gives cn = 1/(n + &agr;n) ∫∞0L(&agr;)n(&lgr;x)g(x) dx. (12) Using (8), cn = ∑n&ngr;=0(n + &agr;n - &ngr;)(n + &agr;n(-&lgr;)&ngr;&ngr;!∫∞0x&ngr;9(x)dx cn = ∑∞&ngr;=0&Ggr;(&agr;+1)/&Ggr;(&agr;+&ngr;+1(n&ngr;)(-&lgr;)&ngr;&agr;&ngr; (13) where &agr;&ngr; is the &ngr;th moment of the distribution about zero. Taking the logarithm of the characteristic function (5), log [EeizQ] = - 1/2 ∑nj=1 log (1 - 2i&lgr;jz), and expanding in powers of iz, one obtains for the cumulants of the distribution (Cramèr [3, p. 186])Xn = (n - 1)!2n-1 ∑j&lgr;jn. (14) The cumulants are related to the moments about zero by the relation 1 + &agr;1(iz) + &agr;2/2!(iz)2 + &agr;3/3!(iz)3 + ··· = exp [ &khgr;1(iz) + &khgr;2/2! (iz)2 + ··· ]. (15) The mean and standard deviation of the frequency function are arrived at by equating powers of iz: m = &khgr;1; &sgr;2 = &khgr;2. (16) The weight function will have the same mean and standard deviation if &agr; = m2/&sgr;2 - 1; &lgr; = m/&sgr;2. (17) Using (13) the remembering that &agr;1 = m and &agr;2 = &sgr;2 + m2, one obtains c0 = 1, c1 = 0, c2 = 0.(18) Now (10) becomes g(x) = &lgr;&agr;+1/&Ggr;(&agr; + 1)x&agr;e-&lgr;x [1 + c3L(&agr;)3(&lgr;x) + c4L(&agr;)4(&lgr;x) + ···] (19) which together with (17), (15), (14), (13) and (8), gives the frequency function in terms of the eigenvalues.Szegö [8] showed that a sufficient condition for convergence, when expanding in the form g(x) = ∑∞&ngr; = 0 a&ngr;L(&agr;)&ngr; (x), is g(x) = O(ex/2x-&agr;/2-1/4-&dgr;), &dgr; > 0, x → ∞. In our case, this reduces to ƒ(x) = O(e-x/2x&agr;/2-1/4-&dgr;), &dgr; > 0, x → ∞.But, (Gurland [6]) ƒ(x) = O(e-x/2&lgr;mxn/2-1, &lgr;m = max &lgr;j, x → ∞. Therefore, the series will converge if &lgr;&lgr;m < 1, which is the same restriction imposed by Gurland.For equal eigenvalues, the result is a &khgr;2-distribution with n degrees of freedom, which serves as an estimate of the computational error.1It may be remarked that Toeplitz theory [10] gives the asymptotic distribution of the eigenvalues &lgr;&ngr; and for large enough sample sizes (see [2]) it is sufficient to use these results instead of the exact eigenvalue distribution. In these cases, the present method is particularly efficient.The method of computation proceeds as follows. The desired number of cumulants divided by n! is calculated from equation (14), to give &khgr;n/n! = 2n-1/n ∑j&lgr;jn; after substitution in equation (15), powers of iz are compared to give the moments &agr;&ngr;.The constant K follows from (11), with the gamma function calculated by fitting an eighth order polynomial to &Ggr; (&agr; + 1) in the range 0 ≦ &agr; ≦ 1 and using the recurrence relation &Ggr; (&agr; + 1) = &agr;&Ggr(&agr;) to extend the range to any &agr; > - 1. Next, &agr; and &lgr; are computed and the series tested for convergence. In all applications to spectral analysis tried so far, the product &lgr;&lgr;m has been between 0.5 (for the case of equal eigenvalues) and 0.75, so that there has been no trouble with convergence. Successively higher approximations are now calculated, using intermediate quantities Jn = &Ggr;(&agr; + 1)/&Ggr;(&agr; = n + 1) (-&lgr;)n = (-&lgr;)n/&agr; + 1)(&agr; + 2) ··· (&agr; + n)′ Kn = Jn&agr;n, and, with the binomial coefficients (n &ngr;), cn = ∑∞&ngr;=0 Kn (n &ngr;). The Laguerre polynomials may now be expressed in the form Ln(&agr;) (&lgr;x) = ∑ n&ngr; = 0E(n)&ngr;x&ngr; where E(n)&ngr; = J&ngr; (&ngr; + 1)(&agr; + 2) ··· (&agr; + n/n!, and the final series (19) emerges term by term. The speed of convergence depends how close &lgr;&lgr;m is to 0.5.As a numerical example we present, in the figure, curves for the frequency function (10) computed for a spectral density ƒ(&lgr;) corresponding to white noise (or bandlimited noise with a correlation function sin &pgr;t/&pgr;t sampled at the zeros), a rectangular spectral window w(&lgr;) of bandwidth &pgr;/10 and sample sizes n = 20, 30, 40, 50, 70, 100, 150. The computation time for each curve on the IBM 650 was approximately 9 minutes, except for n = 70, 100 and 150 where the Toeplitz approximation led to equal eigenvalues and a computation time of 1 minute each. The eigenvalues of RW, where both R and W are symmetric but not their product, were computed by Jacobi's method; for orders 20, 30, 40 and 50 all eigenvalues were found in 0.8, 2.7, 6.4 and 12.5 hours respectively.The authors are indebted to Professor Ulf Grenander, formerly of Brown University, now of the University of Stockholm, for stimulating advice.The assignment of prime numbers to the branches of directed or non-directed nets, enables one to construct transition matrices in a numerical, rather than a symbolic, form. The numerical form greatly facilitates the construction of higher-order transition matrices, needed for the topological analysis of a given net. The proposed method is especially useful for the mechanical determination of the non-repeating paths and cycles in the net.The consistency of precedence matrices is studied in the very natural geometric setting of the theory of directed graphs. An elegant recent procedure (Marimont [7]) for checking consistency is further justified by means of a graphical lemma. In addition, the “direction of future work” mentioned in [7] (to which the present communication may be regarded as a sequel) is developed here using graph theoretic methods. This is based on the relationship between the occurrence of directed cycles and the recognition of “strongly connected components” in a directed graph. An algorithm is included for finding these components in any directed graph. This is necessarily more complicated than determining whether there do not exist any directed cycles, i.e., whether or not a given precedence matrix is consistent.A number of papers have been written from time to time about logical counters of a certain type which have quite simple logic and have been variously referred to as Binary Ring Counters, Shift Register Counters, Johnson Counters, etc. To my knowledge, most of these papers confine themselves to certain special cases and usually leave the subject with some speculation as to the possibility of generating periods of any desired length by the use of these special types. The point of view of this paper is to consider all possible counters of this general type to see how one would obtain a particular period. Special emphasis is placed on determining the least number of bits, n, required to produce a given period, K.The rules for counting are as follows. If an n-bit counter is in state (an-1, an-2 ···, a2, a1, a0) at a given time, T, then at T + 1 its state is (bn-1, bn-2, ···, b1, b0) where b0 = an-1, bi = ai-1 + cian-1 for i = 1, 2, ···, n - 1.The a's, b's, and c's are all 0's or 1's, the c's being constants, and the indicated operations are carried out using modulo 2 arithmetic. This is equivalent to considering the state of the counter as an (n - 1)th degree polynomial in X, multiplying said polynomial by X and reducing it modulo m(X), where m(X) is a polynomial of degree n which is relatively prime to X. At time T the state of the counter corresponds to: A(X) = an-1Xn-1 + an-2Xn-2 + ··· + a1X + a0. The polynomial which corresponds to the state of the counter at time T + 1 is obtained by forming X·A (X) and reducing, if necessary, modulo m (X) = Xn + cn-1Xn-1 + cn-2Xn-2 + ··· + c1X + 1.Since an-1·m(X) = 0 mod m(X), X·A(X) = X·A(X)+ an-1m(X) mod m(X), so X·A(X) = (an-2 + cn-1·an-1)Xn-1 + (an-3 + cn-2an-1)Xn-2 + ··· + (a0 + c1an-1)X + an-1 = bn-1Xn-1 + bn-2Xn-2 + ··· + b1X + b0.It is well known that more than one possible period may be obtained depending upon the initial state of the counter. Several examples are given by Young [4]. However, starting with X itself will always yield the longest possible period for any given m(X) and, furthermore, any other periods possible will always be divisors of the major period (Theorem I below). Since these minor periods can always be obtained with moduli of lower degree they are of no real interest here, and throughout the remainder of this paper the expression “period of the counter” will be assumed to refer to the major period.The set of all polynomials whose coefficients are the integers modulo 2 is the polynomial domain GF(2, X), which has among other things unique factorization into primes (irreducibles). If m(X) is in GF(2, X), then GF(2, X) modulo m(X) is a commutative ring. Thus it is closed under multiplication, but it may have proper divisors of zero. However, any element which is relatively prime to m(X) in GF(2, X) has an inverse in GF(2, X)/m(X) [1].
A compiled computer language for the manipulation of symbolic expressions organized in storage as Newell-Shaw-Simon lists has been developed as a tool to make more convenient the task of programming the simulation of a geometry theorem-proving machine on the IBM 704 high-speed electronic digital computer. Statements in the language are written in usual Fortran notation, but with a large set of special list-processing functions appended to the standard Fortran library. The algebraic structure of certain statements in this language corresponds closely to the structure of an NSS list, making possible the generation and manipulation of complex list expressions with a single statement. The many programming advantages accruing from the use of Fortran, and in particular, the ease with which massive and complex programs may be revised, combined with the flexibility offered by an NSS list organization of storage make the language particularly useful where, as in the case of our theorem-proving program, intermediate data of unpredictable form, complexity, and length may be generated.Three types of floating-point arithmetics with error control are discussed and compared with conventional floating-point arithmetic. General multiplication and division shift criteria are derived (for any base) for Metropolis-type arithmetics. The limitations and most suitable range of application for each arithmetic are discussed.The transfer of credit and the processing of receivables play important roles in most business systems. Present procedures require a dual handling of payments— they are processed both by the payee and again by the banks involved in the transfer of credit. The banks must necessarily process payments made by check; it is therefore reasonable to inquire whether the processing now required of the payee is not largely redundant, and whether it would not therefore be profitable to alter the present system.Three questions deserve examination:

Can the present system be simplified?
Will simplification lead to net savings?
How should the savings be shared?
Can the present system be simplified?Will simplification lead to net savings?How should the savings be shared?Certain higher order iterative procedures for the numerical solution of ordinary differential equations require a separate procedure for determining starting values. Such is the case in the Adams and Milne methods. The work involved in programming the computation of starting values may be of the same magnitude as that required in programming the method itself. This paper shows that the three-point Adams formula leads to a method which may be considered “self-starting” in the sense that very minor modifications of the method provide starting values of sufficient accuracy for continuing the solution.Let the equation under solution be y′ = ƒ(y, t), and let the values y′(t0) = y0′, y′(t0 + h) = y1′, y(t0) = y0, y(t0 + h) = y1, be given. The general three-point predictor-corrector process consists of estimating (in some unspecified way) a value y2′ of y2′ computing a first estimate y2 by means of a closed three-point integration formula; obtaining the (presumably) better value y2′ = ƒ(y2, t0 + 2h); and then repeating the process until some convergence criterion is satisfied, either for y2′ or for y2.The process could have started by first estimating y2, rather than y2′. It is important to note that as long as the step size h and the first estimate y2′ of y2′ result in a converging process, the values to which there is convergence are independent of the method of prediction.The Adams three-point formula is yn+1 = yn + h/12[5ƒ(tn+1, yn+1) + 8ƒ(tn, yn) - ƒ(tn-1, yn-1)]. (1) Tn, the single step truncation error, is given by Tn = - h4/24ƒ′′′(&xgr;), tn-1 ≦ &xgr; ≦ tn+1. The quantities yn, y′n+1, yn′, y′n-1 are assumed exact. Let the initial conditions y0′, y0 be specified for y′ = ƒ(y, t).As a first approximation, let y(0)-1′ = y0′, y(0)+1′ = y0′, y(0)-1 = y0, y(0)+1 = y0. The superscript zero in parentheses indicates that these are the initial estimates of y, y′. The starting procedure consists of performing corrective iterations in the forward (+1) and backward (-1) direction as follows: y(i)+1 = y0 + h/12[5ƒ(t+1, y(i-1)+1) + 8ƒ(t0, y0) - ƒ(t-1, y(j)-1)], y(j)-1 = y0 - h/12[5ƒ(t-1, y(j-1)-1) + 8ƒ(t0, y0) - ƒ(t+1, y(i)+1)].(2) The superscript notation employed does not imply any order to the forward and backward iterations. In the following analysis, however, it will be assumed that the process starts with a forward iteration, and alternates thereafter.It is necessary to show the conditions under which the process converges and the conditions under which the resulting values are accurate enough to warrant continuing the solution by the Adams method.Let y+1 be the value of y at t+1 to which the iterative process converges (if at all). The error at the ith forward iteration is defined as &agr;(i), such that y(i)+1 = y+1 + &agr;(i).(3) The error in the backward direction is &bgr;(j), such that y(j)-1 = y-1 + &bgr;(j). (3′) Substituting the error definition into equation (2) yields &agr;(i) = 5h/12{ƒ[t+1, y+1 + &agr;(i-1)] - ƒ[t+1, y+1]} - h/12{ƒ[t-1, y-1 + &bgr;(i)] - ƒ[t-1, y-1]}, (4) &bgr;(j) = - 5h/12{ƒ[t-1, y-1 + &bgr;(i-1)] - ƒ[t-1, y-1]} + h/12{ƒ[t+1, y+1 + &agr;(i)] - ƒ[t+1, y+1]}.Let g+1 = ∂ƒ[t+1, y(t0 + &xgr;1)]/∂y, 0 ≦ &xgr;1 ≦ h, and (5) g-1 = ∂ƒ[t-1, y(t0 + &xgr;2)]/∂y, -h ≦ &xgr;2 ≦ 0. Let it be assumed that g+1 and g-1 are insensitive to small changes in &xgr;1 and &xgr;2; then, by the law of the mean, equations (4) can be written as &agr;(i) = 5h/12 g+1&agr;(i-1) - h/12 g-1&bgr;(j), &bgr;(j) = 5h/12 g-1&agr;(j-1) + h/12 g+1&agr;(i).(6) The order in which the iterations are performed may now be specified by letting i = 2k + 3, j = 2k + 2, k = -1, 0, 1, ···, ∞. Equations (6) can be reduced to a single equation in either &agr; or &bgr;. Choosing &bgr; results in &bgr;(2k+4) + 5h/12 (g-1 - g+1)&bgr;(2k+2) - 24h2/12 g+1g-1&bgr;(2k) = 0. (7) The condition for convergence of the starting process is that &bgr; → 0 as k → ∞; i.e., that the roots of equations (7), when considered as a polynomial in &bgr;2, be less than one in magnitude. The conditions for convergence are then ‖- 5h/12 (g+1 - g-1) ± h/12 √[5(g+1 - g-1]2 - 4g+1g-1‖ < 1. (8) Choosing &agr; instead of &bgr; yields the same result.The condition for the convergence of the usual predictor-corrector process, in which y-1 and y0 are given, is ‖5h/12g1‖ < 1. (9) Conditions (8) are quite similar to condition (9). As can be seen, convergence can always be obtained for sufficiently small values of h.To say that the starting process gives values y+1, and y-1 of sufficient accuracy to warrant the further use of the normal Adams procedure implies that the error introduced by the former is not significantly larger than the error introduced by a single step of the latter. Let y-1 and y0 be given exactly, and let y+1 be the value to which successive forward iterations converge. If y+1 is the true value of y at t+1, the error &egr;1 is then defined as y+1 = y+1 + &egr;1, (10) By definition, y+1 satisfies y+1 = y0 + h/12 [5ƒ(t+1, y+1) + 8ƒ(t0, y0) - ƒ(t-1, y-1)].(11) It is known that y+1 = y0 + h/12 [5ƒ(t+1, y+1) + 8ƒ(t0, y0) - ƒ(t-1, y-1)] + T1, (1′) where T1 is the truncation error.From equations (1), (10) and (11), and by the law of the mean, it can be stated that &egr;1 = -T1 + h/12{5&egr;1 ∂ƒ/∂y [t+1, y(t0 + &xgr;1)]}, 0 ≦ &xgr;1 ≦ h. (12) By defining &ggr;1 = h/12 ∂ƒ/∂y [t+1, y(t0 + &xgr;1)], (13) the error equation becomes &egr;1 = -T1/1 - 5&ggr;1.(14) The quantity &ggr;1 may be estimated, since &ggr;1 = h/12 ƒ[t+1, y(i)+1] - ƒ[t+1, y(i-1)+1]/y(i)+1 - y(i-1)+1; (15) the superscript again refer to iterations. As a rule, the step size h is chosen so as to make &ggr;1 small compared to one.A similar analysis for the proposed starting process yields &egr;1 = -T1 - &ggr;-1(5T1 - T-1)/1 - 5&ggr;1 + 5&ggr;1 - 24&ggr;1&ggr;-1.(16) The difference between the error of equation (14) and that of equation (16) is &Dgr; = &ggr;-1T-1 - 5&ggr;1T1 - 24&ggr;1&ggr;-1T1/(1 - 5&ggr;1)(1 + 5&ggr;-1) + &ggr;1&ggr;-1. (17) This equation may be simplified by considering a worst possible case, in which T-1 and T1 are replaced by some maximum T and &ggr;-1 and &ggr;1 by some maximum &ggr;, and the signs adjusted. This yields ‖&Dgr;‖≦‖&ggr;(6 + &ggr;)/1 - 26&ggr;2T‖. (18) Thus, if &ggr; is of the order of 10-2, the difference &Dgr; is less than seven percent of the single step truncation error.In general, if the step size h is chosen so that the error introduced by a single step of the iterative procedure is of the order of magnitude of the single step truncation error (which is the error in yn+1 if y′n+1, yn′, and yn-1 are known exactly), then the proposed procedure results in starting values of sufficient accuracy to warrant continuing the solution by means of the three-point Adams method.Filon's method of numerical integration was developed to deal with integrals of the form I = ∫BA ƒ(x) cos px dx (1) (Filon, 1928; Tranter, 1951). This method, most useful when p is large, is a modified Simpson's rule using an interval no larger than is required to integrate ∫BA ƒ(x) dx alone to the desired accuracy. The derivation proceeds as follows: The range of integration is divided into panels of width 2h, and a second-order curve is fitted to the middle and end ordinates of one panel. After twice integrating by parts over the width of the panel and summing over all the panels, the result is I = h{&agr;}[ƒ(B) sin pB - ƒ(A) sin pA] + &bgr;Ce + &ggr;Co}, (2) where h is the interval, Ce is the sum of all the even ordinates of ƒ(x) cos px less half the end ordinates, Co is the sum of all the odd ordinates of ƒ(x) cospx less half the end ordinates, Co is the sum of all the odd ordinates of ƒ(x) cos px, and &thgr; = hp &thgr;3&agr; = &thgr;2 + &thgr; sin &thgr; cos &thgr; - 2 sin2 &thgr; &thgr;3&bgr; = 2[&thgr;(1 + cos2) - 2 sin &thgr; cos &thgr;] &thgr;3&ggr; = 4[sin &thgr; - &thgr; cos &thgr;]. (3) In the limit as p approaches zero, (2) reduces to Simpson's rule.The present modification was developed to evaluate functions of the form F(T) = ∫T0 ƒ(x) cos px dx using a larger interval for a permissible error than is possible with Filon's formula.A fifth-order instead of a second-order curve may be fitted to the middle and end points of a panel. Substituting the first five terms of a Stirling approximation into (1), integrating over the width of the panel by parts five times, and summing over all the panels, we obtain I = ∫BA ƒ(x) cos px dx = h{S[ƒ(B) sin pB - ƒ(A) sin pA] + hP[ƒ′(B) cos pB - ƒ′(A) cos pA] + RCce + hQ C′se + NCco + hM C′so (4) where primes denote differentiation with respect to the argument, Cco = sum of odd ordinates of ƒ(x) cos px; C′so = sum of odd ordinates of ƒ′(x) sin px; Cce = sum of even ordinates of ƒ(x) cos px, less half the end ordinates; C′se = sum of even ordinates of ƒ′(x) sin px, less half the end ordinates; and &thgr; = hp &thgr;6M = 16&thgr;(15 - &thgr;2) cos &thgr; + 48(2&thgr;2 - 5) sin &thgr; &thgr;6N = 16&thgr;(3 - &thgr;2) sin &thgr; - 48&thgr;2 cos &thgr; &thgr;6P = 2&thgr;(&thgr;2 - 24) sin &thgr; cos &thgr; + 15(&thgr;2 - 4) cos2 &thgr; + &thgr;4 - 27&thgr;2 + 60 &thgr;6Q = 2[&thgr;(12 - 5&thgr;2) + 15(thgr;2 - 4) sin &thgr; cos &thgr; + 2&thgr;(24 - &thgr;2) cos2 &thgr;]&thgr;2R = 2[&thgr;(156 - 7&thgr;2) sin &thgr; cos &thgr; + 3(60 - 17&thgr;2) cos2 &thgr; - 15(12 - 5&thgr;2)] &thgr;6S = &thgr; (&thgr;4 + 8&thgr;2 - 24) + &thgr;(7&thgr;2 - 156) cos2 &thgr; + 3(60 - 17&thgr;2) sin &thgr; cos &thgr;. (5)For &thgr; less than about 0.9, it is better to expand equations (5) in powers of &thgr;: M = -16/105&thgr; + 8/945&thgr;3 - 2/10395&thgr;5 + 1/405405&thgr;7 - 1/48648600&thgr;9 N = 16/15 - 8/105&thgr;2 + 2/945&thgr;4 - 1/31185&thgr;6 + 1/3243240&thgr;8 P = -1/15 + 2/105&thgr;2 - 1/315&thgr;4 + 2/7425&thgr;6 - 62/4729725&thgr;8 Q = -8/105&thgr; + 16/945&thgr;3 - 104/51975&thgr;5 + 256/2027025&thgr;7 - 16/3274425&thgr;9 R = 14/15 - 16/105&thgr;2 + 22/945&thgr;4 - 608/311850&thgr;6 + 268/2837835&thgr;8 S = 19/105&thgr; - 2/63&thgr;3 + 1/275&thgr;5 - 2/8775&thgr;7 + 34/3869775&thgr;9.(6) For sin px instead of cos px in the integrand, the result is ∫BA ƒ(x) sin px dx= h{S[ƒ(A) cos pA - ƒ (B) cos pB] + hP[ƒ′(B) sin pB - ƒ′(A) sin pA] + RCse - hQC′co + NCso - hMC′co} with an obvious change of notation for the C's.In the limit as p approaches zero, equation (4) reduces to the modified Simpson's rule described by Lanczos (1957).Analytical estimation of the error involved in this method has not been carried out. As an empirical check on the error—and on the expansions (6)—the integral ∫1.50.5 ex cos &pgr;x dx was evaluated to nine decimal places by Filon's method and by the modification. The errors, compared with the true value of -1.7718441, were: Interval Number of points Error, Filon's method Error, Modification 0.1 11 .00000141 < 10-8 0.25 5 .00070660 .00000016 0.5 3 .00051522 .00008785As Lanczos (1957) points out, the validity of these integration procedures depends on the convergence of the Stirling approximation of ƒ(x). For Filon's method this necessitates the smooth behaviour of differences up to the fourth order, and for the modification presented here, of differences up to the sixth order.Used with Hitchcock's (1957) approximations, Filon's method, or this modification of it, is also useful with integrals containing J0(px) or J1(px) instead of the trigonometric functions.Luke (1954) considered the fitting of an nth order curve to the middle and end points of a panel, and gave a detailed discussion of the error involved—the results obtained here are special cases of his equations. Luke's formulae are, however, suitable primarily for hand computation using tabulated functions, whereas the results given here are intended for use with an electronic computer.The author wishes to thank the referee for drawing his attention to the paper by Luke.In [1], A. Householder described a method for the unitary triangularization of a matrix. The formulas given there are valid for the real case. In this note we describe the modifications to handle the complex case and also point out a small modification in the real case which will improve the numerical accuracy of the method.At first we are concerned with a complex vector space. The basic tool is the fact that if ‖ u ‖ = √2, then the matrix I - uu* is unitary, as may be readily verified.The following lemma is a modification of the one give in [1].LEMMA. Let a ≠ 0 be an arbitrary vector and let v be an arbitrary unit vector. Then there exists a vector u with ‖ u ‖ = √2 and a scalar &zgr; with | &zgr; | = 1 such that (I - uu*)a = &zgr; ‖ a ‖ v. (1) PROOF: Letting &agr; = ‖ a ‖ and &mgr; = u*a, (1) may be written a - &agr;&zgr;v = &mgr;u.(2) Multiply by a* gives &agr;2 - &agr;&zgr;a*v = &mgr;a*u = ‖ &mgr; ‖2. (3) It follows that &zgr;a*v is real. Assuming for the moment that a*v ≠ 0, we write it in polar form a*v = rw, r > 0, ‖ w ‖ = 1. Then the fact that &zgr;rw is real implies that &zgr; = ± w. (4) Substituting into (3) gives ‖ &mgr; ‖2 = &agr;2 ∓ &agr;r. (5) We now set, arbitrarily, arg (&mgr;) = 0. Then &mgr; = √&agr;(&agr; ∓ r). (6) Next, we select the negative sign in (4) in order to avoid the subtraction of two positive quantities in (6), since such a subtraction may give rise to numerical difficulties. Collecting the formulas, we see that the following sequence of computations will produce the required u and &zgr;: &agr; = ‖ a ‖ (7) r = ‖ a*v ‖ (8) &zgr; = -a*v/r (9) &mgr; = √ &agr;(&agr; + r) (10) u = 1/&mgr;(a - &zgr;&agr;v).(11) The case a*v = 0 may be handled if, instead of using (9), we let &zgr; be an arbitrary number with ‖ &zgr; ‖ = 1. It is easily verified that the formulas thus modified will still work.The computation requires 3 square roots to compare &agr;, r, and &mgr;. A slight modification [1] permits one to avoid the root required to compute &mgr;. In the real case, no root is required to compute r.Now consider the case of a real vector space. The formulas given [1] for this case are essentially the same as ours except that (8) is replaced by r = a*v, and (10) by &mgr; = √&agr;(&agr; - r). If &mgr; is computed this way and if r is positive and near &agr; (as is the case when v is near a/&agr;), cancellation of significant digits will occur. This difficulty, and the need for making a special case when v is exactly a/&agr;, is avoided in the present set of formulas.
Given a sequential machine, in the terminology of E. F. Moore, Annals of Mathematics Studies, No. 34, 1956, a problem of some interest is that of determining testing procedures which will enable one to transform it into a known state starting from an initial situation in which only the set of possible states is given.To treat this problem, we introduce the concept of ambiguity, and show how the functional equation approach of dynamic programming can be applied.Occasionally in the numerical solution of elliptic partial differential equations the rate of convergence of relaxation methods to the solution is adversely affected by the relative proximity of certain points in the grid. It has been proposed that the removal of the unknown functional values at these points by Gaussian elimination may accelerate the convergence.By application of the Perron-Frobenius theory of non-negative matrices it is shown that the rates of convergence of the Jacobi-Richardson and Gauss-Seidel iterations are not decreased and could be increased by this elimination. Although this may indicate that the elimination could improve the convergence rate for overrelaxation, it is still strictly an unsolved problem.In this paper the numerical solution of Laplace's equation for the circle is discussed and consideration is given to the convergence of the solution obtained by the boundary contraction method to the analytic solution. It is proved that in order to achieve this a relation between the mesh sizes in the circumferential and radial direction must exist. It is also demonstrated that the error due to the contraction of boundaries can be made insignificant.In Part I of this paper [1] the authors have shown that instability in Milne's method of solving differential equations numerically [2] can be avoided by the occasional use of Newton's “three eights” quadrature formula. Part I dealt with a single differential equation of first order. In Part II the analysis is extended to equations and systems of equations of higher order.A differential equation of order m or a system of equations of total order m can be expressed by m simultaneous equations of the first order dyi/dt = Fi(y1, …, ym, t), (i = 1, 2, …, m) (1) in m dependent variables yi. Let y denote a vector in m-space which has the components y1, &hellip, ym. Let z, with components z1, z2, …, zm, denote a small variation in the vector y such as might have been produced by a small error occurring at an earlier value of t. The differential system (1) can be written in vector form as dy/dt = F(y, t) (2) and the varied vector y + z satisfies dy/dt + dz/dt = F (y + z, t). (3)Let us assume that the functions Fi have continuous partial derivatives ∂Fi/∂yi = aij and let G denote the m × m matrix [aij]. From equations (2) and (3) it may be shown that to terms of the first order in the small quantity z we have dz/dt = Gz. (4)The matrix G is usually a variable depending on the y's and t, but it is possible to forecast the general behavior of the error by treating the simple case where G is constant. (See [3] of Part I.) Assuming that G is constant we first of all briefly review some well-known facts about the solution of the differential system (4).If we introduce a new vector variable x in place of the variable z by means of a nonsingular linear transformation x = Tz, it is seen that the system (4) is transformed into dx/dt = (TGT-1)x. (5)In particular the transformation T may be chosen so as to put the matrix TGT-1 into the classical canonical form. (Cf. e.g., Turnbull and Aitkin [3].) Then if the latent roots &lgr;1, &lgr;2, … &lgr;m, of the matrix G are all distinct, the new matrix TGT-1 has these latent roots in the principal diagonal and zeros elsewhere. In these case the differential system (5) separates into m independent equations dxi/dt = &lgr;ixi, (i = 1, 2, …, m), (6) where xi is the ith component of x. The solutions are xi = cie&lgr;it, in which the ci are arbitrary constants.If the latent roots are not all distinct there may occur ones instead of zeros in the diagonal just above the principal diagonal. Wherever a one occurs the latent root to its left equals the latent root below it. In such a case we have in addition to equations of type (6) certain nonhomogeneous linear equations. For example, if &lgr;1 = &lgr;2 = &lgr;3, while the remaining roots are distinct, we would have dx1/dt = &lgr;1x1 + x2, dx2/dt = &lgr;1x2 + x3, (7) dx3/dt = &lgr;1x3. The solutions of the system (7) are x3 = c3e&lgr;1t, x2 = (c2 + c3t)e&lgr;1t, x1 = (c1 + c2t + c3t2/2)e&lgr;1t.It should be noted that multiple roots do not always lead to equations of type (7). Such a case as illustrated by example 3 at the end of the paper.Turning now to numerical integration and the problem of stability we introduce a linear operator S defined by the equation Sƒ(t) = ƒ(tn+1) - ƒ(tn-1) - (h/3)[ƒ′ (tn+1) + 4ƒ′ (tn) + ƒ′ (tn-1)], (8) where ƒ′ (t) means df/dt and where tn = nh. If ƒ(tn+1) has been computed by means of Simpson's rule from ƒ(tn-1) and the values of ƒ′ (t), it is clear that Sƒ(t) = 0. Since in Milne's method the final values of the variables yi, are found by Simpson's rule we may assume1 that Syi = 0 for i = 1 to m. Then Sy = 0, and since z represents an error inherited from previous steps we have S(y + z) = 0, whence Sz = 0 also. Moreover, since x = Tz, it follows that Sx = STz = TSz = 0. Hence Sxi = 0, i = 1, …, m. (9)From equations (6), (8), and (9) it is clear that in case &lgr;i is not a multiple root the values xi satisfy the difference equation (1 - s/3)xi(tn+1) - (4s/3) xi (tn) - (1 + s/3) xi (tn-1) = 0, in which s = h&lgr;i. This is identical with equation (1) of Part I. Hence we see by equation (2) of Part I that xi is expressed in the form xi (tn) = Ar1n + Br2n, where r1 and r2 are roots of the quadratic equation (1 - s/3) r2 - (4s/3) r - (1 + s/3) = 0. (10)We now consider the following three cases: Case 1, where &lgr;i is real and simple. In this case the treatment of Part I applies without change to the stabilization of the particular component xi.Case 2, where the root &lgr;i is complex but not a multiple root.Consider equation (10) as the defining equation for a complex function r of a complex variable s. This two-valued function has branch points at s = ± i√3. We make branch cuts in the two-sheeted Riemann surface in the s-plane along the axis of imaginaries from s = + i√3 up to infinity, and from s = -i√3 down to infinity, as shown in figure 1 (here i is the unit of imaginaries).These branch cuts are mapped in the r-plane on the circle BCDC′ with center at (-2, 0) and radius √3. Corresponding points in the s- and r-planes are denoted by the same letters. The function which we have called r2 is mapped conformally on the interior of the circle BCDC′, the function r1 on the exterior. The interior of the unit circle in the s-plane (which is our only concern in this discussion) is mapped on the shaded regions in figure 2. The segment BD of the imaginary axis in the s-plane goes into the unit circle BADA′ with center at the origin in the r-plane.When &lgr;i is a complex latent root, but not a multiple root, we proceed exactly as in Part I, and find that the stability of the solution corresponding to s = h&lgr;i depends on the latent roots u and w of the matrix M. These latent roots are approximately expressed by the formulas u = r1k, w = r2kQ, just as in Part I.(We recall that the above simple forms for u and w were obtained by replacing K(r1) by r13. The resulting error in u and w can be shown to be of the order of s5. In the domain of s that is used in practice we may ignore these errors.)When &lgr;i is a complex latent root, but not a multiple root, we proceed exactly as in Part I, and find that the stability of the solution corresponding to s = h&lgr;i depends on the latent roots u and w of the matrix M. These latent roots are approximately expressed by the formulas u = r1k, w = r2kQ, just as in Part I.(We recall that the above simple forms for u and w were obtained by replacing K(r1) by r13. The resulting error in u and w can be shown to be of the order of s5. In the domain of s that is used in practice we may ignore these errors.)In [1] Carr established propagation error bounds for a particular Runge-Kutta (RK) procedure, and suggested that similar bounds could be established for other RK procedures obtained by choosing the parameters differently. More explicitly, a fourth-order Runge-Kutta procedure for the solution of the equation y′ = ƒ(x,y) is based on the computation: k1 = hƒ(xi, yi) k2 = hƒ(xi, + mh, yi + mk1) k3 = hƒ(xi, + vh, yi + (v - r)k1 + rk2) k4 = hƒ(xi, + ph, yi + (p - s - t)k1 + sk2 + tk3) k = ak1 + bk2 + ck3 + dk4 yi+1 = yi + k, where h is the step size of the integration.Carr considered the case: k1 = hƒ(xi, yi) k2 = hƒ(xi + 1/2h, yi + 1/2 k1) k3 = hƒ(xi + 1/2h, yi + 1/2 k2) k4 = hƒ(xi + h, yi + k3) k = 1/6 (k1 + 2k2 + 2k3 + k4) yi+1 = yi + k, and established the following theorem (we shall use the notation of [1] without further explanation): THEOREM 1. If ∂ƒ/∂y is continuous, negative, and bounded from above and below throughout a region D in the (x, y)-plane, -M2 < ∂ƒ/∂y < - M1 < 0, where M2 > M1 > 0, then for a maximum error (truncation, or round-off, or both) E in absolute value at each step in the Kutta fourth-order numerical integration procedure has total error at the ith step, i arbitrary, in the region D*of | &egr;i | ≦ 2E/hM1, where the step size h is to be taken to be h < min (4M13/M24 , M1/M22 - 2 dsM22 - 2 dsmM1M2)PROOF. Let yi+1 be the value of the solution obtained at step i + 1. If there is no error at step i, let yi+1* be the value of the solution obtained assuming an error of &egr;i introduced at the ith step. Then the propagated error at the (i + 1)- st step is &eegr;i+1 = yi+1* - yi+1.The proof of theorem 1 as given in [1] is based on the inequality | &eegr; i+1 | ≦ | &egr;i || 1 - hM1/2 |. We shall show that this inequality can be obtained under the hypotheses of theorem 2.In the determination of the parameters a, b, c, d, m, v, p, r, s, t for the Runge-Kutta procedure, certain coefficients of Taylor expansions of k1, k2, k3, k4, and k are equated, providing a set of eight equations: a + b + c + d = 1 bm + cv + dp = 1/2 bm2 + cv2 + dp2 = 1/3 bm3 + cv3 + dp3 = 1/4 crm + d(sm + tv) = 1/6 crm2 + d(sm2 + tv2) = 1/12 crmv + dp (sm + tv) = 1/8 drtm = 1/24Assuming, then, that we have a Runge-Kutta procedure obtained this way, we may use the equations (3) when necessary.If there is an error &egr;i at the ith step, the value of k1 (at the (i + 1)-st step) will be (by a simple application of the Mean Value Theorem): k1* = hƒ(xi, yi + &egr;i) = hƒ(xi, yi) + h ∂ƒ/∂y &egr;i = k1 + hƒy&egr;i, where the partial derivative ƒy = ∂ƒ∂y is evaluated in a suitable rectangle.Similarly, (remembering that each occurrence of ƒy is to be evaluated at a possibly different point):Frequently, as in missile control systems, linear differential equations are simultaneous with nonlinear but slower acting differential equations. The numerical solution of this type of system on a digital computer is significantly speeded up by approximating the forcing functions with polynomials, solving the linear equations exactly, and numerically integrating the nonlinear equations with Milne integration. Automatic interval adjustment is possible by comparing errors in the nonlinear integration. The interval selected is related to the shortest time constant of the nonlinear equations rather than the shortest of all the equations. With this system, both detailed transient response and steady state conditions are revealed with a minimum of machine time.Many practiced and proposed methods for the generation of pseudo-random numbers for use in Monte Carlo calculation can be expressed in the following way: One chooses an integer P, the base; an integer &lgr;, the multiplier, prime to P; and an integer &mgr;, the increment, less than P (&mgr; is frequently, but not always, zero). One then defines recursively a sequence x0, x1, x2, … (1) of integers by x0 = a, (2) xn+1 ≡ &lgr;xn + &mgr; (mod P), (3) 0 ≦ xn < P. (4) It is clear that such a sequence is periodic with period P or less. Much work has been done on the determination of the period for various choices of P, &lgr; and &mgr; [1-7]. From this work one concludes that it is relatively easy to assure an adequately long period and that other considerations should determine the choice of these parameters.As for P, machine convenience dictates the choices P = 2q or P = 10q for a q-place binary or decimal machine, respectively. No other choice of P appears to have any practical advantage whatever.As for &lgr; and &mgr; it is the thesis of this note that they should be chosen to reduce serial correlation in the sequence (1), and we proceed to show how this may be done. We assume that &lgr; and &mgr; are such that the sequence (1) has an adequately long period. Then, clearly, one may assume, with error of order 1/P or less, that the sequence (1) is continuously and uniformly distributed on the interval (0, P). Then, if (&lpargt;Z⦔) denotes the mean value of Z, &lpargt;xn⦔ = 1/P ∫P0 x dx = P/2 (5) &lpargt;xn2⦔ = 1/P ∫P0 x2 dx = P2/3 (6) and Var (xn) = P2/3 - P2/4 = P2/12. (7) Further, one may write xn+1 = P{(&lgr;xn + &mgr;)P-1}; (8) where {···} denotes “fractional part of.” (9) Also &lpargt;xnxn+1⦔ = 1/P ∫P0 xP {(&lgr;x + &mgr;)P-1} dx. (10)An elementary but tedious integration yields (see Appendix) &lpargt;xnxn+1⦔ = P2/4 + P2 - 6&mgr;(P - &mgr;)/12&lgr;. (10′) Hence cov (xn, xn+1) = P2 - 6&mgr;(P - &mgr;)/12&lgr;, (11) and, if &rgr; (x, y) = the correlation coefficient of x and y, (12) then &rgr; (xn, xn+1) = 1 - 6 &mgr;/P (1 - &mgr;/P)/&lgr;. (13)One must be cautious of the conclusions one draws from (13) because of the approximate nature of its derivation. But one can say that a method which uses very small values of &lgr; and &mgr; is faulty. Unfortunately, such methods have been repeatedly suggested, and in one case within the author's knowledge serious difficulties were encountered in a Monte Carlo calculation because of the use of one such method.One should not lose sight of the fact that the correlation between a random number and its immediate successor is by no means the only one of importance. It is very difficult to give a precise rule, but it seems plausible that the correlation between the current random number and, at least, its next 10 or 20 successors should be controlled. Fortunately, for any such method one may write xn+p ≡ &lgr;pxn + &mgr;p (mod P), (14) with &lgr;p ≡ &lgr;p (mod P) (15) and &mgr;p ≡ &lgr;p - 1/&lgr; - 1 &mgr; (mod P), (16) and again one may estimate the correlation with the use of equation (13).Although the multiplicative congruential method for generating pseudo-random numbers is widely used and has passed a number of tests of randomness [1, 2], attempts have been made to find an additive congruential method since it could be expected to be faster. Tests on a Fibonacci sequence [1] have shown it to be unsatisfactory. The sequence xi+1 = (2a + 1) xi + c (mod 235) (1) has been tested on the IBM 704. In appendix I it is shown that the sequence generates the full period of 235 numbers for a ≧ 2 and c odd. Similar results obtain for decimal machines. Since multiplication by a power of the base can be accomplished by shifting, which is comparable in speed to addition, this scheme requires essentially three additions. It takes 14 machine cycles on the IBM 704, compared to 28 for the multiplicative method, so that the saving is 168 &mgr;s/random number. The scheme has the further advantage that it does not destroy the multiplier-quotient register.Some tests have been made on the randomness of this sequence for a = 7 and c = 1, and a summary of the results is given in appendix II, where now the random numbers are considered to lie in the interval (0, 1).The serial correlation coefficient between one member of this sequence and the next is shown by Coveyou [3] to be approximately 0.8 per cent. By taking a = 9 this correlation coefficient can be reduced to approximately 0.2 per cent without increasing the time. Taking a = 21 would make this correlation very small but would require one more machine cycle on the IBM 704. Another way to reduce the correlation is to choose c such that the numerator in Coveyou's expression for the correlation coefficient is zero. This cannot be done exactly since it requires that c = (.5 ± √3/6)2P where P is the number of binary digits (excluding sign) in a machine word. However, a machine representation close to either of these numbers should be satisfactory. Some correlations with c = (.788+)235 and a = 7 were obtained and did not differ significantly from those given for c = 1 in the first section of appendix II.The author wishes to thank R. R. Coveyou for communicating his results in advance of publication and Elizabeth Wetherell for carrying out the calculations.In a recent paper1 I stated that von Neumann had originated the suggestion for the use of Schur's canonical form for arbitrary matrices. I have since learned that the suggestion actually is due in the first instance to John Greenstadt, who brought it to von Neumann's attention. The history of this is rather interesting and was communicated to me in a letter from John Greenstadt, which I quote below.“The full story is, that the triangularization occurred to me early in 1953, after trying in vain to find a general iterative diagonalization procedure, even where one knew that it was possible to diagonalize (defective degeneracy being the impossible case). It seemed to me that one thing that made for the stability of the Jacobi method was the fact that all the elements in the transformation matrix were less than 1. A natural generalization embodying this requirement was to consider unitary transformations. Then, a quick check of Murnaghan's book showed that one could hope only to triangularize, but that this was always possible.“I did some hand calculations on this, and lo and behold! it converged in the few cases I tried. I then programmed it for the CPC and tried many other cases. For several months thereafter, Kogbetliantz, John Sheldon, and I tried to prove convergence, when the algorithm involved the sequential annihilation of off-diagonal elements. We (particularly Sheldon) tried many approaches, but with no hint of success. Finally, in the latter part of 1953, we decided to ask von Neumann, who was then a consultant for IBM, when he was in New York at our offices.“I had prepared a writeup describing the procedure, but von Neumann (rightly) didn't want to bother reading it, so I explained it to him in about two minutes. He spent the next 15 minutes thinking up all the approaches we had thought of in three or four months, plus a few ones—all, however, without promise.“At this point he decided that it was a nontrivial problem, and perhaps not worth it anyway, and immediately suggested minimizing the sum of squares of subdiagonal elements, which is, of course, the truly natural generalization of the Jacobi method. For the next 15 minutes he investigated the case when it would be impossible to make an improvement for a particular pivotal element and found that these cases were of measure zero.“I recoded my procedure for the 701 and tried many other matrices of various sizes. I myself never had a failure, but it has since been demonstrated that the method will indeed fail for a class of matrices. Hence, a proof is clearly impossible. However, I think a statistical proof is possible, along lines suggested by Kogbetliantz, which, however, I have not been able to find. I do not think von Neumann's variation of the method would fail. (However, it is more complicated and time consuming.)”
It is shown that for a given problem different machine organizations will vary in time of solution over a nine-fold range. This holds even if it is assumed that the machines used the same arithmetic section and the same memory. The comparison is made on the basis of a typical problem—in this case the Kaczmarz iteration. An efficiency number is developed giving the relative solution time required by each organization. When the efficiency number of a single address machine is taken to be 1.0, the following numbers are obtained: four address machine, 1.25; single address machine with index registers, 2.5; complex organization, 9.In this paper we wish to initiate the study of the application of dynamic programming to the domain of problems arising in the synthesis of logical systems.In a number of fields one encounters the problem of converting a system in one state into another state in a most efficient fashion—in mathematical economics, in the theory of control processes, in network theory, and in trajectory processes. Here we wish to consider a type of question which arises in the design of computers and switching circuits.We shall first treat the problem in general terms, and then consider a special example to illustrate the methods.New series expansions are developed for computing incomplete elliptic integrals of the first and second kind when the values of the amplitude and modulus are large. The classical series, which are obtained after a binomial expansion of the integrands, are used when the values of the amplitude and modulus are small. The range of use of each series is so selected as to maintain a minimum of rounding error. A special criterion is used to determine when the binomial series should be terminated.The calculation of elliptic integrals by these series expansions is compared with the calculation by the previously established Landen transformation, which has been used by Legendre. The new series yield more accurate results and the average time of computation is 30 per cent shorter. The computing program in the NORC subroutine for the calculation of elliptic integrals is described.Several empirical tests were made of the apparent randomness of numbers generated by the additive process Xj = (Xj-1 + Xj-n) mod 1, where the X's are positive fractions. The results show that the numbers are uniformly distributed on the unit interval and that there is no significant serial correlation in the sequence. However, for n < 16, a test of run lengths indicates nonrandomness. This difficulty can be overcome by discarding alternate numbers.The primary objective of this paper is to extend the results of the earlier paper, “A Functional Canonical Form”, in this journal [4]. This extension includes the application of the fundamental functional canonical form to multiple output circuits and the formulation of cost bounds on these circuits. It also includes the use of a multifunctional canonical form in the design of single output circuits.
The PILOT data processor is a high-speed multiple computer system, more than 100 times faster than SEAC. It contains three interconnected computers for rapid processing of data, and also contains multiple input-output channels for rapid transfer of data into and out of the system. All of these units operate concurrently in a coordinated fashion. A summary description is given of the over-all logical plan of the system, including the principal characteristics of the first computer, the second computer, the third computer, the internal controls, and the external controls.The system combines internal processing capabilities that are fast and versatile with external communication capabilities that are exceptionally flexible. This combination permits the internal power of the machine to be exploited readily by the outside world,—either by other automatic devices or by human operators.It has been suggested by Householder [1] and by Householder and Bauer [2] that orthogonal similarity transformations of matrices are particularly stable with respect to the practical computation of proper values. It is the purpose of this note to examine this question and to demonstrate in terms of a “condition number” to be defined below a sense in which this conjecture is true.Broadly speaking, any problem may be termed “ill-conditioned” for which the solution is acutely sensitive to slight variations in the parameters of the problem. Examples of ill-conditioning occur in many contexts. Computers are familiar with the phenomenon as it manifests itself, for example, in the study of matrix inversion. Since our purpose is to study the conditioning of matrices specifically for the proper value problem, it is convenient to have a nomenclature and notation which will avoid confusion with conditioning as it is used in other senses.A necessary and sufficient condition is given for the absolute stability of multipoint numerical integration formulas for differential equations. The condition is that a certain matrix of low order, whose elements are computable from the coefficients of the integration formula, be positive definite. Two simple examples are given.A method of computation for spherical Bessel functions of real and imaginary argument is given which is especially suitable for high speed digital computers. The accuracy and convergence are examined and criterion formulas are given. A procedure based on the Wronskian is used to simplify the final normalization.Two methods recently developed for generating normal deviates within a computer are reviewed along with earlier proposals. A comparison of the various methods for application on an IBM 704 is given. The new direct method gives higher accuracy than previous methods of comparable speed. The detailed inverse technique proposed yields accuracy comparable with, or better than, most previous proposals using about one-quarter the computing time.A class of quadrature formulas is derived which achieve higher accuracy in composite rules (i.e., where the interval of integration is broken up into a number of subintervals) than analogous Newton-Cotes or Gaussian formulas. The cost of this higher accuracy is the computation of one or two more ordinates over the whole interval of integration.The high accuracy is obtained by using Gaussian techniques in the interior of each subinterval and by using the endpoints of each subinterval as abscissas with weights of equal magnitude and opposite sign. In this way when the subintervals are put together only the endpoints of the whole interval of integration remain. It is proved that the abscissas are all real and interior to the subinterval and that the weights corresponding to the interior abscissas are positive.Since the abscissas are not equally spaced, the method is not suited to tabular functions but rather to analytically given functions. The roundoff properties of the formulas are discussed and are shown to be quite good.Algorithms for floating point computer arithmetic are described, in which fractional parts are not subject to the usual normalization convention. These algorithms give results in a form which furnishes some indication of their degree of precision. An analysis of one-stage error propagation is developed for each operation; a suggested statistical model for long-run error propagation is also set forth.A radio telegrapher can, by operating a key, turn a transmitter on or off for any desired period. International radio-telegraph (Morse) code is predicated on controlling these parameters of key position and duration of the signal. The messages to be sent are represented by a sequence of five elementary symbols: the dot, dash, intra-character space, inter-character space, and inter-word space. In terms of an arbitrary time unit determined by the sender1, a dot results from closing the key for one and a dash for three time units. An intra-character space results from opening the key for one unit, an inter-character space for three, and an inter-word space for seven time units (see table 1).All letters, numerals, punctuation marks, and several brevity codes are represented by sequences of dots, dashes, and intra-character spaces between successive occurrences of inter-character or inter-word spaces. The sequences that are used in international radio-telegraph are shown in table 2. The letter “A”, for example, is represented by an inter-character space (shared with the preceding character) followed by a dot, an intra-character space, a dash, and an inter-character space (shared with the succeeding character). Thus any message that can be transmitted by a sequence of characters can be broken down into a sequence of the five primary elements for transmission by radio-telegraph.If manual Morse senders transmitted accurately, it would be easy to construct equipment to copy them. Indeed, just such devices have been constructed for signals sent by machine. Most human operators, however, are unable to control the duration of elements with sufficient precision to be copied by these.An idea of the variability of durations formed by a manual Morse operator can be gained from figure 1. Although all of these patterns are a single operator's sending of the same symbol (a question mark), a great deal of variation is apparent in their formation. Despite these variations, the basic structure of the symbol, that is, the · · —— · · pattern, is immediately evident to both the eye and the ear. This remains true, although the variation is greater, when symbols sent by different operators are compared. To make a machine transcribe Morse as accurately as a human being, we must find some means of conserving the basic information conveyed by the form of the pattern and eliminating the effects of the nonsignificant variations.Many attempts have been made to construct a machine that will automatically transcribe hand-sent Morse into printed copy.2 In the past, each new proposal has been tested by constructing an operating model of the device. The results of these efforts have been uniformly disappointing, with resultant waste of time, talent and equipment. At the outset of our research in this field, it seemed clear that we could learn a valuable lesson from our predecessors.Clearly, we needed a flexible means of implementing translation techniques to insure that no great loss would accrue from the changes that would undoubtedly be necessary as we better understood its shortcomings. Second, previously constructed devices have suffered from faults that, owing to the nature of the equipment, could not definitely be assigned to engineering or logic. We determined, therefore, to solve the problems of logic independently of those of engineering. These considerations led to the decision to attempt to simulate manual Morse transcription devices on a general-purpose digital computer.3A really good manual Morse operator can transmit about 35 words per minute. Thirty-five words per minute corresponds to approximately 28 short elements a second (a short element is a dot or an intra-character space). At this rate, therefore, the shortest duration is about 36 milliseconds. A digital computer which requires 100 microseconds to perform its simple operations (add, etc.) is considered to be moderately slow; yet a computer which operates at this rate is capable of performing 360 instructions during the duration of the shortest element at the highest speed that one normally expects to encounter in manual Morse. Most Morse transmissions are substantially slower than 35 w.p.m., and many computers have substantially faster operation times than 100 microseconds. It seemed reasonable, therefore, to expect a computer to have the speed capability of transcribing manual Morse.In order to use a digital computer for transcribing Morse code, we had to devise a method of converting the keying information into a numerical form which represents the variables of key position and duration. To minimize programming complications, we decided to use an external device to convert the audible signal into a facsimile of the original key operations (i.e., a demodulator). For laboratory-type signals (no noise, fading, etc.), there are demodulators which are able to perform this conversion accurately. The state of the key can be specified by a single binary digit; the duration can be designated by the number of time units4 that the key has remained in a given position.5 The values of these two parameters arranged in time sequence convey the information contained in a radio-telegraph signal.6There are several ways of converting the original signal into numerical form. First we shall consider the method which requires a minimum of special purpose equipment. Assuming the machine has a conditional jump which is contingent upon the position of an external switch—a facility available in nearly all digital computers—we need only attach the contacts of the telegraph key or demodulator relay in parallel with the contacts of this external switch. The program is written so that the elapsed time between successive conditional jumps, regardless of the path taken, is constant. The amount of time that the key remains in a given position is determined by “sampling” (performing the conditional jump) at these periodic intervals and counting the number of samples between successive changes of position.7For example, prior to the beginning of the message, the key is open. The operator closes the key to send the first element of the first character and the next conditional jump detects that the key has been closed; this jump causes the program to add one to a previously cleared counter and, after a predetermined period, sample the key again. If the key is still closed, the program again adds one to the counter and proceeds as before. Several milliseconds later the operator opens the key to indicate the end of the first element of the first character of the message; the next conditional jump detects the change. At this time the accumulated count is the number of time units that the key remained closed; the program stores this value in the first position of the keying time sequence, clears the counter and resumes sampling to determine the duration of the succeeding key-open. The key-open duration is measured in precisely the same manner and stored as the next item in the list. The process continues until a very long key-open indicates the end of the transmission. Clearly, this procedure will convert the key information into a sequence of numbers which indicate the successive states of the key with the duration of each state.8The necessity for periodic examination of the key becomes quite burdensome. For, in order to perform all the functions involved in transcribing a signal into printed copy, the program must have many alternative paths. If the machine is to sample the key periodically, it is necessary to insure that every path that can be traced through these alternatives requires the same amount of time, so that the difference between successive samplings is constant. Although this can be and has been done, it places a very severe burden on the programmer. Recall that the computer must sample the key periodically only because it must maintain durations in terms of a fixed time interval. Programming is made far simpler by supplying an independent clock which is advanced periodically and which can be examined by the program to determine element durations. Of course, it is still necessary for the program to examine the key frequently. However, it is considerably less difficult to write a program that samples the key about every 10 milliseconds than it is to write a program which must sample it exactly every 10 milliseconds. The addition of a program interrupt when the key changes state would further simplify programming.The distinguishing, and crucial, characteristic of manual Morse transcription schemes is the method of determining which element is to be assigned to each duration. The sequence of durations is, in effect, a numerical representation of a set of approximations to the five basic elements. Assigning these approximate patterns (durations) to the “ideal” patterns (elements) which they represent is a one-dimensional pattern recognition problem. Virtually each attempt to transcribe Morse has used a different procedure for assigning durations to elements.Even a cursory discussion of each of these would lengthen this paper considerably; we shall limit ourselves, therefore, to the class of discrimination techniques which includes our final proposal. This class of assignment processes assumes that durations associated with the same element tend to be approximately equal and that durations of different elements differ by an amount greater than the variation between durations representing the same element.Figure 2 illustrates the characteristic distributions of Morse elements produced by a typical operator. The abscissa, calibrated in milliseconds, gives the range of durations observed in the transmission of a message. The ordinate represents the number of times that a given duration was observed in the transmission of the message. The values above the center line represent key-closed durations; those below are key-open durations. From this arrangement we see that, although the operator has not associated a unique time interval with each of the elements, he has managed to group durations associated with each element into a cluster. In the key-closed range, for example, there is a cluster of values on the left representing dots, and on the right, another cluster representing dashes. In the key-open range there is a cluster, approximately under the dot distribution, representing intra-character spaces. Under the dash distribution, a second cluster represents inter-character spaces; and further to the right, a very poorly defined cluster represents inter-word spaces.To assign each duration to the correct one of the five basic elements, we must devise a method of determining which observations belong to each cluster. It is evident from examination of the typical distribution in figure 2 that the separation between dots and dashes can be made with relative ease by taking a value lying between the two distributions as the dividing line. This dividing line will be called the discrimination value. And key-closed element which has a duration less than the discrimination value will be called a dot; and any other will be called a dash. Examination of the distributions furnished by a number of operators indicates that this example is typical; that is, there is almost always a clear-cut separation between the two distributions of key-closed elements.9The situation is quite different for the key-open distributions. The dividing line between intra- and inter-character spaces is difficult to ascertain and the line between inter-character and inter-word spaces is even more difficult. Clearly, an excellent discrimination technique is needed to determine the assignment of key-open durations to the correct elements.The sample distributions shown are taken from a single operator over a fairly short span of homogeneous text. The parameters of the distribution can change quite markedly with the passage of time (from fatigue for example), or when the message form changes (say from English to random text) or when another operator takes over the circuit. These considerations eliminate the possibility of using fixed thresholds to separate the elements.One might consider the possibility of allowing a person to control variable thresholds on the basis of the distributions and the printed text (and our experience indicates that this approach is possible) but it has the considerable disadvantage of requiring continuing supervision.10 We have concentrated on simulating a far more useful device which can transcribe Morse without assitance. Thus, it was necessary to devise automatic means of determining the separation between duration distributions.The basic discrimination technique is an iterative process for calculating a dividing line such that the “distance” in lower distribution standard deviations from the mean of the lower distribution is equal to the “distance” in upper distribution standard deviations from the mean of the upper distribution. In practice, it is convenient to perform the equivalent process of finding the discrimination value for which the averaged “distance” (which we call the “goodness of separation”) is maximized.Initial dividing lines are based on the expected behavior of a Morse transmission. For example, in random text, about 54 percent of the key-closures are dots (46 percent dashes), and 68 percent of the key-opens are intra-character spaces (32 percent inter-character). If the text is sent in five letter groups, 20 percent of the inter-character spaces are also inter-word spaces. Similar calculations made from other types of text give values which do not differ greatly from these. (English, for example, has 61 percent dots and intra-character spaces and about 20 percent inter-word spaces.) Since these values are only used as a first approximation, the effect of the small variations is negligible.To separate two distributions, the computer uses the initial dividing line to calculate the mean, X, and the standard deviation, S, of the two distributions; then computes and stores the “goodness-of-separation” statistic,11k = (pxu - qx1) ÷ (psu + qs1). The dividing line is moved down one unit and k is recalculated; if the new k is greater than or equal to the previous k, this step is repeated. When a new k is found that is less than a previous k, the computer repeats this process while moving the dividing line upward until the value of k is again reduced. It then uses the dividing line which gave the previous k (i.e., the maximum value of k) as the line separating the distributions. The behavior of k for a typical pair of distributions is shown in figure 3.Note that this procedure assumes there are exactly two distributions; the key-open measurements, however, come from four distributions. The effects of two of these distributions are minimized by using the a priori estimates to eliminate these observations. Since the easiest separation (determined empirically) is intra- from inter-character, this is done first with the upper 20 percent of the observations removed. Using the separation line so determined to remove the effect of the intra-character distribution and the 80th percentile to eliminate extremely large observations (representing the undefined but existing inter-line durations), the computer determines the value which divides inter-character from inter-word spaces.Finally, this separation is used to eliminate the effects of the lower distributions from the attempt to separate inter-word from inter-line spaces.For effective transcription, the computer must recognize the point at which a large change in sending speed occurs so it can transcribe the information prior to the shift using discrimination points based on durations occurring before the change and transcribe the succeeding signals using discrimination points formed from new durations alone. In order to do this, the computer continually monitors the “goodness-of-separation” statistic and, whenever this drops below a preset value, indicating that the current samples are drawn from populations having different parameters, stops revising the discrimination points but continues to read more information and to calculate the “goodness-of-separation” values until the minimum occurs when half the observations are from one population and half from the other. The old discrimination points are then used for durations before the minimum and the duration immediately after the minimum is treated as if it were the first element of a new message.12This type of discrimination process can be economically realized by means of an unconventional, special-purpose analog device.Once the sequence of elements has been determined, its transcription into the characters it represents is accomplished by a straightforward recoding and table look-up. To recode, the machine examines the elements in order. For each dot, 1 is added to a counter; for each dash 2 is added. At each intra-character space, the counter is multiplied by 2. When an inter-character or inter-word space occurs, the value which has been formed is used as the entry address of a table which determines the character this sequence of elements represents. The relationship between the values so formed and Morse characters is given in table 3.Occasionally the machine produces a number which is not associated with any of the defined Morse characters. Clearly, either the sender has made a mistake, or one of the elements has been incorrectly assigned. In this case, the machine could print some special symbol as an indication that a character is unassignable; but this symbol does not convey all the information that the machine has about the garbled character. It is far more desirable to produce the best possible transcription under the circumstances together with an indication that it has been necessary to perform a special process to obtain it.13 An incorrect assignment of a word or an end-of-line space cannot cause an incorrect character assignment. An incorrect assignment of a dot or a dash, as can be seen from the keying-time distributions, is very improbable and, since it usually produces a possible character, would not normally be detected. Interpretation of an intra-character space as an inter-character space although more probable, is equally difficult to detect. But when an inter-character space is transcribed as an intra-character space (i.e., when two characters are run together), the resulting value usually does not correspond to a possible character. Thus, if the table look-up indicates that a character has an incorrectly assigned element, it seems most probable than an inter-character has been transcribed as an intra-character space. Since experience has shown that the “intra-character” space that has the longest duration should be the inter-character space, the computer is programmed to make that change. Since every character can ultimately be resolved into a series of dots and dashes (that is, e's and t's), this process must eventually result in valid characters.14 With some extremely poor senders, as many as 10 percent of the characters may be resolved by this procedure.When using a general purpose digital computer to translate manual Morse, there is a continuing temptation to use the information conveyed by the context of the message to improve the quality of the transcription. Comparative studies of human and machine copies of the same signal clearly reveal that people do use this information—even to the extent of subconsciously correcting the sender's spelling errors. Despite the obvious advantages, we have not included context analysis in our programs because it would complicate considerably the special-purpose device the computer program is simulating. Fortunately, virtually all the information conveyed by context is preserved in the transcribed text, so that context analysis can be performed independently of transcription.Now that we have considered the details involved in converting radio-telegraph signals into printed copy, let us consider the way that they are combined into a single useful program. Between transmissions the computer is sampling the key and measuring the amount of time that has elapsed since the end of the last message. As the first element starts, the computer begins to measure its duration and prints the idle time notation. When the number of durations sufficient to insure the accuracy of the statistical process have been received, the computer calculates the discrimination points.If the “goodness-of-separation” is greater than a previously assigned minimum, the machine assigns elements to the first durations of the message, converts the elements into character values, looks them up in the table, corrects any inter-character mistakes and begins printing the message. Of course, while this analysis is going on, the computer is continuing to sample the key periodically and to measure the durations of the new elements. Since the typewriter can type out information faster than the Morse operator can send it, it will eventually reduce the length of the list of durations to a number that is too small to insure the accuracy of the statistical process. At this point the typing is delayed until additional elements are read into the machine. The discrimination calculation is based only on those elements that have been received but not transcribed, so that the dividing lines will follow any slow changes in sending speed. From this point on, unless something unusual happens, the computer continues to print the letters at approximately the same rate at which they are arriving (after a slight delay). Of course, if the operator suddenly shifts his rate of sending, the “goodness-of-separation” value goes down. The machine waits until it finds the point at which the rate was shifted and then treats the sequence of elements as if it were sent as two messages. Eventually the operator will stop sending. The resulting very long key-open duration indicates that the message has ended, so the machine can use the discrimination points based on the last set of durations to print the remaining characters, even though the list is reduced below the minimum length. When the final character of the message has been printed, the computer reassumes the state between messages.Thus, we see that the computer is able to receive a message in any language, print it, automatically adjust to slow or rapid changes in the sending rate, indicate the points in the text where the transcription technique was so poor that it was necessary to resort to fractionation, and even keep track of the amount of idle time between messages. There are a number of minor “fringe” benefits to be obtained from the program which hardly need to be mentioned; for example, the computer can indicate the number of words per minute at which the message was sent or the overall goodness of assignment of the characters in the message.Testing a manual Morse transcription procedure is complicated by the fact that one cannot obtain an exact copy of the test message; for no human sender can produce precisely the message given to him15 and no receiver can copy a signal without making mistakes. It is virtually impossible therefore to make an absolute statement about the accuracy of a manual Morse transcription device. One can, however, compare the output of the machine with copy made by a man. In one sets of tests, for example, seven operators each sent two messages which were recorded and a “standard” text was established by replaying them for our best human receiver, at normal and reduced rates, until he was unable to improve his copy by further listening. This type of comparison is given in table 4. In these 14 messages the computer copied between 0 and 6 percent of the characters “incorrectly.” On the whole, the machine copies are 96.5 percent “correct.”On the other hand, this procedure gives no information about the relative merits of the human and the machine under realistic operating conditions. We decided, therefore, to include tests in which each person has one opportunity to monitor each message.16 In these tests, two groups of five members each sent four context-free messages17 a piece—forty in all—which were recorded on magnetic tape. Each sender copied all the signals from his group, including his own, and his copy was compared with the machine's. Whenever the two were not in agreement, if either had copied the original text the other was charged with an error; if neither copied it the difference was attributed to the sender and no errors were charged. The four messages sent by each operator were then considered to be a single long message and the percent characters copied incorrectly by man and machine was calculated. The percentage of incorrectly copied characters in these tests is summarized in tables 5 and 6. The error percentages of the machine are italicized and appear under the human percentages for those messages with which they were compared. Although each of the operators used in these comparisons is considered competent, the difference in their ability both to send and receive is apparent. While the machine does not copy as well as some of the operators (K, L, and M), it does better than the others (H, I, J, N, O, P, and Q). Furthermore, with the exception of sender18 L, there is no great disparity between even the best human copy and the machine copy of any message. The relative merits of the two are more easily seen in table 7, which classifies the 50 man-machine comparisons in these two groups according to the errors made by man and machine.Entries below and to the left of the main diagonal (boldface type) represent those messages in which the machine produced better copy than the man (24 messages or 48 percent). Entries above and to the right represent messages copied better by the man (10 messages or 20 percent). The remainder were copied equally well by each (16 messages or 32 percent). These tests indicate that the machine does about as well as a man in copying hand-sent Morse code.





I must begin by admitting, as is scarcely necessary, that. I am at least several months away from being able to feed a new piece of French text into a computer and have an English translation come out at the other end. In trying out my basic idea, with verbally expressed rules on filing cards, I found it was possible to arrive in about 110 hours of work at a system that would translate 200 consecutive sentences from a French chemical journal into passable English. This was during last December and January. It was so encouraging to me that it seemed reasonable to try to mechanize the system immediately and use a computer to speed up further research on the linguistic side of the problem, rather than to perfect the linguistic system by hand, so to speak, and then mechanize it. The score still remains at 220 sentences. Since February, the computer programming needed to handle the essentially linguistic part of the system has been completed, and it is now in operation on ILLIAC. It does not look French words up in a mechanical dictionary. This seems to me to be an operation whose mechanization can legitimately be left until later. Quite closely similar problems must have been solved already for many other applications of computers. At any rate, I have to convert a French sentence by hand into a series of what would be the entries for the words in my mechanical dictionary. At the other end of the process, the computer produces a translation consisting of numbers that have to be looked up in a one-for-one table of English words. This, again, seems a legitimate and indeed trivial simplification in the development stage.My choice of French as the language to work on was obviously not dictated by a consideration of the market. The obvious choice is Russian; if I knew Russian, I too would have chosen it. However, my original intention was to demonstrate that a certain method of attack would yield results with surprising speed, and the method was one which second applicable to most pairs of languages.In common with most of those who have worked on mechanical translation, I have assumed that a set of rules could be devised by which most texts on a given subject in a given language could be transformed into texts in another language that were recognizably translations. One must also assume that the set of rules is not too large and complicated for human investigators to complete, or for a computer to apply. There are then two large questions; how to devise the rules, and how to enable the computer to apply them. In the beginning of my work I sat down to make up some rules before I had any clear idea of what sort of rules I would want; but in describing the approach now it is more convenient to begin by saying what sort of rules are involved.To begin with, it is assumed that the French words in a sentence have been looked up in a special dictionary, and that what I shall refer to as “items” have been brought out of the dictionary, one for each French word. Each item begins with a fixed number of digits that indicate the grammatical characteristics of the French word, in fairly conventional terms. Then comes a number indicating what the French word was, and then the English equivalent that will come out as part of the translation, unless it is changed in the course of working out the sentence. After that there may follow one or more instructions, then one or more constants that are used in carrying out instructions, and finally one or more diacritics whose presence or absence may be a necessary condition for executing various instructions.The sentence, in the form in which the computer gets it from the hypothetical dictionary search routine, contains instructions that will have to be carried out before translation is produced. These instructions correspond to the rules invented during the non-mechanical consideration of the problems. Some of the rules, however, are so general in their application that it is inefficient to plant them in individual dictionary items. For instance, there has to be a rule providing that adjectives, which mostly follow the nouns they modify in French, should be moved around to the English position, before the noun. This rule would apparently have to be included in the dictionary item for almost every French adjective. So it is more practical to have a number of general instructions, 12 of them at the moment, put at the beginning of each new sentence before instructions begin to be carried out.After each instruction is carried out, it is discarded; and when there are no instructions left, the English words remaining in the items of the sentence are printed out; they compose, one hopes, a translation of the original French sentence. An ordinary instruction is done once and then thrown away, but a general instruction has to be done once for each item in the sentence, it is treated as though it were found at the right moment in each item in turn.The order in which instructions are to be carried out has to be controlled very carefully, to avoid conflict. The most important reason for this is the need to make all the decisions that depend on French word order before the items are shuffled around into English word order. The sequence of execution is fixed by beginning each instruction with a priority number, within a somewhat arbitrary range of one to 126. Whenever the computer has to decide which instruction to follow next, it looks for the one with the lowest priority number.In case of a tie, the instruction occurring earlier in the sentence is done first. Within each item the instructions are listed in the order in which they are to be done, so that actually the computer only has to look at the first of the remaining instructions in each item, and at the first remaining one in the series of general instructions, to decide which one to take up next.Besides its priority number, seven binary digits in the present system, an instruction contains the name of an operation, nine digits, and a parameter, three digits. The parameter has a rather minor function, enabling reference to be made to comparison constants included in the same item with the instruction. To carry out the instruction, the name is used to refer to the operation, a series of consecutive computer words carried permanently in magnetic drum storage. The operation may begin with a series of one or more comparison constants which can be referred to by number, and after any such constants follows a series of words that might be called sub-instructions. These are converted by an interpretive routine into a program of sub-operations, which are computer routines permanently stored in the Williams memory. Such a program contains orders for making changes in the sentence when appropriate. It may contain logical decisions and loops of all kinds, but unlike a computer program it does not have much ability to alter itself.There are 48 basic sub-operations, and all the operations I have concocted or imagined so far can be conveniently programmed in terms of them by reference to simple tables, without having to do any computer programming. The various sub-operations can make the item containing the current instruction the “current” item, to be looked at or altered or moved, or can make the item before or after the presently current one into the new current one.They can ask whether the current item has the grammatical characteristics, or the English word, or the French word, indicated by one of the comparison constants; or whether it contains a given instruction or diacritic. Or they can look forward or backward in the sentence until they find an item that satisfies some such condition, making it the new current item if it is found, and returning a negative answer if none is found. Other sub-operations can change the grammatical characteristics or the English of an item, or insert an instruction or a diacritic into it, or delete or insert a whole item, or change the order of items in the sentence.The whole process of translation by this method can be described as a double interpretive routine. Raw material is got from the dictionary and then subjected to the first interpretive routine, which causes instructions to be performed in the correct order, and a series of English words to be printed out after the last instruction is performed. Each instruction, in turn, takes an operation from storage and interprets it as a program of sub-operations. Now the sub-operations, and the interpretive routines, are so general as to be almost independent of what languages are concerned in the translation. So the method has the possible advantage that one master program, with only slight changes, might be used for several different sorts of translation. A different dictionary would have to be used in each case, of course, and a different set of instructions would have to be stored on the drum. But, as the basic program has taken several months of part time work to get ready, it is encouraging to think that this work may not have to be repeated if I should get mechanical translation of chemical French into actual operation, and then turn to some language more in demand at the present time.It remains to describe the method by which the rules, and the dictionary items for them to work on, have been arrived at. I opened a recent French chemical journal at random, went to the beginning of the article, and set out to formulate verbal rules that would translate the first sentence. It had about forty words, and it took ten hours to work out the rules. Turning to the second sentence, I added new items to the dictionary invented new rules, and modified existing rules until the system would handle both sentences. The third sentence was attacked in the same way, and so on up to 220.The time required to add each new sentence to the repertory tapered off rapidly, and by the two hundredth sentence it averaged about fifteen minutes per sentence. And this time was mostly consumed in shuffling cards in and out of the file, and writing cards for new items of vocabulary, without much thought necessary. To my own satisfaction at least, this showed that by the time two hundred sentences of running text have been processed in this way, in French at least, most of the major difficulties have been met and solved moderately well. Further progress, once the mechanical version of the system is working smoothly, should be very rapid. Fresh text can be fed into the machine in batches of say ten sentences at a time. If a mechanical dictionary system is already working, then something purporting to be a translation will be produced for each sentence, and the existence of new French words that need to have dictionary items written will be signalled in the translations. If dictionary lookup is still a hand operation, then the new dictionary items have to be written, by analogy with items for comparable words, before the fresh text is fed in. In either case some of the sentences will be translated acceptably, and others will not. The unacceptable translations will show fairly clearly where the existing system goes wrong.Some operations will have to be modified, and new instructions will have to be added. These can be tested without much trouble on a selection of the earlier sentences, to make sure that the new rules are not conflicting with old ones, and that the old ones have not been spoiled in modification.Ultimately, a point should be reached at which 90 percent or 95 percent of the sentences in each new batch of text are adequately translated, with no prior additions to the dictionary needed, and it might then be claimed that a useful, though no doubt uneconomic, system for machine translation of chemical French had been achieved. The objection may be made that the day of 90 percent effectiveness will be a long time away if it is approached simply by going ahead from one sentence of text to the next, and accumulating the system accordingly. I do not think this objection is valid. In the first place, every little rule that is added to the system as I am trying to build it up would have to be discovered and formulated in some form or other in any method of research. The job might be done in large batches instead of bit by bit, yet the size of the job could not be much different. In the second place, the sentence by sentence approach is the only one I can see in which a computer can be made to do most of the dirty work, leaving the investigator to develop the system according to the necessities indicated by the computer.The opposite approach to the one I am following involves the attempt to find a few very powerful rules, rather than a lot of rules with limited application. The attempt is often to make rules that will recognize large syntactic patterns and units in the input language, produce the corresponding patterns and units in the output language, and then, so to speak, fill in the blanks with the correct words of the output language. This may turn out to be the best method for converting, say, Japanese sentences into English ones, since the patterns are so different. But if one takes a sentence in a European language and translates it into English, one generally sees that a few special idiom rules, and some well-defined changes of word order, would convert the word-for-word translation into an acceptable translation. And the easiest way to provide for these is to have a rule for translating each idiom attached to one of the words in the idiom, and to make up specific rules for the changes of word order. No one has even hinted, so far, at a description of French linguistic structure that does not involve inter-locking structures within structures. So I doubt that a “technological breakthrough” is likely by which the rules could be made not only few in number and powerful, but also simple enough to represent much of a net gain.The method of a few powerful rules makes research much more difficult. In the first place, rules of this kind will take much time and thought to devise, and at least the rough drafts of them have to be made up before the system can be tried out on any text. While if one progresses from one sentence to the next, making up reasonable but ad hoc rules as one goes along, one always has a system that will actually handle all the text that has been processed so far, and will point out its own specific inadequacies as more text is processed.In the second place, it is hard to tinker with a system that consists mainly of powerful rules. A powerful rule has to be so involved that any modification may well mean rewriting it from scratch, and this may mean fresh computer programming. A system of many small rules, however, can be modified at one point, by changing or inventing one rule, without necessarily disturbing the rest. And as new operations can be composed of standard sub-operations, no new programming is needed.Another method of research which I have not been tempted to use involves making a number of studies of general problems, and then combining the results. One may study prepositions in general, and plausibly solve the problems connected with translating them. Separately, one may solve the problems of pronouns, and of verb tenses and moods, and so on. But before these solutions can be of actual use, they have to be combined into one over-all system, and it would be extremely difficult to know whether the tactics used on the pronouns, say, might not be disturbing the evidence on which the treatment of the past participle was to be based. All this is avoided if the system is developed as a whole from the very beginning, all its parts being made to fit each other by controlling the order in which instructions are carried out.A further disadvantage of making general studies in the temptation to study too much and try to solve too many problems. To prevent a waste of effort, supposing French chemical text is being considered, a big sample of that sort of text has to be studied to see how many of the resources of the French language in general are used often enough to bother with. Here again, an investigator who builds up a total system by working through continuous text is prevented from wasting time on too many of the complexities which he can imagine, but which are in practice very uncommon.There are three specific points I would like to discuss next, which are brought up almost too often in discussions of machine translation. My excuse is that I think it can be shown that the problems are not nearly as difficult as they are generally made out to be. First, idioms. For example, it was some time before I realized that eau oxygenée was not to be literally translated oxygenated water, but meant hydrogen peroxide in English. A simple rule now provides for this; to make sure it is brought into play as rarely as possible, it is planted in the item for the least common word in the idiom. Thus the item for oxygenée contains an instruction which is eventually interpreted: “Look at the item next preceding. Does it contain the French word eau? If so, change its English word to hydrogen peroxide, and delete this item (the one for oxygenée).” Most idioms can be provided for by equally straightforward instructions, and in fact they seem to be the least difficult of the problems of mechanical translation.The second question is that of how to store vocabulary in a language with many inflections; whether there should be one item in the dictionary for each form of a verb, for example, or whether there should be one item for the stem of each regular verb, with the affixes listed separately. The assumption too generally made is that having a separate item for every inflected form will make the glossary too large to be practical. This may be true, but in the first place nobody knows, or at any rate nobody has yet stated, how many binary digits will compose the average dictionary item in his system. For my own system, I would estimate about 120 digits, plus five for each letter of the English word or words contained in the item; say 160 bits on an average. In any case, where no such estimate has been made, it is fruitless to worry about the size of one's dictionary. In the second place, it will probably be a couple of years before mechanical translation gets into any sort of commercial production, and by that time, we are presumably confident, large advances will be made in the techniques of data storage. It does not seem ridiculous to hope that storing and referring to an immensely large dictionary will be quite practicable. In the meantime, the most sensible strategy would seem to be to develop a system using separable endings, but in such a way that very few changes need to be made if a dictionary containing each individual form of an inflected word turns out to be practical. In my own work, I think I have achieved a method which is compatible in this way, by listing the endings that each stem can take immediately after that stem in the dictionary, each one followed by the indications of its fraction of the total meaning. This might seem wasteful of space, compared with listing every suffix just once in a master-list of suffixes.But it has the advantage of compatibility with a system of completely multiple storage, and it greatly simplifies the use of the dictionary in certain other ways.The third often-raised point is how to refer to a dictionary rapidly, if it has to be carried on a one-dimensional medium like magnetic tape or punched tape. This, again, is not as awkward as it may seem. On the ILLIAC, for instance, it looks as though 200,000 binary digits of drum storage will be available for use in dictionary lookup; this amount of space will probably hold the items for six to eight hundred different French words comfortably. A continuous passage of up to 1000 words of text may be handled with this number of different items. So if a passage of that length is first read, the words can be, in effect, sorted into alphabetical order; and one reading of the dictionary tape or tapes from end to end can be used for extracting all the necessary items. For the ILLIAC, the sheer bulk of the paper tape may well make this impractical, but if the entire dictionary could be got onto a single tape which the ILLIAC could read through as a single operation, and if the delay for rewinding the dictionary tape after each use could be eliminated, the lookup time might be of the order of five seconds per item. This is not an impressive speed, but the ILLIAC was designed with quite different uses in mind. The same system on an advanced IBM computer could probably consult the dictionary at the rate of five or ten items per second.This method can perhaps be pushed a stage further. If there were a hundred thousand items in the dictionary, and enough drum storage for ten thousand French words, without their items, a stretch of about eight thousand words might be read, containing say five thousand different words. A temporary dictionary tape of five thousand items might then be selectively copied from the complete dictionary tape; and this small tape could be consulted five times in translating the eight-thousand-word piece of text. The whole process would be rather faster than consulting the master dictionary tape five times in the first place. However, this process looks so involved that I keep a sneaking hope that some more accessible storage medium than tape will be available soon for very large-scale storage1.Actually, the size of the dictionary needed for translating chemical literature from French into English will not be nearly so large as at first I supposed. The great bulk of the inorganic terminology does not have to be entered in the dictionary at all. Suppose the dictionary does contain entries for the singular and plural forms of sulfurique. Then it is not necessary to provide entries for nitrique. One simply provides a rule that when a word with the suffix -ique or -iques is not found in the dictionary, it is to be given an item containing an English word made by changing the ending to -ic, and otherwise identical with the dictionary item for sulfurique(s). So large families of nouns and adjectives can be disposed of by listing only one member of each in the dictionary. Organic terminology, though it contains a much vaster number of words, should be even easier to provide for. The names of organic compounds are almost all recognizable as such by their suffixes.So instead of listing them in the dictionary, one provides a rule that any French word not found in the dictionary but possessing an organic ending is to be given an item consisting of an English word spelt just like the French one—let the accents fall where they may—with the instructions and diacritics appropriate to all names of organic compounds, whatever they turn out to be. Occasionally, of course, an odd but still perfectly recognizable spelling would appear in the translation.The same plan could probably be adapted for translation among most European languages. It would not be available, of course, for translation between European and Asiatic languages. Even Russian, from the little I know of it, would give the machine translator a hard time with its chemical vocabulary, though I think some of the dictionary storage could be saved by systematization of this kind. At any rate, I feel lucky to have picked French to try my hand on for the present.The solution for a Dirichlet problem on a given plane domain and with given boundary values is usually approximated in numerical computation by its discrete analog defined and determined on an approximating set of net points. It can be proved that the approximation thus obtained converges to the exact solution, when the net becomes denser indefinitely, independently of the domain and the boundary values subject to rather weak conditions. Nevertheless, the irregularity of the boundary curve and of the boundary values affects strongly the convergence rate. For instance, if both are analytic and if a proper boundary interpolation scheme is used, then the simplest net analog leads to an error which decreases asymptotically at least proportional to the square of the mesh constant h, as proved by Gerschgorin [2] and Collatz [1]: &dgr;h = O(h2). In introducing the convergence exponent (1) &kgr; = limh→0 inf maxPhCDh log | &dgr;h(Ph) |/log h, Dh being the approximating net domain with the mesh constant h and Ph its node, this result can be expressed also by the inequality &kgr; ≧ 2.On the other hand, the author of this paper has shown in [4] that if the boundary of the domain is piecewise analytic, i.e., composed of a finite number of analytic arcs, and the boundary values are analytic on each closed arc, but jump discontinuities at the join of two arcs are permitted, then the convergence exponent depends on the angles at those corners where two adjacent arcs are connected. (Of course, the analytic character of the prescribed boundary values may alter at a finite number of interior points of an analytic boundary arc; it is sufficient to partition this arc at these points into different arcs with angles &pgr; at the related corners.) Actually, if &agr;&pgr; is the largest of these angles, then, by using, for instance, the extrapolation scheme proposed by Collatz in [1] in order to determine the boundary values for the approximating net domain, one obtains the inequality (2) &kgr; ≧ min (2, 1/&agr;).In the paper [4] it has been proved, moreover, that if the boundary values are not only piecewise analytic but also continuous, then for &agr; < 1 the relation &dgr;h = O (h2) holds. This implies that (3) &kgr; ≧ 2, (&agr; < 1). However, no information is given in the case where &agr; ≧ 1, i.e., if either the domain has corners where two different analytic arcs are connected to form an angle greater than or equal to &pgr;, or at some interior point of an analytic boundary are the analytic character of the prescribed boundary values changes. The result (2) is valid, of course; however, since (3) is an essentially better result than (2) for 1/2 < &agr; < 1, one could expect that some improvement is possible also in the cases &agr; ≧ 1, if the prescribed values are continuos.The results obtained theoretically in the paper [4] may be summarized as follows. If the domain has a piecewise analytic boundary and the boundary values are also piecewise analytic, both in the sense described above, then (2) &kgr; ≧ min (2, 1/&agr;), (&agr; > 0), where the boundary values may be discontinuous. However, if the boundary values are continuous, then (3) &kgr; ≧ 2, (0 < &agr; < 1).The purpose of the present paper is to develop further the considerations in [4] and to demonstrate those theoretical results by some experimental ones. These seem to indicate that in the two inequalities above, at least in some cases, the equality sign holds, and, moreover, that the latter theoretical rule is just a part of a more general hypothetical rule, namely (4) &kgr; ≧ min (2, 2/&agr;), (&agr; > 0).Table I presents the results of some experiments with discontinuous but piecewise analytic boundary values. In each case the domain D is a polygon such that its boundary B contains the boundary nodes set of the approximating square net domain Dh. Moreover, the boundary values are assumed to vanish, except at those corner points with the greatest angle &agr;&pgr;, and, hence, the asymptotic solution for h → 0 is known to be identically zero. It is true that the boundary values thus defined do not fall strictly under the definition of piecewise analytic boundary functions, given above, but they can be interpreted as the difference of two such admissible functions. If uh and uh are the discrete solutions of the related problems, then it is obvious that uh is their difference, and its rate of convergence, as h → 0, is at least the smaller of those for uh and uh. Accordingly, the rate of convergence obtained by using such degenerate boundary functions is now an upper bound for the rate originally investigated.Now, if the truncation error were exactly &dgr;h = uh - u = Ch&kgr;, with some C independent of h, then for two different values h with the ratio 2 (for instance h = 2 and h = 1) the corresponding approximations u2 and u1 would have the ratio u2:u1 = 2&kgr;, since u is identically zero. Accordingly, the expression (5) &kgr; = (log u2:u1)/log 2 gives an approximation for the asymptotic convergence exponent.In table I, &agr; is the magnitude of the greatest angle divided by &pgr; ; n is the number of the interior points at which the values u1 and u2 are determined and then used to compute &kgr; from (5); &kgr;m is the arithmetic mean of these n values and s the related quartile deviation; finally, &kgr; in the last column is the theoretical lower bound (6) &kgr = min (2, 1/&agr;).In the investigation of problems with continuous and piecewise analytic boundary values the domains are similar to those previously described. Now, however, the limit solution for h → 0 is not known, and, therefore, the approximate values &kgr; are based on three consecutive approximations to the solution at the same point, with three different values of h which are chosen so that they are in the proportion 4:2:1. If these are denoted by u4, u2, and u1, and if the truncation error were &dgr;h = uh - u = Ch&kgr;, with C independent of h, then &kgr; could be computed from (7) &kgr; = (log u4 - u2/u2 - u1)/log 2. This is now the formula from which the approximate values &kgr; are computed at n (quite uniformly distributed) interior points.&kgr;m is the arithmetic mean and s the quartile deviation. In addition to these values, table II gives also &kgr;, which is defined by (8) &kgr; = min (2, 2/&agr;).The remarkably good coincidence between the corresponding values &kgr;m and &kgr; is also indicated in figure 1; it contains, in addition to the experimental values &kgr;m, also the graphs of the analytic expressions for &kgr; and &kgr;, from (6) and (8), respectively, with respect to 1/&agr;. Those values which are theoretically proved to be lower bounds for the convergence exponent are represented as a solid line and those values in the hypothetical case as a broken one.In trying to prove theoretically the hypothesis that, for continuous boundary values, the relation &kgr; ≧ &kgr; is true, the decisive difficulty lies in finding proper estimates for the variation of the discrete solution function uh in the vicinity of a corner. As long as the angle of this corner is less than &pgr;, i.e., &agr; < 1, then the variation of uh is of the order O(r), as shown in [4], r being the distance from the corner point. This agrees with the variation O(r) of the solution function u of Laplace's differential equation. Now, for &agr; > 1, the variation of u can be proved to be of the order O(r1/&agr;).1 In order to obtain some experimental knowledge about the variation of uh, the following numerical experiments were made.First, suppose that the corner point P(0) is a node for the nets considered. Take a direction from the corner such that two points Ph(1) and Ph(2) of the net domain are located on the line in this direction. Moreover, suppose that the Ph(1) is the closest node on this line and Ph(2) the second closet, so that their distances have the ratio 1:2. Let uh(1) and uh(2) be the values of uh at Ph(1) and Ph(2), respectively, and u(0) the prescribed boundary value at P(0). Then, for indefinitely decreasing h, the expression (9) &lgr;h = (log uh(2) - u(0)/uh(1) - u(0))/log 2 will converge to the limit &lgr;0, provided the asymptotic behavior of uh in the vicinity of P(0) is characterized by an expression (10) uh ∼ u(0) + Cr&lgr;0, where C is independent of h but dependent on the direction of approach.Of course, the converse is not in general true. The statement that the expressions (9) tend to a definite limit does not justify the conclusion that uh behaves asymptotically like (10). Nevertheless it is interesting to find the values of (9) for various directions and for decreasing mesh constants h. Table III again gives in its first column the value &agr; of the domain, i.e., the greatest angle divided by &pgr;, in the second column the number of those directions in which the expressions &lgr;h were determined. The three following columns contain the mean values of these &lgr;h's determined for three different mesh constants which are proportional to 4:2:1. Finally, the last column gives the expected lower bound for &lgr;0: &lgr; = min (1, 1/&agr;).These results seem to indicate that if there exists a limit &lgr;0 for indefinitely decreasing h, then this limit cannot be less than &lgr;. (The values &lgr;4, &lgr;2, and &lgr;1 for &agr; = 1, seemingly not supporting this conjecture, are not yet near enough to their limit value, which for &agr; = 1 - &egr; has been proved to be greater than or equal to 1.) Furthermore, this statement makes the estimate uh - u(0) = O(r&lgr;) plausible, which has been proved for &agr; < 1, as has been already mentioned. If it is true also for &agr; > 1, i.e., if uh - u(0) = O(r1/&agr;), (&agr; > 1) then it follows, from the similar estimate for the solution of the differential equation, that (11) &dgr;h = O(r1/&agr;).But now, this estimate can be used to find an estimate for &dgr;h in terms of h. In reference [4], it is shown that &dgr;h = O(h(2)) follows from &agr; < 1 and &dgr;h = O(r). Without going into details it may be remarked that a similar reasoning leads to the assertion that &agr; > 1 and (11) imply the estimate &dgr;h = O(h(1/&agr;)+&kgr;1), where &kgr;1 is any number less than 1/&agr;.Accordingly, this estimate yields &dgr;h = O(h(2/&agr;)-&egr;) where &egr; is any positive quantity: that is, the convergence exponent defined in (1) is at least 2/&agr;.Briefly, the results are as follows:Let D be a plane domain those boundary B is composed of a finite number of regular analytic arcs, &agr;&pgr; being the greatest angle at the corner Let u be the solution of a Dirichlet problem with prescribed, piecewise analytic boundary by using the interpolation scheme from [1] in order to assign the values at the boundary nodes of Dh. Let uh be the solution of the corresponding discrete problem and &dgr;h = uh - u the truncation error. Specifically, this truncation error for an indefinitely decreasing mesh constant h is examined by introducing the convergence exponent &kgr; = limh→0 inf maxPh(Dh log &dgr;h (Ph)/logh.For boundary values which may be discontinuous, the rate of convergence is determined by an exponent, whose lower bound has been proved in [4] to be &kgr; ≧ min (2, 1/&agr;).For continuous boundary values, the hypothesis &kgr; &gE min (2, 2/&agr;) s introduced. It has been shown in [4] that it holds for &agr; < 1; for various values &kgr; &gE 1, experimental results seem to be in good agreement with this law. Moreover, it has been remarked that another hypothetical law,uhAlthough numerous writers have stated that the class of single-step (“Runge-Kutta”—like) methods of numerical integration of ordinary differential equations are stable under calculational or round-off error, no one has given formal equations for the bounds on the propagated error to indicate this stability. Rutishauser [1] justifies the stability by noting that there is only one solution to the approximating difference equation, and Hildebrand [2] calculates a propagated error bound for the simplest (Euler) case. However, the latter bound does not indicate the stability for even that case.It is the purpose of this paper to investigate this “stability” of the Kutta fourth order procedure for integration of the ordinary differential equation dy/dx = ƒ(x, y), (1) where ƒ(x, y) possesses a continuous first-order partial derivative with respect to y throughout a region D in which the integration is to take place. (By alteration of the proof below, this condition can be replaced by a Lipschitz condition.) Since the Kutta process is the most complicated of such single-step procedures, it should be apparent that similar error bounds can be derived for the various other single-step methods of same or lower order (and in particular the Gill variant method, probably most often used in machine integration because of the storage savings). It is plausible that such error bounds can also be extended to the stable (extrapolation) multi-step methods, such as the Adams method, and to systems of ordinary differential equations.If the variational equation d&eegr;/dx = ∂ƒ(x, y)/∂y &eegr; (2) for the above ordinary differential equation has ƒv(x, y) < 0 throughout a region D the ordinary differential equation is termed stable in D, and for small enough variations in the initial conditions the absolute value of the propagated error decreases with growing x.Todd [3], Milne [4], Rutishauser [1], and others have shown that numerous multi-step numerical integration techniques are unstable in that even when the differential equation is stable, the difference equation will introduce spurious solutions for any step-size h. For the Kutta fourth order process, as seen below, this is not the case; for the stable differential equation the propagated error in the difference approximation remains bounded for small enough (but not too small!) step-size h; and for a given value of x, bounds on the propagated error decrease to a minimum given as a function of the round-off error as the step-size is decreased. Similar statements can be proved (but are not proved here) for other single-step processes. For no round-off (an “infinite word-length machine”) the process converges as h goes to zero.In addition, an algorithm for determining the step-size as a function of the partial derivative is given below so as to keep the propagated error within a given bound.The classical Kutta procedure [5] gives the value of y at the (i + 1)th step in terms of the value at the ith step and the step-size h as follows: yi+1 = yi + h/6(k1 + 2k2 + 2k3 + k4) + O(h5) k1 = ƒ(xi, yi) k2 = ƒ(xi + h/2, yi + k1h/2) k3 = ƒ(xi + h/2, yi + k2h/2) k4 = ƒ(xi + h, yi + k3h) (3) When the O(h5) term is neglected, the value of yi+1, here called yti+1, is only an approximation.An error bound for the truncation error at each step is given by Bieberbach [6, 7] and Lotkin [8]. This guarantees that for h small enough the truncation error at a single step, starting at yi, will be bounded by |yi+1 - yti+1| ≦ Ci+1h5, (4) where Ci+1 ≧ 0 is a function only of i, the function ƒ(x, y), and its partial derivatives of the first three orders; and yti is the true solution to the equations (3) truncated so that the O(h5) term does not appear. If the function and its derivaties of the first three orders exist and are bounded throughout a region, then all Ci would be bounded in the region.Suppose there exists an error &egr;1 in yi at the ith step, which might be due either to previous truncation or round-off error.Then the calculated value y*i+1 (as opposed to the true value if &egr;i were zero) at the next step is given, after several applications of the mean value theorem, by y*i+1 = yi + &egr;i + h/6{k1 + ∂ƒ/∂y &egr;i + 2k2 + 2[∂ƒ/∂y + (∂ƒ/∂y)2 h/2]&egr;i + 2k3 + 2[∂ƒ/∂y(1 + ∂ƒ/∂y h/2(1 + ∂ƒ/∂y h/2))]&egr;i + k4 + [∂ƒ/∂y(1 + ∂ƒ/∂y h(1 + ∂ƒ/∂y h/2(1 + ∂ƒ/∂y h/2)))]&egr;i} (5) where the various partial derivatives are evaluated within a rectangle given by xi ≦ x ≦ xi + h; |yi - y| ≦ Qh + | &egr;i | where Q is given below.Consider the true solution yit to the difference equation. During its evaluation, in order that all values of ƒ(x, y) used in its calculation lie within a region D, the true solution should not approach the y-boundaries of D closer than | yit; - y| > Qh, where if Q is chosen as Q ≧ maxx,y&egr;D | ƒ(x, y) | ≧ max (| k1/2 |, | k2/2 |, | k3 |) then certainly only values within D will be involved in the calculations.The same argument holds for the difference equation containing error &egr;i at any step. If such a solution does not approach within closer than Qh + | &egr;i | to the y-boundary of D (such a region will be called D* below), then the true solution will not approach closer than Qh to the boundary. In the latter case the rectangle within which the partial derivatives are to be evaluated will always be within the given region D.This gives for the propagated error at the (i + 1)th step (if yi+1 is the value at step i + 1 given no error at step i, and y*i+1 is the value at step i + 1 with error &egr;i present): &eegr;i+1 = y*i+1 - yi+1 = &egr;i[1 + h/6(∂ƒ/∂y + 2∂ƒ/∂y + 2∂ƒ/∂y + ∂ƒ/∂y) + h2/6(∂ƒ/∂y ∂ƒ/∂y + ∂ƒ/∂y ∂ƒ/∂y + ∂ƒ/∂y ∂ƒ/∂y) + h3/12(∂ƒ/∂y ∂ƒ/∂y ∂ƒ/∂y + ∂ƒ/∂y ∂ƒ/∂y ∂ƒ/∂y) + h4/24(∂ƒ/∂y ∂fnof;/∂y ∂ƒ/∂y ∂ƒ/∂y)] (6) where the partial derivaties are written out to indicate that each is evaluated at what may be a different point in the rectangle. One can now prove the following:THEOREM: If ∂ƒ/∂y is continuous, negative, and bounded from above and below throughout a region D in the(x, y) plane, -M2 < ∂ƒ/∂y < -M1 < 0, where M2 > M1 < 0, then for a maximum error (truncation, or round-off, or both) E in absolute value at each step the Kutta fourth-order numerical integration procedure has total error at the i-th step, i arbitrary, in the region D*, of | &egr;i | ≦ 2E/hM1 (7) where the step-size h is taken to be h < min (M1/M22, 4M13/M24. (Obviously if D extends to infinity, the boundaries of D and D* will coincide at infinity. Better (but more complex) bounds could certainly be obtained on Q and therefore D*.)PROOF: For h as given, the multiplier of | &egr;i | within the absolute value signs on the right-hand side of (8) below is positive and the inequality does indeed hold, by applying the bounds on ƒv of the theorem to (6) above.In fact, for h as given &eegr;i+1 | = | y*i+1 - yi+1 | ≦ | &egr;i | | 1 - hM1 + (hM2)2/2! - (hM1)3/3! + (hM2)4/4!| ≦ | &egr;i| |1 - hM1/2|. (8) The total error at the (i + 1)th step is, in absolute value, | &egr;i+1| = |&rgr;i+1| + |&eegr;i+1| ≦ E + | &egr;i | (1 - hM1/2) (9) where &rgr;i+1 is the sum of the round-off and truncation error in that step and &eegr;i+1 the propagated error, 1 - hM1/2 < 1, and E > | &rgr;i | for all i.For i = 0, &egr;0 = 0, and therefore certainly | &egr;0 | ≦ 2E/hM1. (10) If for a given i, | &egr;i | ≦ 2E/hM1, then from (9) | &egr;i+1 | ≦ E + 2E/hM1 (1 - hM1/2) ≦ 2E/hM1, (11) and the proof holds by induction1 on i.Note that, if &rgr;i consists only of truncation error, | &egr;i+1 | ≦ 2Ch5/hM1, (12) where C > Ci for all i (if sufficient bounded derivatives on ƒ(x, y) are assumed), and limh→0 | &egr;i+1 | = 0. (13)On the other hand, if &rgr;i consists of round-off error &rgr;* also, bounded by | &rgr;* | for all i, then limh→0 | &egr;i+1 | ≦ 2(| &rgr;* | + Ch5)/hM1 = ∞, (14) provided | &rgr;* | is bounded from below by a positive non-zero value, which is generally true for fixed word-length computers; and the error bound is of no value in the limit. In that case, for h small enough | &eegr;i+1 | = | y*i+1 - yi+1 | ≧ | &egr;i | (1 - hM2) ≧ K | &egr;i |, (15) where now K = (1 - hM2).If, for example, &rgr;k > R > 0, for all k, one obtains | &egr;i+1 | = &rgr;i+1 + | &eegr;i+1 | ≧ &rgr;i+1 + K(&rgr;i + K(&rgr;i-1 + ··· + Ki+1&rgr;0) ···) (16) ≧ R(1 + K + K2 + ··· + Ki+1) ≧ R(i + 2)Ki+1 ∴limh→0 | &egr;i+1 | ≧ (i + 2)R (17)Therefore, in the case &rgr;k > R > 0, the error can increase without limit when (i + 1)h is held constant. Thus, as perhaps seems obvious from the start, decreasing the step-size will in this case, speaking roughly, improve the propagated error until the point at which the round-off and truncation error have the same order of magnitude. If on the other hand the round-off error can be made a function of the step-size of order hk, where k > 1, then the total error will go to zero as h → 0, or will remain bounded in the case k = 1. Users of variable wordlength digital computing machines should therefore increase their word-length as h is decreased in order to avoid this lower limit due to round-off error. Users of fixed word-length computers must use multi-precision. This result is not surprising; a specific algorithm for doing it is given below.If ƒy is allowed also to be zero or positive, but bounded throughout D, 0 ≦ |∂ƒ/∂y| < M. (18) Then the propagated error in D* at the (i + 1)th step, due to a given (round-off or truncation error) &egr;i at the ith step, is |y*i+1 - yi+1 | ≦ | &egr;i | (1 + hM + (hM)2)/2 + (hM)3)/3! + (hM)4)/4!) (19) or | &eegr;i+1 | ≦ | &egr;i | ehM, (20) where &eegr;i+1 is the propagated error and E and h are given as before; and one obtains an error bound similar to most classical ones | &egr;i+1 | ≦ E(1 + K + K2 + ··· + Ki) ≦ E Ki+1 - 1/K - 1 = E e(i+1)h M - 1/eh M - 1, (21) where K = eh M.In both the stable and unstable case, ignoring round-off, the process is obviously convergent in D as h → 0, since D* approaches D as h → 0 and | &egr;i+1 | → 0. For the case ƒy < 0 equations (7) and (14) above give an algorithm for a bound on the stepsize h; a similar analysis could be based on equation (21). Suppose the round-off error per single-precision step is bounded by | &rgr;*(1)(h) |, per n-precision step by | &rgr;*(n)(h) |. (Here n could just as well be number of digits as number of words.) Then, since | &egr;i+1 | ≦ 2(|&rgr;*(n)(h)| + Ch5)/hM1, (22) for | &egr;i+1 | required to be less than B > 0, choose h < M1/M22 such that 2 Ch5/hM1 < B/2 (23) or h4 < M1B/4C. (23a) Then for that value of h, choose the value of n such that | &rgr;*(n)(h) | < BhM1/4. (24) This will guarantee | &egr;i+1 | < B.Obviously, such an h is not necessarily the best possible. The computation of | &rgr;*(n)(h) |, while intricate, could very well be done by the computer itself, if procedures for forming formal derivatives of machine functions were included in whatever compiling techniques were used with the routine. If the calculated solution approached the boundaries of D*, the step-size h would have to be decreased, and the values of h accordingly readjusted in order to continue to apply the theorem.If h and n are picked in this fashion, the numerical machine solution can be made as close to the true solution as desired, and the process will converge. As for “stability” of the solution process under round-off, as would appear intuitive, the propagated error can be guaranteed to go to zero with the step-size h only if the round-off error per step varies with h as O(hk) with k > 1. The Kutta process is stable in the sense that for a stable differential equation, for a given h, the error will be bounded by (7), where E is given without round-off error by (12) and Bieberbach's bound (4). In the latter case, as h → 0, the truncation error goes to zero, which certainly is not true for many multi-step methods.The results of an investigation of the use of photographic techniques for high-scanning-rate digital storage are presented.The most promising technique, that of utilizing a projection system and a rotating mirror, is described in detail. In this system, the binary-digital information recorded on a photographic medium in the form of spots is projected on a rotating mirror by a device similar to a common slide projector. The image reflected from the rotating mirror sweeps past a row of stationary photoelectric transducers and causes the transducers to “read” the stored information.Experiments conducted with a simple slide projector and a rotating mirror resulted in a reading rate of 250,000 cps, with a mirror speed of 3600 rpm and a memory pattern of 110 lines per inch.A system using a memory pattern that is 4 by 4 inches and that contains 16 × 10 6 bits of information, each bit being 0.0005 by 0.002 inch, is proposed. This information would be written in parallel channels, each approximately 40 bits in width and 4 inches in length. Schemes for reading each of the channels repeatedly and for reading the channels successively are discussed. The reading rate should be greater than 1 million words per second.















As the Association for Computing Machinery enters a new phase of its existence, it seems befitting to review, briefly, the conditions in the computing field just prior to its organization and the events of the past six years of its life. Since its formation, in 1947, the Association has adhered to the originally established policy of informality. That is, meetings and discussions were encouraged and information was generally put out in mimeographed form and more formal publications were discouraged. The function of the organization was to maintain a mailing list of members paying only such dues as were necessary to cover the cost of printing or mimeographing and mailing. Such an organization served its purpose excellently, but times have changed.Prior to the formation of the Association, the automatic computing field, as such, hardly existed. Probably the first meeting of those interested in the field was held at the Massachusetts Institute of Technology in 1945. The occasion was to introduce the differential analyzer, designed by Dr. Vannevar Bush and Dr. Samuel H. Caldwell, to the public. This machine is a refinement of the original machine built by Dr. Bush in 1925. The earlier machine served as a pattern for several machines which were in operation in 1945, including those at the Aberdeen Proving Ground, the Moore School of Electrical Engineering, the General Electric Company and in Manchester, England.It is interesting to note that, at the time of this first meeting, other analog type machines were in operation. Network analyzers were employed to simulate power distribution systems and aid in their study. None of these machines employed digital representation but represented the values in analog form, such as voltage, current or angular position. Digital computation was possible only by hand operated calculators or by some business machines.Although automatic digital computation by machinery was the goal Charles Babbage strove to reach, it was not until the Hollerith rotary counter was suggested in 1890 and the International Business Machines Corporation began producing machines employing such counters for accounting purposes in the period from 1903 to 1905, that such goal was reached. The automatic multiplying punch machine was not produced until 1931.Computation by means of telephone relays was first introduced in the Bell System Complex Computer, known as Model I, in 1939. The method of employing the relays was suggested by Dr. George R. Stibitz and the machine was designed by Samuel B. Williams. This was not a fully automatic machine. The complex quantities for a singleThe IBM Magnetic Drum Calculator Type 650 is an electronic calculator intermediate in speed, capacity and cost. It takes a logical position between the IBM Card Programmed Electronic Calculator and the IBM Electronic Data Processing Machines Type 701. It is a more powerful computing tool as required by those who have “outgrown” the Card Programmed Electronic Calculator. It is also a machine which may be used economically by those who are not as yet ready for a large scale computer such as the 701. It will serve not only to perform their required computing tasks, but it will also result in gaining valuable experience for later use of large scale equipment. The Magnetic Drum Calculator, through its stored program control, comprehensive order list, punched card input-output, self-checking and moderate memory capacity, gains the flexibility required of a computer which is to serve in both the commercial and scientific computing fields.Approximately 2000 tubes are used in the machine. They are the 5965, 6211, 12AY7, 6AL5, 2D21 and 5687. Types 6211, and 5965 are similar to the 12AV7. These are used because they meet special IBM acceptance tests.
Let f be a function on a set of variables V. For each x ∈ V, let c(x) be the cost of reading the value of x. An algorithm for evaluating f is a strategy for adaptively identifying and reading a set of variables U ⊆ V whose values uniquely determine the value of f. We are interested in finding algorithms which minimize the cost incurred to evaluate f in the above sense. Competitive analysis is employed to measure the performance of the algorithms. We address two variants of the above problem. We consider the basic model in which the evaluation algorithm knows the cost c(x), for each x ∈ V. We also study a novel model where the costs of the variables are not known in advance and some preemption is allowed in the reading operations. This model has applications, for example, when reading a variable coincides with obtaining the output of a job on a CPU and the cost is the CPU time.For the model where the costs of the variables are known, we present a polynomial time algorithm with the best possible competitive ratio γcf for each function f that is representable by a threshold tree and for each fixed cost function c(⋅). Remarkably, the best-known result for the same class of functions is a pseudo-polynomial algorithm with competitiveness 2 γcf. Still in the same model, we introduce the Linear Programming Approach (LPA), a framework that allows the design of efficient algorithms for evaluating functions. We show that different implementations of this approach lead in general to the best algorithms known so far—and in many cases to optimal algorithms—for different classes of functions considered before in the literature.Via the LPA, we are able to determine exactly the optimal extremal competitiveness of monotone Boolean functions. Remarkably, the upper bound which leads to this result, holds for a much broader class of functions, which also includes the whole set of Boolean functions.We also show how to extend the LPA (together with these results) to the model where the costs of the variables are not known beforehand. In particular, we show how to employ the extended LPA to design a polynomial-time optimal (with respect to competitiveness) algorithm for the class of monotone Boolean functions representable by threshold trees.We consider Fisher and Arrow--Debreu markets under additively separable, piecewise-linear, concave utility functions and obtain the following results. For both market models, if an equilibrium exists, there is one that is rational and can be written using polynomially many bits. There is no simple necessary and sufficient condition for the existence of an equilibrium: The problem of checking for existence of an equilibrium is NP-complete for both market models; the same holds for existence of an ε-approximate equilibrium, for ε = O(n−5). Under standard (mild) sufficient conditions, the problem of finding an exact equilibrium is in PPAD for both market models. Finally, building on the techniques of Chen et al. [2009a] we prove that under these sufficient conditions, finding an equilibrium for Fisher markets is PPAD-hard.This article is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the ℓ1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.This article focuses on computations on large graphs (e.g., the web-graph) where the edges of the graph are presented as a stream. The objective in the streaming model is to use small amount of memory (preferably sub-linear in the number of nodes n) and a smaller number of passes.In the streaming model, we show how to perform several graph computations including estimating the probability distribution after a random walk of length l, the mixing time M, and other related quantities such as the conductance of the graph. By applying our algorithm for computing probability distribution on the web-graph, we can estimate the PageRank p of any node up to an additive error of &sqrt;ε p+ε in Õ(&sqrt;M/α) passes and Õ(min(nα+1/ε&sqrt;M/α+(1/ε)Mα, α n&sqrt;Mα + (1/ε)&sqrt;M/α)) space, for any α ∈ (0,1]. Specifically, for ε = M/n, α = M−1/2, we can compute the approximate PageRank values in Õ(nM−1/4) space and Õ(M3/4) passes. In comparison, a standard implementation of the PageRank algorithm will take O(n) space and O(M) passes. We also give an approach to approximate the PageRank values in just Õ(1) passes although this requires Õ(nM) space.
Locally decodable codes are error-correcting codes that admit efficient decoding algorithms; any bit of the original message can be recovered by looking at only a small number of locations of a corrupted codeword. The tradeoff between the rate of a code and the locality/efficiency of its decoding algorithms has been well studied, and it has widely been suspected that nontrivial locality must come at the price of low rate. A particular setting of potential interest in practice is codes of constant rate. For such codes, decoding algorithms with locality O(k∈) were known only for codes of rate ∈Ω(1/∈), where k is the length of the message. Furthermore, for codes of rate > 1/2, no nontrivial locality had been achieved.In this article, we construct a new family of locally decodable codes that have very efficient local decoding algorithms, and at the same time have rate approaching 1. We show that for every ∈ > 0 and α > 0, for infinitely many k, there exists a code C which encodes messages of length k with rate 1 − α, and is locally decodable from a constant fraction of errors using O(k∈) queries and time.These codes, which we call multiplicity codes, are based on evaluating multivariate polynomials and their derivatives. Multiplicity codes extend traditional multivariate polynomial codes; they inherit the local-decodability of these codes, and at the same time achieve better tradeoffs and flexibility in the rate and minimum distance.We study the problem of “privacy amplification”: key agreement between two parties who both know a weak secret w, such as a password. (Such a setting is ubiquitous on the internet, where passwords are the most commonly used security device.) We assume that the key agreement protocol is taking place in the presence of an active computationally unbounded adversary Eve. The adversary may have partial knowledge about w, so we assume only that w has some entropy from Eve’s point of view. Thus, the goal of the protocol is to convert this nonuniform secret w into a uniformly distributed string R that is fully secret from Eve. R may then be used as a key for running symmetric cryptographic protocols (such as encryption, authentication, etc.).Because we make no computational assumptions, the entropy in R can come only from w. Thus, such a protocol must minimize the entropy loss during its execution, so that R is as long as possible. The best previous results have entropy loss of Θ(κ2), where κ is the security parameter, thus requiring the password to be very long even for small values of κ. In this work, we present the first protocol for information-theoretic key agreement that has entropy loss linear in the security parameter. The result is optimal up to constant factors. We achieve our improvement through a somewhat surprising application of error-correcting codes for the edit distance.The protocol can be extended to provide also “information reconciliation,” that is, to work even when the two parties have slightly different versions of w (e.g., when biometrics are involved).Today’s hardware technology presents a new challenge in designing robust systems. Deep submicron VLSI technology introduces transient and permanent faults that were never considered in low-level system designs in the past. Still, robustness of that part of the system is crucial and needs to be guaranteed for any successful product. Distributed systems, on the other hand, have been dealing with similar issues for decades. However, neither the basic abstractions nor the complexity of contemporary fault-tolerant distributed algorithms match the peculiarities of hardware implementations.This article is intended to be part of an attempt striving to bridge over this gap between theory and practice for the clock synchronization problem. Solving this task sufficiently well will allow to build an ultra-robust high-precision clocking system for hardware designs like systems-on-chips in critical applications. As our first building block, we describe and prove correct a novel distributed, Byzantine fault-tolerant, probabilistically self-stabilizing pulse synchronization protocol, called FATAL, that can be implemented using standard asynchronous digital logic: Correct FATAL nodes are guaranteed to generate pulses (i.e., unnumbered clock ticks) in a synchronized way, despite a certain fraction of nodes being faulty. FATAL uses randomization only during stabilization and, despite the strict limitations introduced by hardware designs, offers optimal resilience and smaller complexity than all existing protocols. Finally, we show how to leverage FATAL to efficiently generate synchronized, self-stabilizing, high-frequency clocks.A Markov sequence is a basic statistical model representing uncertain sequential data, and it is used within a plethora of applications, including speech recognition, image processing, computational biology, radio-frequency identification (RFID), and information extraction. The problem of querying a Markov sequence is studied under the conventional semantics of querying a probabilistic database, where queries are formulated as finite-state transducers. Specifically, the complexity of two main problems is analyzed. The first problem is that of computing the confidence (probability) of an answer. The second is the enumeration of the answers in the order of decreasing confidence (with the generation of the top-k answers as a special case), or in an approximate order thereof. In particular, it is shown that enumeration in any subexponential-approximate order is generally intractable (even for some fixed transducers), and a matching upper bound is obtained through a proposed heuristic. Due to this hardness, a special consideration is given to restricted (yet common) classes of transducers that extract matches of a regular expression (subject to prefix and suffix constraints), and it is shown that these classes are, indeed, significantly more tractable.
Two upper bounds for the total path length of binary trees are obtained. One is for node-trees, and bounds the internal (or root-to-node) path length; the other is for leaf-trees, and bounds the external (or root-to-leaf) path length. These bounds involve a quantity called the balance, which allows the bounds to adapt from the n log n behavior of a completely balanced tree to the n2 behavior of a most skewed tree. These bounds are illustrated for the case of Fibonacci trees.Consider the set of multistep formulas ∑l-1jmn-k &agr;ijxmn+j - h ∑l-1jmn-k&bgr;ijxmn+j = 0, i = 1, ···, l, where xmn+j = ymn+j for j= -k, ···, -1 and xn = ƒn = ƒ(xn , tn). These formulas are solved simultaneously for the xmn+j with j = 0, ···, l - 1 in terms of the xmn+j with j = -k, ··· , - 1, which are assumed to be known.Then ymn+j is defined to be xmn+j for j = 0, ··· , m - 1. For j = m, ··· , l - 1, xmn+j is discarded. The set of y's generated in this manner for successive values of n provide an approximate solution of the initial value problem: y = ƒ(y, t), y(t0) = y0. It is conjectured that if the method, which is referred to as the composite multistep method, is A-stable, then its maximum order is 2l. In addition to noting that the conjecture conforms to Dahlquist's bound of 2 for l = 1, the conjecture is verified for k = 1. A third-order A-stable method with m = l = 2 is given as an example, and numerical results established in applying a fourth-order A-stable method with m = 1 and l = 2 are described. A-stable methods with m = l offer the promise of high order and a minimum of function evaluations—evaluation of ƒ(y, t) at solution points. Furthermore, the prospect that such methods might exist with k = 1—only one past point—means that step-size control can be easily implementedTridiagonal linear systems of equations can be solved on conventional serial machines in a time proportional to N, where N is the number of equations. The conventional algorithms do not lend themselves directly to parallel computation on computers of the ILLIAC IV class, in the sense that they appear to be inherently serial. An efficient parallel algorithm is presented in which computation time grows as log2 N. The algorithm is based on recursive doubling solutions of linear recurrence relations, and can be used to solve recurrence relations of all orders.A combinatorial problem arising from the analysis of a model of interleaved memory systems is studied. The performance measure whose calculation defines this problem is based on the distribution of the number of modules in operation during a memory cycle, assuming saturated demand and an arbitrary but fixed number of modules.In general terms the problem is as follows. Suppose we have a Markov chain of n states numbered 0, 1, ···, n - 1. For each i assume that the one-step transition probability from state i to state (i + 1) mod n is given by the parameter &agr; and from state i to any other state is &bgr; = (1 - &agr;)/(n - 1). Given an initial state, the problem is to find the expected number of states through which the system passes before returning to a state previously entered. The principal result of the paper is a recursive procedure for computing this expected number of states. The complexity of the procedure is seen to be small enough to enable practical numerical studies of interleaved memory systems.The problem of multiprogram scheduling on a single processor is studied from the viewpoint of the characteristics peculiar to the program functions that need guaranteed service. It is shown that an optimum fixed priority scheduler possesses an upper bound to processor utilization which may be as low as 70 percent for large task sets. It is also shown that full processor utilization can be achieved by dynamically assigning priorities on the basis of their current deadlines. A combination of these two scheduling techniques is also discussed.A particular decision-theoretic approach to the problem of detecting straight edges and lines in pictures is discussed. A model is proposed of the appearance of scenes consisting of prismatic solids, taking into account blurring, noise, and smooth variations in intensity over faces. A suboptimal statistical decision procedure is developed for the identification of a line within a narrow band in the field of view, given an array of intensity values from within the band. The performance of this procedure is illustrated and discussed.Characterizations of digital “simple arcs” and “simple closed curves” are given. In particular, it is shown that the following are equivalent for sets S having more than four points: (1) S is a simple curve; (2) S is connected and each point of S has exactly two neighbors in S; (3) S is connected, has exactly one hole, and has no deletable points. It follows that if a “shrinking” algorithm is applied to a connected S that has exactly one hole, it shrinks to a simple curve.The problem of finding a minimum number of patterns to exercise the logic elements of a combinational switching net is investigated. Throughout, the word “testing” refers to exercising of this kind; or, equivalently, to fault diagnosis where each line of the net can be directly observed. Any set of permanent faults can be selected to test against, examples of which range from “stuck-at” faults (allowing the most economical test) to “any possible fault” (requiring the most complete test).The method used depends upon exact structural analysis rather than upon search algorithms or random pattern generation. The types of results presented appear to be fundamentally new. In particular, the maximum number of patterns required to test any one of an infinite class of nets is frequently found to be finite and extremely small. For example, any nontrivial connected tree of 2-input nand gates can be tested for “any possible fault” by exactly five patterns—no more and no less.The method in brief: Given a set F of switching functions and a set of required inputs for each (collectively denoted T), a “testing” function is defined for each element of F for each positive integer r. If the lines of a net can be mapped to the domain of the testing functions P(T, r) so that the gates perform consistent with these functions, we say the net “accepts” P(T, r)—and then r patterns are sufficient to test the net for T.Only nets in which each logic element is intended to realize the same switching function are discussed here. Trees (nets without fanout) are studied first, and the conditions under which a tree of identical gates “accepts” a partial function on an arbitrary domain is established. Then the common symmetric switching functions are separately investigated to find for each a minimum value of r such that all trees composed solely of the function accept P(T, r) (for various T). In most cases, as in the example given, the number of patterns required to test any such tree is extremely low.The conditions under which all nets (nontrees included) accept a set of partial functions with arbitrary domain are then established. These conditions are rarely met in practice, even where F consists of a single function. However, many subclasses of nets can be identified which require only a few patterns at most (depending on the function and the class of faults selected). These subclasses often contain nets of arbitrary size and complexity, and frequently consist of exactly those nets for which a related graph can be “colored” (i.e., h-node colored for some particular h) in the classical graph-theoretic sense. For example, any net of 2-input nand gates can be tested by five patterns if one of its related graphs is 2-colorable and another one is 3-colorable (!).The detailed results and methods used to obtain them are summarized, and in conclusion coloring problems and test construction are commented upon.In connection with the development of an actual question-answering system, the Relational Data File of The Rand Corporation, a class of formulas of the predicate calculus, the definite formulas of Kuhns, was proposed as the class of symbolic representations of the “reasonable” questions to put to this retrieval system. Roughly speaking, the definite formulas are those formulas F such that the set of true substitution instances of F in a finite interpretation I are preserved on passage to a certain special extension I′ of I. The proper formulas are those definite formulas which are supposedly especially suitable for machine processing. It has previously been shown by the author that the decision problem for the class of definite formulas is recursively unsolvable. In this paper, however, it is shown that the decision problem for various classes of proper formulas is solvable. Thus, for each of these classes there is a mechanical procedure to decide of an arbitrary formula whether it is a member of the class. It follows that for each of these classes there is no effective transformation 
@@@@ which takes an arbitrary definite formula F into a proper equivalent @@@@(F). In addition, it is shown that the decision problems for a number of classes of formulas which bear a structural similarity to any class of proper formulas are recursively unsolvable.An improved procedure for resolution theorem proving, called Z-resolution, is described. The basic idea of Z-resolution is to “compile” some of the axioms in a deductive problem. This means to automatically transform the selected axioms into a computer program which carries out the inference rules indicated by the axioms. This is done automatically by another program called the specializer. The advantage of doing this is that the compiled axioms run faster, just as a compiled program runs faster than an interpreted program.A proof is given that the inference rule used in Z-resolution is complete, provided that the axioms “compiled” have certain properties.Suppose we are given two disjoint linearly ordered subsets A and B of a linearly ordered set C, say A = {a1 < a2 < ··· < am} and B = {b1 < b2 < ··· < bn}. The problem is to determine the linear ordering of their union (i.e. to merge A and B) by means of a sequence of pairwise comparisons between an element of A and an element of B (which we refer to in the paper as the (m, n) problem). Given any algorithm s to solve the (m, n) problem, we are interested in the maximum number of comparisons Ks(m, n) required under all possible orderings of A ∪ B. An algorithm s is said to be minimax if Ks(m, n) = K(m, n) where K(m, n) = mins Ks(m, n)It is a rather difficult task to determine K(m, n) in general. In this study the authors are only concerned with the minimax over a particular class of merging algorithms. This class includes the tape merge algorithm, the simple binary algorithm, and the generalized binary algorithm.Subtree replacement systems form a broad class of tree-manipulating systems. Systems with the “Church-Rosser property” are appropriate for evaluation or translation processes: the end result of a complete sequence of applications of the rules does not depend on the order in which the rules were applied. Theoretical and practical advantages of such flexibility are sketched. Values or meanings for trees can be defined by simple mathematical systems and then computed by the cheapest available algorithm, however intricate that algorithm may be.We derive sufficient conditions for the Church-Rosser property and discuss their applications to recursive definitions, to the lambda calculus, and to parallel programming. Only the first application is treated in detail. We extend McCarthy's recursive calculus by allowing a choice between call-by-value and call-by-name. We show that recursively defined functions are single-valued despite the nondeterminism of the evaluation algorithm. We also show that these functions solve their defining equations in a “canonical” manner.
Locally decodable codes are error-correcting codes that admit efficient decoding algorithms; any bit of the original message can be recovered by looking at only a small number of locations of a corrupted codeword. The tradeoff between the rate of a code and the locality/efficiency of its decoding algorithms has been well studied, and it has widely been suspected that nontrivial locality must come at the price of low rate. A particular setting of potential interest in practice is codes of constant rate. For such codes, decoding algorithms with locality O(k∈) were known only for codes of rate ∈Ω(1/∈), where k is the length of the message. Furthermore, for codes of rate > 1/2, no nontrivial locality had been achieved.In this article, we construct a new family of locally decodable codes that have very efficient local decoding algorithms, and at the same time have rate approaching 1. We show that for every ∈ > 0 and α > 0, for infinitely many k, there exists a code C which encodes messages of length k with rate 1 − α, and is locally decodable from a constant fraction of errors using O(k∈) queries and time.These codes, which we call multiplicity codes, are based on evaluating multivariate polynomials and their derivatives. Multiplicity codes extend traditional multivariate polynomial codes; they inherit the local-decodability of these codes, and at the same time achieve better tradeoffs and flexibility in the rate and minimum distance.We study the problem of “privacy amplification”: key agreement between two parties who both know a weak secret w, such as a password. (Such a setting is ubiquitous on the internet, where passwords are the most commonly used security device.) We assume that the key agreement protocol is taking place in the presence of an active computationally unbounded adversary Eve. The adversary may have partial knowledge about w, so we assume only that w has some entropy from Eve’s point of view. Thus, the goal of the protocol is to convert this nonuniform secret w into a uniformly distributed string R that is fully secret from Eve. R may then be used as a key for running symmetric cryptographic protocols (such as encryption, authentication, etc.).Because we make no computational assumptions, the entropy in R can come only from w. Thus, such a protocol must minimize the entropy loss during its execution, so that R is as long as possible. The best previous results have entropy loss of Θ(κ2), where κ is the security parameter, thus requiring the password to be very long even for small values of κ. In this work, we present the first protocol for information-theoretic key agreement that has entropy loss linear in the security parameter. The result is optimal up to constant factors. We achieve our improvement through a somewhat surprising application of error-correcting codes for the edit distance.The protocol can be extended to provide also “information reconciliation,” that is, to work even when the two parties have slightly different versions of w (e.g., when biometrics are involved).Today’s hardware technology presents a new challenge in designing robust systems. Deep submicron VLSI technology introduces transient and permanent faults that were never considered in low-level system designs in the past. Still, robustness of that part of the system is crucial and needs to be guaranteed for any successful product. Distributed systems, on the other hand, have been dealing with similar issues for decades. However, neither the basic abstractions nor the complexity of contemporary fault-tolerant distributed algorithms match the peculiarities of hardware implementations.This article is intended to be part of an attempt striving to bridge over this gap between theory and practice for the clock synchronization problem. Solving this task sufficiently well will allow to build an ultra-robust high-precision clocking system for hardware designs like systems-on-chips in critical applications. As our first building block, we describe and prove correct a novel distributed, Byzantine fault-tolerant, probabilistically self-stabilizing pulse synchronization protocol, called FATAL, that can be implemented using standard asynchronous digital logic: Correct FATAL nodes are guaranteed to generate pulses (i.e., unnumbered clock ticks) in a synchronized way, despite a certain fraction of nodes being faulty. FATAL uses randomization only during stabilization and, despite the strict limitations introduced by hardware designs, offers optimal resilience and smaller complexity than all existing protocols. Finally, we show how to leverage FATAL to efficiently generate synchronized, self-stabilizing, high-frequency clocks.A Markov sequence is a basic statistical model representing uncertain sequential data, and it is used within a plethora of applications, including speech recognition, image processing, computational biology, radio-frequency identification (RFID), and information extraction. The problem of querying a Markov sequence is studied under the conventional semantics of querying a probabilistic database, where queries are formulated as finite-state transducers. Specifically, the complexity of two main problems is analyzed. The first problem is that of computing the confidence (probability) of an answer. The second is the enumeration of the answers in the order of decreasing confidence (with the generation of the top-k answers as a special case), or in an approximate order thereof. In particular, it is shown that enumeration in any subexponential-approximate order is generally intractable (even for some fixed transducers), and a matching upper bound is obtained through a proposed heuristic. Due to this hardness, a special consideration is given to restricted (yet common) classes of transducers that extract matches of a regular expression (subject to prefix and suffix constraints), and it is shown that these classes are, indeed, significantly more tractable.
Locally decodable codes are error-correcting codes that admit efficient decoding algorithms; any bit of the original message can be recovered by looking at only a small number of locations of a corrupted codeword. The tradeoff between the rate of a code and the locality/efficiency of its decoding algorithms has been well studied, and it has widely been suspected that nontrivial locality must come at the price of low rate. A particular setting of potential interest in practice is codes of constant rate. For such codes, decoding algorithms with locality O(k∈) were known only for codes of rate ∈Ω(1/∈), where k is the length of the message. Furthermore, for codes of rate > 1/2, no nontrivial locality had been achieved.In this article, we construct a new family of locally decodable codes that have very efficient local decoding algorithms, and at the same time have rate approaching 1. We show that for every ∈ > 0 and α > 0, for infinitely many k, there exists a code C which encodes messages of length k with rate 1 − α, and is locally decodable from a constant fraction of errors using O(k∈) queries and time.These codes, which we call multiplicity codes, are based on evaluating multivariate polynomials and their derivatives. Multiplicity codes extend traditional multivariate polynomial codes; they inherit the local-decodability of these codes, and at the same time achieve better tradeoffs and flexibility in the rate and minimum distance.We study the problem of “privacy amplification”: key agreement between two parties who both know a weak secret w, such as a password. (Such a setting is ubiquitous on the internet, where passwords are the most commonly used security device.) We assume that the key agreement protocol is taking place in the presence of an active computationally unbounded adversary Eve. The adversary may have partial knowledge about w, so we assume only that w has some entropy from Eve’s point of view. Thus, the goal of the protocol is to convert this nonuniform secret w into a uniformly distributed string R that is fully secret from Eve. R may then be used as a key for running symmetric cryptographic protocols (such as encryption, authentication, etc.).Because we make no computational assumptions, the entropy in R can come only from w. Thus, such a protocol must minimize the entropy loss during its execution, so that R is as long as possible. The best previous results have entropy loss of Θ(κ2), where κ is the security parameter, thus requiring the password to be very long even for small values of κ. In this work, we present the first protocol for information-theoretic key agreement that has entropy loss linear in the security parameter. The result is optimal up to constant factors. We achieve our improvement through a somewhat surprising application of error-correcting codes for the edit distance.The protocol can be extended to provide also “information reconciliation,” that is, to work even when the two parties have slightly different versions of w (e.g., when biometrics are involved).Today’s hardware technology presents a new challenge in designing robust systems. Deep submicron VLSI technology introduces transient and permanent faults that were never considered in low-level system designs in the past. Still, robustness of that part of the system is crucial and needs to be guaranteed for any successful product. Distributed systems, on the other hand, have been dealing with similar issues for decades. However, neither the basic abstractions nor the complexity of contemporary fault-tolerant distributed algorithms match the peculiarities of hardware implementations.This article is intended to be part of an attempt striving to bridge over this gap between theory and practice for the clock synchronization problem. Solving this task sufficiently well will allow to build an ultra-robust high-precision clocking system for hardware designs like systems-on-chips in critical applications. As our first building block, we describe and prove correct a novel distributed, Byzantine fault-tolerant, probabilistically self-stabilizing pulse synchronization protocol, called FATAL, that can be implemented using standard asynchronous digital logic: Correct FATAL nodes are guaranteed to generate pulses (i.e., unnumbered clock ticks) in a synchronized way, despite a certain fraction of nodes being faulty. FATAL uses randomization only during stabilization and, despite the strict limitations introduced by hardware designs, offers optimal resilience and smaller complexity than all existing protocols. Finally, we show how to leverage FATAL to efficiently generate synchronized, self-stabilizing, high-frequency clocks.A Markov sequence is a basic statistical model representing uncertain sequential data, and it is used within a plethora of applications, including speech recognition, image processing, computational biology, radio-frequency identification (RFID), and information extraction. The problem of querying a Markov sequence is studied under the conventional semantics of querying a probabilistic database, where queries are formulated as finite-state transducers. Specifically, the complexity of two main problems is analyzed. The first problem is that of computing the confidence (probability) of an answer. The second is the enumeration of the answers in the order of decreasing confidence (with the generation of the top-k answers as a special case), or in an approximate order thereof. In particular, it is shown that enumeration in any subexponential-approximate order is generally intractable (even for some fixed transducers), and a matching upper bound is obtained through a proposed heuristic. Due to this hardness, a special consideration is given to restricted (yet common) classes of transducers that extract matches of a regular expression (subject to prefix and suffix constraints), and it is shown that these classes are, indeed, significantly more tractable.
We describe an efficient, purely functional implementation of deques with catenation. In addition to being an intriguing problem in its own right, finding a purely functional implementation of catenable deques is required to add certain sophisticated programming constructs to functional programming languages. Our solution has a worst-case running time of O(1) for each push, pop, inject, eject and catenation. The best previously known solution has an O(log*k) time bound for the kth deque operation. Our solution is not only faster but simpler. A key idea used in our result is an algorithmic technique related to the redundant digital representations used to avoid carry propagation in binary counting.The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authorative” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages” that join them together in  the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link-based analysis.In a timestamping system, processors repeatedly choose timestamps so that the order of the timestamps obtained reflects the real-time order in which they were requested. Concurrent timestamping systems permit requests by multiple processors to be issued concurrently; in bounded timestamping systems the sizes of the timestamps and the size and number of shared variables are bounded. An algorithm is wait-free if there exists an a priori bound on the number of steps a processor must take in order to make progress, independent of the action or inaction of other processors. Letting n denote the number of procesors, we construct a simple wait-free bounded concurrent timestamping system    requiring O(n) steps (accesses to shared memory) for a processor to read the current timestamps and determine the order among them, and O(n) steps to generate a timestamp, independent of the actions of the other processors. In addition, we introduce and implement the traceable use abstraction, a new primitive providing “inventory control” over values introduced by processors in the course of an algorithm execution. This abstraction has proved to be of great value in converting unbounded algorithms to bounded ones {Attiya and Rachman 1998; Dwork et al. 1992; 1993].Consider the set   H   of all linear (or affine) transformations between two vector spaces over a finite field F. We study how good   H is as a class of hash functions, namely we consider hashing a set S of size n into a range having the same cardinality n by a randomly chosen function from   H and look at the expected size of the largest hash bucket.   H is a universal class of hash functions for any finite field, but with respect to our measure different fields behave differently.If the finite  field   F has n elements, then there is a bad set S   ⊂ F2 of size n with expected maximal bucket size   H(n1/3). If n is a perfect square, then there is even a bad set with largest bucket size always at least    n. (This is worst possible, since with respect to a universal class of hash functions every set of size n has expected largest bucket size below    n +   1/2.)If, however, we consider the field of  two  elements, then we get much better bounds. The best previously known upper bound on the  expected size of the largest bucket for this class was  O(2   log n). We reduce this upper bound to O(log n log logn). Note that this is not far from the guarantee for a random function. There, the average largest bucket would be &THgr;(log n/ log log n).In the course of our proof we develop a tool which may be of independent interest. Suppose we have a subset S of a vector space D over   Z2, and consider a random linear mapping of  D to a smaller vector space R. If the cardinality of S is larger than   c&egr;|R|log|R|, then with probability 1 - &egr;, the image of S will cover all elements in the range.In this paper, we prove various results about PAC learning in the presence of malicious noise. Our main interest is the sample size behavior of learning algorithms. We prove the first nontrivial sample complexity lower bound in this model by showing that order of &egr;/&Dgr;2 + d/&Dgr; (up to logarithmic factors) examples are necessary for PAC learning any target class of {0,1}-valued functions of VC dimension d, where &egr; is the desired accuracy and &eegr; = &egr;/(1 + &egr;) - &Dgr; the malicious noise rate (it is well known that any nontrivial target class cannot be PAC learned with accuracy &egr; and malicious noise rate &eegr; ≥ &egr;/(1 + &egr;), this irrespective to sample complexity). We also show that this result  cannot be significantly improved in general by presenting efficient learning algorithms for the class of all subsets of d elements and the class of unions of at most d intervals on the real line. This is especialy interesting as we can also show that the popular minimum disagreement strategy needs samples of size d &egr;/&Dgr;2, hence is not optimal with respect to sample size. We then discuss the use of randomized hypotheses. For these the bound &egr;/(1 + &egr;) on the noise rate is no longer true and is replaced by 2&egr;/(1 + 2&egr;). In fact, we present a generic algorithm using randomized hypotheses that can tolerate noise rates slightly larger than &egr;/(1 + &egr;) while using samples of size  d/&egr; as in the noise-free case. Again one observes a quadratic powerlaw (in this case d&egr;/&Dgr;2, &Dgr; = 2&egr;/(1 + 2&egr;) - &eegr;) as &Dgr; goes to zero. We show upper and lower bounds of this order.This paper studies the problem of efficiently schedulling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is “work stealing,” in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies.Specifically, our analysis shows that the expected time to execute a fully strict computation on P processors using our work-stealing scheduler is T1/P + O(T  ∞ , where  T1 is the minimum serial execution time of the multithreaded computation and (T  ∞ is the minimum execution time with an infinite number of processors. Moreover, the space required by the execution is at most S1P, where  S1 is the minimum serial space requirement. We also show that the expected total communication of the algorithm is at most O(PT  ∞ ( 1 +  nd)Smax), where Smax is the size of the largest activation record of any thread and  nd is the maximum number of times that any thread synchronizes with its parent. This communication bound justifies the folk wisdom that work-stealing schedulers are more communication efficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.We develop principles and rules for achieving secrecy properties in security protocols. Our approach is based on traditional classification techniques, and extends those techniques to handle concurrent processes that use shared-key cryptography. The rules have the form of typing rules for a basic concurrent language with cryptographic primitives, the spi calculus. They guarantee that, if a protocol typechecks, then it does not leak its secret inputs.
The main contribution of this work is an O(n log n + k)-time algorithm for computing all k intersections among n line segments in the plane. This time complexity is easily shown to be optimal. Within the same asymptotic cost, our algorithm can also construct the subdivision of the plane defined by the segments and compute which segment (if any) lies right above (or below) each intersection and each endpoint. The algorithm has been implemented and performs very well. The storage requirement is on the order of n + k in the worst case, but it is considerably lower in practice. To analyze the complexity of the algorithm, an amortization argument based on  a new combinatorial theorem on line arrangements is used.A deterministic O(log N)-time algorithm for the problem of routing an aribitrary permutation on an N-processor bounded-degree network with bounded buffers is presented.Unlike all previous deterministic solutions to this problem, our routing scheme does not reduce the routing problem to sorting and does not use the sorting network of Ajtai, et al. [1]. Consequently, the constant in the run time of our routing scheme is substantially smaller, and the network topology is significantly simpler.A domain-independent formula of first-order predicate calculus is a formula whose evaluation in a given interpretation does not change when we add a new constant to the interpretation domain. The formulas used to express queries, integrity constraints or deductive rules in the database field that have an intuitive meaning are domain independent. That is the reason why this class is of great interest in practice. Unfortunately, this class is not decidable, and the problem is to characterize new subclasses, as large as possible, which are decidable. A syntactic characterization of a class of formulas, the Evaluable formulas, which are proved to be domain independent are provided. This class is defined only for function-free formulas. It is also proved that the class of evaluable   formulas contains the other classes of syntactically characterized domain-independent formulas usually found in the literature, namely, range-separable formulas and range-restricted formulas. Finally, it is shown that the expressive power of evaluable formulas is the same as that of domain-independent formulas. That is, each domain-independent formula admits an equivalent evaluable one. An important advantage of this characterization is that, to check if a formula is evaluable, it is not necessary to transform it to a normal form, as is the case for range-restricted formulas.There is a population explosion among the logical systems used in computing science. Examples include first-order logic, equational logic, Horn-clause logic, higher-order logic, infinitary logic, dynamic logic, intuitionistic logic, order-sorted logic, and temporal logic; moreover, there is a tendency for each theorem prover to have its own idiosyncratic logical system. The concept of institution is introduced to formalize the informal notion of “logical system.” The major requirement is that there is a satisfaction relation between models and sentences that is consistent under change of notation. Institutions enable abstracting away from syntactic and semantic detail when working on language structure “in-the-large”; for example, we can   define language features for building large logical system. This applies to both specification languages and programming languages. Institutions also have applications to such areas as database theory and the semantics of artificial and natural languages. A first main result of this paper says that any institution such that signatures (which define notation) can be glued together, also allows gluing together theories (which are just collections of sentences over a fixed signature). A second main result considers when theory structuring is preserved by institution morphisms. A third main result gives conditions under which it is sound to use a theorem prover for one institution on theories from another. A fourth main result shows how to extend institutions so that their theories may include, in addition  to the original sentences, various kinds of constraint that are useful for defining   abstract data types, including both “data” and “hierarchy” constraints. Further results show how to define institutions that allow sentences and constraints from two or more institutions. All our general results apply to such “duplex” and “multiplex” institutions.In this paper, a process algebra that incorporates explicit representations of successful termination, deadlock, and divergence is introduced and its semantic theory is analyzed. Both an operational and a denotational semantics for the language is given and it is shown that they agree. The operational theory is based upon a suitable adaptation of the notion of bisimulation preorder. The denotational semantics for the language is given in terms of the initial continuous algebra that satisfies a set of equations E, CIE. It is shown that CIE is fully abstract with respect to our choice of behavioral preorder. Several results of independent interest are obtained; namely, the finite  approximability of the behavioral preorder and a partial completeness result for the set of equations E with respect to the preorder.In a closed, separable, queuing network model of a computer system, the number of customer classes is an input parameter. The number of classes and the class compositions are assumptions regarding the characteristics of the system's workload. Often, the number of customer classes and their associated device demands are unknown or are unmeasurable parameters of the system. However, when the system is viewed as having a single composite customer class, the aggregate single-class parameters are more easily obtainable.This paper addresses the error made when constructing a single-class model of a multi-class system. It is shown that the single-class model pessimistically bounds, the performance of the multi-class system. Thus, given a multi-class system, the corresponding  single-class model can be constructed with the assurance that the actual system performance is better than that given by the single-class model. In the worst case, it is shown that the throughput given by the single-class model underestimates the actual multi-class throughput by, at most, 50%. Also, lower bounds are provided for the number of necessary customer classes, given observed device utilizations. This information is useful to clustering analysis techniques as well as to analysts who must obtain class-specific device demands.A digital signature scheme is presented, which is based on the existence of any trapdoor permutation. The scheme is secure in the strongest possible natural sense: namely, it is secure against existential forgery under adaptive chosen message attack.
The problem of synthesizing a procedure from example computations is examined. An algorithm for this task is presented, and its success is considered. To do this, a model of procedures and example computations is introduced, and the class of acceptable examples is defined. The synthesis algorithm is shown to be successful, with respect to the model of procedures and examples, from two perspectives. First, it is shown to be sound, that is, that the procedure synthesized from a set of examples produces the same result as the intended one on the inputs used to generate that set of examples. Second, it is shown to be complete, that is, that for any procedure in the class of procedures, there exists a finite set of examples such that the procedure synthesized behaves as the intended one on all inputs for which the intended one halts.The costs of subsumption algorithms are analyzed by an estimation of the maximal number of unification attempts (worst-case unification complexity) made for deciding whether a clause C subsumes a clause D. For this purpose the clauses C and D are characterized by the following parameters: number of variables in C, number of literals in C, number of literals in D, and maximal length of the literals. The worst-case unification complexity immediately yields a lower bound for the worst-case time complexity.First, two well-known algorithms (Chang-Lee, Stillman) are investigated. Both algorithms are shown to have a very high worst-case time complexity. Then, a new subsumption algorithm is defined, which is based on an analysis of the connection between variables and predicates in C. An upper bound for the worst-case unification complexity of this algorithm, which is much lower than the lower bounds for the two other algorithms, is derived. Examples in which exponential costs are reduced to polynomial costs are discussed. Finally, the asymptotic growth of the worst-case complexity for all discussed algorithms is shown in a table (for several combinations of the parameters).The problem of finding a minimum cardinality feedback vertex set of a directed graph is considered. Of the classic NP-complete problems, this is one of the least understood. Although Karp showed the general problem to be NP-complete, a linear algorithm for its solution on reducible flow graphs was given by Shamir. The class of reducible flow graphs is the only nontrivial class of graphs for which a polynomial-time algorithm to solve this problem is known. The main result of this paper is to present a new class of graphs—the cyclically reducible graphs—for which minimum feedback vertex sets can be found in polynomial time. This class is not restricted to flow graphs, and most small graphs (10 or fewer nodes) fall into this class. The identification of this class is particularly important since there do not exist approximation algorithms for this problem having a provably good worst case performance. Along with the class and a simple polynomial-time algorithm for finding minimum feedback vertex sets of graphs in the class, several related results are presented. It is shown that there is no “forbidden subgraph” characterization of the class and that there is no particular inclusion relationship between this class and the reducible flow graphs. In addition, it is shown that a class of (general) graphs, which are related to the reducible flow graphs, are contained in the cyclically reducible class.Many database systems maintain the consistency of the data by using a locking protocol to restrict access to data items. It has been previously shown that if no information is known about the method of accessing items in the database, then the two-phase protocol is optimal. However, the use of structural information about the database allows development of non-two-phase protocols, called graph protocols, that can potentially increase efficiency. Yannakakis developed a general class of protocols that included many of the graph protocols. Graph protocols either are only usable in certain types of databases or can incur the performance liability of cascading rollback. In this paper, it is demonstrated that if the system has a priori information as to which data items will be locked first by various transactions, a new graph protocol that is outside the previous classes of graph protocols and is applicable to arbitrarily structured databases can be constructed. This new protocol avoids cascading rollback and its accompanying performance degradation, and extends the class of serializable sequences allowed by non-two-phase protocols. This is the first protocol shown to be always as effective as the two-phase protocol, and it can be more effective for certain types of database systems.Parallel algorithms for data compression by textual substitution that are suitable for VLSI implementation are studied. Both “static” and “dynamic” dictionary schemes are considered.The consensus problem involves an asynchronous system of processes, some of which may be unreliable. The problem is for the reliable processes to agree on a binary value. In this paper, it is shown that every protocol for this problem has the possibility of nontermination, even with only one faulty process. By way of contrast, solutions are known for the synchronous case, the “Byzantine Generals” problem.This paper analyzes decomposition properties of a graph that, when they occur, permit a polynomial solution of the traveling salesman problem and a description of the traveling salesman polytope by a system of linear equalities and inequalities. The central notion is that of a 3-edge cutset, namely, a set of 3 edges that, when removed, disconnects the graph. Conversely, our approach can be used to construct classes of graphs for which there exists a polynomial algorithm for the traveling salesman problem. The approach is illustrated on two examples, Halin graphs and prismatic graphs.Criteria for adequacy of a data flow semantics are discussed and Kahn's successful semantics for functional (deterministic) data flow is reviewed. Problems arising from nondeterminism are introduced and the paper's approach to overcoming them is introduced. The approach is based on generalizing the notion of input-output relation, essentially to a partially ordered multiset of input-output histories. The Brock-Ackerman anomalies concerning the input-output relation model of nondeterministic data flow are reviewed, and it is indicated how the proposed approach avoids them. A new anomaly is introduced to motivate the use of multisets. A formal theory of asynchronous processes is then developed. The main result is that the operation of forming a process from a network of component processes is associative. This result shows that the approach is not subject to anomalies such as that of Brock and Ackerman.A distributed computer system that consists of a set of heterogeneous host computers connected in an arbitrary fashion by a communications network is considered. A general model is developed for such a distributed computer system, in which the host computers and the communications network are represented by product-form queuing networks. In this model, a job may be either processed at the host to which it arrives or transferred to another host. In the latter case, a transferred job incurs a communication delay in addition to the queuing delay at the host on which the job is processed. It is assumed that the decision of transferring a job does not depend on the system state, and hence is static in nature. Performance is optimized by determining the load on each host that minimizes the mean job response time. A nonlinear optimization problem is formulated, and the properties of the optimal solution in the special case where the communication delay does not depend on the source-destination pair is shown.Two efficient algorithms that determine the optimal load on each host computer are presented. The first algorithm, called the parametric-study algorithm, generates the optimal solution as a function of the communication time. This algorithm is suited for the study of the effect of the speed of the communications network on the optimal solution. The second algorithm is a single-point algorithm; it yields the optimal solution for given system parameters. Queuing models of host computers, communications networks, and a numerical example are illustrated.Two of the most powerful classes of programs for which interesting decision problems are known to be solvable are the class of finite-memory programs and the class of programs that characterize the Presburger, or semilinear, sets. In this paper, a new class of programs that presents solvable decision problems similar to the other two classes of programs is introduced. However, the programs in the new class are shown to be computationally more powerful (i.e., capable of defining larger sets of input-output relations).A projection of a Boolean function is a function obtained by substituting for each of its variables a variable, the negation of a variable, or a constant. Reducibilities among computational problems under this relation of projection are considered. It is shown that much of what is of everyday relevance in Turing-machine-based complexity theory can be replicated easily and naturally in this elementary framework. Finer distinctions about the computational relationships among natural problems can be made than in previous formulations and some negative results are proved.
Locally decodable codes are error-correcting codes that admit efficient decoding algorithms; any bit of the original message can be recovered by looking at only a small number of locations of a corrupted codeword. The tradeoff between the rate of a code and the locality/efficiency of its decoding algorithms has been well studied, and it has widely been suspected that nontrivial locality must come at the price of low rate. A particular setting of potential interest in practice is codes of constant rate. For such codes, decoding algorithms with locality O(k∈) were known only for codes of rate ∈Ω(1/∈), where k is the length of the message. Furthermore, for codes of rate > 1/2, no nontrivial locality had been achieved.In this article, we construct a new family of locally decodable codes that have very efficient local decoding algorithms, and at the same time have rate approaching 1. We show that for every ∈ > 0 and α > 0, for infinitely many k, there exists a code C which encodes messages of length k with rate 1 − α, and is locally decodable from a constant fraction of errors using O(k∈) queries and time.These codes, which we call multiplicity codes, are based on evaluating multivariate polynomials and their derivatives. Multiplicity codes extend traditional multivariate polynomial codes; they inherit the local-decodability of these codes, and at the same time achieve better tradeoffs and flexibility in the rate and minimum distance.We study the problem of “privacy amplification”: key agreement between two parties who both know a weak secret w, such as a password. (Such a setting is ubiquitous on the internet, where passwords are the most commonly used security device.) We assume that the key agreement protocol is taking place in the presence of an active computationally unbounded adversary Eve. The adversary may have partial knowledge about w, so we assume only that w has some entropy from Eve’s point of view. Thus, the goal of the protocol is to convert this nonuniform secret w into a uniformly distributed string R that is fully secret from Eve. R may then be used as a key for running symmetric cryptographic protocols (such as encryption, authentication, etc.).Because we make no computational assumptions, the entropy in R can come only from w. Thus, such a protocol must minimize the entropy loss during its execution, so that R is as long as possible. The best previous results have entropy loss of Θ(κ2), where κ is the security parameter, thus requiring the password to be very long even for small values of κ. In this work, we present the first protocol for information-theoretic key agreement that has entropy loss linear in the security parameter. The result is optimal up to constant factors. We achieve our improvement through a somewhat surprising application of error-correcting codes for the edit distance.The protocol can be extended to provide also “information reconciliation,” that is, to work even when the two parties have slightly different versions of w (e.g., when biometrics are involved).Today’s hardware technology presents a new challenge in designing robust systems. Deep submicron VLSI technology introduces transient and permanent faults that were never considered in low-level system designs in the past. Still, robustness of that part of the system is crucial and needs to be guaranteed for any successful product. Distributed systems, on the other hand, have been dealing with similar issues for decades. However, neither the basic abstractions nor the complexity of contemporary fault-tolerant distributed algorithms match the peculiarities of hardware implementations.This article is intended to be part of an attempt striving to bridge over this gap between theory and practice for the clock synchronization problem. Solving this task sufficiently well will allow to build an ultra-robust high-precision clocking system for hardware designs like systems-on-chips in critical applications. As our first building block, we describe and prove correct a novel distributed, Byzantine fault-tolerant, probabilistically self-stabilizing pulse synchronization protocol, called FATAL, that can be implemented using standard asynchronous digital logic: Correct FATAL nodes are guaranteed to generate pulses (i.e., unnumbered clock ticks) in a synchronized way, despite a certain fraction of nodes being faulty. FATAL uses randomization only during stabilization and, despite the strict limitations introduced by hardware designs, offers optimal resilience and smaller complexity than all existing protocols. Finally, we show how to leverage FATAL to efficiently generate synchronized, self-stabilizing, high-frequency clocks.A Markov sequence is a basic statistical model representing uncertain sequential data, and it is used within a plethora of applications, including speech recognition, image processing, computational biology, radio-frequency identification (RFID), and information extraction. The problem of querying a Markov sequence is studied under the conventional semantics of querying a probabilistic database, where queries are formulated as finite-state transducers. Specifically, the complexity of two main problems is analyzed. The first problem is that of computing the confidence (probability) of an answer. The second is the enumeration of the answers in the order of decreasing confidence (with the generation of the top-k answers as a special case), or in an approximate order thereof. In particular, it is shown that enumeration in any subexponential-approximate order is generally intractable (even for some fixed transducers), and a matching upper bound is obtained through a proposed heuristic. Due to this hardness, a special consideration is given to restricted (yet common) classes of transducers that extract matches of a regular expression (subject to prefix and suffix constraints), and it is shown that these classes are, indeed, significantly more tractable.
The literature concerned with methods for finding the minimal form of a truth function is, by now, quite extensive. This article extends this knowledge by introducing an algorithm whereby all calculations are performed on decimal numbers obtained from binary-decimal conversion of the terms of the Boolean function. Several computational aids are presented for the purpose of adapting this algorithm to the solution of large-scale problems on a digital computer.It has been observed by many people that a striking number of quite diverse mathematical problems can be formulated as problems in integer programming, that is, linear programming problems in which some or all of the variables are required to assume integral values. This fact is rendered quite interesting by recent research on such problems, notably by R. E. Gomory [2, 3], which gives promise of yielding efficient computational techniques for their solution. The present paper provides yet another example of the versatility of integer programming as a mathematical modeling device by representing a generalization of the well-known “Travelling Salesman Problem” in integer programming terms. The authors have developed several such models, of which the one presented here is the most efficient in terms of generality, number of variables, and number of constraints. This model is due to the second author [4] and was presented briefly at the Symposium on Combinatorial Problems held at Princeton University, April 1960, sponsored by SIAM and IBM. The problem treated is: (1) A salesman is required to visit each of n cities, indexed by 1, ··· , n. He leaves from a “base city” indexed by 0, visits each of the n other cities exactly once, and returns to city 0. During his travels he must return to 0 exactly t times, including his final return (here t may be allowed to vary), and he must visit no more than p cities in one tour. (By a tour we mean a succession of visits to cities without stopping at city 0.) It is required to find such an itinerary which minimizes the total distance traveled by the salesman.Note that if t is fixed, then for the problem to have a solution we must have tp ≧ n. For t = 1, p ≧ n, we have the standard traveling salesman problem.Let dij (i ≠ j = 0, 1, ··· , n) be the distance covered in traveling from city i to city j. The following integer programming problem will be shown to be equivalent to (1): (2) Minimize the linear form ∑0≦i≠j≦n∑ dijxij over the set determined by the relations ∑ni=0i≠j xij = 1 (j = 1, ··· , n) ∑nj=0j≠i xij = 1 (i = 1, ··· , n) ui - uj + pxij ≦ p - 1 (1 ≦ i ≠ j ≦ n) where the xij are non-negative integers and the ui (i = 1, …, n) are arbitrary real numbers. (We shall see that it is permissible to restrict the ui to be non-negative integers as well.)If t is fixed it is necessary to add the additional relation: ∑nu=1 xi0 = t Note that the constraints require that xij = 0 or 1, so that a natural correspondence between these two problems exists if the xij are interpreted as follows: The salesman proceeds from city i to city j if and only if xij = 1. Under this correspondence the form to be minimized in (2) is the total distance to be traveled by the salesman in (1), so the burden of proof is to show that the two feasible sets correspond; i.e., a feasible solution to (2) has xij which do define a legitimate itinerary in (1), and, conversely a legitimate itinerary in (1) defines xij, which, together with appropriate ui, satisfy the constraints of (2).Consider a feasible solution to (2).The number of returns to city 0 is given by ∑ni=1 xi0. The constraints of the form ∑ xij = 1, all xij non-negative integers, represent the conditions that each city (other than zero) is visited exactly once. The ui play a role similar to node potentials in a network and the inequalities involving them serve to eliminate tours that do not begin and end at city 0 and tours that visit more than p cities. Consider any xr0r1 = 1 (r1 ≠ 0). There exists a unique r2 such that xr1r2 = 1. Unless r2 = 0, there is a unique r3 with xr2r3 = 1. We proceed in this fashion until some rj = 0. This must happen since the alternative is that at some point we reach an rk = rj, j + 1 < k. Since none of the r's are zero we have uri - uri + 1 + pxriri + 1 ≦ p - 1 or uri - uri + 1 ≦ - 1. Summing from i = j to k - 1, we have urj - urk = 0 ≦ j + 1 - k, which is a contradiction. Thus all tours include city 0. It remains to observe that no tours is of length greater than p. Suppose such a tour exists, x0r1 , xr1r2 , ··· , xrprp+1 = 1 with all ri ≠ 0. Then, as before, ur1 - urp+1 ≦ - p or urp+1 - ur1 ≧ p.But we have urp+1 - ur1 + pxrp+1r1 ≦ p - 1 or urp+1 - ur1 ≦ p (1 - xrp+1r1) - 1 ≦ p - 1, which is a contradiction.Conversely, if the xij correspond to a legitimate itinerary, it is clear that the ui can be adjusted so that ui = j if city i is the jth city visited in the tour which includes city i, for we then have ui - uj = - 1 if xij = 1, and always ui - uj ≦ p - 1.The above integer program involves n2 + n constraints (if t is not fixed) in n2 + 2n variables. Since the inequality form of constraint is fundamental for integer programming calculations, one may eliminate 2n variables, say the xi0 and x0j, by means of the equation constraints and produce an equivalent problem with n2 + n inequalities and n2 variables.The currently known integer programming procedures are sufficiently regular in their behavior to cast doubt on the heuristic value of machine experiments with our model. However, it seems appropriate to report the results of the five machine experiments we have conducted so far. The solution procedure used was the all-integer algorithm of R. E. Gomory [3] without the ranking procedure he describes.The first three experiments were simple model verification tests on a four-city standard traveling salesman problem with distance matrix [ 20 23 4 30 7 27 25 5 25 3 21 26 ]The first experiment was with a model, now obsolete, using roughly twice as many constraints and variables as the current model (for this problem, 28 constraints in 21 variables). The machine was halted after 4000 pivot steps had failed to produce a solution.The second experiment used the earlier model with the xi0 and x0j eliminated, resulting in a 28-constraint, 15-variable problem. Here the machine produced the optimal solution in 41 pivot steps.The third experiment used the current formulation with the xi0 and x0j eliminated, yielding 13 constraints and 9 variables. The optimal solution was reached in 7 pivot steps.The fourth and fifth experiments were used on a standard ten-city problem, due to Barachet, solved by Dantzig, Johnson and Fulkerson [1]. The current formulation was used, yielding 91 constraints in 81 variables. The fifth problem differed from the fourth only in that the ordering of the rows was altered to attempt to introduce more favorable pivot choices. In each case the machine was stopped after over 250 pivot steps had failed to produce the solution. In each case the last 100 pivot steps had failed to change the value of the objective function.It seems hopeful that more efficient integer programming procedures now under development will yield a satisfactory algorithmic solution to the traveling salesman problem, when applied to this model. In any case, the model serves to illustrate how problems of this sort may be succinctly formulated in integer programming terms.Numerous formulas are available for the computation of the Gamma function [1, 2]. The purpose of this note is to indicate the value of a well-known method that is easily extended for higher accuracy requirements.Using the recursion formula for the Gamma function, &Ggr;(x + 1) = x&Ggr;(x), (1) and Stirling's asymptotic expansion for ln &Ggr;(x) [3], we have ln &Ggr;(x) ∼ (x - 1/2) ln x - x + 1/2 ln 2&pgr; + ∑Nr=1 Cr/x2r-1. (2) It follows that, if k and N are appropriately selected positive integers, &Ggr;(x + 1) can be represented by &Ggr;(x + 1) ∼ √2&pgr; exp (x + k - 1/2) ln (x + k) - (x + k) exp ∑Nr=1 Cr/(x + k)2r-1/(x + 1)(x + 2) ··· (x + k - 1) (3) where Cr = (- 1)r-1 Br/(2r - 1)(2r), Br being the Bernoulli numbers [4]. These coefficients have been published by Uhler [5].Requiring the range 0 ≦ x ≦ 1 is no restriction since, if necessary, &Ggr;(x + 1) can be generated for other arguments using (1).For a given N, the error in (2) can be estimated from |&egr;| < |CN+1|/x2N+1. (4)The curves of Figure 1 show contours of constant error bound as a function of N and x. These curves represent single and double-precision floating-arithmetic requirements of &egr; < 5·10-9 and &egr; < 5·10-17. For a given N, k is defined as the minimum integral x greater than or equal to those on the curves. Then N and k can be chosen to minimize round-off and computing time.For N and k equal to 4, formula (3) yields &Ggr;(x + 1) ∼ &radic2&pgr; exp (x + 4 - 1/2) ln (x + 4) - (x + 4) exp ∑4r=1Cr/(x + 4)2r-1/(x + 1)(x + 2)(x + 3). (5)A similar expression suitable for double precision results for N = 8 and k = 9.The exponents in (5) are split to reduce roundoff. Various algebraic manipulations might result in a further reduction of roundoff.For each item to be sorted by address calculation, a location in the file is determined by a linear formula. It is placed there if the location is empty. If there is an item at the specified location, a search is made to find the closest empty space to this spot. The item at the specified location, together with adjacent items, is moved by a process similar to musical chairs, so that the item to be filed can be entered in its proper order in the file. A generalized flowchart for computer address calculation sorting is presented here. A mathematical analysis using average expectation follows. Formulas are derived to determine the number of computer operations required. Further formulas are derived which determine the time required for an address calculation sort in terms of specific computer orders. Several examples are given. A sorting problem solved elsewhere in the literature by an empirical method is solved by the formulas developed here to demonstrate their practical application.
Two upper bounds for the total path length of binary trees are obtained. One is for node-trees, and bounds the internal (or root-to-node) path length; the other is for leaf-trees, and bounds the external (or root-to-leaf) path length. These bounds involve a quantity called the balance, which allows the bounds to adapt from the n log n behavior of a completely balanced tree to the n2 behavior of a most skewed tree. These bounds are illustrated for the case of Fibonacci trees.Consider the set of multistep formulas ∑l-1jmn-k &agr;ijxmn+j - h ∑l-1jmn-k&bgr;ijxmn+j = 0, i = 1, ···, l, where xmn+j = ymn+j for j= -k, ···, -1 and xn = ƒn = ƒ(xn , tn). These formulas are solved simultaneously for the xmn+j with j = 0, ···, l - 1 in terms of the xmn+j with j = -k, ··· , - 1, which are assumed to be known.Then ymn+j is defined to be xmn+j for j = 0, ··· , m - 1. For j = m, ··· , l - 1, xmn+j is discarded. The set of y's generated in this manner for successive values of n provide an approximate solution of the initial value problem: y = ƒ(y, t), y(t0) = y0. It is conjectured that if the method, which is referred to as the composite multistep method, is A-stable, then its maximum order is 2l. In addition to noting that the conjecture conforms to Dahlquist's bound of 2 for l = 1, the conjecture is verified for k = 1. A third-order A-stable method with m = l = 2 is given as an example, and numerical results established in applying a fourth-order A-stable method with m = 1 and l = 2 are described. A-stable methods with m = l offer the promise of high order and a minimum of function evaluations—evaluation of ƒ(y, t) at solution points. Furthermore, the prospect that such methods might exist with k = 1—only one past point—means that step-size control can be easily implementedTridiagonal linear systems of equations can be solved on conventional serial machines in a time proportional to N, where N is the number of equations. The conventional algorithms do not lend themselves directly to parallel computation on computers of the ILLIAC IV class, in the sense that they appear to be inherently serial. An efficient parallel algorithm is presented in which computation time grows as log2 N. The algorithm is based on recursive doubling solutions of linear recurrence relations, and can be used to solve recurrence relations of all orders.A combinatorial problem arising from the analysis of a model of interleaved memory systems is studied. The performance measure whose calculation defines this problem is based on the distribution of the number of modules in operation during a memory cycle, assuming saturated demand and an arbitrary but fixed number of modules.In general terms the problem is as follows. Suppose we have a Markov chain of n states numbered 0, 1, ···, n - 1. For each i assume that the one-step transition probability from state i to state (i + 1) mod n is given by the parameter &agr; and from state i to any other state is &bgr; = (1 - &agr;)/(n - 1). Given an initial state, the problem is to find the expected number of states through which the system passes before returning to a state previously entered. The principal result of the paper is a recursive procedure for computing this expected number of states. The complexity of the procedure is seen to be small enough to enable practical numerical studies of interleaved memory systems.The problem of multiprogram scheduling on a single processor is studied from the viewpoint of the characteristics peculiar to the program functions that need guaranteed service. It is shown that an optimum fixed priority scheduler possesses an upper bound to processor utilization which may be as low as 70 percent for large task sets. It is also shown that full processor utilization can be achieved by dynamically assigning priorities on the basis of their current deadlines. A combination of these two scheduling techniques is also discussed.A particular decision-theoretic approach to the problem of detecting straight edges and lines in pictures is discussed. A model is proposed of the appearance of scenes consisting of prismatic solids, taking into account blurring, noise, and smooth variations in intensity over faces. A suboptimal statistical decision procedure is developed for the identification of a line within a narrow band in the field of view, given an array of intensity values from within the band. The performance of this procedure is illustrated and discussed.Characterizations of digital “simple arcs” and “simple closed curves” are given. In particular, it is shown that the following are equivalent for sets S having more than four points: (1) S is a simple curve; (2) S is connected and each point of S has exactly two neighbors in S; (3) S is connected, has exactly one hole, and has no deletable points. It follows that if a “shrinking” algorithm is applied to a connected S that has exactly one hole, it shrinks to a simple curve.The problem of finding a minimum number of patterns to exercise the logic elements of a combinational switching net is investigated. Throughout, the word “testing” refers to exercising of this kind; or, equivalently, to fault diagnosis where each line of the net can be directly observed. Any set of permanent faults can be selected to test against, examples of which range from “stuck-at” faults (allowing the most economical test) to “any possible fault” (requiring the most complete test).The method used depends upon exact structural analysis rather than upon search algorithms or random pattern generation. The types of results presented appear to be fundamentally new. In particular, the maximum number of patterns required to test any one of an infinite class of nets is frequently found to be finite and extremely small. For example, any nontrivial connected tree of 2-input nand gates can be tested for “any possible fault” by exactly five patterns—no more and no less.The method in brief: Given a set F of switching functions and a set of required inputs for each (collectively denoted T), a “testing” function is defined for each element of F for each positive integer r. If the lines of a net can be mapped to the domain of the testing functions P(T, r) so that the gates perform consistent with these functions, we say the net “accepts” P(T, r)—and then r patterns are sufficient to test the net for T.Only nets in which each logic element is intended to realize the same switching function are discussed here. Trees (nets without fanout) are studied first, and the conditions under which a tree of identical gates “accepts” a partial function on an arbitrary domain is established. Then the common symmetric switching functions are separately investigated to find for each a minimum value of r such that all trees composed solely of the function accept P(T, r) (for various T). In most cases, as in the example given, the number of patterns required to test any such tree is extremely low.The conditions under which all nets (nontrees included) accept a set of partial functions with arbitrary domain are then established. These conditions are rarely met in practice, even where F consists of a single function. However, many subclasses of nets can be identified which require only a few patterns at most (depending on the function and the class of faults selected). These subclasses often contain nets of arbitrary size and complexity, and frequently consist of exactly those nets for which a related graph can be “colored” (i.e., h-node colored for some particular h) in the classical graph-theoretic sense. For example, any net of 2-input nand gates can be tested by five patterns if one of its related graphs is 2-colorable and another one is 3-colorable (!).The detailed results and methods used to obtain them are summarized, and in conclusion coloring problems and test construction are commented upon.In connection with the development of an actual question-answering system, the Relational Data File of The Rand Corporation, a class of formulas of the predicate calculus, the definite formulas of Kuhns, was proposed as the class of symbolic representations of the “reasonable” questions to put to this retrieval system. Roughly speaking, the definite formulas are those formulas F such that the set of true substitution instances of F in a finite interpretation I are preserved on passage to a certain special extension I′ of I. The proper formulas are those definite formulas which are supposedly especially suitable for machine processing. It has previously been shown by the author that the decision problem for the class of definite formulas is recursively unsolvable. In this paper, however, it is shown that the decision problem for various classes of proper formulas is solvable. Thus, for each of these classes there is a mechanical procedure to decide of an arbitrary formula whether it is a member of the class. It follows that for each of these classes there is no effective transformation 
@@@@ which takes an arbitrary definite formula F into a proper equivalent @@@@(F). In addition, it is shown that the decision problems for a number of classes of formulas which bear a structural similarity to any class of proper formulas are recursively unsolvable.An improved procedure for resolution theorem proving, called Z-resolution, is described. The basic idea of Z-resolution is to “compile” some of the axioms in a deductive problem. This means to automatically transform the selected axioms into a computer program which carries out the inference rules indicated by the axioms. This is done automatically by another program called the specializer. The advantage of doing this is that the compiled axioms run faster, just as a compiled program runs faster than an interpreted program.A proof is given that the inference rule used in Z-resolution is complete, provided that the axioms “compiled” have certain properties.Suppose we are given two disjoint linearly ordered subsets A and B of a linearly ordered set C, say A = {a1 < a2 < ··· < am} and B = {b1 < b2 < ··· < bn}. The problem is to determine the linear ordering of their union (i.e. to merge A and B) by means of a sequence of pairwise comparisons between an element of A and an element of B (which we refer to in the paper as the (m, n) problem). Given any algorithm s to solve the (m, n) problem, we are interested in the maximum number of comparisons Ks(m, n) required under all possible orderings of A ∪ B. An algorithm s is said to be minimax if Ks(m, n) = K(m, n) where K(m, n) = mins Ks(m, n)It is a rather difficult task to determine K(m, n) in general. In this study the authors are only concerned with the minimax over a particular class of merging algorithms. This class includes the tape merge algorithm, the simple binary algorithm, and the generalized binary algorithm.Subtree replacement systems form a broad class of tree-manipulating systems. Systems with the “Church-Rosser property” are appropriate for evaluation or translation processes: the end result of a complete sequence of applications of the rules does not depend on the order in which the rules were applied. Theoretical and practical advantages of such flexibility are sketched. Values or meanings for trees can be defined by simple mathematical systems and then computed by the cheapest available algorithm, however intricate that algorithm may be.We derive sufficient conditions for the Church-Rosser property and discuss their applications to recursive definitions, to the lambda calculus, and to parallel programming. Only the first application is treated in detail. We extend McCarthy's recursive calculus by allowing a choice between call-by-value and call-by-name. We show that recursively defined functions are single-valued despite the nondeterminism of the evaluation algorithm. We also show that these functions solve their defining equations in a “canonical” manner.
We describe an efficient, purely functional implementation of deques with catenation. In addition to being an intriguing problem in its own right, finding a purely functional implementation of catenable deques is required to add certain sophisticated programming constructs to functional programming languages. Our solution has a worst-case running time of O(1) for each push, pop, inject, eject and catenation. The best previously known solution has an O(log*k) time bound for the kth deque operation. Our solution is not only faster but simpler. A key idea used in our result is an algorithmic technique related to the redundant digital representations used to avoid carry propagation in binary counting.The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authorative” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages” that join them together in  the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link-based analysis.In a timestamping system, processors repeatedly choose timestamps so that the order of the timestamps obtained reflects the real-time order in which they were requested. Concurrent timestamping systems permit requests by multiple processors to be issued concurrently; in bounded timestamping systems the sizes of the timestamps and the size and number of shared variables are bounded. An algorithm is wait-free if there exists an a priori bound on the number of steps a processor must take in order to make progress, independent of the action or inaction of other processors. Letting n denote the number of procesors, we construct a simple wait-free bounded concurrent timestamping system    requiring O(n) steps (accesses to shared memory) for a processor to read the current timestamps and determine the order among them, and O(n) steps to generate a timestamp, independent of the actions of the other processors. In addition, we introduce and implement the traceable use abstraction, a new primitive providing “inventory control” over values introduced by processors in the course of an algorithm execution. This abstraction has proved to be of great value in converting unbounded algorithms to bounded ones {Attiya and Rachman 1998; Dwork et al. 1992; 1993].Consider the set   H   of all linear (or affine) transformations between two vector spaces over a finite field F. We study how good   H is as a class of hash functions, namely we consider hashing a set S of size n into a range having the same cardinality n by a randomly chosen function from   H and look at the expected size of the largest hash bucket.   H is a universal class of hash functions for any finite field, but with respect to our measure different fields behave differently.If the finite  field   F has n elements, then there is a bad set S   ⊂ F2 of size n with expected maximal bucket size   H(n1/3). If n is a perfect square, then there is even a bad set with largest bucket size always at least    n. (This is worst possible, since with respect to a universal class of hash functions every set of size n has expected largest bucket size below    n +   1/2.)If, however, we consider the field of  two  elements, then we get much better bounds. The best previously known upper bound on the  expected size of the largest bucket for this class was  O(2   log n). We reduce this upper bound to O(log n log logn). Note that this is not far from the guarantee for a random function. There, the average largest bucket would be &THgr;(log n/ log log n).In the course of our proof we develop a tool which may be of independent interest. Suppose we have a subset S of a vector space D over   Z2, and consider a random linear mapping of  D to a smaller vector space R. If the cardinality of S is larger than   c&egr;|R|log|R|, then with probability 1 - &egr;, the image of S will cover all elements in the range.In this paper, we prove various results about PAC learning in the presence of malicious noise. Our main interest is the sample size behavior of learning algorithms. We prove the first nontrivial sample complexity lower bound in this model by showing that order of &egr;/&Dgr;2 + d/&Dgr; (up to logarithmic factors) examples are necessary for PAC learning any target class of {0,1}-valued functions of VC dimension d, where &egr; is the desired accuracy and &eegr; = &egr;/(1 + &egr;) - &Dgr; the malicious noise rate (it is well known that any nontrivial target class cannot be PAC learned with accuracy &egr; and malicious noise rate &eegr; ≥ &egr;/(1 + &egr;), this irrespective to sample complexity). We also show that this result  cannot be significantly improved in general by presenting efficient learning algorithms for the class of all subsets of d elements and the class of unions of at most d intervals on the real line. This is especialy interesting as we can also show that the popular minimum disagreement strategy needs samples of size d &egr;/&Dgr;2, hence is not optimal with respect to sample size. We then discuss the use of randomized hypotheses. For these the bound &egr;/(1 + &egr;) on the noise rate is no longer true and is replaced by 2&egr;/(1 + 2&egr;). In fact, we present a generic algorithm using randomized hypotheses that can tolerate noise rates slightly larger than &egr;/(1 + &egr;) while using samples of size  d/&egr; as in the noise-free case. Again one observes a quadratic powerlaw (in this case d&egr;/&Dgr;2, &Dgr; = 2&egr;/(1 + 2&egr;) - &eegr;) as &Dgr; goes to zero. We show upper and lower bounds of this order.This paper studies the problem of efficiently schedulling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is “work stealing,” in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies.Specifically, our analysis shows that the expected time to execute a fully strict computation on P processors using our work-stealing scheduler is T1/P + O(T  ∞ , where  T1 is the minimum serial execution time of the multithreaded computation and (T  ∞ is the minimum execution time with an infinite number of processors. Moreover, the space required by the execution is at most S1P, where  S1 is the minimum serial space requirement. We also show that the expected total communication of the algorithm is at most O(PT  ∞ ( 1 +  nd)Smax), where Smax is the size of the largest activation record of any thread and  nd is the maximum number of times that any thread synchronizes with its parent. This communication bound justifies the folk wisdom that work-stealing schedulers are more communication efficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.We develop principles and rules for achieving secrecy properties in security protocols. Our approach is based on traditional classification techniques, and extends those techniques to handle concurrent processes that use shared-key cryptography. The rules have the form of typing rules for a basic concurrent language with cryptographic primitives, the spi calculus. They guarantee that, if a protocol typechecks, then it does not leak its secret inputs.
The problem of synthesizing a procedure from example computations is examined. An algorithm for this task is presented, and its success is considered. To do this, a model of procedures and example computations is introduced, and the class of acceptable examples is defined. The synthesis algorithm is shown to be successful, with respect to the model of procedures and examples, from two perspectives. First, it is shown to be sound, that is, that the procedure synthesized from a set of examples produces the same result as the intended one on the inputs used to generate that set of examples. Second, it is shown to be complete, that is, that for any procedure in the class of procedures, there exists a finite set of examples such that the procedure synthesized behaves as the intended one on all inputs for which the intended one halts.The costs of subsumption algorithms are analyzed by an estimation of the maximal number of unification attempts (worst-case unification complexity) made for deciding whether a clause C subsumes a clause D. For this purpose the clauses C and D are characterized by the following parameters: number of variables in C, number of literals in C, number of literals in D, and maximal length of the literals. The worst-case unification complexity immediately yields a lower bound for the worst-case time complexity.First, two well-known algorithms (Chang-Lee, Stillman) are investigated. Both algorithms are shown to have a very high worst-case time complexity. Then, a new subsumption algorithm is defined, which is based on an analysis of the connection between variables and predicates in C. An upper bound for the worst-case unification complexity of this algorithm, which is much lower than the lower bounds for the two other algorithms, is derived. Examples in which exponential costs are reduced to polynomial costs are discussed. Finally, the asymptotic growth of the worst-case complexity for all discussed algorithms is shown in a table (for several combinations of the parameters).The problem of finding a minimum cardinality feedback vertex set of a directed graph is considered. Of the classic NP-complete problems, this is one of the least understood. Although Karp showed the general problem to be NP-complete, a linear algorithm for its solution on reducible flow graphs was given by Shamir. The class of reducible flow graphs is the only nontrivial class of graphs for which a polynomial-time algorithm to solve this problem is known. The main result of this paper is to present a new class of graphs—the cyclically reducible graphs—for which minimum feedback vertex sets can be found in polynomial time. This class is not restricted to flow graphs, and most small graphs (10 or fewer nodes) fall into this class. The identification of this class is particularly important since there do not exist approximation algorithms for this problem having a provably good worst case performance. Along with the class and a simple polynomial-time algorithm for finding minimum feedback vertex sets of graphs in the class, several related results are presented. It is shown that there is no “forbidden subgraph” characterization of the class and that there is no particular inclusion relationship between this class and the reducible flow graphs. In addition, it is shown that a class of (general) graphs, which are related to the reducible flow graphs, are contained in the cyclically reducible class.Many database systems maintain the consistency of the data by using a locking protocol to restrict access to data items. It has been previously shown that if no information is known about the method of accessing items in the database, then the two-phase protocol is optimal. However, the use of structural information about the database allows development of non-two-phase protocols, called graph protocols, that can potentially increase efficiency. Yannakakis developed a general class of protocols that included many of the graph protocols. Graph protocols either are only usable in certain types of databases or can incur the performance liability of cascading rollback. In this paper, it is demonstrated that if the system has a priori information as to which data items will be locked first by various transactions, a new graph protocol that is outside the previous classes of graph protocols and is applicable to arbitrarily structured databases can be constructed. This new protocol avoids cascading rollback and its accompanying performance degradation, and extends the class of serializable sequences allowed by non-two-phase protocols. This is the first protocol shown to be always as effective as the two-phase protocol, and it can be more effective for certain types of database systems.Parallel algorithms for data compression by textual substitution that are suitable for VLSI implementation are studied. Both “static” and “dynamic” dictionary schemes are considered.The consensus problem involves an asynchronous system of processes, some of which may be unreliable. The problem is for the reliable processes to agree on a binary value. In this paper, it is shown that every protocol for this problem has the possibility of nontermination, even with only one faulty process. By way of contrast, solutions are known for the synchronous case, the “Byzantine Generals” problem.This paper analyzes decomposition properties of a graph that, when they occur, permit a polynomial solution of the traveling salesman problem and a description of the traveling salesman polytope by a system of linear equalities and inequalities. The central notion is that of a 3-edge cutset, namely, a set of 3 edges that, when removed, disconnects the graph. Conversely, our approach can be used to construct classes of graphs for which there exists a polynomial algorithm for the traveling salesman problem. The approach is illustrated on two examples, Halin graphs and prismatic graphs.Criteria for adequacy of a data flow semantics are discussed and Kahn's successful semantics for functional (deterministic) data flow is reviewed. Problems arising from nondeterminism are introduced and the paper's approach to overcoming them is introduced. The approach is based on generalizing the notion of input-output relation, essentially to a partially ordered multiset of input-output histories. The Brock-Ackerman anomalies concerning the input-output relation model of nondeterministic data flow are reviewed, and it is indicated how the proposed approach avoids them. A new anomaly is introduced to motivate the use of multisets. A formal theory of asynchronous processes is then developed. The main result is that the operation of forming a process from a network of component processes is associative. This result shows that the approach is not subject to anomalies such as that of Brock and Ackerman.A distributed computer system that consists of a set of heterogeneous host computers connected in an arbitrary fashion by a communications network is considered. A general model is developed for such a distributed computer system, in which the host computers and the communications network are represented by product-form queuing networks. In this model, a job may be either processed at the host to which it arrives or transferred to another host. In the latter case, a transferred job incurs a communication delay in addition to the queuing delay at the host on which the job is processed. It is assumed that the decision of transferring a job does not depend on the system state, and hence is static in nature. Performance is optimized by determining the load on each host that minimizes the mean job response time. A nonlinear optimization problem is formulated, and the properties of the optimal solution in the special case where the communication delay does not depend on the source-destination pair is shown.Two efficient algorithms that determine the optimal load on each host computer are presented. The first algorithm, called the parametric-study algorithm, generates the optimal solution as a function of the communication time. This algorithm is suited for the study of the effect of the speed of the communications network on the optimal solution. The second algorithm is a single-point algorithm; it yields the optimal solution for given system parameters. Queuing models of host computers, communications networks, and a numerical example are illustrated.Two of the most powerful classes of programs for which interesting decision problems are known to be solvable are the class of finite-memory programs and the class of programs that characterize the Presburger, or semilinear, sets. In this paper, a new class of programs that presents solvable decision problems similar to the other two classes of programs is introduced. However, the programs in the new class are shown to be computationally more powerful (i.e., capable of defining larger sets of input-output relations).A projection of a Boolean function is a function obtained by substituting for each of its variables a variable, the negation of a variable, or a constant. Reducibilities among computational problems under this relation of projection are considered. It is shown that much of what is of everyday relevance in Turing-machine-based complexity theory can be replicated easily and naturally in this elementary framework. Finer distinctions about the computational relationships among natural problems can be made than in previous formulations and some negative results are proved.

The hope that mathematical methods employed in the investigation of formal logic would lead to purely computational methods for obtaining mathematical theorems goes back to Leibniz and has been revived by Peano around the turn of the century and by Hilbert's school in the 1920's. Hilbert, noting that all of classical mathematics could be formalized within quantification theory, declared that the problem of finding an algorithm for determining whether or not a given formula of quantification theory is valid was the central problem of mathematical logic. And indeed, at one time it seemed as if investigations of this “decision” problem were on the verge of success. However, it was shown by Church and by Turing that such an algorithm can not exist. This result led to considerable pessimism regarding the possibility of using modern digital computers in deciding significant mathematical questions. However, recently there has been a revival of interest in the whole question. Specifically, it has been realized that while no decision procedure exists for quantification theory there are many proof procedures available—that is, uniform procedures which will ultimately locate a proof for any formula of quantification theory which is valid but which will usually involve seeking “forever” in the case of a formula which is not valid—and that some of these proof procedures could well turn out to be feasible for use with modern computing machinery.Hao Wang [9] and P. C. Gilmore [3] have each produced working programs which employ proof procedures in quantification theory. Gilmore's program employs a form of a basic theorem of mathematical logic due to Herbrand, and Wang's makes use of a formulation of quantification theory related to those studied by Gentzen. However, both programs encounter decisive difficulties with any but the simplest formulas of quantification theory, in connection with methods of doing propositional calculus. Wang's program, because of its use of Gentzen-like methods, involves exponentiation on the total number of truth-functional connectives, whereas Gilmore's program, using normal forms, involves exponentiation on the number of clauses present. Both methods are superior in many cases to truth table methods which involve exponentiation on the total number of variables present, and represent important initial contributions, but both run into difficulty with some fairly simple examples.In the present paper, a uniform proof procedure for quantification theory is given which is feasible for use with some rather complicated formulas and which does not ordinarily lead to exponentiation. The superiority of the present procedure over those previously available is indicated in part by the fact that a formula on which Gilmore's routine for the IBM 704 causes the machine to computer for 21 minutes without obtaining a result was worked successfully by hand computation using the present method in 30 minutes. Cf. §6, below.It should be mentioned that, before it can be hoped to employ proof procedures for quantification theory in obtaining proofs of theorems belonging to “genuine” mathematics, finite axiomatizations, which are “short,” must be obtained for various branches of mathematics. This last question will not be pursued further here; cf., however, Davis and Putnam [2], where one solution to this problem is given for eleThis paper reports on a novel technique for literature indexing and searching in a mechanized library system. The notion of relevance is taken as the key concept in the theory of information retrieval and a comparative concept of relevance is explicated in terms of the theory of probability. The resulting technique called “Probabilistic Indexing,” allows a computing machine, given a request for information, to make a statistical inference and derive a number (called the “relevance number”) for each document, which is a measure of the probability that the document will satisfy the given request. The result of a search is an ordered list of those documents which satisfy the request ranked according to their probable relevance.The paper goes on to show that whereas in a conventional library system the cross-referencing (“see” and “see also”) is based solely on the “semantical closeness” between index terms, statistical measures of closeness between index terms can be defined and computed. Thus, given an arbitrary request consisting of one (or many) index term(s), a machine can elaborate on it to increase the probability of selecting relevant documents that would not otherwise have been selected.Finally, the paper suggests an interpretation of the whole library problem as one where the request is considered as a clue on the basis of which the library system makes a concatenated statistical inference in order to provide as an output an ordered list of those documents which most probably satisfy the information needs of the user.One of the outstanding problems in the theory of time series analysis is the distribution problem in spectral analysis for small samples. When the number of observations is sufficiently large for the Central Limit Theorem to be applicable, the normal approximation can be used to advantage. In recent years, work has aimed at the discovery of more generally applicable approximate methods and of more rational criteria for the sample size at which the large sample theory becomes useful; the state of the art is summarized in two recent papers by Grenander, Pollak and Slepian [1] and Freiberger and Grenander [2].Consider a sample x = (x1, x2, ··· , xn) of successive values taken from a discrete-valued stationary time series ··· , y-1 , y0 , y1 , ··· , of normally distributed random variables with mean zero and covariance matrix R with elements r&ngr;&mgr; = E(x&ngr;x&mgr;).For stationarity we have: ri,i+&ngr;≡ r&ngr; = E(yiyi+&ngr;), i, &ngr; = 0, ±1, ±2, ···. The Fourier-Stieltjes representation of the covariances is r&ngr; = 1/2&pgr; ∫&pgr;&pgr; ei&lgr;&ngr; dF(&lgr;) (1) where the spectral distribution function F(&lgr;) is bounded and nondecreasing and can, if it is absolutely continuous, be expressed in terms of a spectral density ƒ(&lgr;): F(&lgr;) = ∫&lgr;&pgr;ƒ(&lgr;) (1) The practical determination of ƒ(&lgr;) from observations of the process is effected by the introduction of a quadratic form Q = ∑n&ngr;,&lgr;=1w&ngr;-&mgr;x&ngr;x&mgr; which is taken as an estimate for ƒ(&lgr;); the coefficients w&ngr; can be written in terms of a spectral weight function or “spectral window” w(&lgr;): w&ngr; = 1/2&pgr; ∫&pgr;-&pgr; x(&lgr;)ei&ngr;&lgr; d&lgr;. (4) For details, see the cited references.In order to obtain confidence limits for these estimates Q of the spectral density ƒ(&lgr;), it is important to have methods for computing the distribution of Q. This paper deals with one such method which has proved very efficient for digital computer application.It is well known and shown, for instance, by H. Cramèr [3, p. 118], that the characteristic function of (3), where w&ngr;&mgr; ≡ w&ngr;-&mgr; are elements of a non-negative definite symmetric matrix W, is given by &phgr;(z) = EeizQ = | I - 2izRW | -1/2 = ∏nj=1 (1 - 2i&lgr;jz)-1/2 (5) where the &lgr;j are the n eigenvalues of the matrix product RW. Although RW is not necessarily a symmetric matrix, both R and W are symmetric and non-negative definite, so that all the &lgr;j are real and non-negative. The frequency function g(x) of Q then follows from Fourier's inversion formula: g(x) = 1/2&pgr; ∫∞∞ e-ixz ∫nj=1 (1 - 2iz&lgr;j)-1/2 dz. (6)Several ways have been suggested for evaluating g (x).Slepian [4] obtains a sum of finite integrals by deforming the contour of integration into a set of circles enclosing pairs of the branch points zj = -i/(2&lgr;j) of (6), and collapsing the circles. This method, which was also used in [2], works well when the eigenvalues cluster toward zero, but not otherwise.A method of repeated convolution of the frequency functions of its individual terms to obtain the frequency function of a quadratic form was developed in [5] and programmed for the IBM 650. It was found to be slowly convergent in most cases.Taking the logarithmic derivative of (5) and applying the inverse Fourier transform yields the following singular integral equation for g(x): xg(x) = ∫x0 g(x - y)h(y) dy (7) where h(x) = 1/2 ∑n&ngr;=1 e-i/2&lgr;&ngr;. This observation forms the basis of an ingeneous method [1] for a computational scheme to derive the distribution of Q. The difficulties of obtaining initial values, associated with the high order zero of the frequency function g(x) at x = 0, are solved and the solution of (7) is discussed in detail in [1]. This method is suitable only for large-scale computers of the order of the IBM 704, and becomes, as does the method in [4], slowly convergent when the eigenvalues are densely spaced.Gurland [6] expanded the frequency function in terms of a Laguerre series and ([7]) presented a convergent series for the distribution fucntion using Laguerre polynomials. This method, which was found to converge slowly, formed the basis for the following procedure which has proved fast, accurate and reliable for a variety of problems. Essentially, the Laguerre expansion is now taken around Rice's approximation [8, p. 99], which is a type III distribution with appropriately chosen parameters.The following definition of the Laguerre polynomials Ln(&agr;) (x) = ∑n&ngr;=0 (n+&agr;n-&ngr;)(-x)&ngr;/&ngr;! (8) satisfies the orthogonality relations ∫∞0e-x x&agr;Ln(&agr;) (x) Lm(&agr;) (x) dx = {&Ggr;(&agr; =1)(n+&agr;n); m = nm ≠ n (See Szegö [9]).Replacing x by &lgr;x gives ∫∞0 e-&lgr;xx&agr;L(&agr;)n(&Ggr;&agr;) dx={&Ggr;(&agr;+1/&lgr;&agr;+1) (n+&agr;n);m = nm ≠ n (9) which has the desired weight. The frequency function may now be expanded in a modified Laguerre series g(x) = Kx&agr;e-&lgr;x[c0 + c1L1(&agr;) (&lgr;x) + c2L2(&agr;) (&lgr;x) + …] (10) where K is chosen so that the weight integrates to one: ∫∞0Kx&agr;e-&lgr;x dx = 1, ∴ K = &lgr;&agr;+1/&Ggr;(&agr; + 1).(11) Multiplying both sides of (10) by Ln(&agr;) (&lgr;x) and integrating from 0 to ∞, gives cn = 1/(n + &agr;n) ∫∞0L(&agr;)n(&lgr;x)g(x) dx. (12) Using (8), cn = ∑n&ngr;=0(n + &agr;n - &ngr;)(n + &agr;n(-&lgr;)&ngr;&ngr;!∫∞0x&ngr;9(x)dx cn = ∑∞&ngr;=0&Ggr;(&agr;+1)/&Ggr;(&agr;+&ngr;+1(n&ngr;)(-&lgr;)&ngr;&agr;&ngr; (13) where &agr;&ngr; is the &ngr;th moment of the distribution about zero. Taking the logarithm of the characteristic function (5), log [EeizQ] = - 1/2 ∑nj=1 log (1 - 2i&lgr;jz), and expanding in powers of iz, one obtains for the cumulants of the distribution (Cramèr [3, p. 186])Xn = (n - 1)!2n-1 ∑j&lgr;jn. (14) The cumulants are related to the moments about zero by the relation 1 + &agr;1(iz) + &agr;2/2!(iz)2 + &agr;3/3!(iz)3 + ··· = exp [ &khgr;1(iz) + &khgr;2/2! (iz)2 + ··· ]. (15) The mean and standard deviation of the frequency function are arrived at by equating powers of iz: m = &khgr;1; &sgr;2 = &khgr;2. (16) The weight function will have the same mean and standard deviation if &agr; = m2/&sgr;2 - 1; &lgr; = m/&sgr;2. (17) Using (13) the remembering that &agr;1 = m and &agr;2 = &sgr;2 + m2, one obtains c0 = 1, c1 = 0, c2 = 0.(18) Now (10) becomes g(x) = &lgr;&agr;+1/&Ggr;(&agr; + 1)x&agr;e-&lgr;x [1 + c3L(&agr;)3(&lgr;x) + c4L(&agr;)4(&lgr;x) + ···] (19) which together with (17), (15), (14), (13) and (8), gives the frequency function in terms of the eigenvalues.Szegö [8] showed that a sufficient condition for convergence, when expanding in the form g(x) = ∑∞&ngr; = 0 a&ngr;L(&agr;)&ngr; (x), is g(x) = O(ex/2x-&agr;/2-1/4-&dgr;), &dgr; > 0, x → ∞. In our case, this reduces to ƒ(x) = O(e-x/2x&agr;/2-1/4-&dgr;), &dgr; > 0, x → ∞.But, (Gurland [6]) ƒ(x) = O(e-x/2&lgr;mxn/2-1, &lgr;m = max &lgr;j, x → ∞. Therefore, the series will converge if &lgr;&lgr;m < 1, which is the same restriction imposed by Gurland.For equal eigenvalues, the result is a &khgr;2-distribution with n degrees of freedom, which serves as an estimate of the computational error.1It may be remarked that Toeplitz theory [10] gives the asymptotic distribution of the eigenvalues &lgr;&ngr; and for large enough sample sizes (see [2]) it is sufficient to use these results instead of the exact eigenvalue distribution. In these cases, the present method is particularly efficient.The method of computation proceeds as follows. The desired number of cumulants divided by n! is calculated from equation (14), to give &khgr;n/n! = 2n-1/n ∑j&lgr;jn; after substitution in equation (15), powers of iz are compared to give the moments &agr;&ngr;.The constant K follows from (11), with the gamma function calculated by fitting an eighth order polynomial to &Ggr; (&agr; + 1) in the range 0 ≦ &agr; ≦ 1 and using the recurrence relation &Ggr; (&agr; + 1) = &agr;&Ggr(&agr;) to extend the range to any &agr; > - 1. Next, &agr; and &lgr; are computed and the series tested for convergence. In all applications to spectral analysis tried so far, the product &lgr;&lgr;m has been between 0.5 (for the case of equal eigenvalues) and 0.75, so that there has been no trouble with convergence. Successively higher approximations are now calculated, using intermediate quantities Jn = &Ggr;(&agr; + 1)/&Ggr;(&agr; = n + 1) (-&lgr;)n = (-&lgr;)n/&agr; + 1)(&agr; + 2) ··· (&agr; + n)′ Kn = Jn&agr;n, and, with the binomial coefficients (n &ngr;), cn = ∑∞&ngr;=0 Kn (n &ngr;). The Laguerre polynomials may now be expressed in the form Ln(&agr;) (&lgr;x) = ∑ n&ngr; = 0E(n)&ngr;x&ngr; where E(n)&ngr; = J&ngr; (&ngr; + 1)(&agr; + 2) ··· (&agr; + n/n!, and the final series (19) emerges term by term. The speed of convergence depends how close &lgr;&lgr;m is to 0.5.As a numerical example we present, in the figure, curves for the frequency function (10) computed for a spectral density ƒ(&lgr;) corresponding to white noise (or bandlimited noise with a correlation function sin &pgr;t/&pgr;t sampled at the zeros), a rectangular spectral window w(&lgr;) of bandwidth &pgr;/10 and sample sizes n = 20, 30, 40, 50, 70, 100, 150. The computation time for each curve on the IBM 650 was approximately 9 minutes, except for n = 70, 100 and 150 where the Toeplitz approximation led to equal eigenvalues and a computation time of 1 minute each. The eigenvalues of RW, where both R and W are symmetric but not their product, were computed by Jacobi's method; for orders 20, 30, 40 and 50 all eigenvalues were found in 0.8, 2.7, 6.4 and 12.5 hours respectively.The authors are indebted to Professor Ulf Grenander, formerly of Brown University, now of the University of Stockholm, for stimulating advice.The assignment of prime numbers to the branches of directed or non-directed nets, enables one to construct transition matrices in a numerical, rather than a symbolic, form. The numerical form greatly facilitates the construction of higher-order transition matrices, needed for the topological analysis of a given net. The proposed method is especially useful for the mechanical determination of the non-repeating paths and cycles in the net.The consistency of precedence matrices is studied in the very natural geometric setting of the theory of directed graphs. An elegant recent procedure (Marimont [7]) for checking consistency is further justified by means of a graphical lemma. In addition, the “direction of future work” mentioned in [7] (to which the present communication may be regarded as a sequel) is developed here using graph theoretic methods. This is based on the relationship between the occurrence of directed cycles and the recognition of “strongly connected components” in a directed graph. An algorithm is included for finding these components in any directed graph. This is necessarily more complicated than determining whether there do not exist any directed cycles, i.e., whether or not a given precedence matrix is consistent.A number of papers have been written from time to time about logical counters of a certain type which have quite simple logic and have been variously referred to as Binary Ring Counters, Shift Register Counters, Johnson Counters, etc. To my knowledge, most of these papers confine themselves to certain special cases and usually leave the subject with some speculation as to the possibility of generating periods of any desired length by the use of these special types. The point of view of this paper is to consider all possible counters of this general type to see how one would obtain a particular period. Special emphasis is placed on determining the least number of bits, n, required to produce a given period, K.The rules for counting are as follows. If an n-bit counter is in state (an-1, an-2 ···, a2, a1, a0) at a given time, T, then at T + 1 its state is (bn-1, bn-2, ···, b1, b0) where b0 = an-1, bi = ai-1 + cian-1 for i = 1, 2, ···, n - 1.The a's, b's, and c's are all 0's or 1's, the c's being constants, and the indicated operations are carried out using modulo 2 arithmetic. This is equivalent to considering the state of the counter as an (n - 1)th degree polynomial in X, multiplying said polynomial by X and reducing it modulo m(X), where m(X) is a polynomial of degree n which is relatively prime to X. At time T the state of the counter corresponds to: A(X) = an-1Xn-1 + an-2Xn-2 + ··· + a1X + a0. The polynomial which corresponds to the state of the counter at time T + 1 is obtained by forming X·A (X) and reducing, if necessary, modulo m (X) = Xn + cn-1Xn-1 + cn-2Xn-2 + ··· + c1X + 1.Since an-1·m(X) = 0 mod m(X), X·A(X) = X·A(X)+ an-1m(X) mod m(X), so X·A(X) = (an-2 + cn-1·an-1)Xn-1 + (an-3 + cn-2an-1)Xn-2 + ··· + (a0 + c1an-1)X + an-1 = bn-1Xn-1 + bn-2Xn-2 + ··· + b1X + b0.It is well known that more than one possible period may be obtained depending upon the initial state of the counter. Several examples are given by Young [4]. However, starting with X itself will always yield the longest possible period for any given m(X) and, furthermore, any other periods possible will always be divisors of the major period (Theorem I below). Since these minor periods can always be obtained with moduli of lower degree they are of no real interest here, and throughout the remainder of this paper the expression “period of the counter” will be assumed to refer to the major period.The set of all polynomials whose coefficients are the integers modulo 2 is the polynomial domain GF(2, X), which has among other things unique factorization into primes (irreducibles). If m(X) is in GF(2, X), then GF(2, X) modulo m(X) is a commutative ring. Thus it is closed under multiplication, but it may have proper divisors of zero. However, any element which is relatively prime to m(X) in GF(2, X) has an inverse in GF(2, X)/m(X) [1].
Computational efficiency is a central concern in the design of knowledge representation systems. In order to obtain efficient systems, it has been suggested that one should limit the form of the statements in the knowledge base or use an incomplete inference mechanism. The former approach is often too restrictive for practical applications, whereas the latter leads to uncertainty about exactly what can and cannot be inferred from the knowledge base. We present a third alternative, in which knowledge given in a general representation language is translated (compiled) into a tractable form—allowing for efficient subsequent query answering.We show how propositional logical theories can be compiled into Horn theories that approximate the original information.  The approximations bound the original theory from below and above in terms of logical strength. The procedures are extended to other tractable languages (for example, binary clauses) and to the first-order case. Finally, we demonstrate the generality of our approach by compiling concept descriptions in a general frame-based language into a tractable form.We introduce the concept of unreliable failure detectors and study how they can be used to solve Consensus in asynchronous systems with crash failures. We characterise unreliable failure detectors in terms of two properties—completeness and accuracy. We show that Consensus can be solved even with unreliable failure detectors that make an infinite number of mistakes, and determine which ones can be used to solve Consensus despite any  number of crashes, and which ones require a majority of correct processes. We prove that Consensus and Atomic Broadcast are reducible to each other in asynchronous systems with crash failures; thus, the above results also apply to Atomic Broadcast. A companion paper shows that one of the failure detectors introduced here is the weakest failure detector for solving Consensus [Chandra et al. 1992].The contribution of this paper is two-fold. First, a connection is established between approximating the size of the largest clique in a graph and multi-prover interactive proofs. Second, an efficient multi-prover interactive proof for NP languages is constructed, where the verifier uses very few random bits and communication bits. Last, the connection between cliques and efficient multi-prover interaction proofs, is shown to yield hardness results on the complexity of approximating the size of the largest clique in a graph.Of independent interest is our proof of correctness for the multilinearity test of functions.We present optimal algorithms for sorting on parallel CREW and EREW versions of the pointer machine model. Intuitively, one can view our methods as being based on a parallel mergesort using linked lists rather than arrays (the usual parallel data structure). We also show how to exploit the “locality” of our approach to solve the set expression evaluation problem, a problem with applications to database querying and logic-programming in O(log n) time using O(n) processors. Interestingly, this is an asymptotic improvement over what seems possible using previous techniques.Categorical combinators [Curien 1986/1993; Hardin 1989; Yokouchi 1989] and more recently &lgr;&sgr;-calculus [Abadi 1991; Hardin and Le´vy 1989], have been introduced to provide an explicit treatment of substitutions in the &lgr;-calculus. We reintroduce here the ingredients of these calculi in a self-contained and stepwise way, with a special emphasis on confluence properties. The main new results of the paper with respect to Curien [1986/1993], Hardin [1989], Abadi [1991], and Hardin and Le´vy [1989] are the following:(1) We present a confluent weak calculus of substitutions, where no variable clashes can be feared;
(2) We solve a conjecture raised in Abadi [1991]: &lgr;&sgr;-calculus is not confluent (it is confluent on ground terms only).This  unfortunate result is “repaired” by presenting a confluent version of &lgr;&sgr;-calculus, named the &lgr;Env-caldulus in Hardin and Le´vy [1989], called here the confluent &lgr;&sgr;-calculus.
A statistical model is presented which is useful in the solution of a Fredholm integral equation of the first kind and equivalent to one proposed by Strand and Westwater. The model and the related problem presented here are familiar to statisticians from the study of regression analysis and are essentially, “(GLM): Find the best linear unbiased estimate of &bgr; given the observation y which satisfies y = H&bgr; + e, e distributed as N (0, &Ggr;).” Here y, &bgr; c are vectors, H is an (m + n) × k matrix, and &Ggr; is a certain (m + n) × (m + n) positive definite, known matrix. The main content of the paper is that (GLM) provides an equivalent way of considering the problem of Strand and Westwater and is to be preferred by virtue of the rich store of results available for the study of (GLM) and its intrinsic geometric nature.Efficiently computable a posteriori error bounds are attained by using a posteriori models for bounding roundoff errors in the basic floating-point operations. Forward error bounds are found for inner product and polynomial evaluations. An analysis of the Crout algorithm in solving systems of linear algebraic equations leads to sharper backward a posteriori bounds. The results in the analysis of the iterative refinement give bounds useful in estimating the rate of convergence. Some numerical experiments are included.Compute-output processing times are determined for n-segment jobs that are preloaded into main storage and processed with overlap. A queueing model with tandem servers is utilized for the performance analysis. In particular, the solution presented involves determination of the transient response for a batched arrival of n segments to be processed through two stages of tandem service with unlimited output buffering. The performance results provide insight into conditions arising in systems consisting of a single CPU and I/O channel with overlap capabilities. Two cases, single-segment overlap and unlimited overlap, are considered. Segmental compute and output (or input) service times are taken to be exponentially distributed; however, the approach is not limited to the exponential case if service is independent. The ratio of mean output time to mean compute time is varied to explore the full range between compute-bound and output-bound extremes. Final results are presented as relative gain over sequential processing.The topic of this paper is a probabilistic analysis of demand paging algorithms for storage hierarchies. Two aspects of algorithm performance are studied under the assumption that the sequence of page requests is statistically independent: the page fault probability for a fixed memory size and the variation of performance with memory. Performance bounds are obtained which are independent of the page request probabilities. It is shown that simple algorithms exist which yield fault probabilities close to optimal with only a modest increase in memory.A cost is defined for demand paging algorithms with respect to a formal stochastic model of program behavior. This cost is shown to exist under rather general assumptions, and a computational procedure is given which makes it possible to determine the optimal cost and optimal policy for moderate size programs, when the formal model is known and not time dependent. In this latter case it is shown that these computational procedures may be extended to larger programs to obtain arbitrarily close approximations to their optimal policies. In previous models either unwarranted information is assumed beyond the formal model, or the complete stochastic nature of the model is not taken into account.A class of demand paging algorithms for some two-level memory hierarchies is analyzed. The typical memory hierarchy is comprised of the core and a backing device. A distance matrix characterizes the properties of the latter device. The sequence of address references directed to the hierarchy by the CPU and channels is modeled as a Markov process. A compact expression for the mean time required to satisfy the page demands is derived and this expression provides the basis for some optimization problems concerning partitionings and rearrangements of pages in the backing device. In connection with these problems, a class of random processes is defined in terms of an ordering property of a joint probability matrix which is central to memory hierarchies. Three results are given on the ordering property, its relation specifically to partitionings inherent in hierarchies and the problem of optimal rearrangements. Finally, for such a class of ordered processes, certain results due to the author are specialized to yield the solution to the problem of optimal rearrangement of pages on an assembly of magnetic bubble loops.An analytic model of a single processor scheduling problem is investigated. The scheduling objective is to minimize the total loss incurred by a finite number of initially available requests when each request has an associated linear loss function. The assumptions of the model are that preemption is allowed with negligible loss of processor time, and that the distribution of actual service times is known for each class of requests. A request is associated with a class by any of its characteristics except its actual service time. A contrived example demonstrates that one reasonable scheduling rule does not always minimize expected total loss. The major results of the paper are the definition of a new scheduling rule based on the known service time distributions, and the proof that expected total loss is always minimized by using this new rule. Brief consideration is given to generalizations of the model in which new requests arrive randomly, and preemption requires a non-negligible amount of processor time.Many compilers for higher order languages attempt to translate the source code into “good” object code. Cocke and Schwartz have described an algorithm for discovering when the computation of an expression is redundant (common), and when it can be moved to a less frequently executed region of the program. The present paper includes a tutorial presentation of their basic methods, along with a number of improvements and extensions. These include simplification of the solution method, to save a pass; extension of it to treat the safety constraint, handle multi-entry regions directly, detect additional commonality and code motion after unsafe code motion, and decide where moved code should be put; and combination of the algorithms for commonality and the dead condition, making use of important work of Ken Kennedy.The methods here applied to collecting information for use in code optimization include general algorithms for solving a set of linear equations in Boolean algebra. The algorithms are most useful when the coefficient matrix is sparse.A technique is introduced for analyzing simulations of stochastic systems in the steady state. From the viewpoint of classical statistics, questions of simulation run duration and of starting and stopping simulations are addressed. This is possible because of the existence of a random grouping of observations which produces independent identically distributed blocks from the start of the simulation. The analysis is presented in the context of the general multiserver queue, with arbitrarily distributed interarrival and service times. In this case, it is the busy period structure of the system which produces the grouping mentioned above. Numerical illustrations are given for the M/M/1 queue. Statistical methods are employed so as to obtain confidence intervals for a variety of parameters of interest, such as the expected value of the stationary customer waiting time, the expected value of a function of the stationary waiting time, the expected number of customers served and length of a busy cycle, the tail of the stationary waiting time distribution, and the standard deviation of the stationary waiting time. Consideration is also given to determining system sensitivity to errors and uncertainty in the input parameters.A technique for simulating GI/G/s queues is shown to apply to simulations of discrete and continuous-time Markov chains. It is possible to address questions of simulation run duration and of starting and stopping simulations because of the existence of a random grouping of observations which produces independent identically distributed blocks from the start of the simulation. This grouping allows confidence intervals to be obtained for a general function of the steady-state distribution of the Markov chain. The technique is illustrated with simulation of an (s, S) inventory model in discrete time and the classical repairman problem in continuous time. Consideration is also given to determining system sensitivity to errors and uncertainty in the input parameters.The model elimination (ME) and resolution algorithms for mechanical theorem-proving were implemented so as to maximize shared features. The identical data structures and large amount of common programming permit meaningful comparisons when the two programs are run on standard problems. ME does better on some classes of problems, and resolution better on others. The depth-first search strategy used in this ME implementation affects the performance profoundly. Other novel features in the implementation are new control parameters to govern extensions, and modified rules for generating and rejecting chains. The resolution program incorporates unit preference and set-of-support. An appendix reproduces the steps of a machine-derived ME refutation.Branch-and-bound implicit enumeration algorithms for permutation problems (discrete optimization problems where the set of feasible solutions is the permutation group Sn) are characterized in terms of a sextuple (Bp S,E,D,L,U), where (1) Bp is the branching rule for permutation problems, (2) S is the next node selection rule, (3) E is the set of node elimination rules, (4) D is the node dominance function, (5) L is the node lower-bound cost function, and (6) U is an upper-bound solution cost. A general algorithm based on this characterization is presented and the dependence of the computational requirements on the choice of algorithm parameters, S, E, D, L, and U is investigated theoretically. The results verify some intuitive notions but disprove others.Let r be the total number of cycles required to complete a compromise merge of a given number of initial strings. Define row vectors mr-j and dj whose components represent the number and length respectively of strings at the end of the jth cycle of the merge. It is shown in this paper that there are asymptotic approximations to these vectors, which enables one to compute their respective components directly. Consequently, the number of cycles r can be computed directly, as in the case of the balanced merge.A family of new algorithms is given for evaluating the first m derivatives of a polynomial. In particular, it is shown that all derivatives may be evaluated in 3n - 2 multiplications. The best previous result required 1/2n(n + 1) multiplications. Some optimality results are presented.The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of “edit operations” needed to change the one string into the other. The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings. Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings.
Directory-based coherence protocols in shared-memory multiprocessors are so complex that verification techniques based on automated procedures are required to establish their correctness. State enumeration approaches are well-suited to the verification of cache protocols but they face the problem of state space explosion, leading to unacceptable verification time and memory consumption even for small system configurations. One way to manage this complexity and make the verification feasible is to map the system model to verify onto a symbolic state model (SSM). Since the number of symbolic states is considerably less than the number of system states, an exhaustive state search becomes possible, even for large-scale sytems and complex protocols.In this paper, we develop the  concepts and notations to verifiy some properties of a directory-based protocol designed for non-FIFO interconnection networks. We compare the verification of the protocol with SSM and with the Stanford Mur  4 , a verification tool enumerating system states. We show that SSM is much more efficient in terms of verification time and memory consumption and therefore holds that promise of verifying much more complex protocols. A unique feature of SSM is that it verifies protocols for any system size and therefore provides reliable verification results in one run of the tool.A database query is finite if its result consists of a finite sets tuples. For queries formulated as sets of pure Horn rules, the problem of determining finiteness is, in general, undecidable.In this paper, we consider superfiniteness—a stronger kind of finiteness, which applies to Horn queries whose function symbols are replaced by the abstraction of infinite relations with finiteness constraints (abbr., FC's). We show that superfiniteness is not only decidable but also axiomatizable, and the axiomatization yields an effective decision procedure. Although there are finite queries that are not superfinite, we demonstrate that superfinite queries represent an interesting and  nontrivial subclass within the class of all finite queries.The we turn to the issue of inference of finiteness constraints—an important practical problem that is instrumental in deciding if a query is evaluable by a bottom-up algorithm. Although it is not known whether FC-entailment is decidable for sets of function-free Horn rules, we show that super-entailment, a stronger form of entailment, is decidable. We also show how a decision procedure for super-entailment can be used to enhance tests for query finiteness.Given a collection

 F
 of subsets of S =
{1,…,n}, set
cover is the problem of selecting as few as possible
subsets from   F such that their union covers
S,, and max
k-cover is the problem of selecting
k subsets from
  F such that their union has maximum cardinality. Both these problems are
NP-hard.   We prove that (1 - o(1)) ln
n is a threshold below  

 which set
cover cannot be approximated efficiently, unless NP has slightly
superpolynomial time algorithms. This closes the gap (up to low-order
terms) between the ratio of approximation achievable by the greedy
alogorithm (which is (1 - o(1)) ln
n), and provious results of Lund and Yanakakis, that showed hardness of
approximation within a ratio of 

log2
n/2≃0.72
 ln n. For max
k-cover, we show an approximation
threshold of (1 - 1/e)(up to
low-order terms), under assumption that 

P≠NP
.In this paper, we consider the question of determining whether a function f has property P or is &egr;-far from any function with property P. A property testing algorithm is given a sample of the value of f on instances drawn according to some distribution. In some cases, it is also allowed to query f on instances of its choice. We study this question for different properties and establish some connections to problems in learning theory and approximation.In particular, we focus our attention on testing graph properties. Given access to a graph G in the form of being able to query whether an edge exists or not between a pair of vertices, we devise algorithms to test whether the underlying graph has  properties such as being bipartite, k-Colorable, or having a p-Clique (clique of density p with respect to the vertex set). Our graph property testing algorithms are probabilistic and make assertions that are correct with high probability, while making a number of queries that is independent of the size of the graph. Moreover, the property testing algorithms can be used to efficiently (i.e., in time linear in the number of vertices) construct partitions of the graph that correspond to the property being tested, if it holds for the input graph.
Autoepistemic logic is one of the principal modes of nonmonotonic
reasoning. It unifies several other modes of nonmonotonic reasoning and
has important application in logic programming. In the paper, a theory
of autoepistemic logic is developed. This paper starts with a brief
survey of some of the previously known results. Then, the nature of
nonmonotonicity is studied by investigating how membership of
autoepistemic statements in autoepistemic theories depends on the
underlying objective theory. A notion similar to set-theoretic forcing
is introduced. Expansions of autoepistemic theories are also
investigated. Expansions serve as sets of consequences of an
autoepistemic theory and they can also be used to define semantics for
logic programs with negation. Theories that  have expansions are
characterized, and a normal form that allows the description of all
expansions of a theory is introduced. Our results imply algorithms to
determine whether a theory has a unique expansion. Sufficient conditions
(stratification) that imply existence of a unique expansion are
discussed. The definition of stratified theories is extended and (under
some additional assumptions) efficient algorithms for testing whether a
theory is stratified are proposed. The theorem characterizing expansions
is applied to two classes of theories, K1-theories
and ae-programs. In each case, simple hypergraph characterization of
expansions of theories from each of these classes is given. Finally,
connections with stable model semantics for logic programs with negation
is  discussed. In particular, it is proven that the problem of existence
of stable models is NP-complete.

