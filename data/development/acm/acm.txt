This article is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the ℓ1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.

Let f be a function on a set of variables V. For each x ∈ V, let c(x) be the cost of reading the value of x. An algorithm for evaluating f is a strategy for adaptively identifying and reading a set of variables U ⊆ V whose values uniquely determine the value of f. We are interested in finding algorithms which minimize the cost incurred to evaluate f in the above sense. Competitive analysis is employed to measure the performance of the algorithms. We address two variants of the above problem. We consider the basic model in which the evaluation algorithm knows the cost c(x), for each x ∈ V. We also study a novel model where the costs of the variables are not known in advance and some preemption is allowed in the reading operations. This model has applications, for example, when reading a variable coincides with obtaining the output of a job on a CPU and the cost is the CPU time. For the model where the costs of the variables are known, we present a polynomial time algorithm with the best possible competitive ratio γcf for each function f that is representable by a threshold tree and for each fixed cost function c(⋅). Remarkably, the best-known result for the same class of functions is a pseudo-polynomial algorithm with competitiveness 2 γcf. Still in the same model, we introduce the Linear Programming Approach (LPA), a framework that allows the design of efficient algorithms for evaluating functions. We show that different implementations of this approach lead in general to the best algorithms known so far—and in many cases to optimal algorithms—for different classes of functions considered before in the literature. Via the LPA, we are able to determine exactly the optimal extremal competitiveness of monotone Boolean functions. Remarkably, the upper bound which leads to this result, holds for a much broader class of functions, which also includes the whole set of Boolean functions. We also show how to extend the LPA (together with these results) to the model where the costs of the variables are not known beforehand. In particular, we show how to employ the extended LPA to design a polynomial-time optimal (with respect to competitiveness) algorithm for the class of monotone Boolean functions representable by threshold trees.

We consider Fisher and Arrow--Debreu markets under additively separable, piecewise-linear, concave utility functions and obtain the following results. For both market models, if an equilibrium exists, there is one that is rational and can be written using polynomially many bits. There is no simple necessary and sufficient condition for the existence of an equilibrium: The problem of checking for existence of an equilibrium is NP-complete for both market models; the same holds for existence of an ε-approximate equilibrium, for ε = O(n−5). Under standard (mild) sufficient conditions, the problem of finding an exact equilibrium is in PPAD for both market models. Finally, building on the techniques of Chen et al. [2009a] we prove that under these sufficient conditions, finding an equilibrium for Fisher markets is PPAD-hard.

This article is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the ℓ1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.

This article focuses on computations on large graphs (e.g., the web-graph) where the edges of the graph are presented as a stream. The objective in the streaming model is to use small amount of memory (preferably sub-linear in the number of nodes n) and a smaller number of passes. In the streaming model, we show how to perform several graph computations including estimating the probability distribution after a random walk of length l, the mixing time M, and other related quantities such as the conductance of the graph. By applying our algorithm for computing probability distribution on the web-graph, we can estimate the PageRank p of any node up to an additive error of &sqrt;ε p+ε in Õ(&sqrt;M/α) passes and Õ(min(nα+1/ε&sqrt;M/α+(1/ε)Mα, α n&sqrt;Mα + (1/ε)&sqrt;M/α)) space, for any α ∈ (0,1]. Specifically, for ε = M/n, α = M−1/2, we can compute the approximate PageRank values in Õ(nM−1/4) space and Õ(M3/4) passes. In comparison, a standard implementation of the PageRank algorithm will take O(n) space and O(M) passes. We also give an approach to approximate the PageRank values in just Õ(1) passes although this requires Õ(nM) space.

Locally decodable codes are error-correcting codes that admit efficient decoding algorithms; any bit of the original message can be recovered by looking at only a small number of locations of a corrupted codeword. The tradeoff between the rate of a code and the locality/efficiency of its decoding algorithms has been well studied, and it has widely been suspected that nontrivial locality must come at the price of low rate. A particular setting of potential interest in practice is codes of constant rate. For such codes, decoding algorithms with locality O(k∈) were known only for codes of rate ∈Ω(1/∈), where k is the length of the message. Furthermore, for codes of rate > 1/2, no nontrivial locality had been achieved. In this article, we construct a new family of locally decodable codes that have very efficient local decoding algorithms, and at the same time have rate approaching 1. We show that for every ∈ > 0 and α > 0, for infinitely many k, there exists a code C which encodes messages of length k with rate 1 − α, and is locally decodable from a constant fraction of errors using O(k∈) queries and time. These codes, which we call multiplicity codes, are based on evaluating multivariate polynomials and their derivatives. Multiplicity codes extend traditional multivariate polynomial codes; they inherit the local-decodability of these codes, and at the same time achieve better tradeoffs and flexibility in the rate and minimum distance.

Locally decodable codes are error-correcting codes that admit efficient decoding algorithms; any bit of the original message can be recovered by looking at only a small number of locations of a corrupted codeword. The tradeoff between the rate of a code and the locality/efficiency of its decoding algorithms has been well studied, and it has widely been suspected that nontrivial locality must come at the price of low rate. A particular setting of potential interest in practice is codes of constant rate. For such codes, decoding algorithms with locality O(k∈) were known only for codes of rate ∈Ω(1/∈), where k is the length of the message. Furthermore, for codes of rate > 1/2, no nontrivial locality had been achieved. In this article, we construct a new family of locally decodable codes that have very efficient local decoding algorithms, and at the same time have rate approaching 1. We show that for every ∈ > 0 and α > 0, for infinitely many k, there exists a code C which encodes messages of length k with rate 1 − α, and is locally decodable from a constant fraction of errors using O(k∈) queries and time. These codes, which we call multiplicity codes, are based on evaluating multivariate polynomials and their derivatives. Multiplicity codes extend traditional multivariate polynomial codes; they inherit the local-decodability of these codes, and at the same time achieve better tradeoffs and flexibility in the rate and minimum distance.

We study the problem of “privacy amplification”: key agreement between two parties who both know a weak secret w, such as a password. (Such a setting is ubiquitous on the internet, where passwords are the most commonly used security device.) We assume that the key agreement protocol is taking place in the presence of an active computationally unbounded adversary Eve. The adversary may have partial knowledge about w, so we assume only that w has some entropy from Eve’s point of view. Thus, the goal of the protocol is to convert this nonuniform secret w into a uniformly distributed string R that is fully secret from Eve. R may then be used as a key for running symmetric cryptographic protocols (such as encryption, authentication, etc.). Because we make no computational assumptions, the entropy in R can come only from w. Thus, such a protocol must minimize the entropy loss during its execution, so that R is as long as possible. The best previous results have entropy loss of Θ(κ2), where κ is the security parameter, thus requiring the password to be very long even for small values of κ. In this work, we present the first protocol for information-theoretic key agreement that has entropy loss linear in the security parameter. The result is optimal up to constant factors. We achieve our improvement through a somewhat surprising application of error-correcting codes for the edit distance. The protocol can be extended to provide also “information reconciliation,” that is, to work even when the two parties have slightly different versions of w (e.g., when biometrics are involved).

Today’s hardware technology presents a new challenge in designing robust systems. Deep submicron VLSI technology introduces transient and permanent faults that were never considered in low-level system designs in the past. Still, robustness of that part of the system is crucial and needs to be guaranteed for any successful product. Distributed systems, on the other hand, have been dealing with similar issues for decades. However, neither the basic abstractions nor the complexity of contemporary fault-tolerant distributed algorithms match the peculiarities of hardware implementations. This article is intended to be part of an attempt striving to bridge over this gap between theory and practice for the clock synchronization problem. Solving this task sufficiently well will allow to build an ultra-robust high-precision clocking system for hardware designs like systems-on-chips in critical applications. As our first building block, we describe and prove correct a novel distributed, Byzantine fault-tolerant, probabilistically self-stabilizing pulse synchronization protocol, called FATAL, that can be implemented using standard asynchronous digital logic: Correct FATAL nodes are guaranteed to generate pulses (i.e., unnumbered clock ticks) in a synchronized way, despite a certain fraction of nodes being faulty. FATAL uses randomization only during stabilization and, despite the strict limitations introduced by hardware designs, offers optimal resilience and smaller complexity than all existing protocols. Finally, we show how to leverage FATAL to efficiently generate synchronized, self-stabilizing, high-frequency clocks.

A Markov sequence is a basic statistical model representing uncertain sequential data, and it is used within a plethora of applications, including speech recognition, image processing, computational biology, radio-frequency identification (RFID), and information extraction. The problem of querying a Markov sequence is studied under the conventional semantics of querying a probabilistic database, where queries are formulated as finite-state transducers. Specifically, the complexity of two main problems is analyzed. The first problem is that of computing the confidence (probability) of an answer. The second is the enumeration of the answers in the order of decreasing confidence (with the generation of the top-k answers as a special case), or in an approximate order thereof. In particular, it is shown that enumeration in any subexponential-approximate order is generally intractable (even for some fixed transducers), and a matching upper bound is obtained through a proposed heuristic. Due to this hardness, a special consideration is given to restricted (yet common) classes of transducers that extract matches of a regular expression (subject to prefix and suffix constraints), and it is shown that these classes are, indeed, significantly more tractable.

The problem of multiprogram scheduling on a single processor is studied from the viewpoint of the characteristics peculiar to the program functions that need guaranteed service. It is shown that an optimum fixed priority scheduler possesses an upper bound to processor utilization which may be as low as 70 percent for large task sets. It is also shown that full processor utilization can be achieved by dynamically assigning priorities on the basis of their current deadlines. A combination of these two scheduling techniques is also discussed.

Two upper bounds for the total path length of binary trees are obtained. One is for node-trees, and bounds the internal (or root-to-node) path length; the other is for leaf-trees, and bounds the external (or root-to-leaf) path length. These bounds involve a quantity called the balance, which allows the bounds to adapt from the n log n behavior of a completely balanced tree to the n2 behavior of a most skewed tree. These bounds are illustrated for the case of Fibonacci trees.

Consider the set of multistep formulas ∑l-1jmn-k &agr;ijxmn+j - h ∑l-1jmn-k&bgr;ijxmn+j = 0, i = 1, ···, l, where xmn+j = ymn+j for j= -k, ···, -1 and xn = ƒn = ƒ(xn , tn). These formulas are solved simultaneously for the xmn+j with j = 0, ···, l - 1 in terms of the xmn+j with j = -k, ··· , - 1, which are assumed to be known. Then ymn+j is defined to be xmn+j for j = 0, ··· , m - 1. For j = m, ··· , l - 1, xmn+j is discarded. The set of y's generated in this manner for successive values of n provide an approximate solution of the initial value problem: y = ƒ(y, t), y(t0) = y0. It is conjectured that if the method, which is referred to as the composite multistep method, is A-stable, then its maximum order is 2l. In addition to noting that the conjecture conforms to Dahlquist's bound of 2 for l = 1, the conjecture is verified for k = 1. A third-order A-stable method with m = l = 2 is given as an example, and numerical results established in applying a fourth-order A-stable method with m = 1 and l = 2 are described. A-stable methods with m = l offer the promise of high order and a minimum of function evaluations—evaluation of ƒ(y, t) at solution points. Furthermore, the prospect that such methods might exist with k = 1—only one past point—means that step-size control can be easily implemented

Tridiagonal linear systems of equations can be solved on conventional serial machines in a time proportional to N, where N is the number of equations. The conventional algorithms do not lend themselves directly to parallel computation on computers of the ILLIAC IV class, in the sense that they appear to be inherently serial. An efficient parallel algorithm is presented in which computation time grows as log2 N. The algorithm is based on recursive doubling solutions of linear recurrence relations, and can be used to solve recurrence relations of all orders.

A combinatorial problem arising from the analysis of a model of interleaved memory systems is studied. The performance measure whose calculation defines this problem is based on the distribution of the number of modules in operation during a memory cycle, assuming saturated demand and an arbitrary but fixed number of modules.
In general terms the problem is as follows. Suppose we have a Markov chain of n states numbered 0, 1, ···, n - 1. For each i assume that the one-step transition probability from state i to state (i + 1) mod n is given by the parameter &agr; and from state i to any other state is &bgr; = (1 - &agr;)/(n - 1). Given an initial state, the problem is to find the expected number of states through which the system passes before returning to a state previously entered. The principal result of the paper is a recursive procedure for computing this expected number of states. The complexity of the procedure is seen to be small enough to enable practical numerical studies of interleaved memory systems.

The problem of multiprogram scheduling on a single processor is studied from the viewpoint of the characteristics peculiar to the program functions that need guaranteed service. It is shown that an optimum fixed priority scheduler possesses an upper bound to processor utilization which may be as low as 70 percent for large task sets. It is also shown that full processor utilization can be achieved by dynamically assigning priorities on the basis of their current deadlines. A combination of these two scheduling techniques is also discussed.

A particular decision-theoretic approach to the problem of detecting straight edges and lines in pictures is discussed. A model is proposed of the appearance of scenes consisting of prismatic solids, taking into account blurring, noise, and smooth variations in intensity over faces. A suboptimal statistical decision procedure is developed for the identification of a line within a narrow band in the field of view, given an array of intensity values from within the band. The performance of this procedure is illustrated and discussed.

Characterizations of digital “simple arcs” and “simple closed curves” are given. In particular, it is shown that the following are equivalent for sets S having more than four points: (1) S is a simple curve; (2) S is connected and each point of S has exactly two neighbors in S; (3) S is connected, has exactly one hole, and has no deletable points. It follows that if a “shrinking” algorithm is applied to a connected S that has exactly one hole, it shrinks to a simple curve.

The problem of finding a minimum number of patterns to exercise the logic elements of a combinational switching net is investigated. Throughout, the word “testing” refers to exercising of this kind; or, equivalently, to fault diagnosis where each line of the net can be directly observed. Any set of permanent faults can be selected to test against, examples of which range from “stuck-at” faults (allowing the most economical test) to “any possible fault” (requiring the most complete test).
The method used depends upon exact structural analysis rather than upon search algorithms or random pattern generation. The types of results presented appear to be fundamentally new. In particular, the maximum number of patterns required to test any one of an infinite class of nets is frequently found to be finite and extremely small. For example, any nontrivial connected tree of 2-input nand gates can be tested for “any possible fault” by exactly five patterns—no more and no less.
The method in brief: Given a set F of switching functions and a set of required inputs for each (collectively denoted T), a “testing” function is defined for each element of F for each positive integer r. If the lines of a net can be mapped to the domain of the testing functions P(T, r) so that the gates perform consistent with these functions, we say the net “accepts” P(T, r)—and then r patterns are sufficient to test the net for T.
 Only nets in which each logic element is intended to realize the same switching function are discussed here. Trees (nets without fanout) are studied first, and the conditions under which a tree of identical gates “accepts” a partial function on an arbitrary domain is established. Then the common symmetric switching functions are separately investigated to find for each a minimum value of r such that all trees composed solely of the function accept P(T, r) (for various T). In most cases, as in the example given, the number of patterns required to test any such tree is extremely low.
The conditions under which all nets (nontrees included) accept a set of partial functions with arbitrary domain are then established. These conditions are rarely met in practice, even where F consists of a single function. However, many subclasses of nets can be identified which require only a few patterns at most (depending on the function and the class of faults selected). These subclasses often contain nets of arbitrary size and complexity, and frequently consist of exactly those nets for which a related graph can be “colored” (i.e., h-node colored for some particular h) in the classical graph-theoretic sense. For example, any net of 2-input nand gates can be tested by five patterns if one of its related graphs is 2-colorable and another one is 3-colorable (!).
The detailed results and methods used to obtain them are summarized, and in conclusion coloring problems and test construction are commented upon.

In connection with the development of an actual question-answering system, the Relational Data File of The Rand Corporation, a class of formulas of the predicate calculus, the definite formulas of Kuhns, was proposed as the class of symbolic representations of the “reasonable” questions to put to this retrieval system. Roughly speaking, the definite formulas are those formulas F such that the set of true substitution instances of F in a finite interpretation I are preserved on passage to a certain special extension I′ of I. The proper formulas are those definite formulas which are supposedly especially suitable for machine processing. It has previously been shown by the author that the decision problem for the class of definite formulas is recursively unsolvable. In this paper, however, it is shown that the decision problem for various classes of proper formulas is solvable. Thus, for each of these classes there is a mechanical procedure to decide of an arbitrary formula whether it is a member of the class. It follows that for each of these classes there is no effective transformation 
@@@@ which takes an arbitrary definite formula F into a proper equivalent @@@@(F). In addition, it is shown that the decision problems for a number of classes of formulas which bear a structural similarity to any class of proper formulas are recursively unsolvable.

An improved procedure for resolution theorem proving, called Z-resolution, is described. The basic idea of Z-resolution is to “compile” some of the axioms in a deductive problem. This means to automatically transform the selected axioms into a computer program which carries out the inference rules indicated by the axioms. This is done automatically by another program called the specializer. The advantage of doing this is that the compiled axioms run faster, just as a compiled program runs faster than an interpreted program.
A proof is given that the inference rule used in Z-resolution is complete, provided that the axioms “compiled” have certain properties.

Suppose we are given two disjoint linearly ordered subsets A and B of a linearly ordered set C, say A = {a1 < a2 < ··· < am} and B = {b1 < b2 < ··· < bn}. The problem is to determine the linear ordering of their union (i.e. to merge A and B) by means of a sequence of pairwise comparisons between an element of A and an element of B (which we refer to in the paper as the (m, n) problem). Given any algorithm s to solve the (m, n) problem, we are interested in the maximum number of comparisons Ks(m, n) required under all possible orderings of A ∪ B. An algorithm s is said to be minimax if Ks(m, n) = K(m, n) where K(m, n) = mins Ks(m, n)
It is a rather difficult task to determine K(m, n) in general. In this study the authors are only concerned with the minimax over a particular class of merging algorithms. This class includes the tape merge algorithm, the simple binary algorithm, and the generalized binary algorithm.

Subtree replacement systems form a broad class of tree-manipulating systems. Systems with the “Church-Rosser property” are appropriate for evaluation or translation processes: the end result of a complete sequence of applications of the rules does not depend on the order in which the rules were applied. Theoretical and practical advantages of such flexibility are sketched. Values or meanings for trees can be defined by simple mathematical systems and then computed by the cheapest available algorithm, however intricate that algorithm may be.
We derive sufficient conditions for the Church-Rosser property and discuss their applications to recursive definitions, to the lambda calculus, and to parallel programming. Only the first application is treated in detail. We extend McCarthy's recursive calculus by allowing a choice between call-by-value and call-by-name. We show that recursively defined functions are single-valued despite the nondeterminism of the evaluation algorithm. We also show that these functions solve their defining equations in a “canonical” manner.

Today’s hardware technology presents a new challenge in designing robust systems. Deep submicron VLSI technology introduces transient and permanent faults that were never considered in low-level system designs in the past. Still, robustness of that part of the system is crucial and needs to be guaranteed for any successful product. Distributed systems, on the other hand, have been dealing with similar issues for decades. However, neither the basic abstractions nor the complexity of contemporary fault-tolerant distributed algorithms match the peculiarities of hardware implementations. This article is intended to be part of an attempt striving to bridge over this gap between theory and practice for the clock synchronization problem. Solving this task sufficiently well will allow to build an ultra-robust high-precision clocking system for hardware designs like systems-on-chips in critical applications. As our first building block, we describe and prove correct a novel distributed, Byzantine fault-tolerant, probabilistically self-stabilizing pulse synchronization protocol, called FATAL, that can be implemented using standard asynchronous digital logic: Correct FATAL nodes are guaranteed to generate pulses (i.e., unnumbered clock ticks) in a synchronized way, despite a certain fraction of nodes being faulty. FATAL uses randomization only during stabilization and, despite the strict limitations introduced by hardware designs, offers optimal resilience and smaller complexity than all existing protocols. Finally, we show how to leverage FATAL to efficiently generate synchronized, self-stabilizing, high-frequency clocks.

Locally decodable codes are error-correcting codes that admit efficient decoding algorithms; any bit of the original message can be recovered by looking at only a small number of locations of a corrupted codeword. The tradeoff between the rate of a code and the locality/efficiency of its decoding algorithms has been well studied, and it has widely been suspected that nontrivial locality must come at the price of low rate. A particular setting of potential interest in practice is codes of constant rate. For such codes, decoding algorithms with locality O(k∈) were known only for codes of rate ∈Ω(1/∈), where k is the length of the message. Furthermore, for codes of rate > 1/2, no nontrivial locality had been achieved. In this article, we construct a new family of locally decodable codes that have very efficient local decoding algorithms, and at the same time have rate approaching 1. We show that for every ∈ > 0 and α > 0, for infinitely many k, there exists a code C which encodes messages of length k with rate 1 − α, and is locally decodable from a constant fraction of errors using O(k∈) queries and time. These codes, which we call multiplicity codes, are based on evaluating multivariate polynomials and their derivatives. Multiplicity codes extend traditional multivariate polynomial codes; they inherit the local-decodability of these codes, and at the same time achieve better tradeoffs and flexibility in the rate and minimum distance.

We study the problem of “privacy amplification”: key agreement between two parties who both know a weak secret w, such as a password. (Such a setting is ubiquitous on the internet, where passwords are the most commonly used security device.) We assume that the key agreement protocol is taking place in the presence of an active computationally unbounded adversary Eve. The adversary may have partial knowledge about w, so we assume only that w has some entropy from Eve’s point of view. Thus, the goal of the protocol is to convert this nonuniform secret w into a uniformly distributed string R that is fully secret from Eve. R may then be used as a key for running symmetric cryptographic protocols (such as encryption, authentication, etc.). Because we make no computational assumptions, the entropy in R can come only from w. Thus, such a protocol must minimize the entropy loss during its execution, so that R is as long as possible. The best previous results have entropy loss of Θ(κ2), where κ is the security parameter, thus requiring the password to be very long even for small values of κ. In this work, we present the first protocol for information-theoretic key agreement that has entropy loss linear in the security parameter. The result is optimal up to constant factors. We achieve our improvement through a somewhat surprising application of error-correcting codes for the edit distance. The protocol can be extended to provide also “information reconciliation,” that is, to work even when the two parties have slightly different versions of w (e.g., when biometrics are involved).

Today’s hardware technology presents a new challenge in designing robust systems. Deep submicron VLSI technology introduces transient and permanent faults that were never considered in low-level system designs in the past. Still, robustness of that part of the system is crucial and needs to be guaranteed for any successful product. Distributed systems, on the other hand, have been dealing with similar issues for decades. However, neither the basic abstractions nor the complexity of contemporary fault-tolerant distributed algorithms match the peculiarities of hardware implementations. This article is intended to be part of an attempt striving to bridge over this gap between theory and practice for the clock synchronization problem. Solving this task sufficiently well will allow to build an ultra-robust high-precision clocking system for hardware designs like systems-on-chips in critical applications. As our first building block, we describe and prove correct a novel distributed, Byzantine fault-tolerant, probabilistically self-stabilizing pulse synchronization protocol, called FATAL, that can be implemented using standard asynchronous digital logic: Correct FATAL nodes are guaranteed to generate pulses (i.e., unnumbered clock ticks) in a synchronized way, despite a certain fraction of nodes being faulty. FATAL uses randomization only during stabilization and, despite the strict limitations introduced by hardware designs, offers optimal resilience and smaller complexity than all existing protocols. Finally, we show how to leverage FATAL to efficiently generate synchronized, self-stabilizing, high-frequency clocks.

A Markov sequence is a basic statistical model representing uncertain sequential data, and it is used within a plethora of applications, including speech recognition, image processing, computational biology, radio-frequency identification (RFID), and information extraction. The problem of querying a Markov sequence is studied under the conventional semantics of querying a probabilistic database, where queries are formulated as finite-state transducers. Specifically, the complexity of two main problems is analyzed. The first problem is that of computing the confidence (probability) of an answer. The second is the enumeration of the answers in the order of decreasing confidence (with the generation of the top-k answers as a special case), or in an approximate order thereof. In particular, it is shown that enumeration in any subexponential-approximate order is generally intractable (even for some fixed transducers), and a matching upper bound is obtained through a proposed heuristic. Due to this hardness, a special consideration is given to restricted (yet common) classes of transducers that extract matches of a regular expression (subject to prefix and suffix constraints), and it is shown that these classes are, indeed, significantly more tractable.

A Markov sequence is a basic statistical model representing uncertain sequential data, and it is used within a plethora of applications, including speech recognition, image processing, computational biology, radio-frequency identification (RFID), and information extraction. The problem of querying a Markov sequence is studied under the conventional semantics of querying a probabilistic database, where queries are formulated as finite-state transducers. Specifically, the complexity of two main problems is analyzed. The first problem is that of computing the confidence (probability) of an answer. The second is the enumeration of the answers in the order of decreasing confidence (with the generation of the top-k answers as a special case), or in an approximate order thereof. In particular, it is shown that enumeration in any subexponential-approximate order is generally intractable (even for some fixed transducers), and a matching upper bound is obtained through a proposed heuristic. Due to this hardness, a special consideration is given to restricted (yet common) classes of transducers that extract matches of a regular expression (subject to prefix and suffix constraints), and it is shown that these classes are, indeed, significantly more tractable.

Locally decodable codes are error-correcting codes that admit efficient decoding algorithms; any bit of the original message can be recovered by looking at only a small number of locations of a corrupted codeword. The tradeoff between the rate of a code and the locality/efficiency of its decoding algorithms has been well studied, and it has widely been suspected that nontrivial locality must come at the price of low rate. A particular setting of potential interest in practice is codes of constant rate. For such codes, decoding algorithms with locality O(k∈) were known only for codes of rate ∈Ω(1/∈), where k is the length of the message. Furthermore, for codes of rate > 1/2, no nontrivial locality had been achieved. In this article, we construct a new family of locally decodable codes that have very efficient local decoding algorithms, and at the same time have rate approaching 1. We show that for every ∈ > 0 and α > 0, for infinitely many k, there exists a code C which encodes messages of length k with rate 1 − α, and is locally decodable from a constant fraction of errors using O(k∈) queries and time. These codes, which we call multiplicity codes, are based on evaluating multivariate polynomials and their derivatives. Multiplicity codes extend traditional multivariate polynomial codes; they inherit the local-decodability of these codes, and at the same time achieve better tradeoffs and flexibility in the rate and minimum distance.

We study the problem of “privacy amplification”: key agreement between two parties who both know a weak secret w, such as a password. (Such a setting is ubiquitous on the internet, where passwords are the most commonly used security device.) We assume that the key agreement protocol is taking place in the presence of an active computationally unbounded adversary Eve. The adversary may have partial knowledge about w, so we assume only that w has some entropy from Eve’s point of view. Thus, the goal of the protocol is to convert this nonuniform secret w into a uniformly distributed string R that is fully secret from Eve. R may then be used as a key for running symmetric cryptographic protocols (such as encryption, authentication, etc.). Because we make no computational assumptions, the entropy in R can come only from w. Thus, such a protocol must minimize the entropy loss during its execution, so that R is as long as possible. The best previous results have entropy loss of Θ(κ2), where κ is the security parameter, thus requiring the password to be very long even for small values of κ. In this work, we present the first protocol for information-theoretic key agreement that has entropy loss linear in the security parameter. The result is optimal up to constant factors. We achieve our improvement through a somewhat surprising application of error-correcting codes for the edit distance. The protocol can be extended to provide also “information reconciliation,” that is, to work even when the two parties have slightly different versions of w (e.g., when biometrics are involved).

Today’s hardware technology presents a new challenge in designing robust systems. Deep submicron VLSI technology introduces transient and permanent faults that were never considered in low-level system designs in the past. Still, robustness of that part of the system is crucial and needs to be guaranteed for any successful product. Distributed systems, on the other hand, have been dealing with similar issues for decades. However, neither the basic abstractions nor the complexity of contemporary fault-tolerant distributed algorithms match the peculiarities of hardware implementations. This article is intended to be part of an attempt striving to bridge over this gap between theory and practice for the clock synchronization problem. Solving this task sufficiently well will allow to build an ultra-robust high-precision clocking system for hardware designs like systems-on-chips in critical applications. As our first building block, we describe and prove correct a novel distributed, Byzantine fault-tolerant, probabilistically self-stabilizing pulse synchronization protocol, called FATAL, that can be implemented using standard asynchronous digital logic: Correct FATAL nodes are guaranteed to generate pulses (i.e., unnumbered clock ticks) in a synchronized way, despite a certain fraction of nodes being faulty. FATAL uses randomization only during stabilization and, despite the strict limitations introduced by hardware designs, offers optimal resilience and smaller complexity than all existing protocols. Finally, we show how to leverage FATAL to efficiently generate synchronized, self-stabilizing, high-frequency clocks.

A Markov sequence is a basic statistical model representing uncertain sequential data, and it is used within a plethora of applications, including speech recognition, image processing, computational biology, radio-frequency identification (RFID), and information extraction. The problem of querying a Markov sequence is studied under the conventional semantics of querying a probabilistic database, where queries are formulated as finite-state transducers. Specifically, the complexity of two main problems is analyzed. The first problem is that of computing the confidence (probability) of an answer. The second is the enumeration of the answers in the order of decreasing confidence (with the generation of the top-k answers as a special case), or in an approximate order thereof. In particular, it is shown that enumeration in any subexponential-approximate order is generally intractable (even for some fixed transducers), and a matching upper bound is obtained through a proposed heuristic. Due to this hardness, a special consideration is given to restricted (yet common) classes of transducers that extract matches of a regular expression (subject to prefix and suffix constraints), and it is shown that these classes are, indeed, significantly more tractable.

The consensus problem involves an asynchronous system of processes, some of which may be unreliable. The problem is for the reliable processes to agree on a binary value. In this paper, it is shown that every protocol for this problem has the possibility of nontermination, even with only one faulty process. By way of contrast, solutions are known for the synchronous case, the “Byzantine Generals” problem.

The problem of synthesizing a procedure from example computations is examined. An algorithm for this task is presented, and its success is considered. To do this, a model of procedures and example computations is introduced, and the class of acceptable examples is defined. The synthesis algorithm is shown to be successful, with respect to the model of procedures and examples, from two perspectives. First, it is shown to be sound, that is, that the procedure synthesized from a set of examples produces the same result as the intended one on the inputs used to generate that set of examples. Second, it is shown to be complete, that is, that for any procedure in the class of procedures, there exists a finite set of examples such that the procedure synthesized behaves as the intended one on all inputs for which the intended one halts.

The costs of subsumption algorithms are analyzed by an estimation of the maximal number of unification attempts (worst-case unification complexity) made for deciding whether a clause C subsumes a clause D. For this purpose the clauses C and D are characterized by the following parameters: number of variables in C, number of literals in C, number of literals in D, and maximal length of the literals. The worst-case unification complexity immediately yields a lower bound for the worst-case time complexity.
First, two well-known algorithms (Chang-Lee, Stillman) are investigated. Both algorithms are shown to have a very high worst-case time complexity. Then, a new subsumption algorithm is defined, which is based on an analysis of the connection between variables and predicates in C. An upper bound for the worst-case unification complexity of this algorithm, which is much lower than the lower bounds for the two other algorithms, is derived. Examples in which exponential costs are reduced to polynomial costs are discussed. Finally, the asymptotic growth of the worst-case complexity for all discussed algorithms is shown in a table (for several combinations of the parameters).

The problem of finding a minimum cardinality feedback vertex set of a directed graph is considered. Of the classic NP-complete problems, this is one of the least understood. Although Karp showed the general problem to be NP-complete, a linear algorithm for its solution on reducible flow graphs was given by Shamir. The class of reducible flow graphs is the only nontrivial class of graphs for which a polynomial-time algorithm to solve this problem is known. The main result of this paper is to present a new class of graphs—the cyclically reducible graphs—for which minimum feedback vertex sets can be found in polynomial time. This class is not restricted to flow graphs, and most small graphs (10 or fewer nodes) fall into this class. The identification of this class is particularly important since there do not exist approximation algorithms for this problem having a provably good worst case performance. Along with the class and a simple polynomial-time algorithm for finding minimum feedback vertex sets of graphs in the class, several related results are presented. It is shown that there is no “forbidden subgraph” characterization of the class and that there is no particular inclusion relationship between this class and the reducible flow graphs. In addition, it is shown that a class of (general) graphs, which are related to the reducible flow graphs, are contained in the cyclically reducible class.

Many database systems maintain the consistency of the data by using a locking protocol to restrict access to data items. It has been previously shown that if no information is known about the method of accessing items in the database, then the two-phase protocol is optimal. However, the use of structural information about the database allows development of non-two-phase protocols, called graph protocols, that can potentially increase efficiency. Yannakakis developed a general class of protocols that included many of the graph protocols. Graph protocols either are only usable in certain types of databases or can incur the performance liability of cascading rollback. In this paper, it is demonstrated that if the system has a priori information as to which data items will be locked first by various transactions, a new graph protocol that is outside the previous classes of graph protocols and is applicable to arbitrarily structured databases can be constructed. This new protocol avoids cascading rollback and its accompanying performance degradation, and extends the class of serializable sequences allowed by non-two-phase protocols. This is the first protocol shown to be always as effective as the two-phase protocol, and it can be more effective for certain types of database systems.

Parallel algorithms for data compression by textual substitution that are suitable for VLSI implementation are studied. Both “static” and “dynamic” dictionary schemes are considered.

The consensus problem involves an asynchronous system of processes, some of which may be unreliable. The problem is for the reliable processes to agree on a binary value. In this paper, it is shown that every protocol for this problem has the possibility of nontermination, even with only one faulty process. By way of contrast, solutions are known for the synchronous case, the “Byzantine Generals” problem.

This paper analyzes decomposition properties of a graph that, when they occur, permit a polynomial solution of the traveling salesman problem and a description of the traveling salesman polytope by a system of linear equalities and inequalities. The central notion is that of a 3-edge cutset, namely, a set of 3 edges that, when removed, disconnects the graph. Conversely, our approach can be used to construct classes of graphs for which there exists a polynomial algorithm for the traveling salesman problem. The approach is illustrated on two examples, Halin graphs and prismatic graphs.

Criteria for adequacy of a data flow semantics are discussed and Kahn's successful semantics for functional (deterministic) data flow is reviewed. Problems arising from nondeterminism are introduced and the paper's approach to overcoming them is introduced. The approach is based on generalizing the notion of input-output relation, essentially to a partially ordered multiset of input-output histories. The Brock-Ackerman anomalies concerning the input-output relation model of nondeterministic data flow are reviewed, and it is indicated how the proposed approach avoids them. A new anomaly is introduced to motivate the use of multisets. A formal theory of asynchronous processes is then developed. The main result is that the operation of forming a process from a network of component processes is associative. This result shows that the approach is not subject to anomalies such as that of Brock and Ackerman.

A distributed computer system that consists of a set of heterogeneous host computers connected in an arbitrary fashion by a communications network is considered. A general model is developed for such a distributed computer system, in which the host computers and the communications network are represented by product-form queuing networks. In this model, a job may be either processed at the host to which it arrives or transferred to another host. In the latter case, a transferred job incurs a communication delay in addition to the queuing delay at the host on which the job is processed. It is assumed that the decision of transferring a job does not depend on the system state, and hence is static in nature. Performance is optimized by determining the load on each host that minimizes the mean job response time. A nonlinear optimization problem is formulated, and the properties of the optimal solution in the special case where the communication delay does not depend on the source-destination pair is shown.
Two efficient algorithms that determine the optimal load on each host computer are presented. The first algorithm, called the parametric-study algorithm, generates the optimal solution as a function of the communication time. This algorithm is suited for the study of the effect of the speed of the communications network on the optimal solution. The second algorithm is a single-point algorithm; it yields the optimal solution for given system parameters. Queuing models of host computers, communications networks, and a numerical example are illustrated.

Two of the most powerful classes of programs for which interesting decision problems are known to be solvable are the class of finite-memory programs and the class of programs that characterize the Presburger, or semilinear, sets. In this paper, a new class of programs that presents solvable decision problems similar to the other two classes of programs is introduced. However, the programs in the new class are shown to be computationally more powerful (i.e., capable of defining larger sets of input-output relations).

A projection of a Boolean function is a function obtained by substituting for each of its variables a variable, the negation of a variable, or a constant. Reducibilities among computational problems under this relation of projection are considered. It is shown that much of what is of everyday relevance in Turing-machine-based complexity theory can be replicated easily and naturally in this elementary framework. Finer distinctions about the computational relationships among natural problems can be made than in previous formulations and some negative results are proved.

The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authorative” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages” that join them together in  the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link-based analysis.

We describe an efficient, purely functional implementation of deques with catenation. In addition to being an intriguing problem in its own right, finding a purely functional implementation of catenable deques is required to add certain sophisticated programming constructs to functional programming languages. Our solution has a worst-case running time of O(1) for each push, pop, inject, eject and catenation. The best previously known solution has an O(log*k) time bound for the kth deque operation. Our solution is not only faster but simpler. A key idea used in our result is an algorithmic technique related to the redundant digital representations used to avoid carry propagation in binary counting.

The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authorative” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages” that join them together in  the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link-based analysis.

In a timestamping system, processors repeatedly choose timestamps so that the order of the timestamps obtained reflects the real-time order in which they were requested. Concurrent timestamping systems permit requests by multiple processors to be issued concurrently; in bounded timestamping systems the sizes of the timestamps and the size and number of shared variables are bounded. An algorithm is wait-free if there exists an a priori bound on the number of steps a processor must take in order to make progress, independent of the action or inaction of other processors. Letting n denote the number of procesors, we construct a simple wait-free bounded concurrent timestamping system    requiring O(n) steps (accesses to shared memory) for a processor to read the current timestamps and determine the order among them, and O(n) steps to generate a timestamp, independent of the actions of the other processors. In addition, we introduce and implement the traceable use abstraction, a new primitive providing “inventory control” over values introduced by processors in the course of an algorithm execution. This abstraction has proved to be of great value in converting unbounded algorithms to bounded ones {Attiya and Rachman 1998; Dwork et al. 1992; 1993].

Consider the set   H   of all linear (or affine) transformations between two vector spaces over a finite field F. We study how good   H is as a class of hash functions, namely we consider hashing a set S of size n into a range having the same cardinality n by a randomly chosen function from   H and look at the expected size of the largest hash bucket.   H is a universal class of hash functions for any finite field, but with respect to our measure different fields behave differently.If the finite  field   F has n elements, then there is a bad set S   ⊂ F2 of size n with expected maximal bucket size   H(n1/3). If n is a perfect square, then there is even a bad set with largest bucket size always at least    n. (This is worst possible, since with respect to a universal class of hash functions every set of size n has expected largest bucket size below    n +   1/2.)If, however, we consider the field of  two  elements, then we get much better bounds. The best previously known upper bound on the  expected size of the largest bucket for this class was  O(2   log n). We reduce this upper bound to O(log n log logn). Note that this is not far from the guarantee for a random function. There, the average largest bucket would be &THgr;(log n/ log log n).In the course of our proof we develop a tool which may be of independent interest. Suppose we have a subset S of a vector space D over   Z2, and consider a random linear mapping of  D to a smaller vector space R. If the cardinality of S is larger than   c&egr;|R|log|R|, then with probability 1 - &egr;, the image of S will cover all elements in the range.

In this paper, we prove various results about PAC learning in the presence of malicious noise. Our main interest is the sample size behavior of learning algorithms. We prove the first nontrivial sample complexity lower bound in this model by showing that order of &egr;/&Dgr;2 + d/&Dgr; (up to logarithmic factors) examples are necessary for PAC learning any target class of {0,1}-valued functions of VC dimension d, where &egr; is the desired accuracy and &eegr; = &egr;/(1 + &egr;) - &Dgr; the malicious noise rate (it is well known that any nontrivial target class cannot be PAC learned with accuracy &egr; and malicious noise rate &eegr; ≥ &egr;/(1 + &egr;), this irrespective to sample complexity). We also show that this result  cannot be significantly improved in general by presenting efficient learning algorithms for the class of all subsets of d elements and the class of unions of at most d intervals on the real line. This is especialy interesting as we can also show that the popular minimum disagreement strategy needs samples of size d &egr;/&Dgr;2, hence is not optimal with respect to sample size. We then discuss the use of randomized hypotheses. For these the bound &egr;/(1 + &egr;) on the noise rate is no longer true and is replaced by 2&egr;/(1 + 2&egr;). In fact, we present a generic algorithm using randomized hypotheses that can tolerate noise rates slightly larger than &egr;/(1 + &egr;) while using samples of size  d/&egr; as in the noise-free case. Again one observes a quadratic powerlaw (in this case d&egr;/&Dgr;2, &Dgr; = 2&egr;/(1 + 2&egr;) - &eegr;) as &Dgr; goes to zero. We show upper and lower bounds of this order.

This paper studies the problem of efficiently schedulling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is “work stealing,” in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies.Specifically, our analysis shows that the expected time to execute a fully strict computation on P processors using our work-stealing scheduler is T1/P + O(T  ∞ , where  T1 is the minimum serial execution time of the multithreaded computation and (T  ∞ is the minimum execution time with an infinite number of processors. Moreover, the space required by the execution is at most S1P, where  S1 is the minimum serial space requirement. We also show that the expected total communication of the algorithm is at most O(PT  ∞ ( 1 +  nd)Smax), where Smax is the size of the largest activation record of any thread and  nd is the maximum number of times that any thread synchronizes with its parent. This communication bound justifies the folk wisdom that work-stealing schedulers are more communication efficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.

We develop principles and rules for achieving secrecy properties in security protocols. Our approach is based on traditional classification techniques, and extends those techniques to handle concurrent processes that use shared-key cryptography. The rules have the form of typing rules for a basic concurrent language with cryptographic primitives, the spi calculus. They guarantee that, if a protocol typechecks, then it does not leak its secret inputs.

There is a population explosion among the logical systems used in computing science. Examples include first-order logic, equational logic, Horn-clause logic, higher-order logic, infinitary logic, dynamic logic, intuitionistic logic, order-sorted logic, and temporal logic; moreover, there is a tendency for each theorem prover to have its own idiosyncratic logical system. The concept of institution is introduced to formalize the informal notion of “logical system.” The major requirement is that there is a satisfaction relation between models and sentences that is consistent under change of notation. Institutions enable abstracting away from syntactic and semantic detail when working on language structure “in-the-large”; for example, we can   define language features for building large logical system. This applies to both specification languages and programming languages. Institutions also have applications to such areas as database theory and the semantics of artificial and natural languages. A first main result of this paper says that any institution such that signatures (which define notation) can be glued together, also allows gluing together theories (which are just collections of sentences over a fixed signature). A second main result considers when theory structuring is preserved by institution morphisms. A third main result gives conditions under which it is sound to use a theorem prover for one institution on theories from another. A fourth main result shows how to extend institutions so that their theories may include, in addition  to the original sentences, various kinds of constraint that are useful for defining   abstract data types, including both “data” and “hierarchy” constraints. Further results show how to define institutions that allow sentences and constraints from two or more institutions. All our general results apply to such “duplex” and “multiplex” institutions.

The main contribution of this work is an O(n log n + k)-time algorithm for computing all k intersections among n line segments in the plane. This time complexity is easily shown to be optimal. Within the same asymptotic cost, our algorithm can also construct the subdivision of the plane defined by the segments and compute which segment (if any) lies right above (or below) each intersection and each endpoint. The algorithm has been implemented and performs very well. The storage requirement is on the order of n + k in the worst case, but it is considerably lower in practice. To analyze the complexity of the algorithm, an amortization argument based on  a new combinatorial theorem on line arrangements is used.

A deterministic O(log N)-time algorithm for the problem of routing an aribitrary permutation on an N-processor bounded-degree network with bounded buffers is presented.
Unlike all previous deterministic solutions to this problem, our routing scheme does not reduce the routing problem to sorting and does not use the sorting network of Ajtai, et al. [1]. Consequently, the constant in the run time of our routing scheme is substantially smaller, and the network topology is significantly simpler.

A domain-independent formula of first-order predicate calculus is a formula whose evaluation in a given interpretation does not change when we add a new constant to the interpretation domain. The formulas used to express queries, integrity constraints or deductive rules in the database field that have an intuitive meaning are domain independent. That is the reason why this class is of great interest in practice. Unfortunately, this class is not decidable, and the problem is to characterize new subclasses, as large as possible, which are decidable. A syntactic characterization of a class of formulas, the Evaluable formulas, which are proved to be domain independent are provided. This class is defined only for function-free formulas. It is also proved that the class of evaluable   formulas contains the other classes of syntactically characterized domain-independent formulas usually found in the literature, namely, range-separable formulas and range-restricted formulas. Finally, it is shown that the expressive power of evaluable formulas is the same as that of domain-independent formulas. That is, each domain-independent formula admits an equivalent evaluable one. An important advantage of this characterization is that, to check if a formula is evaluable, it is not necessary to transform it to a normal form, as is the case for range-restricted formulas.

There is a population explosion among the logical systems used in computing science. Examples include first-order logic, equational logic, Horn-clause logic, higher-order logic, infinitary logic, dynamic logic, intuitionistic logic, order-sorted logic, and temporal logic; moreover, there is a tendency for each theorem prover to have its own idiosyncratic logical system. The concept of institution is introduced to formalize the informal notion of “logical system.” The major requirement is that there is a satisfaction relation between models and sentences that is consistent under change of notation. Institutions enable abstracting away from syntactic and semantic detail when working on language structure “in-the-large”; for example, we can   define language features for building large logical system. This applies to both specification languages and programming languages. Institutions also have applications to such areas as database theory and the semantics of artificial and natural languages. A first main result of this paper says that any institution such that signatures (which define notation) can be glued together, also allows gluing together theories (which are just collections of sentences over a fixed signature). A second main result considers when theory structuring is preserved by institution morphisms. A third main result gives conditions under which it is sound to use a theorem prover for one institution on theories from another. A fourth main result shows how to extend institutions so that their theories may include, in addition  to the original sentences, various kinds of constraint that are useful for defining   abstract data types, including both “data” and “hierarchy” constraints. Further results show how to define institutions that allow sentences and constraints from two or more institutions. All our general results apply to such “duplex” and “multiplex” institutions.

In this paper, a process algebra that incorporates explicit representations of successful termination, deadlock, and divergence is introduced and its semantic theory is analyzed. Both an operational and a denotational semantics for the language is given and it is shown that they agree. The operational theory is based upon a suitable adaptation of the notion of bisimulation preorder. The denotational semantics for the language is given in terms of the initial continuous algebra that satisfies a set of equations E, CIE. It is shown that CIE is fully abstract with respect to our choice of behavioral preorder. Several results of independent interest are obtained; namely, the finite  approximability of the behavioral preorder and a partial completeness result for the set of equations E with respect to the preorder.

In a closed, separable, queuing network model of a computer system, the number of customer classes is an input parameter. The number of classes and the class compositions are assumptions regarding the characteristics of the system's workload. Often, the number of customer classes and their associated device demands are unknown or are unmeasurable parameters of the system. However, when the system is viewed as having a single composite customer class, the aggregate single-class parameters are more easily obtainable.
This paper addresses the error made when constructing a single-class model of a multi-class system. It is shown that the single-class model pessimistically bounds, the performance of the multi-class system. Thus, given a multi-class system, the corresponding  single-class model can be constructed with the assurance that the actual system performance is better than that given by the single-class model. In the worst case, it is shown that the throughput given by the single-class model underestimates the actual multi-class throughput by, at most, 50%. Also, lower bounds are provided for the number of necessary customer classes, given observed device utilizations. This information is useful to clustering analysis techniques as well as to analysts who must obtain class-specific device demands.

A digital signature scheme is presented, which is based on the existence of any trapdoor permutation. The scheme is secure in the strongest possible natural sense: namely, it is secure against existential forgery under adaptive chosen message attack.

We study the problem of “privacy amplification”: key agreement between two parties who both know a weak secret w, such as a password. (Such a setting is ubiquitous on the internet, where passwords are the most commonly used security device.) We assume that the key agreement protocol is taking place in the presence of an active computationally unbounded adversary Eve. The adversary may have partial knowledge about w, so we assume only that w has some entropy from Eve’s point of view. Thus, the goal of the protocol is to convert this nonuniform secret w into a uniformly distributed string R that is fully secret from Eve. R may then be used as a key for running symmetric cryptographic protocols (such as encryption, authentication, etc.). Because we make no computational assumptions, the entropy in R can come only from w. Thus, such a protocol must minimize the entropy loss during its execution, so that R is as long as possible. The best previous results have entropy loss of Θ(κ2), where κ is the security parameter, thus requiring the password to be very long even for small values of κ. In this work, we present the first protocol for information-theoretic key agreement that has entropy loss linear in the security parameter. The result is optimal up to constant factors. We achieve our improvement through a somewhat surprising application of error-correcting codes for the edit distance. The protocol can be extended to provide also “information reconciliation,” that is, to work even when the two parties have slightly different versions of w (e.g., when biometrics are involved).

Locally decodable codes are error-correcting codes that admit efficient decoding algorithms; any bit of the original message can be recovered by looking at only a small number of locations of a corrupted codeword. The tradeoff between the rate of a code and the locality/efficiency of its decoding algorithms has been well studied, and it has widely been suspected that nontrivial locality must come at the price of low rate. A particular setting of potential interest in practice is codes of constant rate. For such codes, decoding algorithms with locality O(k∈) were known only for codes of rate ∈Ω(1/∈), where k is the length of the message. Furthermore, for codes of rate > 1/2, no nontrivial locality had been achieved. In this article, we construct a new family of locally decodable codes that have very efficient local decoding algorithms, and at the same time have rate approaching 1. We show that for every ∈ > 0 and α > 0, for infinitely many k, there exists a code C which encodes messages of length k with rate 1 − α, and is locally decodable from a constant fraction of errors using O(k∈) queries and time. These codes, which we call multiplicity codes, are based on evaluating multivariate polynomials and their derivatives. Multiplicity codes extend traditional multivariate polynomial codes; they inherit the local-decodability of these codes, and at the same time achieve better tradeoffs and flexibility in the rate and minimum distance.

We study the problem of “privacy amplification”: key agreement between two parties who both know a weak secret w, such as a password. (Such a setting is ubiquitous on the internet, where passwords are the most commonly used security device.) We assume that the key agreement protocol is taking place in the presence of an active computationally unbounded adversary Eve. The adversary may have partial knowledge about w, so we assume only that w has some entropy from Eve’s point of view. Thus, the goal of the protocol is to convert this nonuniform secret w into a uniformly distributed string R that is fully secret from Eve. R may then be used as a key for running symmetric cryptographic protocols (such as encryption, authentication, etc.). Because we make no computational assumptions, the entropy in R can come only from w. Thus, such a protocol must minimize the entropy loss during its execution, so that R is as long as possible. The best previous results have entropy loss of Θ(κ2), where κ is the security parameter, thus requiring the password to be very long even for small values of κ. In this work, we present the first protocol for information-theoretic key agreement that has entropy loss linear in the security parameter. The result is optimal up to constant factors. We achieve our improvement through a somewhat surprising application of error-correcting codes for the edit distance. The protocol can be extended to provide also “information reconciliation,” that is, to work even when the two parties have slightly different versions of w (e.g., when biometrics are involved).

Today’s hardware technology presents a new challenge in designing robust systems. Deep submicron VLSI technology introduces transient and permanent faults that were never considered in low-level system designs in the past. Still, robustness of that part of the system is crucial and needs to be guaranteed for any successful product. Distributed systems, on the other hand, have been dealing with similar issues for decades. However, neither the basic abstractions nor the complexity of contemporary fault-tolerant distributed algorithms match the peculiarities of hardware implementations. This article is intended to be part of an attempt striving to bridge over this gap between theory and practice for the clock synchronization problem. Solving this task sufficiently well will allow to build an ultra-robust high-precision clocking system for hardware designs like systems-on-chips in critical applications. As our first building block, we describe and prove correct a novel distributed, Byzantine fault-tolerant, probabilistically self-stabilizing pulse synchronization protocol, called FATAL, that can be implemented using standard asynchronous digital logic: Correct FATAL nodes are guaranteed to generate pulses (i.e., unnumbered clock ticks) in a synchronized way, despite a certain fraction of nodes being faulty. FATAL uses randomization only during stabilization and, despite the strict limitations introduced by hardware designs, offers optimal resilience and smaller complexity than all existing protocols. Finally, we show how to leverage FATAL to efficiently generate synchronized, self-stabilizing, high-frequency clocks.

A Markov sequence is a basic statistical model representing uncertain sequential data, and it is used within a plethora of applications, including speech recognition, image processing, computational biology, radio-frequency identification (RFID), and information extraction. The problem of querying a Markov sequence is studied under the conventional semantics of querying a probabilistic database, where queries are formulated as finite-state transducers. Specifically, the complexity of two main problems is analyzed. The first problem is that of computing the confidence (probability) of an answer. The second is the enumeration of the answers in the order of decreasing confidence (with the generation of the top-k answers as a special case), or in an approximate order thereof. In particular, it is shown that enumeration in any subexponential-approximate order is generally intractable (even for some fixed transducers), and a matching upper bound is obtained through a proposed heuristic. Due to this hardness, a special consideration is given to restricted (yet common) classes of transducers that extract matches of a regular expression (subject to prefix and suffix constraints), and it is shown that these classes are, indeed, significantly more tractable.

It has been observed by many people that a striking number of quite diverse mathematical problems can be formulated as problems in integer programming, that is, linear programming problems in which some or all of the variables are required to assume integral values. This fact is rendered quite interesting by recent research on such problems, notably by R. E. Gomory [2, 3], which gives promise of yielding efficient computational techniques for their solution. The present paper provides yet another example of the versatility of integer programming as a mathematical modeling device by representing a generalization of the well-known “Travelling Salesman Problem” in integer programming terms. The authors have developed several such models, of which the one presented here is the most efficient in terms of generality, number of variables, and number of constraints. This model is due to the second author [4] and was presented briefly at the Symposium on Combinatorial Problems held at Princeton University, April 1960, sponsored by SIAM and IBM. The problem treated is: (1) A salesman is required to visit each of n cities, indexed by 1, ··· , n. He leaves from a “base city” indexed by 0, visits each of the n other cities exactly once, and returns to city 0. During his travels he must return to 0 exactly t times, including his final return (here t may be allowed to vary), and he must visit no more than p cities in one tour. (By a tour we mean a succession of visits to cities without stopping at city 0.) It is required to find such an itinerary which minimizes the total distance traveled by the salesman.
 Note that if t is fixed, then for the problem to have a solution we must have tp ≧ n. For t = 1, p ≧ n, we have the standard traveling salesman problem.
Let dij (i ≠ j = 0, 1, ··· , n) be the distance covered in traveling from city i to city j. The following integer programming problem will be shown to be equivalent to (1): (2) Minimize the linear form ∑0≦i≠j≦n∑ dijxij over the set determined by the relations ∑ni=0i≠j xij = 1 (j = 1, ··· , n) ∑nj=0j≠i xij = 1 (i = 1, ··· , n) ui - uj + pxij ≦ p - 1 (1 ≦ i ≠ j ≦ n) where the xij are non-negative integers and the ui (i = 1, …, n) are arbitrary real numbers. (We shall see that it is permissible to restrict the ui to be non-negative integers as well.)
 If t is fixed it is necessary to add the additional relation: ∑nu=1 xi0 = t Note that the constraints require that xij = 0 or 1, so that a natural correspondence between these two problems exists if the xij are interpreted as follows: The salesman proceeds from city i to city j if and only if xij = 1. Under this correspondence the form to be minimized in (2) is the total distance to be traveled by the salesman in (1), so the burden of proof is to show that the two feasible sets correspond; i.e., a feasible solution to (2) has xij which do define a legitimate itinerary in (1), and, conversely a legitimate itinerary in (1) defines xij, which, together with appropriate ui, satisfy the constraints of (2).
Consider a feasible solution to (2).
 The number of returns to city 0 is given by ∑ni=1 xi0. The constraints of the form ∑ xij = 1, all xij non-negative integers, represent the conditions that each city (other than zero) is visited exactly once. The ui play a role similar to node potentials in a network and the inequalities involving them serve to eliminate tours that do not begin and end at city 0 and tours that visit more than p cities. Consider any xr0r1 = 1 (r1 ≠ 0). There exists a unique r2 such that xr1r2 = 1. Unless r2 = 0, there is a unique r3 with xr2r3 = 1. We proceed in this fashion until some rj = 0. This must happen since the alternative is that at some point we reach an rk = rj, j + 1 < k.  Since none of the r's are zero we have uri - uri + 1 + pxriri + 1 ≦ p - 1 or uri - uri + 1 ≦ - 1. Summing from i = j to k - 1, we have urj - urk = 0 ≦ j + 1 - k, which is a contradiction. Thus all tours include city 0. It remains to observe that no tours is of length greater than p. Suppose such a tour exists, x0r1 , xr1r2 , ··· , xrprp+1 = 1 with all ri ≠ 0. Then, as before, ur1 - urp+1 ≦ - p or urp+1 - ur1 ≧ p. But we have urp+1 - ur1 + pxrp+1r1 ≦ p - 1 or urp+1 - ur1 ≦ p (1 - xrp+1r1) - 1 ≦ p - 1, which is a contradiction.
Conversely, if the xij correspond to a legitimate itinerary, it is clear that the ui can be adjusted so that ui = j if city i is the jth city visited in the tour which includes city i, for we then have ui - uj = - 1 if xij = 1, and always ui - uj ≦ p - 1.
 The above integer program involves n2 + n constraints (if t is not fixed) in n2 + 2n variables. Since the inequality form of constraint is fundamental for integer programming calculations, one may eliminate 2n variables, say the xi0 and x0j, by means of the equation constraints and produce an equivalent problem with n2 + n inequalities and n2 variables.
The currently known integer programming procedures are sufficiently regular in their behavior to cast doubt on the heuristic value of machine experiments with our model. However, it seems appropriate to report the results of the five machine experiments we have conducted so far. The solution procedure used was the all-integer algorithm of R. E. Gomory [3] without the ranking procedure he describes.
The first three experiments were simple model verification tests on a four-city standard traveling salesman problem with distance matrix [ 20 23 4 30 7 27 25 5 25 3 21 26 ]
The first experiment was with a model, now obsolete, using roughly twice as many constraints and variables as the current model (for this problem, 28 constraints in 21 variables). The machine was halted after 4000 pivot steps had failed to produce a solution.
 The second experiment used the earlier model with the xi0 and x0j eliminated, resulting in a 28-constraint, 15-variable problem. Here the machine produced the optimal solution in 41 pivot steps.
The third experiment used the current formulation with the xi0 and x0j eliminated, yielding 13 constraints and 9 variables. The optimal solution was reached in 7 pivot steps.
The fourth and fifth experiments were used on a standard ten-city problem, due to Barachet, solved by Dantzig, Johnson and Fulkerson [1]. The current formulation was used, yielding 91 constraints in 81 variables. The fifth problem differed from the fourth only in that the ordering of the rows was altered to attempt to introduce more favorable pivot choices. In each case the machine was stopped after over 250 pivot steps had failed to produce the solution. In each case the last 100 pivot steps had failed to change the value of the objective function.
It seems hopeful that more efficient integer programming procedures now under development will yield a satisfactory algorithmic solution to the traveling salesman problem, when applied to this model. In any case, the model serves to illustrate how problems of this sort may be succinctly formulated in integer programming terms.

The literature concerned with methods for finding the minimal form of a truth function is, by now, quite extensive. This article extends this knowledge by introducing an algorithm whereby all calculations are performed on decimal numbers obtained from binary-decimal conversion of the terms of the Boolean function. Several computational aids are presented for the purpose of adapting this algorithm to the solution of large-scale problems on a digital computer.

It has been observed by many people that a striking number of quite diverse mathematical problems can be formulated as problems in integer programming, that is, linear programming problems in which some or all of the variables are required to assume integral values. This fact is rendered quite interesting by recent research on such problems, notably by R. E. Gomory [2, 3], which gives promise of yielding efficient computational techniques for their solution. The present paper provides yet another example of the versatility of integer programming as a mathematical modeling device by representing a generalization of the well-known “Travelling Salesman Problem” in integer programming terms. The authors have developed several such models, of which the one presented here is the most efficient in terms of generality, number of variables, and number of constraints. This model is due to the second author [4] and was presented briefly at the Symposium on Combinatorial Problems held at Princeton University, April 1960, sponsored by SIAM and IBM. The problem treated is: (1) A salesman is required to visit each of n cities, indexed by 1, ··· , n. He leaves from a “base city” indexed by 0, visits each of the n other cities exactly once, and returns to city 0. During his travels he must return to 0 exactly t times, including his final return (here t may be allowed to vary), and he must visit no more than p cities in one tour. (By a tour we mean a succession of visits to cities without stopping at city 0.) It is required to find such an itinerary which minimizes the total distance traveled by the salesman.
 Note that if t is fixed, then for the problem to have a solution we must have tp ≧ n. For t = 1, p ≧ n, we have the standard traveling salesman problem.
Let dij (i ≠ j = 0, 1, ··· , n) be the distance covered in traveling from city i to city j. The following integer programming problem will be shown to be equivalent to (1): (2) Minimize the linear form ∑0≦i≠j≦n∑ dijxij over the set determined by the relations ∑ni=0i≠j xij = 1 (j = 1, ··· , n) ∑nj=0j≠i xij = 1 (i = 1, ··· , n) ui - uj + pxij ≦ p - 1 (1 ≦ i ≠ j ≦ n) where the xij are non-negative integers and the ui (i = 1, …, n) are arbitrary real numbers. (We shall see that it is permissible to restrict the ui to be non-negative integers as well.)
 If t is fixed it is necessary to add the additional relation: ∑nu=1 xi0 = t Note that the constraints require that xij = 0 or 1, so that a natural correspondence between these two problems exists if the xij are interpreted as follows: The salesman proceeds from city i to city j if and only if xij = 1. Under this correspondence the form to be minimized in (2) is the total distance to be traveled by the salesman in (1), so the burden of proof is to show that the two feasible sets correspond; i.e., a feasible solution to (2) has xij which do define a legitimate itinerary in (1), and, conversely a legitimate itinerary in (1) defines xij, which, together with appropriate ui, satisfy the constraints of (2).
Consider a feasible solution to (2).
 The number of returns to city 0 is given by ∑ni=1 xi0. The constraints of the form ∑ xij = 1, all xij non-negative integers, represent the conditions that each city (other than zero) is visited exactly once. The ui play a role similar to node potentials in a network and the inequalities involving them serve to eliminate tours that do not begin and end at city 0 and tours that visit more than p cities. Consider any xr0r1 = 1 (r1 ≠ 0). There exists a unique r2 such that xr1r2 = 1. Unless r2 = 0, there is a unique r3 with xr2r3 = 1. We proceed in this fashion until some rj = 0. This must happen since the alternative is that at some point we reach an rk = rj, j + 1 < k.  Since none of the r's are zero we have uri - uri + 1 + pxriri + 1 ≦ p - 1 or uri - uri + 1 ≦ - 1. Summing from i = j to k - 1, we have urj - urk = 0 ≦ j + 1 - k, which is a contradiction. Thus all tours include city 0. It remains to observe that no tours is of length greater than p. Suppose such a tour exists, x0r1 , xr1r2 , ··· , xrprp+1 = 1 with all ri ≠ 0. Then, as before, ur1 - urp+1 ≦ - p or urp+1 - ur1 ≧ p. But we have urp+1 - ur1 + pxrp+1r1 ≦ p - 1 or urp+1 - ur1 ≦ p (1 - xrp+1r1) - 1 ≦ p - 1, which is a contradiction.
Conversely, if the xij correspond to a legitimate itinerary, it is clear that the ui can be adjusted so that ui = j if city i is the jth city visited in the tour which includes city i, for we then have ui - uj = - 1 if xij = 1, and always ui - uj ≦ p - 1.
 The above integer program involves n2 + n constraints (if t is not fixed) in n2 + 2n variables. Since the inequality form of constraint is fundamental for integer programming calculations, one may eliminate 2n variables, say the xi0 and x0j, by means of the equation constraints and produce an equivalent problem with n2 + n inequalities and n2 variables.
The currently known integer programming procedures are sufficiently regular in their behavior to cast doubt on the heuristic value of machine experiments with our model. However, it seems appropriate to report the results of the five machine experiments we have conducted so far. The solution procedure used was the all-integer algorithm of R. E. Gomory [3] without the ranking procedure he describes.
The first three experiments were simple model verification tests on a four-city standard traveling salesman problem with distance matrix [ 20 23 4 30 7 27 25 5 25 3 21 26 ]
The first experiment was with a model, now obsolete, using roughly twice as many constraints and variables as the current model (for this problem, 28 constraints in 21 variables). The machine was halted after 4000 pivot steps had failed to produce a solution.
 The second experiment used the earlier model with the xi0 and x0j eliminated, resulting in a 28-constraint, 15-variable problem. Here the machine produced the optimal solution in 41 pivot steps.
The third experiment used the current formulation with the xi0 and x0j eliminated, yielding 13 constraints and 9 variables. The optimal solution was reached in 7 pivot steps.
The fourth and fifth experiments were used on a standard ten-city problem, due to Barachet, solved by Dantzig, Johnson and Fulkerson [1]. The current formulation was used, yielding 91 constraints in 81 variables. The fifth problem differed from the fourth only in that the ordering of the rows was altered to attempt to introduce more favorable pivot choices. In each case the machine was stopped after over 250 pivot steps had failed to produce the solution. In each case the last 100 pivot steps had failed to change the value of the objective function.
It seems hopeful that more efficient integer programming procedures now under development will yield a satisfactory algorithmic solution to the traveling salesman problem, when applied to this model. In any case, the model serves to illustrate how problems of this sort may be succinctly formulated in integer programming terms.

Numerous formulas are available for the computation of the Gamma function [1, 2]. The purpose of this note is to indicate the value of a well-known method that is easily extended for higher accuracy requirements.
Using the recursion formula for the Gamma function, &Ggr;(x + 1) = x&Ggr;(x), (1) and Stirling's asymptotic expansion for ln &Ggr;(x) [3], we have ln &Ggr;(x) ∼ (x - 1/2) ln x - x + 1/2 ln 2&pgr; + ∑Nr=1 Cr/x2r-1. (2) It follows that, if k and N are appropriately selected positive integers, &Ggr;(x + 1) can be represented by &Ggr;(x + 1) ∼ √2&pgr; exp (x + k - 1/2) ln (x + k) - (x + k) exp ∑Nr=1 Cr/(x + k)2r-1/(x + 1)(x + 2) ··· (x + k - 1) (3) where Cr = (- 1)r-1 Br/(2r - 1)(2r), Br being the Bernoulli numbers [4]. These coefficients have been published by Uhler [5].
Requiring the range 0 ≦ x ≦ 1 is no restriction since, if necessary, &Ggr;(x + 1) can be generated for other arguments using (1).
 For a given N, the error in (2) can be estimated from |&egr;| < |CN+1|/x2N+1. (4)
The curves of Figure 1 show contours of constant error bound as a function of N and x. These curves represent single and double-precision floating-arithmetic requirements of &egr; < 5·10-9 and &egr; < 5·10-17. For a given N, k is defined as the minimum integral x greater than or equal to those on the curves. Then N and k can be chosen to minimize round-off and computing time.
For N and k equal to 4, formula (3) yields &Ggr;(x + 1) ∼ &radic2&pgr; exp (x + 4 - 1/2) ln (x + 4) - (x + 4) exp ∑4r=1Cr/(x + 4)2r-1/(x + 1)(x + 2)(x + 3). (5)
A similar expression suitable for double precision results for N = 8 and k = 9.
The exponents in (5) are split to reduce roundoff. Various algebraic manipulations might result in a further reduction of roundoff.

For each item to be sorted by address calculation, a location in the file is determined by a linear formula. It is placed there if the location is empty. If there is an item at the specified location, a search is made to find the closest empty space to this spot. The item at the specified location, together with adjacent items, is moved by a process similar to musical chairs, so that the item to be filed can be entered in its proper order in the file. A generalized flowchart for computer address calculation sorting is presented here. A mathematical analysis using average expectation follows. Formulas are derived to determine the number of computer operations required. Further formulas are derived which determine the time required for an address calculation sort in terms of specific computer orders. Several examples are given. A sorting problem solved elsewhere in the literature by an empirical method is solved by the formulas developed here to demonstrate their practical application.

The problem of multiprogram scheduling on a single processor is studied from the viewpoint of the characteristics peculiar to the program functions that need guaranteed service. It is shown that an optimum fixed priority scheduler possesses an upper bound to processor utilization which may be as low as 70 percent for large task sets. It is also shown that full processor utilization can be achieved by dynamically assigning priorities on the basis of their current deadlines. A combination of these two scheduling techniques is also discussed.

Two upper bounds for the total path length of binary trees are obtained. One is for node-trees, and bounds the internal (or root-to-node) path length; the other is for leaf-trees, and bounds the external (or root-to-leaf) path length. These bounds involve a quantity called the balance, which allows the bounds to adapt from the n log n behavior of a completely balanced tree to the n2 behavior of a most skewed tree. These bounds are illustrated for the case of Fibonacci trees.

Consider the set of multistep formulas ∑l-1jmn-k &agr;ijxmn+j - h ∑l-1jmn-k&bgr;ijxmn+j = 0, i = 1, ···, l, where xmn+j = ymn+j for j= -k, ···, -1 and xn = ƒn = ƒ(xn , tn). These formulas are solved simultaneously for the xmn+j with j = 0, ···, l - 1 in terms of the xmn+j with j = -k, ··· , - 1, which are assumed to be known. Then ymn+j is defined to be xmn+j for j = 0, ··· , m - 1. For j = m, ··· , l - 1, xmn+j is discarded. The set of y's generated in this manner for successive values of n provide an approximate solution of the initial value problem: y = ƒ(y, t), y(t0) = y0. It is conjectured that if the method, which is referred to as the composite multistep method, is A-stable, then its maximum order is 2l. In addition to noting that the conjecture conforms to Dahlquist's bound of 2 for l = 1, the conjecture is verified for k = 1. A third-order A-stable method with m = l = 2 is given as an example, and numerical results established in applying a fourth-order A-stable method with m = 1 and l = 2 are described. A-stable methods with m = l offer the promise of high order and a minimum of function evaluations—evaluation of ƒ(y, t) at solution points. Furthermore, the prospect that such methods might exist with k = 1—only one past point—means that step-size control can be easily implemented

Tridiagonal linear systems of equations can be solved on conventional serial machines in a time proportional to N, where N is the number of equations. The conventional algorithms do not lend themselves directly to parallel computation on computers of the ILLIAC IV class, in the sense that they appear to be inherently serial. An efficient parallel algorithm is presented in which computation time grows as log2 N. The algorithm is based on recursive doubling solutions of linear recurrence relations, and can be used to solve recurrence relations of all orders.

A combinatorial problem arising from the analysis of a model of interleaved memory systems is studied. The performance measure whose calculation defines this problem is based on the distribution of the number of modules in operation during a memory cycle, assuming saturated demand and an arbitrary but fixed number of modules.
In general terms the problem is as follows. Suppose we have a Markov chain of n states numbered 0, 1, ···, n - 1. For each i assume that the one-step transition probability from state i to state (i + 1) mod n is given by the parameter &agr; and from state i to any other state is &bgr; = (1 - &agr;)/(n - 1). Given an initial state, the problem is to find the expected number of states through which the system passes before returning to a state previously entered. The principal result of the paper is a recursive procedure for computing this expected number of states. The complexity of the procedure is seen to be small enough to enable practical numerical studies of interleaved memory systems.

The problem of multiprogram scheduling on a single processor is studied from the viewpoint of the characteristics peculiar to the program functions that need guaranteed service. It is shown that an optimum fixed priority scheduler possesses an upper bound to processor utilization which may be as low as 70 percent for large task sets. It is also shown that full processor utilization can be achieved by dynamically assigning priorities on the basis of their current deadlines. A combination of these two scheduling techniques is also discussed.

A particular decision-theoretic approach to the problem of detecting straight edges and lines in pictures is discussed. A model is proposed of the appearance of scenes consisting of prismatic solids, taking into account blurring, noise, and smooth variations in intensity over faces. A suboptimal statistical decision procedure is developed for the identification of a line within a narrow band in the field of view, given an array of intensity values from within the band. The performance of this procedure is illustrated and discussed.

Characterizations of digital “simple arcs” and “simple closed curves” are given. In particular, it is shown that the following are equivalent for sets S having more than four points: (1) S is a simple curve; (2) S is connected and each point of S has exactly two neighbors in S; (3) S is connected, has exactly one hole, and has no deletable points. It follows that if a “shrinking” algorithm is applied to a connected S that has exactly one hole, it shrinks to a simple curve.

The problem of finding a minimum number of patterns to exercise the logic elements of a combinational switching net is investigated. Throughout, the word “testing” refers to exercising of this kind; or, equivalently, to fault diagnosis where each line of the net can be directly observed. Any set of permanent faults can be selected to test against, examples of which range from “stuck-at” faults (allowing the most economical test) to “any possible fault” (requiring the most complete test).
The method used depends upon exact structural analysis rather than upon search algorithms or random pattern generation. The types of results presented appear to be fundamentally new. In particular, the maximum number of patterns required to test any one of an infinite class of nets is frequently found to be finite and extremely small. For example, any nontrivial connected tree of 2-input nand gates can be tested for “any possible fault” by exactly five patterns—no more and no less.
The method in brief: Given a set F of switching functions and a set of required inputs for each (collectively denoted T), a “testing” function is defined for each element of F for each positive integer r. If the lines of a net can be mapped to the domain of the testing functions P(T, r) so that the gates perform consistent with these functions, we say the net “accepts” P(T, r)—and then r patterns are sufficient to test the net for T.
 Only nets in which each logic element is intended to realize the same switching function are discussed here. Trees (nets without fanout) are studied first, and the conditions under which a tree of identical gates “accepts” a partial function on an arbitrary domain is established. Then the common symmetric switching functions are separately investigated to find for each a minimum value of r such that all trees composed solely of the function accept P(T, r) (for various T). In most cases, as in the example given, the number of patterns required to test any such tree is extremely low.
The conditions under which all nets (nontrees included) accept a set of partial functions with arbitrary domain are then established. These conditions are rarely met in practice, even where F consists of a single function. However, many subclasses of nets can be identified which require only a few patterns at most (depending on the function and the class of faults selected). These subclasses often contain nets of arbitrary size and complexity, and frequently consist of exactly those nets for which a related graph can be “colored” (i.e., h-node colored for some particular h) in the classical graph-theoretic sense. For example, any net of 2-input nand gates can be tested by five patterns if one of its related graphs is 2-colorable and another one is 3-colorable (!).
The detailed results and methods used to obtain them are summarized, and in conclusion coloring problems and test construction are commented upon.

In connection with the development of an actual question-answering system, the Relational Data File of The Rand Corporation, a class of formulas of the predicate calculus, the definite formulas of Kuhns, was proposed as the class of symbolic representations of the “reasonable” questions to put to this retrieval system. Roughly speaking, the definite formulas are those formulas F such that the set of true substitution instances of F in a finite interpretation I are preserved on passage to a certain special extension I′ of I. The proper formulas are those definite formulas which are supposedly especially suitable for machine processing. It has previously been shown by the author that the decision problem for the class of definite formulas is recursively unsolvable. In this paper, however, it is shown that the decision problem for various classes of proper formulas is solvable. Thus, for each of these classes there is a mechanical procedure to decide of an arbitrary formula whether it is a member of the class. It follows that for each of these classes there is no effective transformation 
@@@@ which takes an arbitrary definite formula F into a proper equivalent @@@@(F). In addition, it is shown that the decision problems for a number of classes of formulas which bear a structural similarity to any class of proper formulas are recursively unsolvable.

An improved procedure for resolution theorem proving, called Z-resolution, is described. The basic idea of Z-resolution is to “compile” some of the axioms in a deductive problem. This means to automatically transform the selected axioms into a computer program which carries out the inference rules indicated by the axioms. This is done automatically by another program called the specializer. The advantage of doing this is that the compiled axioms run faster, just as a compiled program runs faster than an interpreted program.
A proof is given that the inference rule used in Z-resolution is complete, provided that the axioms “compiled” have certain properties.

Suppose we are given two disjoint linearly ordered subsets A and B of a linearly ordered set C, say A = {a1 < a2 < ··· < am} and B = {b1 < b2 < ··· < bn}. The problem is to determine the linear ordering of their union (i.e. to merge A and B) by means of a sequence of pairwise comparisons between an element of A and an element of B (which we refer to in the paper as the (m, n) problem). Given any algorithm s to solve the (m, n) problem, we are interested in the maximum number of comparisons Ks(m, n) required under all possible orderings of A ∪ B. An algorithm s is said to be minimax if Ks(m, n) = K(m, n) where K(m, n) = mins Ks(m, n)
It is a rather difficult task to determine K(m, n) in general. In this study the authors are only concerned with the minimax over a particular class of merging algorithms. This class includes the tape merge algorithm, the simple binary algorithm, and the generalized binary algorithm.

Subtree replacement systems form a broad class of tree-manipulating systems. Systems with the “Church-Rosser property” are appropriate for evaluation or translation processes: the end result of a complete sequence of applications of the rules does not depend on the order in which the rules were applied. Theoretical and practical advantages of such flexibility are sketched. Values or meanings for trees can be defined by simple mathematical systems and then computed by the cheapest available algorithm, however intricate that algorithm may be.
We derive sufficient conditions for the Church-Rosser property and discuss their applications to recursive definitions, to the lambda calculus, and to parallel programming. Only the first application is treated in detail. We extend McCarthy's recursive calculus by allowing a choice between call-by-value and call-by-name. We show that recursively defined functions are single-valued despite the nondeterminism of the evaluation algorithm. We also show that these functions solve their defining equations in a “canonical” manner.

The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authorative” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages” that join them together in  the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link-based analysis.

We describe an efficient, purely functional implementation of deques with catenation. In addition to being an intriguing problem in its own right, finding a purely functional implementation of catenable deques is required to add certain sophisticated programming constructs to functional programming languages. Our solution has a worst-case running time of O(1) for each push, pop, inject, eject and catenation. The best previously known solution has an O(log*k) time bound for the kth deque operation. Our solution is not only faster but simpler. A key idea used in our result is an algorithmic technique related to the redundant digital representations used to avoid carry propagation in binary counting.

The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authorative” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages” that join them together in  the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link-based analysis.

In a timestamping system, processors repeatedly choose timestamps so that the order of the timestamps obtained reflects the real-time order in which they were requested. Concurrent timestamping systems permit requests by multiple processors to be issued concurrently; in bounded timestamping systems the sizes of the timestamps and the size and number of shared variables are bounded. An algorithm is wait-free if there exists an a priori bound on the number of steps a processor must take in order to make progress, independent of the action or inaction of other processors. Letting n denote the number of procesors, we construct a simple wait-free bounded concurrent timestamping system    requiring O(n) steps (accesses to shared memory) for a processor to read the current timestamps and determine the order among them, and O(n) steps to generate a timestamp, independent of the actions of the other processors. In addition, we introduce and implement the traceable use abstraction, a new primitive providing “inventory control” over values introduced by processors in the course of an algorithm execution. This abstraction has proved to be of great value in converting unbounded algorithms to bounded ones {Attiya and Rachman 1998; Dwork et al. 1992; 1993].

Consider the set   H   of all linear (or affine) transformations between two vector spaces over a finite field F. We study how good   H is as a class of hash functions, namely we consider hashing a set S of size n into a range having the same cardinality n by a randomly chosen function from   H and look at the expected size of the largest hash bucket.   H is a universal class of hash functions for any finite field, but with respect to our measure different fields behave differently.If the finite  field   F has n elements, then there is a bad set S   ⊂ F2 of size n with expected maximal bucket size   H(n1/3). If n is a perfect square, then there is even a bad set with largest bucket size always at least    n. (This is worst possible, since with respect to a universal class of hash functions every set of size n has expected largest bucket size below    n +   1/2.)If, however, we consider the field of  two  elements, then we get much better bounds. The best previously known upper bound on the  expected size of the largest bucket for this class was  O(2   log n). We reduce this upper bound to O(log n log logn). Note that this is not far from the guarantee for a random function. There, the average largest bucket would be &THgr;(log n/ log log n).In the course of our proof we develop a tool which may be of independent interest. Suppose we have a subset S of a vector space D over   Z2, and consider a random linear mapping of  D to a smaller vector space R. If the cardinality of S is larger than   c&egr;|R|log|R|, then with probability 1 - &egr;, the image of S will cover all elements in the range.

In this paper, we prove various results about PAC learning in the presence of malicious noise. Our main interest is the sample size behavior of learning algorithms. We prove the first nontrivial sample complexity lower bound in this model by showing that order of &egr;/&Dgr;2 + d/&Dgr; (up to logarithmic factors) examples are necessary for PAC learning any target class of {0,1}-valued functions of VC dimension d, where &egr; is the desired accuracy and &eegr; = &egr;/(1 + &egr;) - &Dgr; the malicious noise rate (it is well known that any nontrivial target class cannot be PAC learned with accuracy &egr; and malicious noise rate &eegr; ≥ &egr;/(1 + &egr;), this irrespective to sample complexity). We also show that this result  cannot be significantly improved in general by presenting efficient learning algorithms for the class of all subsets of d elements and the class of unions of at most d intervals on the real line. This is especialy interesting as we can also show that the popular minimum disagreement strategy needs samples of size d &egr;/&Dgr;2, hence is not optimal with respect to sample size. We then discuss the use of randomized hypotheses. For these the bound &egr;/(1 + &egr;) on the noise rate is no longer true and is replaced by 2&egr;/(1 + 2&egr;). In fact, we present a generic algorithm using randomized hypotheses that can tolerate noise rates slightly larger than &egr;/(1 + &egr;) while using samples of size  d/&egr; as in the noise-free case. Again one observes a quadratic powerlaw (in this case d&egr;/&Dgr;2, &Dgr; = 2&egr;/(1 + 2&egr;) - &eegr;) as &Dgr; goes to zero. We show upper and lower bounds of this order.

This paper studies the problem of efficiently schedulling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is “work stealing,” in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies.Specifically, our analysis shows that the expected time to execute a fully strict computation on P processors using our work-stealing scheduler is T1/P + O(T  ∞ , where  T1 is the minimum serial execution time of the multithreaded computation and (T  ∞ is the minimum execution time with an infinite number of processors. Moreover, the space required by the execution is at most S1P, where  S1 is the minimum serial space requirement. We also show that the expected total communication of the algorithm is at most O(PT  ∞ ( 1 +  nd)Smax), where Smax is the size of the largest activation record of any thread and  nd is the maximum number of times that any thread synchronizes with its parent. This communication bound justifies the folk wisdom that work-stealing schedulers are more communication efficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.

We develop principles and rules for achieving secrecy properties in security protocols. Our approach is based on traditional classification techniques, and extends those techniques to handle concurrent processes that use shared-key cryptography. The rules have the form of typing rules for a basic concurrent language with cryptographic primitives, the spi calculus. They guarantee that, if a protocol typechecks, then it does not leak its secret inputs.

The consensus problem involves an asynchronous system of processes, some of which may be unreliable. The problem is for the reliable processes to agree on a binary value. In this paper, it is shown that every protocol for this problem has the possibility of nontermination, even with only one faulty process. By way of contrast, solutions are known for the synchronous case, the “Byzantine Generals” problem.

The problem of synthesizing a procedure from example computations is examined. An algorithm for this task is presented, and its success is considered. To do this, a model of procedures and example computations is introduced, and the class of acceptable examples is defined. The synthesis algorithm is shown to be successful, with respect to the model of procedures and examples, from two perspectives. First, it is shown to be sound, that is, that the procedure synthesized from a set of examples produces the same result as the intended one on the inputs used to generate that set of examples. Second, it is shown to be complete, that is, that for any procedure in the class of procedures, there exists a finite set of examples such that the procedure synthesized behaves as the intended one on all inputs for which the intended one halts.

The costs of subsumption algorithms are analyzed by an estimation of the maximal number of unification attempts (worst-case unification complexity) made for deciding whether a clause C subsumes a clause D. For this purpose the clauses C and D are characterized by the following parameters: number of variables in C, number of literals in C, number of literals in D, and maximal length of the literals. The worst-case unification complexity immediately yields a lower bound for the worst-case time complexity.
First, two well-known algorithms (Chang-Lee, Stillman) are investigated. Both algorithms are shown to have a very high worst-case time complexity. Then, a new subsumption algorithm is defined, which is based on an analysis of the connection between variables and predicates in C. An upper bound for the worst-case unification complexity of this algorithm, which is much lower than the lower bounds for the two other algorithms, is derived. Examples in which exponential costs are reduced to polynomial costs are discussed. Finally, the asymptotic growth of the worst-case complexity for all discussed algorithms is shown in a table (for several combinations of the parameters).

The problem of finding a minimum cardinality feedback vertex set of a directed graph is considered. Of the classic NP-complete problems, this is one of the least understood. Although Karp showed the general problem to be NP-complete, a linear algorithm for its solution on reducible flow graphs was given by Shamir. The class of reducible flow graphs is the only nontrivial class of graphs for which a polynomial-time algorithm to solve this problem is known. The main result of this paper is to present a new class of graphs—the cyclically reducible graphs—for which minimum feedback vertex sets can be found in polynomial time. This class is not restricted to flow graphs, and most small graphs (10 or fewer nodes) fall into this class. The identification of this class is particularly important since there do not exist approximation algorithms for this problem having a provably good worst case performance. Along with the class and a simple polynomial-time algorithm for finding minimum feedback vertex sets of graphs in the class, several related results are presented. It is shown that there is no “forbidden subgraph” characterization of the class and that there is no particular inclusion relationship between this class and the reducible flow graphs. In addition, it is shown that a class of (general) graphs, which are related to the reducible flow graphs, are contained in the cyclically reducible class.

Many database systems maintain the consistency of the data by using a locking protocol to restrict access to data items. It has been previously shown that if no information is known about the method of accessing items in the database, then the two-phase protocol is optimal. However, the use of structural information about the database allows development of non-two-phase protocols, called graph protocols, that can potentially increase efficiency. Yannakakis developed a general class of protocols that included many of the graph protocols. Graph protocols either are only usable in certain types of databases or can incur the performance liability of cascading rollback. In this paper, it is demonstrated that if the system has a priori information as to which data items will be locked first by various transactions, a new graph protocol that is outside the previous classes of graph protocols and is applicable to arbitrarily structured databases can be constructed. This new protocol avoids cascading rollback and its accompanying performance degradation, and extends the class of serializable sequences allowed by non-two-phase protocols. This is the first protocol shown to be always as effective as the two-phase protocol, and it can be more effective for certain types of database systems.

Parallel algorithms for data compression by textual substitution that are suitable for VLSI implementation are studied. Both “static” and “dynamic” dictionary schemes are considered.

The consensus problem involves an asynchronous system of processes, some of which may be unreliable. The problem is for the reliable processes to agree on a binary value. In this paper, it is shown that every protocol for this problem has the possibility of nontermination, even with only one faulty process. By way of contrast, solutions are known for the synchronous case, the “Byzantine Generals” problem.

This paper analyzes decomposition properties of a graph that, when they occur, permit a polynomial solution of the traveling salesman problem and a description of the traveling salesman polytope by a system of linear equalities and inequalities. The central notion is that of a 3-edge cutset, namely, a set of 3 edges that, when removed, disconnects the graph. Conversely, our approach can be used to construct classes of graphs for which there exists a polynomial algorithm for the traveling salesman problem. The approach is illustrated on two examples, Halin graphs and prismatic graphs.

Criteria for adequacy of a data flow semantics are discussed and Kahn's successful semantics for functional (deterministic) data flow is reviewed. Problems arising from nondeterminism are introduced and the paper's approach to overcoming them is introduced. The approach is based on generalizing the notion of input-output relation, essentially to a partially ordered multiset of input-output histories. The Brock-Ackerman anomalies concerning the input-output relation model of nondeterministic data flow are reviewed, and it is indicated how the proposed approach avoids them. A new anomaly is introduced to motivate the use of multisets. A formal theory of asynchronous processes is then developed. The main result is that the operation of forming a process from a network of component processes is associative. This result shows that the approach is not subject to anomalies such as that of Brock and Ackerman.

A distributed computer system that consists of a set of heterogeneous host computers connected in an arbitrary fashion by a communications network is considered. A general model is developed for such a distributed computer system, in which the host computers and the communications network are represented by product-form queuing networks. In this model, a job may be either processed at the host to which it arrives or transferred to another host. In the latter case, a transferred job incurs a communication delay in addition to the queuing delay at the host on which the job is processed. It is assumed that the decision of transferring a job does not depend on the system state, and hence is static in nature. Performance is optimized by determining the load on each host that minimizes the mean job response time. A nonlinear optimization problem is formulated, and the properties of the optimal solution in the special case where the communication delay does not depend on the source-destination pair is shown.
Two efficient algorithms that determine the optimal load on each host computer are presented. The first algorithm, called the parametric-study algorithm, generates the optimal solution as a function of the communication time. This algorithm is suited for the study of the effect of the speed of the communications network on the optimal solution. The second algorithm is a single-point algorithm; it yields the optimal solution for given system parameters. Queuing models of host computers, communications networks, and a numerical example are illustrated.

Two of the most powerful classes of programs for which interesting decision problems are known to be solvable are the class of finite-memory programs and the class of programs that characterize the Presburger, or semilinear, sets. In this paper, a new class of programs that presents solvable decision problems similar to the other two classes of programs is introduced. However, the programs in the new class are shown to be computationally more powerful (i.e., capable of defining larger sets of input-output relations).

A projection of a Boolean function is a function obtained by substituting for each of its variables a variable, the negation of a variable, or a constant. Reducibilities among computational problems under this relation of projection are considered. It is shown that much of what is of everyday relevance in Turing-machine-based complexity theory can be replicated easily and naturally in this elementary framework. Finer distinctions about the computational relationships among natural problems can be made than in previous formulations and some negative results are proved.

The hope that mathematical methods employed in the investigation of formal logic would lead to purely computational methods for obtaining mathematical theorems goes back to Leibniz and has been revived by Peano around the turn of the century and by Hilbert's school in the 1920's. Hilbert, noting that all of classical mathematics could be formalized within quantification theory, declared that the problem of finding an algorithm for determining whether or not a given formula of quantification theory is valid was the central problem of mathematical logic. And indeed, at one time it seemed as if investigations of this “decision” problem were on the verge of success. However, it was shown by Church and by Turing that such an algorithm can not exist. This result led to considerable pessimism regarding the possibility of using modern digital computers in deciding significant mathematical questions. However, recently there has been a revival of interest in the whole question. Specifically, it has been realized that while no decision procedure exists for quantification theory there are many proof procedures available—that is, uniform procedures which will ultimately locate a proof for any formula of quantification theory which is valid but which will usually involve seeking “forever” in the case of a formula which is not valid—and that some of these proof procedures could well turn out to be feasible for use with modern computing machinery.
 Hao Wang [9] and P. C. Gilmore [3] have each produced working programs which employ proof procedures in quantification theory. Gilmore's program employs a form of a basic theorem of mathematical logic due to Herbrand, and Wang's makes use of a formulation of quantification theory related to those studied by Gentzen. However, both programs encounter decisive difficulties with any but the simplest formulas of quantification theory, in connection with methods of doing propositional calculus. Wang's program, because of its use of Gentzen-like methods, involves exponentiation on the total number of truth-functional connectives, whereas Gilmore's program, using normal forms, involves exponentiation on the number of clauses present. Both methods are superior in many cases to truth table methods which involve exponentiation on the total number of variables present, and represent important initial contributions, but both run into difficulty with some fairly simple examples.
In the present paper, a uniform proof procedure for quantification theory is given which is feasible for use with some rather complicated formulas and which does not ordinarily lead to exponentiation. The superiority of the present procedure over those previously available is indicated in part by the fact that a formula on which Gilmore's routine for the IBM 704 causes the machine to computer for 21 minutes without obtaining a result was worked successfully by hand computation using the present method in 30 minutes. Cf. §6, below.
It should be mentioned that, before it can be hoped to employ proof procedures for quantification theory in obtaining proofs of theorems belonging to “genuine” mathematics, finite axiomatizations, which are “short,” must be obtained for various branches of mathematics. This last question will not be pursued further here; cf., however, Davis and Putnam [2], where one solution to this problem is given for ele

The hope that mathematical methods employed in the investigation of formal logic would lead to purely computational methods for obtaining mathematical theorems goes back to Leibniz and has been revived by Peano around the turn of the century and by Hilbert's school in the 1920's. Hilbert, noting that all of classical mathematics could be formalized within quantification theory, declared that the problem of finding an algorithm for determining whether or not a given formula of quantification theory is valid was the central problem of mathematical logic. And indeed, at one time it seemed as if investigations of this “decision” problem were on the verge of success. However, it was shown by Church and by Turing that such an algorithm can not exist. This result led to considerable pessimism regarding the possibility of using modern digital computers in deciding significant mathematical questions. However, recently there has been a revival of interest in the whole question. Specifically, it has been realized that while no decision procedure exists for quantification theory there are many proof procedures available—that is, uniform procedures which will ultimately locate a proof for any formula of quantification theory which is valid but which will usually involve seeking “forever” in the case of a formula which is not valid—and that some of these proof procedures could well turn out to be feasible for use with modern computing machinery.
 Hao Wang [9] and P. C. Gilmore [3] have each produced working programs which employ proof procedures in quantification theory. Gilmore's program employs a form of a basic theorem of mathematical logic due to Herbrand, and Wang's makes use of a formulation of quantification theory related to those studied by Gentzen. However, both programs encounter decisive difficulties with any but the simplest formulas of quantification theory, in connection with methods of doing propositional calculus. Wang's program, because of its use of Gentzen-like methods, involves exponentiation on the total number of truth-functional connectives, whereas Gilmore's program, using normal forms, involves exponentiation on the number of clauses present. Both methods are superior in many cases to truth table methods which involve exponentiation on the total number of variables present, and represent important initial contributions, but both run into difficulty with some fairly simple examples.
In the present paper, a uniform proof procedure for quantification theory is given which is feasible for use with some rather complicated formulas and which does not ordinarily lead to exponentiation. The superiority of the present procedure over those previously available is indicated in part by the fact that a formula on which Gilmore's routine for the IBM 704 causes the machine to computer for 21 minutes without obtaining a result was worked successfully by hand computation using the present method in 30 minutes. Cf. §6, below.
It should be mentioned that, before it can be hoped to employ proof procedures for quantification theory in obtaining proofs of theorems belonging to “genuine” mathematics, finite axiomatizations, which are “short,” must be obtained for various branches of mathematics. This last question will not be pursued further here; cf., however, Davis and Putnam [2], where one solution to this problem is given for ele

This paper reports on a novel technique for literature indexing and searching in a mechanized library system. The notion of relevance is taken as the key concept in the theory of information retrieval and a comparative concept of relevance is explicated in terms of the theory of probability. The resulting technique called “Probabilistic Indexing,” allows a computing machine, given a request for information, to make a statistical inference and derive a number (called the “relevance number”) for each document, which is a measure of the probability that the document will satisfy the given request. The result of a search is an ordered list of those documents which satisfy the request ranked according to their probable relevance.
The paper goes on to show that whereas in a conventional library system the cross-referencing (“see” and “see also”) is based solely on the “semantical closeness” between index terms, statistical measures of closeness between index terms can be defined and computed. Thus, given an arbitrary request consisting of one (or many) index term(s), a machine can elaborate on it to increase the probability of selecting relevant documents that would not otherwise have been selected.
Finally, the paper suggests an interpretation of the whole library problem as one where the request is considered as a clue on the basis of which the library system makes a concatenated statistical inference in order to provide as an output an ordered list of those documents which most probably satisfy the information needs of the user.

One of the outstanding problems in the theory of time series analysis is the distribution problem in spectral analysis for small samples. When the number of observations is sufficiently large for the Central Limit Theorem to be applicable, the normal approximation can be used to advantage. In recent years, work has aimed at the discovery of more generally applicable approximate methods and of more rational criteria for the sample size at which the large sample theory becomes useful; the state of the art is summarized in two recent papers by Grenander, Pollak and Slepian [1] and Freiberger and Grenander [2].
Consider a sample x = (x1, x2, ··· , xn) of successive values taken from a discrete-valued stationary time series ··· , y-1 , y0 , y1 , ··· , of normally distributed random variables with mean zero and covariance matrix R with elements r&ngr;&mgr; = E(x&ngr;x&mgr;). For stationarity we have: ri,i+&ngr;≡ r&ngr; = E(yiyi+&ngr;), i, &ngr; = 0, ±1, ±2, ···. The Fourier-Stieltjes representation of the covariances is r&ngr; = 1/2&pgr; ∫&pgr;&pgr; ei&lgr;&ngr; dF(&lgr;) (1) where the spectral distribution function F(&lgr;) is bounded and nondecreasing and can, if it is absolutely continuous, be expressed in terms of a spectral density ƒ(&lgr;): F(&lgr;) = ∫&lgr;&pgr;ƒ(&lgr;) (1) The practical determination of ƒ(&lgr;) from observations of the process is effected by the introduction of a quadratic form Q = ∑n&ngr;,&lgr;=1w&ngr;-&mgr;x&ngr;x&mgr; which is taken as an estimate for ƒ(&lgr;); the coefficients w&ngr; can be written in terms of a spectral weight function or “spectral window” w(&lgr;): w&ngr; = 1/2&pgr; ∫&pgr;-&pgr; x(&lgr;)ei&ngr;&lgr; d&lgr;. (4) For details, see the cited references. In order to obtain confidence limits for these estimates Q of the spectral density ƒ(&lgr;), it is important to have methods for computing the distribution of Q. This paper deals with one such method which has proved very efficient for digital computer application.
It is well known and shown, for instance, by H. Cramèr [3, p. 118], that the characteristic function of (3), where w&ngr;&mgr; ≡ w&ngr;-&mgr; are elements of a non-negative definite symmetric matrix W, is given by &phgr;(z) = EeizQ = | I - 2izRW | -1/2 = ∏nj=1 (1 - 2i&lgr;jz)-1/2 (5) where the &lgr;j are the n eigenvalues of the matrix product RW. Although RW is not necessarily a symmetric matrix, both R and W are symmetric and non-negative definite, so that all the &lgr;j are real and non-negative. The frequency function g(x) of Q then follows from Fourier's inversion formula: g(x) = 1/2&pgr; ∫∞∞ e-ixz ∫nj=1 (1 - 2iz&lgr;j)-1/2 dz. (6)
 Several ways have been suggested for evaluating g (x).
Slepian [4] obtains a sum of finite integrals by deforming the contour of integration into a set of circles enclosing pairs of the branch points zj = -i/(2&lgr;j) of (6), and collapsing the circles. This method, which was also used in [2], works well when the eigenvalues cluster toward zero, but not otherwise.
A method of repeated convolution of the frequency functions of its individual terms to obtain the frequency function of a quadratic form was developed in [5] and programmed for the IBM 650. It was found to be slowly convergent in most cases.
Taking the logarithmic derivative of (5) and applying the inverse Fourier transform yields the following singular integral equation for g(x): xg(x) = ∫x0 g(x - y)h(y) dy (7) where h(x) = 1/2 ∑n&ngr;=1 e-i/2&lgr;&ngr;. This observation forms the basis of an ingeneous method [1] for a computational scheme to derive the distribution of Q. The difficulties of obtaining initial values, associated with the high order zero of the frequency function g(x) at x = 0, are solved and the solution of (7) is discussed in detail in [1]. This method is suitable only for large-scale computers of the order of the IBM 704, and becomes, as does the method in [4], slowly convergent when the eigenvalues are densely spaced.
 Gurland [6] expanded the frequency function in terms of a Laguerre series and ([7]) presented a convergent series for the distribution fucntion using Laguerre polynomials. This method, which was found to converge slowly, formed the basis for the following procedure which has proved fast, accurate and reliable for a variety of problems. Essentially, the Laguerre expansion is now taken around Rice's approximation [8, p. 99], which is a type III distribution with appropriately chosen parameters.
The following definition of the Laguerre polynomials Ln(&agr;) (x) = ∑n&ngr;=0 (n+&agr;n-&ngr;)(-x)&ngr;/&ngr;! (8) satisfies the orthogonality relations ∫∞0e-x x&agr;Ln(&agr;) (x) Lm(&agr;) (x) dx = {&Ggr;(&agr; =1)(n+&agr;n); m = nm ≠ n (See Szegö [9]). Replacing x by &lgr;x gives ∫∞0 e-&lgr;xx&agr;L(&agr;)n(&Ggr;&agr;) dx={&Ggr;(&agr;+1/&lgr;&agr;+1) (n+&agr;n);m = nm ≠ n (9) which has the desired weight. The frequency function may now be expanded in a modified Laguerre series g(x) = Kx&agr;e-&lgr;x[c0 + c1L1(&agr;) (&lgr;x) + c2L2(&agr;) (&lgr;x) + …] (10) where K is chosen so that the weight integrates to one: ∫∞0Kx&agr;e-&lgr;x dx = 1, ∴ K = &lgr;&agr;+1/&Ggr;(&agr; + 1). (11) Multiplying both sides of (10) by Ln(&agr;) (&lgr;x) and integrating from 0 to ∞, gives cn = 1/(n + &agr;n) ∫∞0L(&agr;)n(&lgr;x)g(x) dx. (12) Using (8), cn = ∑n&ngr;=0(n + &agr;n - &ngr;)(n + &agr;n(-&lgr;)&ngr;&ngr;!∫∞0x&ngr;9(x)dx cn = ∑∞&ngr;=0&Ggr;(&agr;+1)/&Ggr;(&agr;+&ngr;+1(n&ngr;)(-&lgr;)&ngr;&agr;&ngr; (13) where &agr;&ngr; is the &ngr;th moment of the distribution about zero. Taking the logarithm of the characteristic function (5), log [EeizQ] = - 1/2 ∑nj=1 log (1 - 2i&lgr;jz), and expanding in powers of iz, one obtains for the cumulants of the distribution (Cramèr [3, p. 186]) Xn = (n - 1)!2n-1 ∑j&lgr;jn. (14) The cumulants are related to the moments about zero by the relation 1 + &agr;1(iz) + &agr;2/2!(iz)2 + &agr;3/3!(iz)3 + ··· = exp [ &khgr;1(iz) + &khgr;2/2! (iz)2 + ··· ]. (15) The mean and standard deviation of the frequency function are arrived at by equating powers of iz: m = &khgr;1; &sgr;2 = &khgr;2. (16) The weight function will have the same mean and standard deviation if &agr; = m2/&sgr;2 - 1; &lgr; = m/&sgr;2. (17) Using (13) the remembering that &agr;1 = m and &agr;2 = &sgr;2 + m2, one obtains c0 = 1, c1 = 0, c2 = 0. (18) Now (10) becomes g(x) = &lgr;&agr;+1/&Ggr;(&agr; + 1)x&agr;e-&lgr;x [1 + c3L(&agr;)3(&lgr;x) + c4L(&agr;)4(&lgr;x) + ···] (19) which together with (17), (15), (14), (13) and (8), gives the frequency function in terms of the eigenvalues.
Szegö [8] showed that a sufficient condition for convergence, when expanding in the form g(x) = ∑∞&ngr; = 0 a&ngr;L(&agr;)&ngr; (x), is g(x) = O(ex/2x-&agr;/2-1/4-&dgr;), &dgr; > 0, x → ∞. In our case, this reduces to ƒ(x) = O(e-x/2x&agr;/2-1/4-&dgr;), &dgr; > 0, x → ∞. But, (Gurland [6]) ƒ(x) = O(e-x/2&lgr;mxn/2-1, &lgr;m = max &lgr;j, x → ∞. Therefore, the series will converge if &lgr;&lgr;m < 1, which is the same restriction imposed by Gurland.
For equal eigenvalues, the result is a &khgr;2-distribution with n degrees of freedom, which serves as an estimate of the computational error.1
It may be remarked that Toeplitz theory [10] gives the asymptotic distribution of the eigenvalues &lgr;&ngr; and for large enough sample sizes (see [2]) it is sufficient to use these results instead of the exact eigenvalue distribution. In these cases, the present method is particularly efficient.
The method of computation proceeds as follows. The desired number of cumulants divided by n! is calculated from equation (14), to give &khgr;n/n! = 2n-1/n ∑j&lgr;jn; after substitution in equation (15), powers of iz are compared to give the moments &agr;&ngr;. The constant K follows from (11), with the gamma function calculated by fitting an eighth order polynomial to &Ggr; (&agr; + 1) in the range 0 ≦ &agr; ≦ 1 and using the recurrence relation &Ggr; (&agr; + 1) = &agr;&Ggr(&agr;) to extend the range to any &agr; > - 1. Next, &agr; and &lgr; are computed and the series tested for convergence. In all applications to spectral analysis tried so far, the product &lgr;&lgr;m has been between 0.5 (for the case of equal eigenvalues) and 0.75, so that there has been no trouble with convergence. Successively higher approximations are now calculated, using intermediate quantities Jn = &Ggr;(&agr; + 1)/&Ggr;(&agr; = n + 1) (-&lgr;)n = (-&lgr;)n/&agr; + 1)(&agr; + 2) ··· (&agr; + n)′ Kn = Jn&agr;n, and, with the binomial coefficients (n &ngr;), cn = ∑∞&ngr;=0 Kn (n &ngr;). The Laguerre polynomials may now be expressed in the form Ln(&agr;) (&lgr;x) = ∑ n&ngr; = 0 E(n)&ngr;x&ngr; where E(n)&ngr; = J&ngr; (&ngr; + 1)(&agr; + 2) ··· (&agr; + n/n!, and the final series (19) emerges term by term. The speed of convergence depends how close &lgr;&lgr;m is to 0.5.
As a numerical example we present, in the figure, curves for the frequency function (10) computed for a spectral density ƒ(&lgr;) corresponding to white noise (or bandlimited noise with a correlation function sin &pgr;t/&pgr;t sampled at the zeros), a rectangular spectral window w(&lgr;) of bandwidth &pgr;/10 and sample sizes n = 20, 30, 40, 50, 70, 100, 150. The computation time for each curve on the IBM 650 was approximately 9 minutes, except for n = 70, 100 and 150 where the Toeplitz approximation led to equal eigenvalues and a computation time of 1 minute each. The eigenvalues of RW, where both R and W are symmetric but not their product, were computed by Jacobi's method; for orders 20, 30, 40 and 50 all eigenvalues were found in 0.8, 2.7, 6.4 and 12.5 hours respectively.
The authors are indebted to Professor Ulf Grenander, formerly of Brown University, now of the University of Stockholm, for stimulating advice.

The assignment of prime numbers to the branches of directed or non-directed nets, enables one to construct transition matrices in a numerical, rather than a symbolic, form. The numerical form greatly facilitates the construction of higher-order transition matrices, needed for the topological analysis of a given net. The proposed method is especially useful for the mechanical determination of the non-repeating paths and cycles in the net.

The consistency of precedence matrices is studied in the very natural geometric setting of the theory of directed graphs. An elegant recent procedure (Marimont [7]) for checking consistency is further justified by means of a graphical lemma. In addition, the “direction of future work” mentioned in [7] (to which the present communication may be regarded as a sequel) is developed here using graph theoretic methods. This is based on the relationship between the occurrence of directed cycles and the recognition of “strongly connected components” in a directed graph. An algorithm is included for finding these components in any directed graph. This is necessarily more complicated than determining whether there do not exist any directed cycles, i.e., whether or not a given precedence matrix is consistent.

A number of papers have been written from time to time about logical counters of a certain type which have quite simple logic and have been variously referred to as Binary Ring Counters, Shift Register Counters, Johnson Counters, etc. To my knowledge, most of these papers confine themselves to certain special cases and usually leave the subject with some speculation as to the possibility of generating periods of any desired length by the use of these special types. The point of view of this paper is to consider all possible counters of this general type to see how one would obtain a particular period. Special emphasis is placed on determining the least number of bits, n, required to produce a given period, K.
The rules for counting are as follows. If an n-bit counter is in state (an-1, an-2 ···, a2, a1, a0) at a given time, T, then at T + 1 its state is (bn-1, bn-2, ···, b1, b0) where b0 = an-1, bi = ai-1 + cian-1 for i = 1, 2, ···, n - 1.
 The a's, b's, and c's are all 0's or 1's, the c's being constants, and the indicated operations are carried out using modulo 2 arithmetic. This is equivalent to considering the state of the counter as an (n - 1)th degree polynomial in X, multiplying said polynomial by X and reducing it modulo m(X), where m(X) is a polynomial of degree n which is relatively prime to X. At time T the state of the counter corresponds to: A(X) = an-1Xn-1 + an-2Xn-2 + ··· + a1X + a0. The polynomial which corresponds to the state of the counter at time T + 1 is obtained by forming X·A (X) and reducing, if necessary, modulo m (X) = Xn + cn-1Xn-1 + cn-2Xn-2 + ··· + c1X + 1. Since an-1·m(X) = 0 mod m(X), X·A(X) = X·A(X)+ an-1m(X) mod m(X), so X·A(X) = (an-2 + cn-1·an-1)Xn-1 + (an-3 + cn-2an-1)Xn-2 + ··· + (a0 + c1an-1)X + an-1 = bn-1Xn-1 + bn-2Xn-2 + ··· + b1X + b0.
 It is well known that more than one possible period may be obtained depending upon the initial state of the counter. Several examples are given by Young [4]. However, starting with X itself will always yield the longest possible period for any given m(X) and, furthermore, any other periods possible will always be divisors of the major period (Theorem I below). Since these minor periods can always be obtained with moduli of lower degree they are of no real interest here, and throughout the remainder of this paper the expression “period of the counter” will be assumed to refer to the major period.
The set of all polynomials whose coefficients are the integers modulo 2 is the polynomial domain GF(2, X), which has among other things unique factorization into primes (irreducibles). If m(X) is in GF(2, X), then GF(2, X) modulo m(X) is a commutative ring. Thus it is closed under multiplication, but it may have proper divisors of zero. However, any element which is relatively prime to m(X) in GF(2, X) has an inverse in GF(2, X)/m(X) [1].

We introduce the concept of unreliable failure detectors and study how they can be used to solve Consensus in asynchronous systems with crash failures. We characterise unreliable failure detectors in terms of two properties—completeness and accuracy. We show that Consensus can be solved even with unreliable failure detectors that make an infinite number of mistakes, and determine which ones can be used to solve Consensus despite any  number of crashes, and which ones require a majority of correct processes. We prove that Consensus and Atomic Broadcast are reducible to each other in asynchronous systems with crash failures; thus, the above results also apply to Atomic Broadcast. A companion paper shows that one of the failure detectors introduced here is the weakest failure detector for solving Consensus [Chandra et al. 1992].

Computational efficiency is a central concern in the design of knowledge representation systems. In order to obtain efficient systems, it has been suggested that one should limit the form of the statements in the knowledge base or use an incomplete inference mechanism. The former approach is often too restrictive for practical applications, whereas the latter leads to uncertainty about exactly what can and cannot be inferred from the knowledge base. We present a third alternative, in which knowledge given in a general representation language is translated (compiled) into a tractable form—allowing for efficient subsequent query answering.We show how propositional logical theories can be compiled into Horn theories that approximate the original information.  The approximations bound the original theory from below and above in terms of logical strength. The procedures are extended to other tractable languages (for example, binary clauses) and to the first-order case. Finally, we demonstrate the generality of our approach by compiling concept descriptions in a general frame-based language into a tractable form.

We introduce the concept of unreliable failure detectors and study how they can be used to solve Consensus in asynchronous systems with crash failures. We characterise unreliable failure detectors in terms of two properties—completeness and accuracy. We show that Consensus can be solved even with unreliable failure detectors that make an infinite number of mistakes, and determine which ones can be used to solve Consensus despite any  number of crashes, and which ones require a majority of correct processes. We prove that Consensus and Atomic Broadcast are reducible to each other in asynchronous systems with crash failures; thus, the above results also apply to Atomic Broadcast. A companion paper shows that one of the failure detectors introduced here is the weakest failure detector for solving Consensus [Chandra et al. 1992].

The contribution of this paper is two-fold. First, a connection is established between approximating the size of the largest clique in a graph and multi-prover interactive proofs. Second, an efficient multi-prover interactive proof for NP languages is constructed, where the verifier uses very few random bits and communication bits. Last, the connection between cliques and efficient multi-prover interaction proofs, is shown to yield hardness results on the complexity of approximating the size of the largest clique in a graph.Of independent interest is our proof of correctness for the multilinearity test of functions.

We present optimal algorithms for sorting on parallel CREW and EREW versions of the pointer machine model. Intuitively, one can view our methods as being based on a parallel mergesort using linked lists rather than arrays (the usual parallel data structure). We also show how to exploit the “locality” of our approach to solve the set expression evaluation problem, a problem with applications to database querying and logic-programming in O(log n) time using O(n) processors. Interestingly, this is an asymptotic improvement over what seems possible using previous techniques.

Categorical combinators [Curien 1986/1993; Hardin 1989; Yokouchi 1989] and more recently &lgr;&sgr;-calculus [Abadi 1991; Hardin and Le´vy 1989], have been introduced to provide an explicit treatment of substitutions in the &lgr;-calculus. We reintroduce here the ingredients of these calculi in a self-contained and stepwise way, with a special emphasis on confluence properties. The main new results of the paper with respect to Curien [1986/1993], Hardin [1989], Abadi [1991], and Hardin and Le´vy [1989] are the following:
(1) We present a confluent weak calculus of substitutions, where no variable clashes can be feared;
(2) We solve a conjecture raised in Abadi [1991]: &lgr;&sgr;-calculus is not confluent (it is confluent on ground terms only).
This  unfortunate result is “repaired” by presenting a confluent version of &lgr;&sgr;-calculus, named the &lgr;Env-caldulus in Hardin and Le´vy [1989], called here the confluent &lgr;&sgr;-calculus.

The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of “edit operations” needed to change the one string into the other. The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings. Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings.

A statistical model is presented which is useful in the solution of a Fredholm integral equation of the first kind and equivalent to one proposed by Strand and Westwater. The model and the related problem presented here are familiar to statisticians from the study of regression analysis and are essentially, “(GLM): Find the best linear unbiased estimate of &bgr; given the observation y which satisfies y = H&bgr; + e, e distributed as N (0, &Ggr;).” Here y, &bgr; c are vectors, H is an (m + n) × k matrix, and &Ggr; is a certain (m + n) × (m + n) positive definite, known matrix. The main content of the paper is that (GLM) provides an equivalent way of considering the problem of Strand and Westwater and is to be preferred by virtue of the rich store of results available for the study of (GLM) and its intrinsic geometric nature.

Efficiently computable a posteriori error bounds are attained by using a posteriori models for bounding roundoff errors in the basic floating-point operations. Forward error bounds are found for inner product and polynomial evaluations. An analysis of the Crout algorithm in solving systems of linear algebraic equations leads to sharper backward a posteriori bounds. The results in the analysis of the iterative refinement give bounds useful in estimating the rate of convergence. Some numerical experiments are included.

Compute-output processing times are determined for n-segment jobs that are preloaded into main storage and processed with overlap. A queueing model with tandem servers is utilized for the performance analysis. In particular, the solution presented involves determination of the transient response for a batched arrival of n segments to be processed through two stages of tandem service with unlimited output buffering. The performance results provide insight into conditions arising in systems consisting of a single CPU and I/O channel with overlap capabilities. Two cases, single-segment overlap and unlimited overlap, are considered. Segmental compute and output (or input) service times are taken to be exponentially distributed; however, the approach is not limited to the exponential case if service is independent. The ratio of mean output time to mean compute time is varied to explore the full range between compute-bound and output-bound extremes. Final results are presented as relative gain over sequential processing.

The topic of this paper is a probabilistic analysis of demand paging algorithms for storage hierarchies. Two aspects of algorithm performance are studied under the assumption that the sequence of page requests is statistically independent: the page fault probability for a fixed memory size and the variation of performance with memory. Performance bounds are obtained which are independent of the page request probabilities. It is shown that simple algorithms exist which yield fault probabilities close to optimal with only a modest increase in memory.

A cost is defined for demand paging algorithms with respect to a formal stochastic model of program behavior. This cost is shown to exist under rather general assumptions, and a computational procedure is given which makes it possible to determine the optimal cost and optimal policy for moderate size programs, when the formal model is known and not time dependent. In this latter case it is shown that these computational procedures may be extended to larger programs to obtain arbitrarily close approximations to their optimal policies. In previous models either unwarranted information is assumed beyond the formal model, or the complete stochastic nature of the model is not taken into account.

A class of demand paging algorithms for some two-level memory hierarchies is analyzed. The typical memory hierarchy is comprised of the core and a backing device. A distance matrix characterizes the properties of the latter device. The sequence of address references directed to the hierarchy by the CPU and channels is modeled as a Markov process. A compact expression for the mean time required to satisfy the page demands is derived and this expression provides the basis for some optimization problems concerning partitionings and rearrangements of pages in the backing device. In connection with these problems, a class of random processes is defined in terms of an ordering property of a joint probability matrix which is central to memory hierarchies. Three results are given on the ordering property, its relation specifically to partitionings inherent in hierarchies and the problem of optimal rearrangements. Finally, for such a class of ordered processes, certain results due to the author are specialized to yield the solution to the problem of optimal rearrangement of pages on an assembly of magnetic bubble loops.

An analytic model of a single processor scheduling problem is investigated. The scheduling objective is to minimize the total loss incurred by a finite number of initially available requests when each request has an associated linear loss function. The assumptions of the model are that preemption is allowed with negligible loss of processor time, and that the distribution of actual service times is known for each class of requests. A request is associated with a class by any of its characteristics except its actual service time. A contrived example demonstrates that one reasonable scheduling rule does not always minimize expected total loss. The major results of the paper are the definition of a new scheduling rule based on the known service time distributions, and the proof that expected total loss is always minimized by using this new rule. Brief consideration is given to generalizations of the model in which new requests arrive randomly, and preemption requires a non-negligible amount of processor time.

Many compilers for higher order languages attempt to translate the source code into “good” object code. Cocke and Schwartz have described an algorithm for discovering when the computation of an expression is redundant (common), and when it can be moved to a less frequently executed region of the program. The present paper includes a tutorial presentation of their basic methods, along with a number of improvements and extensions. These include simplification of the solution method, to save a pass; extension of it to treat the safety constraint, handle multi-entry regions directly, detect additional commonality and code motion after unsafe code motion, and decide where moved code should be put; and combination of the algorithms for commonality and the dead condition, making use of important work of Ken Kennedy.
The methods here applied to collecting information for use in code optimization include general algorithms for solving a set of linear equations in Boolean algebra. The algorithms are most useful when the coefficient matrix is sparse.

A technique is introduced for analyzing simulations of stochastic systems in the steady state. From the viewpoint of classical statistics, questions of simulation run duration and of starting and stopping simulations are addressed. This is possible because of the existence of a random grouping of observations which produces independent identically distributed blocks from the start of the simulation. The analysis is presented in the context of the general multiserver queue, with arbitrarily distributed interarrival and service times. In this case, it is the busy period structure of the system which produces the grouping mentioned above. Numerical illustrations are given for the M/M/1 queue. Statistical methods are employed so as to obtain confidence intervals for a variety of parameters of interest, such as the expected value of the stationary customer waiting time, the expected value of a function of the stationary waiting time, the expected number of customers served and length of a busy cycle, the tail of the stationary waiting time distribution, and the standard deviation of the stationary waiting time. Consideration is also given to determining system sensitivity to errors and uncertainty in the input parameters.

A technique for simulating GI/G/s queues is shown to apply to simulations of discrete and continuous-time Markov chains. It is possible to address questions of simulation run duration and of starting and stopping simulations because of the existence of a random grouping of observations which produces independent identically distributed blocks from the start of the simulation. This grouping allows confidence intervals to be obtained for a general function of the steady-state distribution of the Markov chain. The technique is illustrated with simulation of an (s, S) inventory model in discrete time and the classical repairman problem in continuous time. Consideration is also given to determining system sensitivity to errors and uncertainty in the input parameters.

The model elimination (ME) and resolution algorithms for mechanical theorem-proving were implemented so as to maximize shared features. The identical data structures and large amount of common programming permit meaningful comparisons when the two programs are run on standard problems. ME does better on some classes of problems, and resolution better on others. The depth-first search strategy used in this ME implementation affects the performance profoundly. Other novel features in the implementation are new control parameters to govern extensions, and modified rules for generating and rejecting chains. The resolution program incorporates unit preference and set-of-support. An appendix reproduces the steps of a machine-derived ME refutation.

Branch-and-bound implicit enumeration algorithms for permutation problems (discrete optimization problems where the set of feasible solutions is the permutation group Sn) are characterized in terms of a sextuple (Bp S,E,D,L,U), where (1) Bp is the branching rule for permutation problems, (2) S is the next node selection rule, (3) E is the set of node elimination rules, (4) D is the node dominance function, (5) L is the node lower-bound cost function, and (6) U is an upper-bound solution cost. A general algorithm based on this characterization is presented and the dependence of the computational requirements on the choice of algorithm parameters, S, E, D, L, and U is investigated theoretically. The results verify some intuitive notions but disprove others.

Let r be the total number of cycles required to complete a compromise merge of a given number of initial strings. Define row vectors mr-j and dj whose components represent the number and length respectively of strings at the end of the jth cycle of the merge. It is shown in this paper that there are asymptotic approximations to these vectors, which enables one to compute their respective components directly. Consequently, the number of cycles r can be computed directly, as in the case of the balanced merge.

A family of new algorithms is given for evaluating the first m derivatives of a polynomial. In particular, it is shown that all derivatives may be evaluated in 3n - 2 multiplications. The best previous result required 1/2n(n + 1) multiplications. Some optimality results are presented.

The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of “edit operations” needed to change the one string into the other. The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings. Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings.

Given a collection

 F
 of subsets of S =
{1,…,n}, set
cover is the problem of selecting as few as possible
subsets from   F such that their union covers
S,, and max
k-cover is the problem of selecting
k subsets from
  F such that their union has maximum cardinality. Both these problems are
NP-hard.   We prove that (1 - o(1)) ln
n is a threshold below  

 which set
cover cannot be approximated efficiently, unless NP has slightly
superpolynomial time algorithms. This closes the gap (up to low-order
terms) between the ratio of approximation achievable by the greedy
alogorithm (which is (1 - o(1)) ln
n), and provious results of Lund and Yanakakis, that showed hardness of
approximation within a ratio of 

log2
n/2≃0.72
 ln n. For max
k-cover, we show an approximation
threshold of (1 - 1/e)(up to
low-order terms), under assumption that 

P≠NP
.

Directory-based coherence protocols in shared-memory multiprocessors are so complex that verification techniques based on automated procedures are required to establish their correctness. State enumeration approaches are well-suited to the verification of cache protocols but they face the problem of state space explosion, leading to unacceptable verification time and memory consumption even for small system configurations. One way to manage this complexity and make the verification feasible is to map the system model to verify onto a symbolic state model (SSM). Since the number of symbolic states is considerably less than the number of system states, an exhaustive state search becomes possible, even for large-scale sytems and complex protocols.In this paper, we develop the  concepts and notations to verifiy some properties of a directory-based protocol designed for non-FIFO interconnection networks. We compare the verification of the protocol with SSM and with the Stanford Mur  4 , a verification tool enumerating system states. We show that SSM is much more efficient in terms of verification time and memory consumption and therefore holds that promise of verifying much more complex protocols. A unique feature of SSM is that it verifies protocols for any system size and therefore provides reliable verification results in one run of the tool.

A database query is finite if its result consists of a finite sets tuples. For queries formulated as sets of pure Horn rules, the problem of determining finiteness is, in general, undecidable.In this paper, we consider superfiniteness—a stronger kind of finiteness, which applies to Horn queries whose function symbols are replaced by the abstraction of infinite relations with finiteness constraints (abbr., FC's). We show that superfiniteness is not only decidable but also axiomatizable, and the axiomatization yields an effective decision procedure. Although there are finite queries that are not superfinite, we demonstrate that superfinite queries represent an interesting and  nontrivial subclass within the class of all finite queries.The we turn to the issue of inference of finiteness constraints—an important practical problem that is instrumental in deciding if a query is evaluable by a bottom-up algorithm. Although it is not known whether FC-entailment is decidable for sets of function-free Horn rules, we show that super-entailment, a stronger form of entailment, is decidable. We also show how a decision procedure for super-entailment can be used to enhance tests for query finiteness.

Given a collection

 F
 of subsets of S =
{1,…,n}, set
cover is the problem of selecting as few as possible
subsets from   F such that their union covers
S,, and max
k-cover is the problem of selecting
k subsets from
  F such that their union has maximum cardinality. Both these problems are
NP-hard.   We prove that (1 - o(1)) ln
n is a threshold below  

 which set
cover cannot be approximated efficiently, unless NP has slightly
superpolynomial time algorithms. This closes the gap (up to low-order
terms) between the ratio of approximation achievable by the greedy
alogorithm (which is (1 - o(1)) ln
n), and provious results of Lund and Yanakakis, that showed hardness of
approximation within a ratio of 

log2
n/2≃0.72
 ln n. For max
k-cover, we show an approximation
threshold of (1 - 1/e)(up to
low-order terms), under assumption that 

P≠NP
.

In this paper, we consider the question of determining whether a function f has property P or is &egr;-far from any function with property P. A property testing algorithm is given a sample of the value of f on instances drawn according to some distribution. In some cases, it is also allowed to query f on instances of its choice. We study this question for different properties and establish some connections to problems in learning theory and approximation.In particular, we focus our attention on testing graph properties. Given access to a graph G in the form of being able to query whether an edge exists or not between a pair of vertices, we devise algorithms to test whether the underlying graph has  properties such as being bipartite, k-Colorable, or having a p-Clique (clique of density p with respect to the vertex set). Our graph property testing algorithms are probabilistic and make assertions that are correct with high probability, while making a number of queries that is independent of the size of the graph. Moreover, the property testing algorithms can be used to efficiently (i.e., in time linear in the number of vertices) construct partitions of the graph that correspond to the property being tested, if it holds for the input graph.

Autoepistemic logic is one of the principal modes of nonmonotonic
reasoning. It unifies several other modes of nonmonotonic reasoning and
has important application in logic programming. In the paper, a theory
of autoepistemic logic is developed. This paper starts with a brief
survey of some of the previously known results. Then, the nature of
nonmonotonicity is studied by investigating how membership of
autoepistemic statements in autoepistemic theories depends on the
underlying objective theory. A notion similar to set-theoretic forcing
is introduced. Expansions of autoepistemic theories are also
investigated. Expansions serve as sets of consequences of an
autoepistemic theory and they can also be used to define semantics for
logic programs with negation. Theories that  have expansions are
characterized, and a normal form that allows the description of all
expansions of a theory is introduced. Our results imply algorithms to
determine whether a theory has a unique expansion. Sufficient conditions
(stratification) that imply existence of a unique expansion are
discussed. The definition of stratified theories is extended and (under
some additional assumptions) efficient algorithms for testing whether a
theory is stratified are proposed. The theorem characterizing expansions
is applied to two classes of theories, K1-theories
and ae-programs. In each case, simple hypergraph characterization of
expansions of theories from each of these classes is given. Finally,
connections with stable model semantics for logic programs with negation
is  discussed. In particular, it is proven that the problem of existence
of stable models is NP-complete.

—Authors' Abstract

